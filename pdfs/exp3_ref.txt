To design a model for multi-party dialogue response generation and make it compatible with the EM training algorithm, there are two important things to consider: how to model p(r t | c t , z t ; θ) in the maximization step, and how to compute p(z t | c t , r t ; θ) in the expectation step. In this section, we will first address these two problems, then mathematically derive the feasibility of our EM pre-training algorithm.Given an input sequence of the dialogue history and the speaker of the response at time step t, X = { S 1 : U 1 [SEP]S 2 : U 2 [SEP] . . . S t-1 : U t-1 [SEP]S t : } , together with the addressee of the response z t = j, our goal is to train a model that can generatean response Y = U t . Here each S i is the name of the speaker at time step i, which is represented as Speaker #S i like those in Figure 1. U i = { w i1 , w i2 , . . . , w in i } is the content of the ith  utterance with n i words. z t = j represents that St  speaks to S j , who utters U j , and [SEP] is a special token that indicates the end of a dialogue turn.In this section, we answer the first question: how to model p(r t | c t , z t ; θ), or in other words, how to incorporate the addressee information z t = j into the process of generating a response r t . We design a straightforward method that adds addressee embeddings to the positional encodings and word embeddings, before they are further encoded by a PLM. The left part of Figure 2 illustrates this method, where we use an embedding look-up table with 2 entries to indicate whether a word belongs to the addressee utterance or not. Specifically, if a word is in the addressee utterance, it will get its addressee embedding from entry 1, otherwise from entry 0. Since addressee modeling is not the key contribution of this work, we just adopt the most straightforward and effective way. In our experiments, we use BART (Lewis et al., 2020) as the backbone PLM, following previous works (Gu et al., 2022). Due to the page limit, the proverbial architecture of Transformer and BART are omitted here.In this section, we answer the second question: how to compute p(z t | c t , r t ; θ) in the expectation step, or in other words, how to predict the distribution of the unlabeled addressee z t , given the current dialogue context c t , response r t , under parameters θ. The solution to this question is essentially the mostimportant part of our method since it delicately solves the problem of data scarcity in MPDRG. Let’s consider what humans will do to participate in a multi-party conversation. First, we will read the dialogue history c t , then choose an addressee z t to reply. Once c t and z t are determined, we will utter a response according to the content of the whole dialogue and the addressee utterance. The right part of Figure 2 gives the Bayesian Network of the above process, where the joint distribution of (c t , z t , r t ) can be factorized as:Here we omit the subscript t and model parameters θ for simplicity. Given Eq. (1), p(z | c, r; θ) can be derived as:We assume that the probability of choosing any previous utterance as the addressee is the same given the current dialogue history, which means p(z | c) obeys a uniform distribution. Meanwhile, the denominator p(r | c) is independent of z, leaving only the term p(r | c, z). Now, we can induce that:Therefore, for each z i , i = 1, 2, . . . , t−1, we have:In practice, we can use the generative model p(r t | c t , z t ; θ) to compute the probability distribution of p(z t | c t , r t ; θ) by Eq. (4).Figure 3 illustrates the overview of our EM training process. During the E-steps, we compute the probability distribution of the latent variable (the addressee z). During the M-steps, we sample (c, r, z) triplets from this distribution and optimize the generative model by standard training algorithms. The Expectation Step is to compute the conditional distribution of the latent variable z t , given the observed data (c t , r t ) and the current modelparameters θ, where Eq. (4) gives a reasonable approximation of this value. Specifically, for a sample (c t , r t ), with the model parameters θ fixed, we first calculate the un-normalized probability of each of the i th (i < t) utterance being the addressee: p(r t | c t , z t i ; θ) using Eq. (3), then normalize them to get the conditional distribution of z t using Eq. (4). Once P(z t | c t , r t ; θ) is obtained, we sample (c t , r t , z t ) triplets from this distribution, which is further used in the maximization step.The Maximization Step is analogical to the normal training process. Given the sampled { (c t k , r t k , z t k ) } k=1 N triplets, where N is the total number of samples, our goal is to minimize the auto-regressive language modeling loss:where w i k is the i th word in the response of the kth  sample: r t k = { w i k } i=1 n i , and n i is the length of this response. Compared with the vanilla EM algorithm, there are several differences in our implementations. First of all, we do not use the initial model to generate the training data for the first round of the maximization step. Instead, we utilize the discourse parser provided by Shi and Huang (2019) to predict the addressee of each utterance in the unlabeled corpus to get a coarse initial training dataset. The reason for this initialization method is that the initialization of training data (or model parameters) is vital to the EM method, which helps it converge to a better point. Second, rather than sampling z t from its conditional distribution, we adopt a hard EM approach which takes the value z t i with highest probability as the predicted label, where i = arg max p(z t i | c t , r t ; θ). This hard EMapproach is proved as more effective to boost the performance (Min et al., 2019). Finally, to ensure the quality of the generated training data in the maximization step, we set a hyper-parameter α ∈ [0, 1] to control the proportion of training data that is actually used. Specifically, we first rank the prediction confidence of each z t k according to the value of p(z t k | c t k , r t k ; θ), then pick the top α×N samples with the highest confidence scores. In our experiments, α is dynamically set to ensure the addressee prediction accuracy of the selected samples is over 80% in an annotated validation set.In a multi-party dialogue corpus without annotated addressee labels, a usual solution to train a response generation model is to maximize the marginal loglikelihood (or incomplete log-likelihood) over all possible addressees:However, this objective is hard to optimize since the distribution of z is hard to obtain. Here, we define an expected complete log-likelihood where our estimation of p(z t | c t , r t ; θ) can come to rescue:Our new objective now becomes maximizing the expected complete log-likelihood. The relation ˆ between ℓ and ℓ can be derived as follows:where the third line is derived from the Jensen Inequality, and H q(z) is the entropy of the distribution of z. Since H q(z) ≥ 0, we can derive that ˆ ℓ(c, r; θ) ≤ ℓ(c, r; θ), which means ℓ is the lowerbound of ℓ. By maximizing the lower bound ℓ, we can indirectly maximize ℓ, which is originally hard to optimize. Another important observation is hat ℓ = ℓ if and only if q(z) = p(z t | c t , r t ; θ), which is exactly what we calculate during the E-steps in Eq. (7). Though the derivation of the posterior distribution of z is not exact since we assume uniform prior in Eq. (2), it is still much closer to the real distribution compared to random q(z). It is worth noting that the global optimal point is not guaranteed to be reached by this algorithm, and it depends heavily on the initialization of model parameters or the training data for the first round of the maximization step. This explains the reason why we utilize a discourse parser to get a coarse initial training dataset instead of using the expectation step at the first iteration in Section 3.4.