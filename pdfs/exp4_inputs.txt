Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MILDecoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network. MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit.Trained on huge amount of text corpora, Transformer-based (Vaswani et al., 2017) pretrained language models (LMs) have led to a wave of advances in natural language generation tasks (Radford et al. (2019); Lewis et al. (2019); Roberts et al. (2019)).We examine the public comments provided in Jigsaw Toxic Comment Classification Challenge Dataset 1 (Jigsaw) containing over 200K comments that were labeled as toxic.Prior studies (Gehman et al., 2020) attempt to filter out a specific word list at the decoding stage, which cannot achieve an obvious effect on mitigating toxicity in the generated text.Therefore, we present MIL-Decoding, a tokenlevel detoxification in consideration of the contextual information with a multiple instance learning (MIL) neural network.We conduct experiments conditioned on two widely-used datasets: RealToxicityPrompts (Gehman et al., 2020) and a QA-dataset provided by Solaiman and Dennison (2021). \n\n In the classical supervised learning problem, one aims at finding a model that predicts a label y, for a given instance x ∈ R D . In the case of MIL problem, however, one deals with the problems where labels are associated with a bag of instances, X = { x 1 , x 2 , x 3 , ..., x k } , while instance labels are unobserved. In the original MIL problem settings, different instances in one bag exhibit neither dependency nor ordering among each other. Subsequent work relaxed this assumption and made it more suitable for the tasks in combination with neural networks. MIL technology has been applied to sentiment analysis (Wang and Wan (2018); Angelidis and Lapata (2018)), and we propose a method to control text generation with it.Although large-scale pre-trained LMs (Wick et al. (2020); Keskar et al. (2019a); Raffel et al. (2019)) have demonstrated excellent performance in manyNLP tasks, recent studies show that LMs can generate toxic and biased language (Kumar et al., 2022). Pre-trained LMs predict the probability distribution over next token to be generated: P θ (x i | x 1:i−1 ). Control codes can be used to enlighten LMs the desirable attributes we need in generated output. Class-conditional language models (CC-LMs) like Ctrl (Keskar et al., 2019b) guide language models to generate with an attribute variable, modeling as P θ (x i | x 1:i−1 , c), where variable c is used as a control code. Qian et al. (2022) and Clive et al. (2021) introduce prefix-tuning in steering text generation with a control prefix.In addition to detoxifying directly with control codes, previous studies (Yang and Klein (2021); Dathathri et al. (2019)) propose methods steering generation at decoding stage. Methods based on weighted decoding (Holtzman et al. (2018); Ghazvininejad et al. (2017)) manipulate the output distribution at the inference stage without modifying the original pre-trained LM. With application of Bayesian factorization, the problem can be transferred into maximizing the product of P θ (x i | x 1:i−1 ) and P θ (c | x 1:i ):Moreover, recent studies further paid attention to how LMs produce toxicity and the problems with existing detoxification methods. Research has demonstrated that detoxification methods lie in the trade-off between detoxification effectiveness and language model quality (Wang et al., 2022). Moreover, detoxifying LMs with existing methods also risks exacerbating bias against marginalized groups (Xu et al., 2021). Hartvigsen et al. (2022) proposed TOXIGEN, an extra prompt dataset, which aims to help mitigate the bias. Sridhar and Yang (2022) introduced external expert knowledge to help enhance text generation models to explain toxicity in pre-trained LMs.