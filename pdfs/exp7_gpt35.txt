'METHOD: \n- Iterative Learning from User Feedback: The researchers design and deploy an iterative approach where information-seeking users ask questions, receive model-predicted answers, and provide feedback. This iterative process allows the extractive QA system to continually learn and improve over time based on user feedback.\n- Three-Option Feedback Signal: The researchers use a simple three-option feedback signal for users to provide feedback on the model\'s answers. The options include "correct," "partially correct," or "wrong." This feedback signal helps in mapping the user feedback to reward values and facilitates offline contextual bandit learning.\n- Robustness in Low Data Regimes: The researchers conduct multiple studies focusing on low data regimes to demonstrate the robustness of the system to challenging scenarios where only a small fraction of users provide feedback. This research aims to show that the model can rapidly improve even with limited feedback.\n- Different Approach to Human Feedback: Unlike existing reinforcement learning from human feedback (RLHF) methods that rely on pair-wise comparisons from annotators, the researchers adopt a different approach by soliciting feedback from actual users on single outputs to their queries. This approach allows for a more practical interaction with users and facilitates the mapping of user feedback to reward values for contextual bandit learning.\n- Comparison with Other Approaches: The paper highlights that the choice of using single-output feedback from users instead of pair-wise comparisons is motivated by the suitability for soliciting feedback from actual users. The researchers also acknowledge the importance of comparing their approach to learning a reward model directly from feedback, as future work.\n- Previous Work on Learning from Feedback: The paper discusses previous work on learning from feedback for language tasks, including machine translation, semantic parsing, question answering, and chatbots. However, it emphasizes that previous work did not explore iterative continual learning, which is the focus of this research.\n- RL Using Human Feedback: The paper acknowledges previous work on RL using human feedback for non-language problems and mentions that RL from human feedback has also been studied for QA using synthetic interactions and simulated feedback. This research differs as it focuses on human feedback from information-seeking users posing challenging questions.\n- Crowdworkers Providing Feedback: The paper mentions a previous approach where crowdworkers provide rating and explanation for question-answer pairs to improve a QA model post-deployment. However, the researchers in this study focus on feedback from actual information-seeking users rather than crowdworkers.'