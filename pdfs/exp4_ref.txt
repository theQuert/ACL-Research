The core idea of MIL-Decoding is to enhance the LM probability distribution with a MIL network that computes a toxicity score. In section 3.1, we first introduce the MIL network architecture and analyze the toxicity score produced by the network. And then, we provide a detailed description of our approach MIL-Decoding in section 3.2.For a given text with m tokens C = (w 1 , w 2 , ..., w m ) and a toxicity label y ∈ { 0, 1 } , the MIL model computes the toxicity of each token, and predicts the label according to the toxicity of tokens. In our network, token embeddings are encoded with a bidirectional GRU layer so that token representation is not merely based on the token itself, but also integrates contextual information:Toxicity score of each token in the text is computed with a token classification module containing attention layers and activation function based on the token representation, represented by function f:Toxicity scores are fed into a document classifier based on a bidirectional GRU component with attention mechanism, represented by function g:With label y as the ground truth, the CE loss between y pred and y is used to optimize the MIL model. Figure 1 illustrates our network architecture. Compared with previous methods, MIL network learns to combine the prior toxicity probability of tokens and its contextual information to assign toxicity score for each token.Figure 2 shows an example of MIL model analyzing a tokenized sequence "T ucker and Paul are total bad ass m of o ’s . <eos>". Some of the tokens have a toxicity score of 0, which indicatesthat they are harmless in this context, while others are toxic to some extent in the sentence. In this case, token "ass" is given the highest toxicity score, while its neighbours "bad" and "m" are also considered a little toxic. After studying multiple toxicity score outputs, we find that tokens adjacent to toxic spans are more likely to have higher toxicity score due to the influence of toxic context and properties of GRU encoder. Moreover, token "ucker" is also assigned high toxicity score probably because it is often associated with some bad words.Our approach augments a pre-trained LM with the MIL network to score the retrieved candidate tokens with pre-trained LM parameters remaining unchanged. At inference time, given a context sequence of tokens c t = (w 1 , w 2 , ..., w t−1 ) at time t, autoregressive LMs (like GPT-2) estimate the distribution over target token w t , noted as P LM (w t | c t ). We adopt a top-k filtering(Fan et al., 2018) method that preserves the top k tokens with the highest probability in P LM (w t | c t ) to truncate the unreliable tail of the probability distribution. Formally, let q 1 , q 2 , ..., q k denote the top-k retrieved tokens at time t, the MIL network is used to rate the toxicity of the top-k retrieved tokens by concatenating each candidate token q i to the context ct  which produces the potential generated sequencec t+1 i = (w 1 , w 2 , ..., w t−1 , q i ) at the next time step. The MIL model takes c t+1 i as the input sequence and assigns a toxicity score to each token in the sequence according to the network output:We measure the potential toxicity of token q i with the output score p t i . As illustrated in section 3.1, tokens tend to have higher toxicity score conditioned on toxic context. Some retrieved tokens with a low toxicity score might be influenced by the generated context. Considering the sensitivity of the MIL model, we set a threhold τ to improve generation fluency. After a softmax operation, toxicity scores p t 1 , p t 2 , ..., p t k are filtered with τ, where scores less than τ are manually set to 0. Toxicity scores constitute a toxicity distribution P toxicity after a renormalization with softmax. The last step is to interpolate the toxicity distribution P toxicity with the LM distribution PLM  with a tuned hyper-parameter λ and normalize to produce the final distribution we use to sample the next token (Khandelwal et al., 2019):Figure 3 illustrates the overall procedure of MILDecoding. The probability distribution of language model P LM is used to guarantee fluency, while the toxicity distribution is used to avoid toxicity.