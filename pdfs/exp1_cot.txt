"We elaborate on our framework in three subsections: multiple instance learning, building toxicity dataset, and MIL-Decoding for Integrated Detoxification. In MIL, the goal is to predict a label y for a given set of instances from a bag X = {x1, x2, ..., xn}. In our setting, instances are tokens in context and the bag is the set of all tokens for each given sentence exists as. We fine-tuned an MIL-model on a binary classification task to predict the toxicity at sentence-level, and the toxicity of each token in context as well. We also make an attempt as a ternary generation: positive, negative, and neutral. Our MIL model handles both binary and ternary generation, and we implemented it with pre-trained RoBERTa-base (Liu et al., 2019). \n\nOur MIL model works like two parallel decoders, the first decoder generates tokens only when the token is not toxic, and the second decoder is responsible for generating toxic tokens. For each token, we learn the probability of it being toxic as a binary classification task. For reduction operation, we calculated the overall toxicity by aggregating token-level toxicity with max pooling. \n\nExpanding on earlier work on MIL technology sentiment analysis (Wang and Wan, 2018; Angelidis and Lapata, 2018), we crafted an MIL-detoxifier on the basis of the RoBERTa-based language model to steer generated text away from toxicity. Selecting our generator model as the RoBERTa-base (Liu et al., 2019), we generate tokens in an autoregressive fashion. When generating the i-th token, we calculate the toxicity distribution for the next token and interpolate the original distribution from the language model with the toxicity distribution. This way, the MIL-detoxifier discourages the generator from producing tokens with high toxicity scores. The integrated MIL-Decoding method realizes detoxification of a language model by integrating our MIL model into the decoding stage.\n\nStep-by-step, the process can be outlined as follows: Once a token x1 is generated ,its toxicity context is obtained as x1's representation from MIL-model. Then, we run MIL-model again to get the toxicity context of next token. Lastly, the log-probability log P is obtained from LM, and the toxicity log-probability log P toxic is obtained from MIL-model. The final score of token x combined these two scores. \n\nFor the toxicity training dataset, we curated a dataset with fine-grained toxicity labels for every token in its context. To this end, we made use of publicly available toxicity-annotated datasets. The goal here is not to make a perfectly accurate dataset, rather we aimed at creating a corpus with as many discernible toxicity class labels as possible. \n\nHuman evaluation of the results showed that our MIL-Decoding model outperforms other baselines in detoxification, with only minor effects on fluency aspect of the generated text. Due to the structure of our MIL-Decoding model, we managed to retain the language generation properties of the pre-training model while minimizing the occurrence of toxicity at token level."