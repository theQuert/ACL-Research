k-nearest-neighbor machine translation (kNNMT) (Khandelwal et al., 2021) boosts the translation performance of trained neural machine translation (NMT) models by incorporating example-search into the decoding algorithm. However, decoding is seriously time-consuming, i.e., roughly 100 to 1,000 times slower than standard NMT, because neighbor tokens are retrieved from all target tokens of parallel data in each timestep. In this paper, we propose “Subset kNN-MT”, which improves the decoding speed of kNN-MT by two methods: (1) retrieving neighbor target tokens from a subset that is the set of neighbor sentences of the input sentence, not from all sentences, and (2) efﬁcient distance computation technique that is suitable for subset neighbor search using a look-up table. Our subset kNNMT achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT’19 De-En translation task and the domain adaptation tasks in De-En and En-Ja.Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Vaswani et al., 2017) has achieved state-of-the-art performance and become the focus of many studies.We propose “Subset kNN-MT”, which improves the decoding speed of kNN-MT by two methods.Our subset kNN-MT achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT’19 German-to-English general domain translation task and the domain adaptation tasks in German-to-English and English-to-Japanese with open-domain settings. kNN-MT (Khandelwal et al., 2021) retrieves the k-nearest-neighbor target tokens in each timestep, computes the kNN probability from the distances of retrieved tokens, and interpolates the probability with the model prediction probability. The method consists of two steps: (1) datastore creation, which creates key–value translation memory, and (2) generation, which calculates an output probability according to the nearest neighborsof the cached translation memory. Datastore Construction A typical NMT model is composed of an encoder that encodes a source x sentence x = (x 1 , x 2 , . . . , x x | | ) ∈ V | X | and a decoder that generates target tokens y = y (y 1 , y 2 , . . . , y y | | ) ∈ V Y | | where | x | and | y | are the lengths of sentences x and y , respectively, and VX  and V Y are the vocabularies of the source language and target language, respectively. The t-th target token y t is generated according to its output probability P(y t | x, y <t ) over the target vocabulary, calculated from the source sentence x and generated target tokens y <t . kNN-MT stores pairs of Ddimensional vectors and tokens in a datastore, represented as key–value memory M ⊆ R D × V Y . The key (∈ R D ) is an intermediate representation of the ﬁnal decoder layer obtained by teacher forcing a parallel sentence pair (x, y ) to the NMT model, and the value is a ground-truth target token y t . The datastore is formally deﬁned as follows:where D is parallel data and f : V | X | × V t−1 Y → R D is a function that returns the D-dimensional intermediate representation of the ﬁnal decoder layer from the source sentence and generated target tokens. In our model, as in (Khandelwal et al., 2021), the key is the intermediate representation before it is passed to the ﬁnal feed-forward network. Generation During decoding, kNN-MT generates output probabilities by computing the linear interpolation between the kNN and MT probabili-ties, p kNN and p MT , as follows:where λ is a hyperparameter for weighting the kNN probability. Let f(x, y <t ) be the query vector at timestep t. The top i-th key and value in the k-nearest-neighbor are k i ∈ R D and v i ∈ V Y , respectively. Then p kNN is deﬁned as follows:where τ is the temperature for p kNN , and we set τ = 100. Note that this kNN search is seriously time-consuming 1 (Khandelwal et al., 2021).