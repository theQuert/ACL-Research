In this section, we first introduce the previous practices in binarization and ternarization. Then, we introduce a unified statistic-based weight binarization / ternarization method that can alleviate the gradient mismatch issue and enhance the quantized weights entropy. Lastly, we analyze the difference between weight quantization and activation quantization and propose an elastic ternarization method for activations. We abbreviate our method as TBT, short for “Ternary / Binary Transformer”. Ternary neural networks, where real values are quantized to three levels, are first introduced in (Li et al., 2016). Thus, these values can be repre-sented in 2 bits, leading to a 16× reduction in size and computation. Moreover, the computations can be calculated multiplication-free, leading to even further computation gains on suitable hardware. The recent work integrates the ternarization algorithm in natural language models for quantizing the weights and activations in classification tasks (Zhang et al., 2020) and ternarizing the weight (8bit activations are used) in generative models (Li et al., 2022; Tao et al., 2022). The general formula (Li et al., 2016) for ternarization is as follows:Here X T denotes the ternary weights/activations, and X R represents their real-valued counterparts. n X R denotes the total number of elements in the tensor. ∆ is the ternary threshold, and α T is the scaling factor that minimizes l2-loss between XT  and X R .The neural network binarization denotes representing the weights and/or activation with bi-level values. It is first proposed in BNN (Courbariaux et al., 2016) and has evolved in the follow-up works (Rastegari et al., 2016; Liu et al., 2018). Rastegari et al. (2016) formulates binarization as:Here X B can represent binary weights or binary activations. α B denotes the scaling-factor that minimize the l2 loss between X R and α B ·Sign(X ). The acceleration and compression effectR of ternary/binary neural networks is significant. By representing the weights and activations with { −1, 0, 1 } , the network enjoys ∼16× memory saving compared to its 32-bit floating-point counterpart. When further binarize the weights and activations to only 1-bit (i.e., { −1, 1 } ), up to 32×model-size reduction and 58× speedup on CPUs have been achieved (Rastegari et al., 2016), where the matrix multiplication operations are replaced with light-weighted bitwise XNOR operations. Despite its appealing characteristics, naively binarizing or ternarizing the transformer model for natural language generation results in several accuracy drops or even a total failure in training. It has been observed that the attention layers of the transformer network are difficult to quantize to low bits. Also, the auto-regressive decoding tends to accumulate errors due to quantization. Given the nature of generative language networks that require highprecision output, quantizing both the activations and weights in these models to extreme bit values is non-trivial and has not been explored before.We propose a statistics-based method for weight binarization/ternarization. Particularly, this novel quantization method considers maximizing the entropy of the quantized weights and reducing the gradient mismatch in the backward pass. Previous works (Courbariaux et al., 2016; Bai et al., 2021b; Zhang et al., 2020) are mainly focused on minimizing the l2 loss between the quantized weights and the real-valued weights to find the optimal quantization scheme,where W Q denotes binary/ternary weights and α ∗ denotes the optimal scaling factor calculated. Despite the broad application and great success of the classic quantization scheme, we found that merely minimizing the l2 loss neglects several critical but intractable issues in ultra-low-bit weight quantization: (1) The information entropy of the quantized weights is not considered. Eq. 1 and Eq. 4 calculate the quantized weights to minimize the distance to the real-valued weights, which could lead to imbalanced quantized weight distribution and harm the quantized weights representation capacity. (2) The quantization function Eq. 1 and Eq. 4 are not isometric, meaning that it does not consider the magnitude consistency between the quantized weights and real-valued weights, while we find that magnitude consistency contributes significantly to accurate gradient estimation.Considering the above two limitations in previous solutions, we are motivated to design a novelquantization function that enhances information entropy and reduces gradient mismatch. To boost the weights representation capability, in information theory, more information is preserved when the quantized weights contain higher entropy:with p i denoting the proportion of real-valued weights being quantized to i th quantization level in total N levels. Eq. 7 can be easily solved with a Lagrange multiplier, and the optimal p i ∗ = 1 N , i ∈ { 1, 2, . . . , N } , suggesting the best quantization scheme to preserve maximum information entropy is to distribute the real-valued weights in all quantization levels as evenly as possible.For reducing the gradient mismatch, as suggested by the previous binarization work (Liu et al., 2020b), the magnitude difference between the quantized weight and the real-valued weight will greatly influence the gradient scale and a mismatch in magnitude will be amplified in back-propagation and cause gradient vanishing or explosion during training. Thus it is important to ensure the magnitude of real-valued weights and quantized weights are consistent.Combining two requirements discussed above, we proposed max-entropy isometric weight quantization. In ternarization, it is formulated asWhere W T and W R refer to the ternary weights and real-valued weights, respectively. The rounding function ⌊ · ⌉ and Clip(·) function quantize weights to { −1, 0, 1 } . µ T is the mean of realvalued weights and n W R denotes the number of weights in the weight matrix. Scaling factor α is calculated from the weight statistics and follows the entropy rule to scale the real-valued weight W R to be evenly distributed in quantization levels. In the ternary case, the weights are quantized to { −α T , 0, α T } . When the real-valued weights are initialized as uniformly and symmetrically distributed (He et al., 2015; Glorot and Bengio, WR i  2010), the scaling factor α T will distribute α T to [−1.5, 1.5], such that the output ternary weightswill have near uniform distribution in three ternary levels. Meanwhile, Eq. 8 is an isometric mapping where the real-valued weights are scaled by α 1 to T near [-1, 1] and time α T to scale back after quantization. In this way, the magnitude is preserved.Here W B denotes the binary weights, where substracting the average µ B makes the realvalued weight zero-centered before binarization and thus encourages an even distribution in binarized weights. Then the scaling factor α B matches the magnitude between real-valued and binary weights. Particularly, in Eq. 9, W B i = WR i  i α B · Sign( α µ B ) = α B · Sign(W R − µ B ), we B explicitly include the α B in the denominator to keep the binarization function isometric and the gradients w.r.t. weights can be calculated straight-forwardly as:STE is abbreviated for straight-through estimator (Bengio et al., 2013), which replaces the nondifferentiable Sign function with Clip function in the backward pass. We show that the proposed maxentropy isometric weight quantization improves the accuracy of weight binarization / ternarization by 6.0 / 11.53 RougeL scores on the CNN/DailyMail benchmark, respectively. More details can be found in Sec. 3.2.In contrast to neural network weights that are stored on the disk, activations are calculated on-the-fly. The distribution of activations in a particular layer depends on the network weights as well as the corresponding input sequence, and thus varies from batch to batch. In order to have the quantization function better capture the underlying activation distribution, we propose learning-based activation quantization.Inspired by BiT (Liu et al., 2022), we divide the activation layers into two categories: the activationlayers with non-negative values (X R ∈ R + ), i.e., Softmax/ReLU layer outputs and the rest of the layers with both positive and negative activations (X R ∈ R). We binarize / ternarize the first activation category (X R ∈R + ) to { 0, α } / { 0, α, 2α } , and symmetrically quantize the later activation category (X R ∈ R) to { −α, α } and { −α, 0, α } in binary and ternary cases respectively. In this way, the activation distribution matches the original fullprecision activations and thus reduces the quantization error. Further, we learn to scale the real-valued activations to better fit quantization thresholds, and this learnable scaling factor can be updated end-to-end with the gradients from the network loss to better account for overall network optimization. In the ternary case, we propose the elastic ternarization function formulated as,where X R and X T denote real-valued and ternary activations, respectively. To keep the formula concise, we set X R ′ = X R − X R , denoting the zeromean real-valued activations. α T is the scaling factor. Different from the weight quantization, the scaling factor in Eq. 11 is learned with the gradient update. We follow the practice in (Zhou et al., 2016; Esser et al., 2019) to calculate the gradients with straight-through estimation (STE) bypassing the non-differentiable rounding function:The learnable scaling factor can dynamically adapt to different activation distributions and improve the ternarization accuracy. In the binary case, it is formulated as.We demonstrate that with the learning-based activation quantization method and statistics-based weight quantization scheme, the proposed TBT for the first time is able to quantize the BART model for natural language generation tasks to ternary and even binary weights and activations, and achieve reasonable accuracy on summarization and translation benchmarks.
Generally, methods for handling hallucinations can be either internal, i.e. using only information coming from the translation model itself, or external, i.e. using auxiliary models. In addition to these, we also consider “oracles” relying on reference translation. Note that these cannot be used in preventive settings when references are not available; here we use them only for analysis.Following previous work (Müller and Sennrich, 2021; Guerreiro et al., 2022), we use: • chrF: character n-gram F score of the translation with respect to the reference. We use the CHR F++ version that also takes into account word unigrams and bigrams (Popovi´c, 2017); • COMET: a neural quality estimation metric by Rei et al. (2020a) which was shown to be the state-of-the-art reference-based method (Kocmi et al., 2021).Baseline: Seq-Logprob. This is the standard length-normalized sequence log-probability. Compared to previously introduced methods specifically targeting hallucinations, this simple metric performs the best (Guerreiro et al., 2022). We use ALTI: percentage of source contribution. We compute the percentage of source impact on the generated translation using the recently introduced ALTI+ (Ferrando et al., 2022). At a high level, it decomposes each transformer block into a sum of functions of individual tokens and views an output representation as a summation of transformed input vectors. Then it evaluates contribution of these vectors to the resulting sum. Among other things, ALTI+ (as well as an earlier Layerwise Relevance Propagation (LRP) -based method by Voita et al. (2021)) was used to show that for artificially created hallucinations, source influence is much lower than for “healthy” translations. Our work is the first to test this intuition in a real setting where hallucinations are generated naturally.3  Formally, for a model and its generated translation, we compute the total source contribution as the sum of contributions of all source tokens. We do it for each target token individually and then average across target tokens. The scores are computed by the same model that produced the translations (Section 2.1).Baseline: COMET-QE. For a reference-free model, we use the state-of-the-art COMETQE (Rei et al., 2020b) for its superior performancecompared to other quality estimators (Mathur et al., 2020; Freitag et al., 2021; Kocmi et al., 2021). We use: sentence similarity. Overall, we consider three measures based on pretrained models that evaluate semantic similarity of two sentences: • LASER: cosine similarity of source and translation sentence embeddings from LASER2. LASER2 (Heffernan et al., 2022) improves LASER (Artetxe and Schwenk, 2019) by replacing LSTM encoder with a Transformer and using teacher-student training; • LaBSE: cosine similarity of source and translation sentence embeddings from LaBSE (Feng et al., 2022). LaBSE is a dual-encoder approach based on pretrained transformers and fine-tuned for translation ranking with an additive margin softmax loss; • XNLI: product of the entailment probabilities of source to translation and translation to source. We compute entailment scores with RoBERTa (Conneau et al., 2020) finetuned on a combination of NLI data in 15 languages (Conneau et al., 2018).
To design a model for multi-party dialogue response generation and make it compatible with the EM training algorithm, there are two important things to consider: how to model p(r t | c t , z t ; θ) in the maximization step, and how to compute p(z t | c t , r t ; θ) in the expectation step. In this section, we will first address these two problems, then mathematically derive the feasibility of our EM pre-training algorithm.Given an input sequence of the dialogue history and the speaker of the response at time step t, X = { S 1 : U 1 [SEP]S 2 : U 2 [SEP] . . . S t-1 : U t-1 [SEP]S t : } , together with the addressee of the response z t = j, our goal is to train a model that can generatean response Y = U t . Here each S i is the name of the speaker at time step i, which is represented as Speaker #S i like those in Figure 1. U i = { w i1 , w i2 , . . . , w in i } is the content of the ith  utterance with n i words. z t = j represents that St  speaks to S j , who utters U j , and [SEP] is a special token that indicates the end of a dialogue turn.In this section, we answer the first question: how to model p(r t | c t , z t ; θ), or in other words, how to incorporate the addressee information z t = j into the process of generating a response r t . We design a straightforward method that adds addressee embeddings to the positional encodings and word embeddings, before they are further encoded by a PLM. The left part of Figure 2 illustrates this method, where we use an embedding look-up table with 2 entries to indicate whether a word belongs to the addressee utterance or not. Specifically, if a word is in the addressee utterance, it will get its addressee embedding from entry 1, otherwise from entry 0. Since addressee modeling is not the key contribution of this work, we just adopt the most straightforward and effective way. In our experiments, we use BART (Lewis et al., 2020) as the backbone PLM, following previous works (Gu et al., 2022). Due to the page limit, the proverbial architecture of Transformer and BART are omitted here.In this section, we answer the second question: how to compute p(z t | c t , r t ; θ) in the expectation step, or in other words, how to predict the distribution of the unlabeled addressee z t , given the current dialogue context c t , response r t , under parameters θ. The solution to this question is essentially the mostimportant part of our method since it delicately solves the problem of data scarcity in MPDRG. Let’s consider what humans will do to participate in a multi-party conversation. First, we will read the dialogue history c t , then choose an addressee z t to reply. Once c t and z t are determined, we will utter a response according to the content of the whole dialogue and the addressee utterance. The right part of Figure 2 gives the Bayesian Network of the above process, where the joint distribution of (c t , z t , r t ) can be factorized as:Here we omit the subscript t and model parameters θ for simplicity. Given Eq. (1), p(z | c, r; θ) can be derived as:We assume that the probability of choosing any previous utterance as the addressee is the same given the current dialogue history, which means p(z | c) obeys a uniform distribution. Meanwhile, the denominator p(r | c) is independent of z, leaving only the term p(r | c, z). Now, we can induce that:Therefore, for each z i , i = 1, 2, . . . , t−1, we have:In practice, we can use the generative model p(r t | c t , z t ; θ) to compute the probability distribution of p(z t | c t , r t ; θ) by Eq. (4).Figure 3 illustrates the overview of our EM training process. During the E-steps, we compute the probability distribution of the latent variable (the addressee z). During the M-steps, we sample (c, r, z) triplets from this distribution and optimize the generative model by standard training algorithms. The Expectation Step is to compute the conditional distribution of the latent variable z t , given the observed data (c t , r t ) and the current modelparameters θ, where Eq. (4) gives a reasonable approximation of this value. Specifically, for a sample (c t , r t ), with the model parameters θ fixed, we first calculate the un-normalized probability of each of the i th (i < t) utterance being the addressee: p(r t | c t , z t i ; θ) using Eq. (3), then normalize them to get the conditional distribution of z t using Eq. (4). Once P(z t | c t , r t ; θ) is obtained, we sample (c t , r t , z t ) triplets from this distribution, which is further used in the maximization step.The Maximization Step is analogical to the normal training process. Given the sampled { (c t k , r t k , z t k ) } k=1 N triplets, where N is the total number of samples, our goal is to minimize the auto-regressive language modeling loss:where w i k is the i th word in the response of the kth  sample: r t k = { w i k } i=1 n i , and n i is the length of this response. Compared with the vanilla EM algorithm, there are several differences in our implementations. First of all, we do not use the initial model to generate the training data for the first round of the maximization step. Instead, we utilize the discourse parser provided by Shi and Huang (2019) to predict the addressee of each utterance in the unlabeled corpus to get a coarse initial training dataset. The reason for this initialization method is that the initialization of training data (or model parameters) is vital to the EM method, which helps it converge to a better point. Second, rather than sampling z t from its conditional distribution, we adopt a hard EM approach which takes the value z t i with highest probability as the predicted label, where i = arg max p(z t i | c t , r t ; θ). This hard EMapproach is proved as more effective to boost the performance (Min et al., 2019). Finally, to ensure the quality of the generated training data in the maximization step, we set a hyper-parameter α ∈ [0, 1] to control the proportion of training data that is actually used. Specifically, we first rank the prediction confidence of each z t k according to the value of p(z t k | c t k , r t k ; θ), then pick the top α×N samples with the highest confidence scores. In our experiments, α is dynamically set to ensure the addressee prediction accuracy of the selected samples is over 80% in an annotated validation set.In a multi-party dialogue corpus without annotated addressee labels, a usual solution to train a response generation model is to maximize the marginal loglikelihood (or incomplete log-likelihood) over all possible addressees:However, this objective is hard to optimize since the distribution of z is hard to obtain. Here, we define an expected complete log-likelihood where our estimation of p(z t | c t , r t ; θ) can come to rescue:Our new objective now becomes maximizing the expected complete log-likelihood. The relation ˆ between ℓ and ℓ can be derived as follows:where the third line is derived from the Jensen Inequality, and H q(z) is the entropy of the distribution of z. Since H q(z) ≥ 0, we can derive that ˆ ℓ(c, r; θ) ≤ ℓ(c, r; θ), which means ℓ is the lowerbound of ℓ. By maximizing the lower bound ℓ, we can indirectly maximize ℓ, which is originally hard to optimize. Another important observation is hat ℓ = ℓ if and only if q(z) = p(z t | c t , r t ; θ), which is exactly what we calculate during the E-steps in Eq. (7). Though the derivation of the posterior distribution of z is not exact since we assume uniform prior in Eq. (2), it is still much closer to the real distribution compared to random q(z). It is worth noting that the global optimal point is not guaranteed to be reached by this algorithm, and it depends heavily on the initialization of model parameters or the training data for the first round of the maximization step. This explains the reason why we utilize a discourse parser to get a coarse initial training dataset instead of using the expectation step at the first iteration in Section 3.4.
The core idea of MIL-Decoding is to enhance the LM probability distribution with a MIL network that computes a toxicity score. In section 3.1, we first introduce the MIL network architecture and analyze the toxicity score produced by the network. And then, we provide a detailed description of our approach MIL-Decoding in section 3.2.For a given text with m tokens C = (w 1 , w 2 , ..., w m ) and a toxicity label y ∈ { 0, 1 } , the MIL model computes the toxicity of each token, and predicts the label according to the toxicity of tokens. In our network, token embeddings are encoded with a bidirectional GRU layer so that token representation is not merely based on the token itself, but also integrates contextual information:Toxicity score of each token in the text is computed with a token classification module containing attention layers and activation function based on the token representation, represented by function f:Toxicity scores are fed into a document classifier based on a bidirectional GRU component with attention mechanism, represented by function g:With label y as the ground truth, the CE loss between y pred and y is used to optimize the MIL model. Figure 1 illustrates our network architecture. Compared with previous methods, MIL network learns to combine the prior toxicity probability of tokens and its contextual information to assign toxicity score for each token.Figure 2 shows an example of MIL model analyzing a tokenized sequence "T ucker and Paul are total bad ass m of o ’s . <eos>". Some of the tokens have a toxicity score of 0, which indicatesthat they are harmless in this context, while others are toxic to some extent in the sentence. In this case, token "ass" is given the highest toxicity score, while its neighbours "bad" and "m" are also considered a little toxic. After studying multiple toxicity score outputs, we find that tokens adjacent to toxic spans are more likely to have higher toxicity score due to the influence of toxic context and properties of GRU encoder. Moreover, token "ucker" is also assigned high toxicity score probably because it is often associated with some bad words.Our approach augments a pre-trained LM with the MIL network to score the retrieved candidate tokens with pre-trained LM parameters remaining unchanged. At inference time, given a context sequence of tokens c t = (w 1 , w 2 , ..., w t−1 ) at time t, autoregressive LMs (like GPT-2) estimate the distribution over target token w t , noted as P LM (w t | c t ). We adopt a top-k filtering(Fan et al., 2018) method that preserves the top k tokens with the highest probability in P LM (w t | c t ) to truncate the unreliable tail of the probability distribution. Formally, let q 1 , q 2 , ..., q k denote the top-k retrieved tokens at time t, the MIL network is used to rate the toxicity of the top-k retrieved tokens by concatenating each candidate token q i to the context ct  which produces the potential generated sequencec t+1 i = (w 1 , w 2 , ..., w t−1 , q i ) at the next time step. The MIL model takes c t+1 i as the input sequence and assigns a toxicity score to each token in the sequence according to the network output:We measure the potential toxicity of token q i with the output score p t i . As illustrated in section 3.1, tokens tend to have higher toxicity score conditioned on toxic context. Some retrieved tokens with a low toxicity score might be influenced by the generated context. Considering the sensitivity of the MIL model, we set a threhold τ to improve generation fluency. After a softmax operation, toxicity scores p t 1 , p t 2 , ..., p t k are filtered with τ, where scores less than τ are manually set to 0. Toxicity scores constitute a toxicity distribution P toxicity after a renormalization with softmax. The last step is to interpolate the toxicity distribution P toxicity with the LM distribution PLM  with a tuned hyper-parameter λ and normalize to produce the final distribution we use to sample the next token (Khandelwal et al., 2019):Figure 3 illustrates the overall procedure of MILDecoding. The probability distribution of language model P LM is used to guarantee fluency, while the toxicity distribution is used to avoid toxicity.
Our Subset kNN-MT (Figure 1) drastically accelerates vanilla kNN-MT by reducing the kNN search space by using sentence information (Section 3.1) and efﬁciently computing the distance between a query and key by performing table lookup (Section 3.2). Sentence Datastore Construction In our method, we construct a sentence datastore that stores pairs comprising a source sentence vectorand a target sentence. Concretely, a sentence datastore S is deﬁned as follows:where h : V | X | → R D ′ represents a sentence encoder, which is a function that returns a D ′ dimensional vector representation of a source sentence.Decoding At the beginning of decoding, the model retrieves the n-nearest-neighbor sentences of the given input sentence from the sentence dataˆ store S. Let S ⊂ S be the subset comprising nnearest-neighbor sentences. The nearest neighbor search space for target tokens in kNN-MT is then drastically reduced by constructing the datastore ˆ corresponding to S as follows: where is the reduced datastore for the translation examples coming from the n-nearestneighbor sentences. During decoding, the model uses the same algorithm as kNN-MT except that M is used as the datastore instead of M . The proposed method reduces the size of the nearest neighbor search space for the target tokens from | D | to n ( ≪ | D | ) sentences.Subset kNN-MT retrieves the k-nearest-neighbor target tokens by an efﬁcient distance computation method that uses a look-up table. In the original kNN-MT, inverted ﬁle index (IVF) is used for retrieving kNN tokens. IVF divides the search space into N list clusters and retrieves tokens from the neighbor clusters. In contrast, in subset kNNMT, the search space varies dynamically depending on the input sentence. Therefore, clusteringbased search methods cannot be used; instead, it is necessary to calculate the distance for each key in the subset. For this purpose, we use asymmetric distance computation (ADC) (Jégou et al., 2011) instead of the usual distance computation between ﬂoating-point vectors. In ADC, the number of table lookup is linearly proportional to the number of keys N in the subset. Therefore, it is not suitable for searching in large datastore M , but in a ˆ small subset M , the search is faster than the direct calculation of the L2 distance. The kNN-MT datastore M may become too large because it stores high-dimensional intermediate representations of all target tokens of parallel data. For instance, the WMT’19 German-to-English parallel data, which is used in our experiments, contains 862M tokens on the target side. Therefore, if vectors were stored directly, the datastore would occupy 3.2 TiB when a 1024-dimensional vector as a key 2 , and this would be hard to load into RAM. To solve this memory problem, product quantization (PQ) (Jégou et al., 2011) is used in both kNNMT and our subset kNN-MT, which includes both source sentence and target token search.PQ splits a D-dimensional vector into M subvectors and quantizes for each M D -dimensional sub-vector. Codebooks are learned by k-means clustering of key vectors in each subspace. It is computed iteratively by: (1) assigning the code of a key to its nearest neighbor centroid (2) and updating the centroid of keys assigned to the code. The m-th sub-space’s codebook C m is formulated as follows: In this work, each codebook size is set to L = 256. A vector q ∈ R D is quantized and its code vector ¯q is calculated as follows:Asymmetric Distance Computation (ADC) Our method efﬁciently computes the distance between a query vector and quantized key vectors using ADC (Jégou et al., 2011) (Figure 2). ADC computes the distance between a query vector ¯K ¯ki  q ∈ R D and N key codes = { } i=1 N ⊆ { 1, . . . , L } M . First, the distance look-up table A m ∈ R L is computed by calculating the distance between a query q m and the codes c l m ∈ C m in each sub-space m, as follows:Second, the distance between a query and each key ¯ki  d( q , ) is obtained by looking up the distance table as follows:A look-up table in each subspace, A m ∈ R L , consists of the distance between a query and codes. The number of codes in each subspace is L and a distance is a scalar; therefore, A m has L distances. And the table look-up key is the code of a key itself, i.e., if the m-th subspace’s code of a key is 5, ADC looks-up A 5 m . By using ADC, the distance is computed only once 3 (Equation 9) and does not decode PQ codes into D-dimensional key vectors; therefore, it can compute the distance while keeping the key in the quantization code, and the k-nearest-neighbor tokens are efﬁciently ˆ retrieved from M .In our subset kNN-MT, a variety of sentence encoder models can be employed. The more similar sentences extracted from M , the more likely the ˆ subset M comprises the target tokens that are useful for translation. Hence, we need sentence encoders that compute vector representations whose distances are close for similar sentences. In this work, we employ two types of representations: neural and non-neural. We can employ pre-trained neural sentence encoders. While they require to support the source language, we expect that the retrieved sentences are more similar than other encoders because we can use models that have been trained to minimize the vector distance between similar sentences (Reimers and Gurevych, 2019). An NMT encoder can also be used as a sentence encoder by applying average pooling to its intermediate representations. This does not require any external resources, but it is not trained from the supervision of sentence representations. Alternatively, we can also use nonneural models like TF-IDF. However, it is not clear whether TF-IDF based similarity is suitable for our method. This is because even if sentences with close surface expressions are retrieved, they do not necessarily have similar meanings and may not yield the candidate tokens needed for translation.
We present W URA, 2 a multilingual dataset comprising 16 African languages and 4 high-resource languages popularly spoken on the African continent – Arabic, English, French, and Portuguese. The curation of W URA was carried out in a threepart process: – (i) Auditing and cleaning mC4 (ii) Crawling indigenous websites and (iii) Combination with existing language resources. Kreutzer et al. (2022) reports mC4’s high ratio of non-linguistic content and sentences in incorrect languages, with African languages being of particular concern. The authors report significant loss (up to 50%) in recall of correct in-language sentences as they increased precision of their automatic language classification. Our manual audit of mC4 corroborates the documented issues. We highlight three important findings: (1) The distribution of mC4 document sources has a long tail. Many individual news publications yield thousands of documents in the mC4. (2) Documents from news publications are more likely to be of higher quality i.e., both inlanguage and grammatical compared to documents from other web sources. (3) Some documents are from websites which translate content using online translation tools. Such documents are often a mix of in-language and noisy or non-linguistic text, and may best be filtered at sentence-level. Noting all of these issues and findings, we filter at three levels: Corpus-level. We first rank unique websites in descending order of the number of documents they contribute to the mC4 corpus for each language. Then, we select the top 20% of websites for each language and collect documents sourced from websites in this list. This preserves high potential sources for further document level filtering. Document-level. At document level, we filter out documents that do not contain at least 5 stopwords in them (Caswell et al., 2020) using stopwords from Stopword Lists for African Languages dataset.3  Passage-level. After document-level filtering, we chunk the dataset into passages of roughly 512 tokens. Finally, we filter out passages that contain fewer than 4 unique words or contain repetition for more than 20% of its word length; have more than 40% of its characters are numeric or contain markers of possibly offensive content such as included in the Toxicity-200 dataset (NLLB Team et al., 2022) for the relevant language. While Kreutzer et al. (2022)’s audit of mC4 did not yield a significant amount of offensive content (0.06% of sentences they audited) and our web crawls mainly focused on verified news publications, these filters ensure that non-linguistic and offensive contents are removed at the passage level. Xue et al. (2021)’s inclusion of the URL each document is sourced from makes the mC4 corpus even more useful as a data source. Commonly, multiple articles are collected from the same base website, e.g., news publications. For many news publications that provide a sitemap, we find that there are fewer articles in mC4 than is actually available on the websites. Further, mC4 only covers up to August, 2020 so updating the crawls up to the current day yields more data. We initiate focused crawls for such websites and this leads to significant increase (> 100% for Hausa and Somali) in the amount of articles available per language. For all languages we consider except Chichewa, Sesotho, Xhosa and Zulu, we collect 1.39M articles (see Table 6) from credible sources found in mC4. Following previous works (Alabi et al., 2022; Adebara et al., 2022), we include certain non-African languages in our pretraining data. Specifically, we include over 240, 000 articles newly crawled from 10 African news websites reporting in English, French and Portuguese. We also include a sample of 1.5M Wikipedia articles for English and French, as well as Wikipedia articles written in Egyptian Arabic. For the African languages, we include all Wikipedia articles. Finally, we deduplicate using the document URLs. In doing this, we prioritize news articles in our focused crawls over their existing counterparts in mC4. Final Dataset Statistics Table 6 presents a statistical summary of our dataset. The combined dataset from crawling, combining with existing sources and deduplication amounts to ∼30GB of data across all languages and ∼19GB for African languages.
We initialize the model with supervised data, and improve it by learning from user feedback through an offline contextual bandit learning process. We use a standard BERT-style architecture (Devlin et al., 2019). The input to the model is a concatenation of the question ¯q and the context text ¯c. We separately classify over the context tokens for the answer span start and end to compute the span distribution P s (Seo et al., 2017), and compute the binary answerability distribution P u with a classification head on the CLS token (Liu et al., 2019). We initialize the model parameters with DeBERTaV3 weights (He et al., 2023), 4 and fine-tune us-ing supervised data to get our initial model. This is critical to get a tolerable experience to early users. We usually use a small number of examples ( ≤ 512 examples), except when studying domain transfer. The training loss sums over the three cross-entropy classification losses, with a coefficient λ to weigh the binary answerable classification term.We learn through iterative deployment rounds. In each round ρ , we first deploy our model to interact with users (Section 3) and then fine-tune it using the data aggregated during the interactions. Each user interaction generates a tuple (¯q, ¯c, uˆ, i, j, f, θ ρ ), where ¯q is a question, ¯c is a context text, uˆ is the answerability classification decision, i and j are the returned span boundaries if a span was returned, and θ ρ are the model parameters when the interaction took place. Policy We formulate a policy that casts answer prediction as generating a sequence of one or two actions, given a question ¯q and a context ¯c. This sequential decision process formulation, together with the multi-head model architecture, allow to control the losses of the different classification heads by assigning separate rewards to the answerability classification and the span extraction. The policy action space includes classifying if the question is answerable (ANS) or not (UNANS) and actions for predicting any possible answer span [i, j] in ¯c. The set of possible action sequences is constrained. At the start of an episode, the policy first predicts if the question is answerable or not. The probability of the action a ∈ { ANS, UNANS } is P u (a | ¯q, ¯c; θ). Span prediction action are not possible, so their probabilities are set to 0. If the UNANS action is selected, the episode terminates. Otherwise, the second action selects a span [i, j] from ¯c as an answer, and the episode terminates. The probability of each span selection action is P s (i, j | ¯q, ¯c; θ). Answerability prediction actions are not possible, so their probabilities are set to 0. Reward Values We do not have access to a reward function. Instead, we map the user feedback f to a reward value depending on the action (Table 2), and cannot compute rewards for actions not observed during the interaction. The policy formulation, which casts the prediction problem as a sequence of up to two actions, allows to assign different rewards to answerability classification andspan extraction. For example, if we get WRONG feedback when an answer is given, we cannot tell if the answerability classification was correct or not. Our formulation allows us to set the reward value of the first action to zero in such cases, thereby zeroing the answerability classification loss. The reward values were determined through pilot studies. For example, we observed that models overpredict unanswerable, so we set a relatively large penalty of -1 for wrongly predicting unanswerable. Learning Objective We use a policy gradient REINFORCE (Williams, 1992) objective with a clipped inverse propensity score coefficient (IPS; Horvitz and Thompson, 1952; Gao et al., 2022) and an entropy term for the answerability binary classification. IPS de-biases the offline data (Bietti et al., 2021), and also prevents unbounded negative loss terms (Kojima et al., 2021). The entropy term regularizes the learning (Williams, 1992; Mnih et al., 2016). If we substitute the policy terms with the predicted model distributions, the gradient for an answerable example with two actions with respect to the model parameters θ is:where the α 1 and α 2 are IPS coefficients for the first (answerability classification) and second (span extraction) actions, r 1 and r 2 are the corresponding reward values, γ is a hyperparameter, and H(·) is the entropy function. For examples the model predict as unanswerable, the second term is omitted.Deployment and Learning Process Algorithm 1 outlines our process. Each round (Line 2) includes interaction (Lines 4–14) and learning (Lines 1518) phases. During interaction, given a question and context (Line 5), we classify if it is answerable in the given context (Line 7) and extract the answer span (Line 8). Depending on the classification, weeither display the answer (Line 10) or return that the question is not answerable in the given context (Line 12), and solicit feedback (Line 13). We aggregate the interaction data over time (Line 14). During learning, we use rehearsal (Rebuffi et al., 2017) for each update, creating a batch of size B by mixing examples from the most recent interactions (Line 16) and previous rounds (Line 17) to update the model parameters (Line 18).
In this section, we formalize the multi-question generation task and introduce our mQG. We first formulate our task and then explain how our model’s training process incorporates a maximum question similarity loss L MQS . Finally, we provide a detailed outline of our recursive generation framework. The QG task in this paper aims to generate each question using a given context, question type, and the history of questions generated from the same context with the same question type. We use seven wh-words (what, when, where, which, who, why, how) as question types. Mathematically, given the context C, question type QT, and history of generated questions H i = (GQ 1 , GQ 2 , ..., GQ i−1 ), this task can be defined as generating a question, GQ, where: For the training process, we extract wh-words from each question by applying part-of-speech tagging with the Spacy 2 English Model. Due to the absence of a history of generated questions and an insufficient number of questions per context per question type in the FairytaleQA dataset, we utilize groundtruth questions that only share the context as the history of questions within the training process. mQG is built upon BART (Lewis et al., 2020), which has demonstrated remarkable performance in various natural language processing tasks. The primary pre-training objective of BART is to reconstruct the masked input based on unmasked input. To further leverage the capabilities of the pre-trained BART, we introduce a maximum question similarity loss L MQS . This loss is designed to promote similar representations for different questions from the encoder and decoder. As shown in Figure 1, the encoder takes in three inputs: the question type, which signifies the type of question to be generated; the context, which pro-vides the necessary information for question generation; and ground-truth questions from the same context, serving as reference questions. These three inputs are concatenated, with a [SEP] token inserted between them. The encoder processes the input sequence and produces its corresponding representations. Subsequently, the decoder generates the representation for the target question. To calculate the maximum question similarity loss L MQS , we use mean pooling layers to convert question representations into sentence-level representations. The maximum question similarity loss L MQS is calculated between the sentence-level representation of the reference questions and the sentencelevel representation of a generated question. By encouraging the representation of different questions to be similar, we promote the generation of diverse questions that differ from reference questions. Given a set of reference questions sentencelevel representation as Q = { Q 1 , ..., Q m } and a sentence-level representation of the target question as TQ, the maximum question similarity loss L MQS is computed as follows: where s(Q i , TQ) is a cosine similarity calculation between representations. By optimizing the model parameters to maximize the sentence-level similarity between these different representations, we guide mQG to generate diverse questions within the range of semantic correctness. This is achieved by ensuring that all the representations, which are the ground truth questions, are semantically correct. In doing so, we maintain a balance between diversity and accuracy in the generated questions. The overall training objective L is defined as L CE refers to the cross-entropy loss from a target question. As cross-entropy loss is calculated at the token level, the use of cross-entropy loss enhances mQG to generate syntactically correct questions. Figure 2 illustrates the generation process of mQG. First, the encoder takes question type, and context as input. The decoder then generates a question based on the information provided by the encoder. For the subsequent generation steps, the previously generated questions are recursively fed back into the model. Specifically, the previous questions are concatenated with the same question type and context, separated by a [SEP] token. This concatenated sequence is then used as input for the next generation step. This recursive generation process continues until the desired number of questions per context per question type is achieved. The use of this recursive generation process allows mQG to generate multiple questions while considering the previously generated questions. Following the training process of mQG, this generation process enables mQG to build upon its own previous outputs and generate different questions from previous outputs. We use beam search for the decoding method and return multiple sequences to exclude pre-generated questions. By leveraging a recursive framework, mQG demonstrates its capability to generate a variety of diverse questions that are contextually relevant and coherent.
In this section, the overall architecture of our proposed model Dual-Span is shown in Figure 2, which consists of four main components: sentence encoding, feature enhancing module, dual-channel span generation and triplet module. For a sentence X = { w 1 , w 2 , . . . , w n } of length n, the ASTE task is to extract the set of aspect sentiment triplets T = { (a, o, s) m } m=1 | T | from the given sentence X, where a, o and s ∈ { POS, NEU, NEG } represent the aspect term, opinion term and sentiment polarity, respectively. | T | is the number of sentiment triplets contained sentence X. 3.2 Sentence Encoding To obtain contextual representations for each word, we explore two sentence encoding methods, namely, BiLSTM and BERT. BiLSTM We first use the GloVe (Pennington et al., 2014) embedding to get the embedding matrix E ∈ R V | | ∗d w of the corpus, where | V | represents the vocabulary size, and d s represents the embedding dimension. For the embedding tokens E x = { e 1 , e 2 , . . . , e n } in the sentence, we use BiLSTM to get its hidden representation H = { h 1 , h 2 , . . . , h n } , where h ∈ R 2d n is obtained by → splicing the hidden state h ∈ R d n generated byforward LSTM and the hidden state h ∈ R d n generated by backward LSTM: (1) BERT An alternative approach is to utilize BERT (Devlin et al., 2019) as the sentence encoder to generate contextualized word representations. Given a sentence X = { w 1 , w 2 , . . . , w n } with n words, the hidden representation sequence H = { h 1 , h 2 , . . . , h n } is the output of the encoding layer of BERT at the last transformer block. As aforementioned, spans (or intra-span words) involve syntactical dependency and part-of-speech correlation, therefore incorporating those information into feature representations can be beneficial for span pairing and sentiment prediction. To capture the high order dependency relations, here we devise a graph neural network based method to encode the syntactic dependency and part-of-speech relations of intra- and inter-spans in high orders. In particular, we construct the part-of-speech relational graph (corresponding to a multi-relation matrix as shown in Figure 3 (b)). Then we apply two relational graph attention networks to learn the high order interactions between words on syntactic dependency tree of the sentence in question and constructed part-of-speech graph, respectively. 3.3.1 Part-of-speech Graph Construction modify aspect terms, leading to better extraction of aspect-opinion pairs. Finally, for each word’s part-of-speech, we add a self-loop relational edge to itself, as the diagonal elements shown in Figure 3. Figure 3: An example sentence with dependency tree and part-of-speech adjacency matrices in ASTE task. On the other hand, the syntactic dependency graph G Syn = (V, R Syn ) is constructed according to the dependency parsing tree, where edges are represented by syntactic relation types. Moreover, we define the self-dependency for each word. So for a given sentence of length n, the syntactic relation between words w i and w j is denoted as R i,j Syn , whose corresponding vectorization representation is denoted as the vector r i,j s ∈ R d s , where d s is the dimension of syntactic relation embeddings. 3.3.2 High-order Feature Learning with Relational Graph Attention Network The goal of part-of-speech graph construction is to characterize the word formation patterns of aspect and opinion terms so as to better identify the possible spans. Specifically, we adopt the following three rules to construct the part-of-speech graph G Pos = (V, R Pos ) of a given sentence X. First, following previous work (Chakraborty et al., 2022), assuming that aspect terms are usually nouns and opinion terms are usually adjectives, we can define part-of-speech relations based on part-of-speech tags NN or JJ. In particular, we consider the relations between words in a given window that contains words tagged with NN or JJ. Therefore, a relational edge R i,j Pos of G Pos is defined for two words i and j as the combination of part-of-speech tags of the two words, whose representation vector is r i,j p ∈ R d p , where d p is the dimension of part-of-speech combination embedding. Besides, we consider the special syntactic relation nsubj, since opinion terms are usually directly used to Next, we use relational graph attention networks (RGAT) to capture the multiple types of linguistic features and high-order interaction between spans/words on syntactic dependency graph and part-of-speech graph, respectively. Moreover, we use two graph attentional network based modules, namely, SynGAT and PosGAT to learn syntactic dependency graphs and part-of-speech graphs, respectively, which will distinguish between various syntactic relationships and part-of-speech relationships when calculating the attention weight between nodes. In particular, following previous work (Bai et al., 2021), we denote two specific relations on each edge by r i,j s and r i,j p , respectively. Specifically, for the i−th node, the update process is as follows:where W s2 l ∈ R z ×d and W p2 l ∈ R z ×d are parameter matrices. z denotes the number of attention heads, and σ is the sigmoid activation. N i is the set ˆ of immediate neighbors of node i. α i,j (lz) ˆ , β i,j (lz) are the normalized attention coefficients for the z-th head at the l-th layer. To fuse syntactic dependency and part-of-speech relation features, we introduce a gating mechanism (Cho et al., 2014) to merge the two views as follows:where ◦ is element-wise product operation. [h syn : h pos ] is the concatenation of h syn and h pos , and W g and b g are model parameters. This way, g is learned to optimize the feature fusion. 3.4 Dual-Channel Span Generation In this section, we propose a dual-channel span generation module, which consists of two parts: dual-channel span generation and span classification. 3.4.1 Dual-Channel Span Generation Syntactic Span Generation Given a sentence X whose syntactic dependency graph is G Syn = (V, R Syn ), if there is a dependency edge e ij between words w i and w j , then all words positioned between them are considered to be a span s i,j syn . In particular, self-dependent edges represents spans of length L s = 1. We define the representation of s i,j syn as followswhere f width (i, j) denotes trainable embedding of span length (i.e., j − i + 1 ). e i,j = 1 suggests that there is an edge between w i and w j . Part-of-speech Span Generation For a given sentence X = { w 1 , w 2 , . . . , w n } , if the part-of-speech tag of word w o is NN or JJ, the words in a predefined window will be exhaustively enumerated and then the enumeration is further combined with central word w o to form spans. The part-of-speech induced span s k,l pos can be represented as: s k,l pos =[h k : h l : f width (k, l)], (7) if pos o = NN or JJ, and o ∈ [k, l] where f width (k, l) refers to the trainable embedding of span length. Finally, we merge the two types of span candidates: S = s i,j syn ∪ s k,l pos . Compared to exhaustive enumeration on the whole sentence in previous span-based approaches, whose time complexity of enumerated spans is O(n 2 ), for a sentence of length n. However, in our syntactic span generation, the parsing tree containing 2n edge dependencies (Qi et al., 2020) (including self-dependent edges), so the number of generated spans is O(2n). On the other hand, the statistics shows that in the benchmark datasets, there are about 2.5 part-of-speech NN and JJ in each sentence on average. Therefore, in the part-of-speech span generation procedure, the number of span candidates is O(2.5S (S window − 1)) ≤ n, where S window is thewindow window size to restrict span length and generally set to be a small value (e.g., S window = 3 in our experiments). That is, the time complexity of our method to generate the span is O(n), which significantly reduce the span candidate size. After obtaining the span candidates S, we further narrow down the pool of possible spans by leveraging two auxiliary tasks, namely, ATE and OTE tasks. Specifically, all span candidates in S will be classified into one of the three categories: { Aspect, Opinion, Invalid } by a span classifier. Next, nz spans are singled out with higher prediction scores Φ aspect or Φ opinion , where z is the threshold hyper-parameter and Φ aspect and Φ opinion are obtained by where FFNN denotes a feed-forward neural network with non-linear activation. 3.5 Triplet Module Based on the shrinked candidate pool of aspect and opinion terms, the aspect candidate s a,b a ∈ Sa  and opinion candidate s c,d o ∈ S o are paired and represented as g s a,b a ,s c,d o = [ s a,b a : s c,d o : r ab,cd s : f distance (a, b, c, d) ] . (10) where f distance (a, b, c, d) denotes trainable embeddings of span length. r ab,cd s is a trainable embedding vector which is the average pooling of the dependency vectors between words ab and cd. Additionally, since opinions are more likely to modify the aspects that match them, we consider the dependency relationship r ab,cd s ∈ R Syn between them. Then, sentiment classification is performed for the obtained span pairs, where the sentiment types are defined as r ∈ R = { Positive, Negative, Neutral, Invalid } Formally, the triplet prediction is written as The loss function for training is defined as the sum of the negative log-likelihoods from the span-pair classification in the span-classification and triplets modules.
Here we present the key high-level ideas of our approach. We first present a graph-centric view of the standard image-text matching objective in CLIP, which serves as a motivation to develop our approach (Sec. 3.2). We create scene graphs derived from the text, decompose them into multiple sub-graphs (Sec. 3.3) and apply augmentations on these sub-graphs to create negative sub-graphs (Sec. 3.4) which are used as hard negatives in a batch. Sec. 3.5 formally defines the Image-to-Multi-Text and Text-to-Image losses used for a batch of V-L inputs which is key for learning from multiple positive and negative texts derived from sub-graphs. Matching images with coarse-to-fine sub-graphs results in improved fine-grained and hierarchical understanding of text. Sec. 3.6 provides a twostage curriculum learning strategy for improved fine-tuning performance. Our approach builds on the idea that the standard image-text contrastive learning in CLIP can be viewed as a matching between an image scene graph and its sub-graph. Formally, given an imagetext pair (I, T), the image can be viewed by its scene graph, G I = (V I , E I ). The text scene graph is given by G T = (V T , E T ). Then G T ⊂ G I . According to this assumption, during contrastive learning in CLIP, we implicitly bring the representation of the image scene graph close to one of its sub-graph (the text scene graph). Now, let S G = { g | g ⊂ G } represent the set of sub-graphs of a graph G . According to the assumption above, g ∈ S G T ⇒ g ∈ S G I . Hence ∀g ∈ S G T , (g, G I ) becomes a correct matching pair during contrastive learning. We match multiple sub-graphs of the text scene graph to the same image, while also including hard negative sub-graphs in the batch. Matching between graphs is an implicit concept, and all graphs are first converted to text via templates, converted to embeddings using transformerbased (text) encoders, and matched to image embeddings. Scene graphs are succinct representations of images. However, an image scene graph generator used for generating a scene graph for any given input image is expensive to train since it requires supervised scene graph annotations for training (Li et al., 2017; Xu et al., 2017; Zhang et al., 2019), and also leads to issues like low coverage or biased generations against the long tail nature of objects and relationship annotations. We instead use the text scene graph created using an off-the-shelf text scene graph parser 1 (Wu et al., 2019). This serves as a proxy for the scene graph of (part of) the image and is assumed to be a sub-graph of the image scene graph, as also depicted by Figure 2. Let the text scene graph obtained be G T = (V T , E T ), where V T represent the nodes of the graph, which are either objects or their attributes. E T are the edges of the graph that represent relations between objects. See Fig. 2 for an example of a text scene graph. As shown in the figure, we decompose this scene graph into multiple positive sub-graphs P g = { g 1 , g 2 , g 3 , · · · , g k } , k ≤ M, where M is the max number of decomposed subgraphs and is a hyperparameter. Each sub-graph is a representation of a part of the image. We then convert sub-graphs to sentences so that they can be easily processed by transformer-based (text) encoders commonly used to train CLIP. For this, we use a simple template-based approach. For e.g., we create templates of the form " { N 1 } { R } { N 2 } " if we need to convert a graph having two nodes (N 1 , N 2 ) and a relation R, into a sentence format. Corresponding to each sub-graph, we obtain one positive text for the image, creating a positive text set P t = { t 1 , t 2 , t 3 , · · · , t k } . Corresponding to sub-graphs in P g , we create negative sub-graphs N g = { n g 1 , n g 2 , n g 3 , · · · } . Subgraphs in N g are a minimally perturbed versions of the positive sub-graphs in P g . Similar to positive sub-graphs, we convert sub-graphs in N g to text using the same template-based approach, and obtain N t = { n t 1 , n t 2 , n t 3 , · · · } . Texts in N t serve as hard negative texts in a given batch, see Fig. 2. We focus on creating negative sub-graphs that improve the attribute binding and relation understanding capabilities of the model, for which we use the following strategies for negative graph creation: We first consider an external set of objects ( N ), attributes ( A ), and relations (R). 1) Node Swapping and Replacement: We swap nodes in sub-graphs, these can be swaps of nodes which are attributes or objects. We also replace nodes with external nodes from N , A based on their type. 2) Edge Replacement: We replace edges with randomly sampled edges from the external relations set, R. 3) Connecting Sub-graphs: Here we join two sub-graphs. For this, we use one sub-graph from P g , and another random graph created using nodes and edges sampled from external sets N , A , R. This creates an overall hard negative graph. Sub-graphs are joined by simply joining nodes from both graphs through a randomly sampled edge from R. These strategies result in minimally perturbed hard negative subgraphs for improving attribute and relation understanding. We define multiple graph transformations { f g : G −→ P(G) } – f rel , f attr , f obj using the above techniques and create hard negative subgraphs. See Appendix Sec. B for more details regarding negative sub-graph creation. Given an image-text batch during training B = { (x i , t i ) } i=1 n , consider separately the batch of images B I = { x i } i=1 n and a batch of texts B T = { t i } i=1 n . The sentences in the text batch are first converted to scene graphs to obtain a batch of scene graphs B G = { G i } i=1 n , followed by decomposition to sub-graphs to obtain the positive sub-graph batch B g pos = { g i } i=1 m , m > n. r negative subgraphs are sampled and added to the batch to obtain B g = { g i } i=1 m+r . We convert these sub-graphs to text to obtain the final text batch B t = { t i g } i=1 m+r . Consider an image encoder model f θ parameterized by θ, a text encoder f ϕ parameterized by ˜u ϕ . For any image x, text t, = f θ (x) is the un˜v normalized image feature, and = f ϕ (t) is the unnormalized text feature. As common practice, ˜u ˜u the features are normalized to obtain u = / ∥ ∥ ˜v ˜v and v = / ∥ ∥ . The Image-to-Multi-Text contrastive loss is given by: L MC ∑ | − P (i) = | ∑ log | i=1 k ∈P (i) ∑ j=1 B t exp(τu i T ) exp(τui T  | ) vj where P (i) = { k | k ∈ [1, | B pos t | ], g k ⊆ G i } . The Text-to-Image contrastive loss is only calcu-lated for the positive texts. It is given by: | B pos t | exp(τu p(j) T v j ) log L MC t2i = − ∑ | | vj  j=1 ∑ i=1 B I exp(τu i T ) where g p(j) ⊆ G j . B t = [B t pos ; B t neg ], in which B t pos , B t neg represent the texts in B t , obtained from positive and negative sub-graphs respectively. The overall loss is L MosaiCLIP = (L MC t2i + L MC i2t )/2. For fine-tuning experiments, we develop a twostage curriculum learning strategy motivated by recent work (Goyal et al., 2022; Wortsman et al., 2022; Kumar et al., 2022) that show how finetuning can distort pre-trained features and closely mimicking the contrastive pre-training objective while fine-tuning CLIP can help mitigate this problem (Goyal et al., 2022). However, our coarse-tofine contrastive learning objective naturally deviates from pre-training in two ways. a) Existence of hard negative texts in the batch, and b) Having multiple positive and negative texts for an image. This can lead to a gap in pre-training vs finetuning objective, and a lower than optimal performance after fine-tuning. To solve this, our twostage curriculum learning strategy first fine-tunes the model while sampling (at max) a single positive and negative sub-graph per image, followed by fine-tuning it with multiple positive and negative sub-graphs. The hardness of data in this curriculum learning setup is defined by the amount of difference the fine-tuning setup has as compared to the pre-training setup. According to this intuition, it is easier for the model to first learn to handle hard negatives in a batch and then learn to handle multiple positive and hard negative sentences at once. We see consistent improvements using this strategy compared to a direct one-step fine-tuning, which we term as MosaiCLIP NoCurric in our ablations. For better performance on non-compositonal tasks, we use the robust fine-tuning approach (Wortsman et al., 2022) of weight space ensembling of the vision encoder, before and after fine-tuning. This model is called MosaiCLIPWiSE-FT.