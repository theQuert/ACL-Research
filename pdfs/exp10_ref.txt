Here we present the key high-level ideas of our approach. We first present a graph-centric view of the standard image-text matching objective in CLIP, which serves as a motivation to develop our approach (Sec. 3.2). We create scene graphs derived from the text, decompose them into multiple sub-graphs (Sec. 3.3) and apply augmentations on these sub-graphs to create negative sub-graphs (Sec. 3.4) which are used as hard negatives in a batch. Sec. 3.5 formally defines the Image-to-Multi-Text and Text-to-Image losses used for a batch of V-L inputs which is key for learning from multiple positive and negative texts derived from sub-graphs. Matching images with coarse-to-fine sub-graphs results in improved fine-grained and hierarchical understanding of text. Sec. 3.6 provides a twostage curriculum learning strategy for improved fine-tuning performance. Our approach builds on the idea that the standard image-text contrastive learning in CLIP can be viewed as a matching between an image scene graph and its sub-graph. Formally, given an imagetext pair (I, T), the image can be viewed by its scene graph, G I = (V I , E I ). The text scene graph is given by G T = (V T , E T ). Then G T ⊂ G I . According to this assumption, during contrastive learning in CLIP, we implicitly bring the representation of the image scene graph close to one of its sub-graph (the text scene graph). Now, let S G = { g | g ⊂ G } represent the set of sub-graphs of a graph G . According to the assumption above, g ∈ S G T ⇒ g ∈ S G I . Hence ∀g ∈ S G T , (g, G I ) becomes a correct matching pair during contrastive learning. We match multiple sub-graphs of the text scene graph to the same image, while also including hard negative sub-graphs in the batch. Matching between graphs is an implicit concept, and all graphs are first converted to text via templates, converted to embeddings using transformerbased (text) encoders, and matched to image embeddings. Scene graphs are succinct representations of images. However, an image scene graph generator used for generating a scene graph for any given input image is expensive to train since it requires supervised scene graph annotations for training (Li et al., 2017; Xu et al., 2017; Zhang et al., 2019), and also leads to issues like low coverage or biased generations against the long tail nature of objects and relationship annotations. We instead use the text scene graph created using an off-the-shelf text scene graph parser 1 (Wu et al., 2019). This serves as a proxy for the scene graph of (part of) the image and is assumed to be a sub-graph of the image scene graph, as also depicted by Figure 2. Let the text scene graph obtained be G T = (V T , E T ), where V T represent the nodes of the graph, which are either objects or their attributes. E T are the edges of the graph that represent relations between objects. See Fig. 2 for an example of a text scene graph. As shown in the figure, we decompose this scene graph into multiple positive sub-graphs P g = { g 1 , g 2 , g 3 , · · · , g k } , k ≤ M, where M is the max number of decomposed subgraphs and is a hyperparameter. Each sub-graph is a representation of a part of the image. We then convert sub-graphs to sentences so that they can be easily processed by transformer-based (text) encoders commonly used to train CLIP. For this, we use a simple template-based approach. For e.g., we create templates of the form " { N 1 } { R } { N 2 } " if we need to convert a graph having two nodes (N 1 , N 2 ) and a relation R, into a sentence format. Corresponding to each sub-graph, we obtain one positive text for the image, creating a positive text set P t = { t 1 , t 2 , t 3 , · · · , t k } . Corresponding to sub-graphs in P g , we create negative sub-graphs N g = { n g 1 , n g 2 , n g 3 , · · · } . Subgraphs in N g are a minimally perturbed versions of the positive sub-graphs in P g . Similar to positive sub-graphs, we convert sub-graphs in N g to text using the same template-based approach, and obtain N t = { n t 1 , n t 2 , n t 3 , · · · } . Texts in N t serve as hard negative texts in a given batch, see Fig. 2. We focus on creating negative sub-graphs that improve the attribute binding and relation understanding capabilities of the model, for which we use the following strategies for negative graph creation: We first consider an external set of objects ( N ), attributes ( A ), and relations (R). 1) Node Swapping and Replacement: We swap nodes in sub-graphs, these can be swaps of nodes which are attributes or objects. We also replace nodes with external nodes from N , A based on their type. 2) Edge Replacement: We replace edges with randomly sampled edges from the external relations set, R. 3) Connecting Sub-graphs: Here we join two sub-graphs. For this, we use one sub-graph from P g , and another random graph created using nodes and edges sampled from external sets N , A , R. This creates an overall hard negative graph. Sub-graphs are joined by simply joining nodes from both graphs through a randomly sampled edge from R. These strategies result in minimally perturbed hard negative subgraphs for improving attribute and relation understanding. We define multiple graph transformations { f g : G −→ P(G) } – f rel , f attr , f obj using the above techniques and create hard negative subgraphs. See Appendix Sec. B for more details regarding negative sub-graph creation. Given an image-text batch during training B = { (x i , t i ) } i=1 n , consider separately the batch of images B I = { x i } i=1 n and a batch of texts B T = { t i } i=1 n . The sentences in the text batch are first converted to scene graphs to obtain a batch of scene graphs B G = { G i } i=1 n , followed by decomposition to sub-graphs to obtain the positive sub-graph batch B g pos = { g i } i=1 m , m > n. r negative subgraphs are sampled and added to the batch to obtain B g = { g i } i=1 m+r . We convert these sub-graphs to text to obtain the final text batch B t = { t i g } i=1 m+r . Consider an image encoder model f θ parameterized by θ, a text encoder f ϕ parameterized by ˜u ϕ . For any image x, text t, = f θ (x) is the un˜v normalized image feature, and = f ϕ (t) is the unnormalized text feature. As common practice, ˜u ˜u the features are normalized to obtain u = / ∥ ∥ ˜v ˜v and v = / ∥ ∥ . The Image-to-Multi-Text contrastive loss is given by: L MC ∑ | − P (i) = | ∑ log | i=1 k ∈P (i) ∑ j=1 B t exp(τu i T ) exp(τui T  | ) vj where P (i) = { k | k ∈ [1, | B pos t | ], g k ⊆ G i } . The Text-to-Image contrastive loss is only calcu-lated for the positive texts. It is given by: | B pos t | exp(τu p(j) T v j ) log L MC t2i = − ∑ | | vj  j=1 ∑ i=1 B I exp(τu i T ) where g p(j) ⊆ G j . B t = [B t pos ; B t neg ], in which B t pos , B t neg represent the texts in B t , obtained from positive and negative sub-graphs respectively. The overall loss is L MosaiCLIP = (L MC t2i + L MC i2t )/2. For fine-tuning experiments, we develop a twostage curriculum learning strategy motivated by recent work (Goyal et al., 2022; Wortsman et al., 2022; Kumar et al., 2022) that show how finetuning can distort pre-trained features and closely mimicking the contrastive pre-training objective while fine-tuning CLIP can help mitigate this problem (Goyal et al., 2022). However, our coarse-tofine contrastive learning objective naturally deviates from pre-training in two ways. a) Existence of hard negative texts in the batch, and b) Having multiple positive and negative texts for an image. This can lead to a gap in pre-training vs finetuning objective, and a lower than optimal performance after fine-tuning. To solve this, our twostage curriculum learning strategy first fine-tunes the model while sampling (at max) a single positive and negative sub-graph per image, followed by fine-tuning it with multiple positive and negative sub-graphs. The hardness of data in this curriculum learning setup is defined by the amount of difference the fine-tuning setup has as compared to the pre-training setup. According to this intuition, it is easier for the model to first learn to handle hard negatives in a batch and then learn to handle multiple positive and hard negative sentences at once. We see consistent improvements using this strategy compared to a direct one-step fine-tuning, which we term as MosaiCLIP NoCurric in our ablations. For better performance on non-compositonal tasks, we use the robust fine-tuning approach (Wortsman et al., 2022) of weight space ensembling of the vision encoder, before and after fine-tuning. This model is called MosaiCLIPWiSE-FT.