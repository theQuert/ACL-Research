In this study, we highlight the importance of enhancing the quality of pretraining data in multilingual language models. Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages. Consequently, we introduce a new multilingual pretraining corpus for 16 African languages, designed by carefully auditing existing pretraining corpora to understand and rectify prevalent quality issues. Our model demonstrates better downstream effectiveness over existing pretrained models across four NLP tasks, underscoring the critical role data quality plays in pretraining language models in low-resource scenarios. Specifically, on cross-lingual QA evaluation, our new model is more than twice as effective as multilingual T5. As language models have scaled up in size and multilingual capability in recent years, commensurate effort has followed to curate pretraining data (Raffel et al., 2020) to support this growth and improve the alignment of language models.Earlier multilingual models such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2019) were trained on monolingual data from Wikipedia and/or other large-scale web crawls which included only a few African languages.Against this backdrop, indigenous efforts to build language resources for African languages have converged to two approaches: (1) Small highquality data (e.g., 1GB) pretraining where most data are from the clean or verified sources like news domain (Ogueji et al., 2021).This tradeoff between quantity and quality is forced by the unavailability of large, quality pretraining data for African languages.More notably, we demonstrate how large-scale web crawls and document-level datasets, such as mC4, can be enhanced through meticulous auditing of their document sources i.e., base URLs (e.g., www.voahausa.com).To evaluate the quality of our new corpus, we pretrain a new T5-based LM on the collected dataset and benchmark its performance on multiple downstream tasks.