'### RESEARCH METHOD:\n\nWe will employ a two-phase mixed-method approach for this research that combines quantitative method with a computational modelling.\n\nIn the first phase of the research, we will use a computational approach for implementing the ternary and binary transformer models. We plan to mix statistics-based quantization for the weights and elastic quantization for the activations in the BERT, ternary BART base and binary models. We will also apply very low-bit quantization, down to ternary and binary weights and activations, in generative pretrained transformers.\n\nThe second phase will involve evaluation and quantitative analysis of the implemented models. We will apply these models to downstream tasks of summarization and machine translation. It will allow us to quantitatively assess efficiency and the accuracy of the models. We will run the tests for models on the CNN/DailyMail benchmark for summarization, and WMT16 En-Ro benchmark for machine translation, to evaluate respective R1 and BLEU scores.\n\nFinally, we will compare results of our models with other relevant works in the field, such as 8-bit quantization models, to ascertain which method or model achieves better efficiency and accuracy. This will provide a robust measure of comparison for our binary and ternary models. Throughout this research, we will ensure replication of the procedures and methods employed in the referenced works to ascertain the reliability and validity of our results. \n\nThe completion of these phases should allow us to identify and discuss the impact and implications of using ternary and binary neural networks in transformer text generation models.'