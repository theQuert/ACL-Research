'### METHOD:\n\nTo address the optimization difficulty of ternary and binary neural networks, we propose a hybrid approach. The key idea is to use full-precision during early cycles of the training phase. The rationale is to take advantage of the high fidelity of full-precision networks for laying out a basic parameter framework, serving as a solid foundation for further optimization. Once the model is in a tractable state, we gradually reduce the precision to ternary and binary. This gradual transformation would ensure a smoother transition than a sudden switch, thus helping to reduce the risk of getting trapped into poor local minimal.\n\nFor the issues related to the transformer text generation model, we could employ a mixture of precision levels in different operations within the transformer architectures. For instance, keep high precision in the attention module since it is sensitive to quantization meanwhile quantize other operations with lower precision. This mixed-precision technique would contribute to reducing the impact of noise-compounding effects on autoregressive decoding and potentially improve the accuracy of the models.\n\nIn terms of the insufficiency of the reduced precision representation in the output space, we propose deploying full-precision softmax output layer and cross-entropy loss layer during training while keeping the rest dense layers in ternary or binary. Following this, during inference, efficiently approximate the full-precision output layer with a quantized version.\n\nTo optimize performance on specialized hardware, early rounding scheme can be introduced. This approach rounds off the numbers as early as possible during the computation instead of conducting the process on full precision and then rounding. This saves significant computation capacity and leverages efficiency gains over the full-precision networks.\n\nAll these proposed methods would not only mitigate the optimization difficulty in ternary and binary neural networks but could further reduce the efficiency gap between the full model and the quantized versions. It also provides the potential of improving the accuracy of models as compared to previous works.'