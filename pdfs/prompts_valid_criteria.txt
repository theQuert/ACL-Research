given reference
gpt-4 <-
multiple criteria
- similarity
- reasonable
- valid solutions
- innovative

prompt = """
As a researcher of a computer science department major in natural language processing, I have human generated research METHOD based on given ABSTRACT and METHOD. Now, I have machine generated method, please calculate the similarity between human generated method and machine generated method and return if the machine gerneration is reasonable to latent solution in percentage.
\n
### Human Generated METHOD: \n

### Machine Generated METHOD: \n

"""

2023/12/21
high similarity, low reasonable (human check validation)
innovation:  low similarity, high reasonable contents (target for model)

2023/12/28
	
To prevent generative Large Language Models (LLMs) from producing similar contexts or mere summarizations of given input contexts, especially in tasks they are not adept at, several methods and research directions can be explored. These methods aim to enhance the diversity, creativity, and interpretability of the content generated by these models. Here are some key approaches:

1. **Contrastive Learning**: This involves training models to distinguish between correct and incorrect (or less relevant) responses in a given context. By doing so, the model learns to generate more diverse and contextually appropriate responses. Research in this area focuses on developing contrastive loss functions and training regimes that encourage diversity.

2. **Controlled Generation Techniques**: Techniques like prompt engineering or fine-tuning with specific instructions can guide LLMs to generate content that is less similar to the input. For instance, prompts can be designed to encourage creative or out-of-the-box thinking, steering the model away from standard summarization.

3. **Incorporating External Knowledge Sources**: Integrating external databases or knowledge graphs can help LLMs generate more accurate and less repetitive content. This approach is particularly useful for tasks where the model lacks expertise, as it can draw on external sources for information.

4. **Adversarial Training**: This involves training the model against an adversary that tries to make the model generate repetitive or similar content. The adversarial challenge forces the model to explore a wider range of responses and become more adept at generating unique content.

5. **Hierarchical or Modular Approaches**: Breaking down the generation task into smaller, more manageable sub-tasks can help in generating more focused and less generic content. Modular approaches allow different parts of the model to specialize in different aspects of the task, leading to more nuanced outputs.

6. **Interpretability and Explainability Research**: Focusing on making LLMs more interpretable can indirectly improve their content generation. By understanding how the model arrives at a particular output, developers can tweak the model to avoid repetitive or overly similar content.

7. **Evaluation Metrics for Diversity**: Developing and employing metrics that specifically measure the diversity and novelty of the generated content can help in iteratively improving the model. These metrics can be used during both training and post-hoc analysis.

8. **User Feedback Loops**: Incorporating user feedback into the training process can guide the model towards generating more relevant and diverse content. This can be particularly effective in real-world applications where user engagement is a key metric.

9. **Cross-Domain Training**: Training the model on a diverse set of domains and text styles can improve its ability to generate novel content. Exposure to a wide range of topics and writing styles can help the model learn more varied ways of expression.

10. **Ethical and Bias Considerations**: Research into the ethical implications and inherent biases of LLMs is crucial. Ensuring that the model does not replicate or amplify biases in its training data is important for generating fair and unbiased content.

In summary, preventing generative LLMs from producing similar or summary-like content involves a combination of training techniques, model architecture adjustments, incorporation of external knowledge, and continuous evaluation for diversity and novelty. Research in these areas is ongoing and rapidly evolving, offering promising avenues for improving the capabilities of generative models.
-- 
chain of thought on scientific paper


topic inputs/related research -> extract 3 best methods -> 3 ideas/methods -> eval if valid, and results

other work: methods optimization with chain-of-thought

2024/01/04
build interface with Gradio

2024/01/11
5 methods -> scoring -> refine
method eval -> results eval (given problems and methods)
