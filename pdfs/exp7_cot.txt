'We propose an iterative feedback-based learning process aimed at improving an extractive question answering (QA) system over time. This process involves user-system interaction, where users ask questions and receive model-provided answers. Upon receiving an answer, the user provides feedback which is then used to update the model.\n\nThe first step involves the initialization of a strong pre-trained model on a supervised extractive QA task. Post-initialization, users interact with the system to ask questions and receive answers. Each interaction is logged and consists of a dialogue, document, user question, model-predicted answer and user feedback.\n\nFeedback is obtained in the form of a three-level rating scale - correct, partially correct, and wrong. Each rating corresponds to a numerical reward, which is utilized as the learning signal for our model. The reward mapping for correct, partially correct, and wrong is +1, 0, and -1 respectively.\n\nWe steer away from comparisons of model-predicted answers to actual gold-standard answers due to the limited availability of such gold-standard references in a real-world scenario. Instead, we use learned reward models combined with counterfactual reasoning methods in a contextual bandit learning framework.\n\nIn order to update the QA model based on the collected feedback, we utilize a selection of training instances from logged interactions that have reward annotations. The model is then fine-tuned on these instances through min-loss training with the logged propensity weights.\n\nImportantly, we alternate between the period of model serving (interaction and feedback logging) and model updating (re-training the model with the most recent feedback). This allows us to simulate a practical scenario of user interaction unfolding over time while creating room for model improvement from fresh user feedback.\n\nThis approach offers several advantages. Not only does it offer the potential for rapid and significant system improvement with minimal feedback, but it also accommodates varied data scenarios. By leveraging user feedback, we can additionally explore domain adaptation potential.\n\nOn another note, the simplicity of the feedback mechanism employed (three-option rating scale) can be effectively integrated in real-world systems, thereby enabling practical system deployment with continual learning capabilities. \n\nWe believe that this approach provides a robust framework for improving extractive QA systems via continual learning from human user feedback in a broad range of scenarios. We also expect our findings to stimulate wider interest in the relatively understudied area of continual learning from user feedback in the context of NLP tasks.'