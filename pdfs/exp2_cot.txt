'1. Hallucination Detection: The central idea is to exploit the intrinsic characteristics of the machine translation model, without any external tools. Specifically, they use sequence log probability to identify hallucinations. Each translated sentence\'s sequence log-probability score is obtained from the trained translation model, reflecting the model\'s own certainty in generating the translation. Instances flagged by the sequence log probability often correlate with instances of hallucination.\n\n2. Severity Classification: Once potential hallucinations are detected, the system classifies them based on severity according to a predefined taxonomy: fully detached hallucinations (the whole content is not supported by the source) and strongly, but not fully, detached hallucinations (a significant proportion of output is not supported by the source). This is done by leveraging self-attention scores, which provide insight into which target words the model deems important when generating a particular output word.\n\n3. Hallucination Mitigation: For mitigation, the system devises a method that attempts to fix existing hallucinations at test time. This involves iterative beam decoding, a method that iteratively generates new translations while penalizing phrases that were part of the hallucinated content in previous iterations.\n\n*These solutions seem to address the problem presented in the abstract by using only the model\'s features and internal log probabilities to detect and potentially correct "hallucinations" or errors that arise and seem unrelated to the source text. Therefore, these methods can be deemed valid.*\n\n4. Sentence Similarity Tool: As a further improvement to internal model workings, the method integrates an external sentence similarity tool that compares the translated sentences to the original source sentences. Sentence pairs that show significant discrepancy in terms of content can be identified as probable hallucinations.\n\n5. Token Level Attention: The model is fine-tuned to pay greater attention to the alignment of tokens from the source sentence to target sentence. This method is expected to improve the quality of translation by ensuring more accurate alignment and translation of tokens.\n\n6. Enhanced Training: For hallucination mitigation, the model is further trained on a supplemented dataset that includes both hallucinated translations and their corresponding corrections. Providing direct examples of hallucinated outputs coupled with their proper corrections can help the model learn the patterns of hallucination occurrences and rectifications.'