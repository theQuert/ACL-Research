We begin by reviewing generalizations of the concepts of precision and recall in the field of generative modeling. We then discuss the shortcomings of current language generation models and how sampling adapters may address these shortcomings. A series of recent papers have related the precision of a learned distribution p θ to the average quality of generated samples, where high-quality samples are assumed to be those with high probability under the data-generating distribution p. 5 Additionally, they relate the recall of p θ to its coverage of p (Sajjadi et al., 2018; Lucic et al., 2018; Djolonga et al., 2020, inter alia), i.e., high overlap in the support of p θ and p. Following this line of reasoning, the notions of precision and recall can naturally be operationalized using measures of the difference between two distributions—specifically, ones that enable different penalizations of over- and undercoverage of our reference distribution. There are several measures that, when considered together, naturally operationalize precision, recall, or some combination of the two. 6 In this paper, we focus on cross-entropy, KL divergence, total variation distance (TVD), and Jensen–Shannon (JS) divergence. We introduce each in greater detail below. We note that for all these measures, a larger value indicates a greater discrepancy between two distributions, and that all but the cross-entropy will be zero when the two distributions are identical. Further, we note that not all the measures are symmetric, i.e., their values change depending on the order in which the distributions are given as arguments to the measure. Out of convention, in the case that the reference distribution is provided first, we call this the forward variant of the measure. We call the case where the reference distribution is the second argument the reverse variant of the measure. We define all measures in terms of generic distributions p 1 and p 2 , which we assume both have (not necessarily identical) supports that are a subset of V. Upon inspection, we can see that the reverse crossentropy, i.e., where p 1 is the distribution being evaluated and p 2 is a (fixed) reference distribution, rewards high precision. 7 Specifically, it rewards p1  for assigning probability mass where p 2 is large, implicitly penalizing p 1 for assigning high probability where p 2 is small. In fact, the reverse crossentropy is minimized in the case where p 1 places all probability on the most probable token under p 2 . A related measure is the reverse KL divergence which is equivalent to the cross-entropy up to the subtraction of the entropy term H(p 1 ). As with cross-entropy, the reverse KL divergence rewards high precision. This property is reflected by a common intuition provided about this measure when it is used as a learning objective: It is referred to as a mode-seeking objective, i.e., it aims to place mass on the modes of p 1 . 8 Importantly, the distributions that minimize the reverse variants of Eq. (9) and (10a) will not necessarily be equivalent because the latter takes into account p 1 ’s entropy. So which of these two metrics should we use? As we are interested in using metrics that operationalize the notion of precision, the entropy of the distribution under evaluation is irrelevant. Thus, we will use the reverse cross-entropy as our primary precision-emphasizing metric. Recall-emphasizing Measures. On the other hand, the forward variants of Eq. (9) and (10a), where p 2 is now the distribution under evaluation and p 1 is assumed to be fixed, reward recall. This is evident when taking a closer look at their definitions. If p 2 fails to place probability on all elements y assigned probability by p 1 , then both the cross-entropy and KL divergence will be ∞. 9 Analogously to the reverse KL’s description as mode-seeking, the forward KL is referred to as mean-seeking. Note that using the forward variants of cross-entropy and KL divergence as learning objectives is equivalent since H(p 1 ) is constant with respect to p 2 . Further, the forward KL and cross-entropy, as well as the reverse KL, are minimized when p 2 = p 1 . Balanced Measures. The definitions for TVD and JS divergence, which are both symmetric measures, suggest a balance between the characteristics of precision and recall: where m(y) = p 1 (y)+p 2 2 (y) for y ∈ V is a pointwise average. Practically, the JS divergence can informally be viewed as an interpolation between the forward and reverse KL divergences. Indeed, several divergences that generalize the forward and reverse KL recover the JS divergence given a particular choice of hyperparameter (Huszár, 2015; Meister et al., 2020; Pillutla et al., 2021). TVD can be similarly motivated: Sajjadi et al. (2018) recover TVD in their precision–recall operationalization for generative models when assigning equal importance to precision and recall. Further, a standard result demonstrates that the JS divergence is a lower bound on TVD (Lin, 1991). With these measures in hand, we can more effectively assess the shifts to precision and recall that sampling adapters induce in a model. It is not clear that the objective with which probabilistic language generators are typically trained imparts characteristics that align with the goals of building good language generators. 10 Any form of maximum-likelihood training is equivalent to minimizing H(p D , p θ )—often with an additional form of regularization. Thus, it encourages high recall: p θ (y t | y <t ) must be nonzero for all tokens y t in every string y in the training set D for the objective to be finite. This, in turn, results in p θ allocating some probability mass to all (sub)words y ∈ V for all contexts y <t . In language modeling, this is perhaps a desirable property: We often care about the relative probabilities of strings, and assigning strings 0 probability would be counter-productive towards this goal. Yet, this property can potentially prove problematic when such models are used out of the box as language generators. 11 For language generation systems, high precision is arguably a higher priority, i.e., the goal is for all of the generated sequences to be of high quality. An intuitive argument for this is that a single bad output can leave a lasting poor impression on the user. Yet, the inability to generate a single sequence may go unnoticed—especially if the difference between that sequence and one the model can produce is a single, exchangeable token. In this light, a possible explanation for the efficacy of sampling adapters is as follows: While model parameters are chosen to minimize a recall-prioritizing objective, sampling adapters re-align the distribution with a more appropriate precision-prioritizing probabilistic objective, i.e., sampling adapter hyperparameter combinations that work well perhaps do so because they minimize an objective that balances between precision and recall. If this is indeed the case, it should not be surprising that the transformation induced by sampling adapters leads to worse models according to standard, recall-emphasizing measures: Any generator that assigns zero probability to a valid string—as is the case when top-π or top-k sampling are applied—will have both infinite cross-entropy and perplexity with respect to the natural language distribution. They may, however, lead to better models according to more balanced (or even precision-emphasizing) measures, which is what we now empirically test. To test the hypothesis that the operations performed by sampling adapters are akin to a re-prioritization of precision over recall in the output of the model, we evaluate the effects of sampling adapters on measures that emphasize recall, precision or a balance of the two, as outlined in §4.1. We then observe how these measures vary as a function of the sampling adapters’ hyperparameters. Further, we also look at these measures’ Spearman correlations with M AUVE, a sequence-level quality metric. We consider five different adapters: temperature, η (eta), top-π, top-k and locally typical sampling, each over a wide range of hyperparameters. Note that for the latter three adapters, a smaller hyperparameter value corresponds to a larger shift between p θ and pθ ~ . For η -sampling, the reverse is true, and for temperature sampling, hyperparameter values farther from 1 imply a larger shift. For reproducibility, we leverage the Hugging Face framework (Wolf et al., 2020) and its implementation of sampling adapters for all but η -sampling, for which we rely on the original authors’ implementation. 12 Error bars for all plots indicate 95% confidence intervals for the observed values; note that bars are often small enough that they are not visible. We focus on the task of open-ended text generation. We use GPT-2 small and large (Radford et al., 2019), as well as, GPT-Neo (small) (Gao et al., 2020) as our generation models. The main results of this paper use the test set of a public version of the WebText dataset 13 as our reference text. Results using the WikiText test set (Merity et al., 2016) are qualitatively similar and can be found in App. A. Sequence-level Metrics. Following Pillutla et al. (2021), we use the first 35 tokens of samples from our reference text as a prompt to generate continuations y ∼ p θ (· | y <t ) until | y | = 512, or EOS is sampled. We generate 1000 samples for each combination of model, sampling adapter, and hyperparameter. We compute M AUVE scores (where higher implies the samples are closer to the reference text), aggregated over 5 seeds, for each of these sets of text samples. Token-level Measures. In this analysis, we compare (sub)word-level distributions pθ ~ (· | y <t ) and p(· | y <t ). The former is our generation model after the application of a sampling adapter and the latter is a reference distribution. We present results using both the empirical distribution induced by our test set and the distribution given by the GPTJ model (Wang and Komatsuzaki, 2021) 14 as our reference distribution. Here, y is a string from the test set. Results are mean-aggregated across both t = 1, . . . , | y | and all y . Note that when we compute either the cross-entropy or KL divergence and it is not guaranteed that the support of p 1 is a subset of the support of p 2 , we make use of the ε version of the metrics, as specified in §4.1, with ε = 1e-6. 5.2 Results Trends in Probabilistic Measures. We first present our analysis of how different adapterhyperparameter settings affect the relationship of the model to a reference distribution (either probabilities according to GPT-J or the empirical distribution). Note that if our hypothesis in §4.1 is correct, we would expect to see that certain sampling adapter–hyperparameter settings lead to lower values of measures that emphasize precision, such as reverse cross-entropy, while simultaneously increasing measures that emphasize recall, such as forward cross-entropy. We show the reverse and forward cross-entropy, as well as TVD, in Fig. 1.15  Both the forward and reverse cross-entropy results align closely with our hypothesis: A larger adapter shift generally leads to a higher forward cross-entropy and lower reverse cross-entropy.16  This observation holds when using either the empirical distribution or GPT-J as our reference. Interestingly, we see that the trends reverse when we consider the reverse KL divergence (as opposed to the reverse cross-entropy; see Fig. 3). This is perhaps expected given that the entropy of the model’s distribution monotonically decreases after the application of sampling adapters (see Fig. 7). Lastly, the trends in TVD differ largely depending on the distribution used as a reference. When GPT-J is used, we see that TVD monotonically increases as adapter strength increases. The reverse trend appears to hold when considering the empirical distribution: TVD generally decreases with adapter strength. The reason for this difference is not immediately obvious. Closer inspection reveals that when GPT-J is the reference, the trends in TVD mimic what we would expect from a metric that interpolates between forward and reverse crossentropies. Since TVD is motivated as a metric that balances between precision and recall, our results therefore make intuitive sense. On the other hand, the observed trends for the empirical distribution do not have a clear explanation. Critically, we find that the observed trends are stable across various design choices; see App. A for results with the WikiText dataset and with different choices of ε for the ε-smoothed versions of metrics.