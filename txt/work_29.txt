In many sequence-to-sequence tasks, certain rare concepts have a high probability to appear in the reference sequence (y) if they also appear in the source sequence (x). We call these concepts “high utilization concepts” (c ∈ C HU ) and formally define them in Equation 1. These concepts are comprised of one or more tokens c = [ν 0 , ν 1 , ...]. We hypothesize that a source of factuality errors in many sequence-to-sequence tasks is that learned model underestimate the conditional probability of high utilization concepts pˆ(y i = ν, | y <i , x, ν ∈ c, c ∈ x, c ∈ C HU ) < p(...), where pˆ denotes the model estimated probability and p is the true probability. Definition 2.1 (High utilization concepts) Given a universe of concepts C, the set of high utilization concepts C HU is defined as Equation 1 answers the question “How do we know which rare tokens have a propensity to appear in both source and target?” while at the same time it works for rare tokens. This key insight leads us to define two goals for this work: learn to identify high utilization concepts, and build a utilization-rate-aware training objective. The major challenge in identifying high utilization concepts in real datasets is that the concepts we are interested in are present in very few examples. This means that it is hard to directly estimate p(c ∈ y | c ∈ x) and p(c ∈ y) from Equation 1 due to the high variance. In particular, a frequency-based estimate of probability has an uncertainty proportional to 1/sqrt(N) where N is the number of samples for a given concept. However, these rare concepts can still be very impactful to the overall performance of the model. This is because, for a given reference, y, it is unlikely that a particular high utilization concept will be present (∀c ∈ C HU , p(c ∈ y) ≪ 1), but it is also unlikely that no high utilization concept will be present ( ∏ c ∈ C HU p(c ∈ ̸ y) ≪ 1). This is well documented in the medical domain, where medical concepts have a very long-tailed distribution (Prabhu et al., 2019; Mottaghi et al., 2020), yet may appear in almost every relevant sequence. As an illustration, imagine a list of medication instructions. Every instruction may have a different medication so no medication token appears more than once; however, each instruction is rendered useless if it doesn’t include the relevant medication (e.g. see “Medication Plan” instructions in Figure 1). To overcome this challenge, we propose computing what we call “utilization rate”, r ϕ , which we define in Equation 2. This function relies on the concept equivalence class map ϕ : C sel → E where C sel ⊆ C and E is a set of equivalence classes. ( ϕ , C sel , E) cannot be derived from the data or the model, but instead are provided from an external source of knowledge. If ϕ is an iden-tity (id) then r id (c n ) = pˆ(c n ∈ y | c n ∈ x), (x, y) ∈ D. 1. Develop a method for identifying high utilization concepts, C HU for a dataset D = { (x i , y i ) } i=1 N . 2. Develop a method for augmenting the training procedure of sequence-to-sequence models to correctly estimate the conditional probability of tokens forming high utilization concepts. Here, Equation 2 tries to make the intuition from Equation 1 applicable to a real dataset. We gener-ally cannot compute the lift because for rare words the dataset frequency derived probability estimates are poor. Note that Equation 2 combines both externally provided knowledge ( ϕ , C sel , E) and dataset derived values. This allows us to inject domain-specific information. Because concepts are mapped to equivalence classes, every concept in a particular equivalence class has the same utilization rate. If a concept c n ∈ C sel has marginal probability to appear in the reference sequence that is much lower than r ϕ (c n ) then it is a high utilization concept. 2.2 Utilization-rate-aware seq2seq training Our analysis in section 5 (see Figure 3) shows that conventionally trained seq2seq models underestimate the utilization rate (r ϕ ) for many rare concepts. While we cannot optimize the utilization rate directly, we can optimize the approximate marginal probability p(ν | x) of a token ν given a source sequence x, as seen in Equation 3. Given the source sequence x, the tokens for which we aim to optimize the marginal probability are { ν ∈ c, c ∈ x ∩ C HU } . We define the unweighted utilization loss. However, not all concepts in C HU are equally likely to appear in the reference given their appearance in the source. To better reflect we also propose a weighted utilization loss where the weight for each token is determined by its utilization rate. Note that Equation 6 directly injects externally provided knowledge through its dependence on ϕ . We use utilization loss as a regularization term and augment the objective function. We use α > 0 to balance the strength of the regularization: where l nll = − ∑ t=1 y | | log p(y | y <t , x) and l u or w is either l u from Equation 5 or lw t from Equation 6. 