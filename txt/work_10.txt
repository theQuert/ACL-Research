Kenton and Toutanova (2019) propose BERT, a pre-trained masked language model (MLM), which succeeds in capturing the syntactic and semantic meaning of contextualized texts by large-scale selfsupervised pretraining. Recent researches explore and strengthen BERT. XLNet (Yang et al., 2019) addresses the issue of pretrain-finetune discrepancy simultaneously considering bidirectional contexts by a permutation language modeling objective. RoBERTa (Liu et al., 2019) exhaustively explores the pretraining setup, such as data processing, training task, hyper-parameters, etc., to boost the model. ELECTRA (Clark et al., 2019) trains a discriminator to detect replaced tokens, which are substituted by an MLM generator, and improve the modelâ€™s efficiency. Due to space limitations, we can not elaborate on BERT variants. Sun et al. (2022); Naseem et al. (2021); Min et al. (2021) surveyed the pre-trained language models. As far as pretrained machine translation is concerned, a lot of powerful deep learning approaches have been introduced. For instance, XLM (Conneau and Lample, 2019) introduces the crosslingual language model pretraining and get significant improvements on unsupervised and supervised NMT. MASS (Song et al., 2019) adopts the encoder-decoder framework to reconstruct a sentence fragment. mBART (Liu et al., 2020) can be directly finetuned by pretraining a complete model. mRASP (Lin et al., 2020) and mRASP2 (Pan et al., 2021) improve NMT by using code-switching strategy and contrastive learning. CeMAT (Li et al., 2022) utilizes a bidirectional decoder to improve the representation capability. Knowledge distillation is an effective technique for model compression and was first proposed by Hinton et al. (2015), in which knowledge is transferred from a teacher model to a student model. Sanh et al. (2019) distill a BERT-base model (Kenton and Toutanova, 2019) into smaller models by defining loss on the pre-trained predictions, which results in a task-agnostic pretraining distillation. Turc et al. (2019) conduct exhaustive analyses about the initialization of students in a task-specific setting, they show that students initialized by pretraining are better than that initialized from a truncated teacher (Sun et al., 2019; Sanh et al., 2019). Jiao et al. (2020); Wang et al. (2020, 2021); Choi et al. (2022) make assumptions about the student and teacher architectures and investigate aligning layer representations as well as attention matrices. Zhou et al. (2022) utilizes confidence-based knowledge distillation to incorporate bidirectional global context into NMT models.