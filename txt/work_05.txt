kNN-MT (Khandelwal et al., 2021) retrieves the k-nearest-neighbor target tokens in each timestep, computes the kNN probability from the distances of retrieved tokens, and interpolates the probability with the model prediction probability. The method consists of two steps: (1) datastore creation, which creates key–value translation memory, and (2) generation, which calculates an output probability according to the nearest neighbors Datastore Construction A typical NMT model is composed of an encoder that encodes a source x sentence x = (x 1 , x 2 , . . . , x x | | ) ∈ V | X | and a decoder that generates target tokens y = y (y 1 , y 2 , . . . , y y | | ) ∈ V Y | | where | x | and | y | are the lengths of sentences x and y , respectively, and VX  and V Y are the vocabularies of the source language and target language, respectively. The t-th target token y t is generated according to its output probability P(y t | x, y <t ) over the target vocabulary, calculated from the source sentence x and generated target tokens y <t . kNN-MT stores pairs of Ddimensional vectors and tokens in a datastore, represented as key–value memory M ⊆ R D × V Y . The key (∈ R D ) is an intermediate representation of the ﬁnal decoder layer obtained by teacher forcing a parallel sentence pair (x, y ) to the NMT model, and the value is a ground-truth target token y t . The datastore is formally deﬁned as follows: M = { (f(x, y <t ), y t ) | (x, y ) ∈ D, 1 ≤ t ≤ | y |} , (1) x where D is parallel data and f : V | X | × V t−1 Y → R D is a function that returns the D-dimensional intermediate representation of the ﬁnal decoder layer from the source sentence and generated target tokens. In our model, as in (Khandelwal et al., 2021), the key is the intermediate representation before it is passed to the ﬁnal feed-forward network. Generation During decoding, kNN-MT generates output probabilities by computing the linear interpolation between the kNN and MT probabili- ties, p kNN and p MT , as follows: P(y t | x, y <t ) = λp kNN (y t | x, y <t ) + (1 − λ)p MT (y t | x, y <t ), (2) where λ is a hyperparameter for weighting the kNN probability. Let f(x, y <t ) be the query vector at timestep t. The top i-th key and value in the k-nearest-neighbor are k i ∈ R D and v i ∈ V Y , respectively. Then p kNN is deﬁned as follows: p kNN (y t | x, y <t ) k f(x, ) ki  y<t  ∥ ∥2 2  , (3) ∝ ∑  y t =v i exp τ i=1 ( ) where τ is the temperature for p kNN , and we set τ = 100. Note that this kNN search is seriously time-consuming 1 (Khandelwal et al., 2021).