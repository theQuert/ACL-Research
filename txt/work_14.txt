Compositional Generalization has attracted increasing attention with dedicated datasets (Lake and Baroni, 2018; Keysers et al., 2020a; Kim and Linzen, 2020; Li et al., 2021; Shaw et al., 2021; Dankers et al., 2022). One line of research considers dedicated model architectures (Chen et al., 2020b; Gordon et al., 2020; Kim, 2021), which perform well on small scaled data but can face difficulties scaling to large or practical data. For example, Chen et al. (2020b) propose a differentiable neural network to operate a symbolic stack machine. Another line of research enhances the compositionality of standard architectures (i.e., Transformer) by introducing new modules (Bergen et al., 2021; Yin et al., 2022; Zheng and Lapata, 2022). However, significant architecture changes can bring about extra training cost or decoding latency. For example, Edge Transformer (Bergen et al., 2021) uses vectorbased attention weights, and Dangle Transformer (Zheng and Lapata, 2022) re-encodes source representations at each decoding step, which increase model complexity to O(n 3 ). Proto-Transformer (Yin et al., 2022) uses an additional attention module to incorporate prototype vectors obtained by clustering algorithms (e.g., K-Means). Different from them, we improve Transformer from the perspective of regularization training without any architecture changes. Recently, Csord√°s et al. (2021) and Ontanon et al. (2022) empirically make slight changes of Transformer components, and find its capability of compositionality is underestimated. Meta-learning (Conklin et al., 2021) and data augmentation (Andreas, 2020; Guo et al., 2020a) are also introduced to improve the base models, but the experiment results are limited. Along the line of compositional generalization studies without modifying the model architectures, our method focuses on the internal consistency of Transformer, and achieves better performance. Regularization training has been shown effective in semi-supervised training (Sajjadi et al., 2016; Tarvainen and Valpola, 2017), robust training (Cheng et al., 2018; Liang et al., 2021), continual training (Kirkpatrick et al., 2016; Lopez-Paz and Ranzato, 2017), etc. To encourage compositional behavior, Guo et al. (2020a) softly combine source/target sequence embeddings during training, and Conklin et al. (2021) introduce gradient based meta learning to simulate distribution shift. In addition, contrastive learning serving as regularization has achieved success in various NLP tasks (Chi et al., 2021; Su et al., 2022; Zhang et al., 2022). Different form them, we explore the effectiveness of the regularization training on the two different tasks in compositional generalization.