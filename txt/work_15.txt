Definition 2.1 (Differential Privacy (DP) (Dwork et al., 2006)). A randomized algorithm M : D → S is (ϵ, δ)-differentially private if for any two neighboring datasets D, D ′ ∈ D that differ exactly in a single data sample, and for all sets S ⊆ S: P[M(D) ∈ S] ≤ e ϵ P[M(D ′ ) ∈ S] + δ. This definition provides a rigorous privacy guarantee by theoretically bounding the effect of a single data sample in the dataset. For a differentially private algorithm, the output distribution is statistically similar whether any individual data sample appears in the input dataset or not. The privacy parameter ϵ quantifies the maximum allowable impact of a single individual’s data on the outcome. δ specifies the maximum probability that the privacy guarantee may fail. An algorithm can typically be made (ϵ, δ)-DP by bounding the contribution of a single data sample and adding controlled noise from a predetermined distribution (e.g., Gaussian) (Dwork and Roth, 2014). Setting ϵ and δ in practice often requires careful consideration of the specific use case and the acceptable trade-off between privacy and utility. We discuss our choice of ϵ and δ in Section 4.1. An appealing property of DP crucial to this work is robustness to post-processing. This property ensures that if the algorithm M satisfies (ϵ, δ)-DP, then so does F ◦ M for any deterministic or randomized function F (which is independent of M). Namely, one can perform arbitrary post-processing without incurring additional privacy loss. Deep learning models can be trained with DP via a modification of the stochastic gradient descent (SGD) algorithm (Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016). The modified algorithm clips per-sample gradients to bound the contribution of individual examples. Noise from a Gaussian distribution is sampled and added to the sum of the clipped gradients in a batch to obfuscate the gradient update. The resulting algorithm, called Differentially Private Stochastic Gradient Descent (DP-SGD), can be shown to be DP for some (ϵ, δ) for each update of the model. Privacy parameters at the end of training can be computed via privacy composition algorithms (Abadi et al., 2016; Gopi et al., 2021a). In the next section, we will utilize DP-SGD to train a language model with privacy for synthetic text generation.