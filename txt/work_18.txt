Most language generation systems are based on probabilistic models, i.e., models of the probability distribution over natural language strings 2 V ∗ , where V ∗ is the Kleene closure of an alphabet V. In words, V ∗ is the set of all strings that can be generated from a vocabulary of (sub)words V. A common modeling choice is to break down string probabilities autoregressively and locally normalize p θ , i.e., instead of directly modeling the full sequence probability p θ ( y ), one models (sub)word probabilities p θ (y | y <t ) conditioned on def the prior context y <t = ⟨ y 1 , . . . , y t−1 ⟩ ∈ V ∗ . Note def that here, we have y ∈ V for V = V ∪ { EOS } where EOS is a special end of string token required for an autoregressive p θ to define a valid probability distribution over V ∗ . The sequence-level probability can then be computed via the chain rule of probability: See Du et al. (2023) for a characterization of when these models are tight, i.e., when the probability mass assigned to finite-length strings is 1. The parameters θ of these models are typically chosen by (numerically) maximizing the log-likelihood of the training data D, where log-likelihood is defined as: Note this is equivalent to minimizing the (forward) cross-entropy between the empirical distribution p D induced by the training data D. In order to produce text from a model, one must use a decoding strategy, which provides a set of decision rules according to which tokens are sequentially chosen from the distribution p θ to form a string. Decoding strategies can be broadly taxonomized as either maximization-based or samplingbased. Maximization-based strategies aim to find the candidate string that scores highest under some objective. Finding the string with the highest probability under the model is a common maximizationbased strategy. Sampling-based strategies instead sample tokens according to some distribution derived from the model. While maximization-based strategies may make intuitive sense, they often lead to dull or degenerate text in open-generation settings (Cohen and Beck, 2019; Eikema and Aziz, 2020; Nadeem et al., 2020). Sampling-based strategies likewise have shortcomings: They introduce randomness into the generated text, which may lead to a disruption in coherence or fluency when units are sampled from low-probability regions of the distribution (Holtzman et al., 2020; Hewitt et al., 2022). A class of methods has been developed to address the problems observed when sampling directly from the model, specifically by altering the distribution from which tokens are sampled. We term these methods sampling adapters, formally defining them in the next section. Formally, sampling adapters are simplex-tosimplex mappings, i.e., functions α : ∆ V | | −1 → ∆ V | | −1 that take a probability distribution over V as input and map it to another one over V. 3 We use the notation p~ to denote the output of this map, as applied to the distribution p: similarly denoting the individual adapted probabilities as p~(y | y <t ) = α ( p(· | y <t ) ) (y). We now give two examples of common sampling adapters. Example 3.1. We recover standard ancestral sampling when α ( p(· | y <t ) ) (y) = p(y | y <t ). Example 3.2. We recover temperature sampling 1 when α ( p(· | y <t ) ) (y) ∝ p(y | y <t ) T for temperature parameter T.4 One popular way of formulating sampling adapters in the literature has been via truncation functions, i.e., functions where vocabulary units that do not meet a certain criterion are re-assigned zero probability. We write these functions as: where C : ∆ V | | −1 → P (V) is a function that finds the set of (sub)words that meets said criterion; P (·) denotes the powerset operator. Truncation sampling methods aim to eliminate probability mass placed on tokens deemed likely to lead to undesirable text, reallocating their probability mass to the remaining options. We now specify several common truncation-based sampling adapters. Other methods can similarly be cast in the sampling adapter framework, such as Mirostat (Basu et al., 2021) and the re-calibration method proposed by Braverman et al. (2020). Moreover, the general equation for sampling adapters given in Eq. (3) suggests that one direction for future research is learning a sampling adapter α. While many previously proposed adapters are truncation-based, adapters that reallocate mass in a different manner may also prove effective. Indeed, equipping α with tunable parameters could prove useful as a lightweight finetuning method. An Unintuitive Effect. The motivation behind the use of sampling adapters with language generation models is to readjust their distribution, shifting mass away from tokens deemed likely to lead to undesirable text and onto tokens that will generate high-quality text. Yet why are such transformations even necessary? Standard measures of distribution quality, such as perplexity, would suggest that our models’ estimates of the ground-truth distribution over natural language strings are quite good (Brown et al., 2020; Wang and Komatsuzaki, 2021; Hoffmann et al., 2022). This, in turn, implies that the heuristic shifts performed by sampling adapters should lead to worse language generators. We argue that the disparity between the quality of language generation systems using sampling-adapted models and the quality of these same models according to standard measures can be reconciled using probabilistic analogs of precision and recall. 