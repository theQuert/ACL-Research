Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users’ values could enhance conversation engagement and user connections. In this paper, we propose a framework, M ORAL D IAL to train and evaluate moral dialogue systems. In our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. Based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. The constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. Furthermore, we propose a novel evaluation method under the framework. We evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values in discussions, where the multifaceted nature of morality is particularly considered. Automatic and manual experiments demonstrate that our framework is promising to train and evaluate moral dialogue systems. Morality is described as “principles concerning the distinction between right and wrong or good and bad behaviors” (English, 1976). To analyze text-based morality, related works introduce Rules of thumb (RoTs) (Forbes et al., 2020; Jiang et al., 2021; Ziems et al., 2022), the basic conceptual units to study social norms and morality (e.g. you shouldn’t slap or punch others’ face). Adopting RoTs to model morality is proved effective.