There are three key technical parts in TECHS: temporal graph encoder, logical decoder, and extrapolation prediction. Figure 2 shows its architecture. Generally, GCNs follow an iterative messagepassing strategy to continuously aggregate information from neighbor nodes. As conventional GCNs cannot model time information, we propose a temporal graph encoder. The generic time encoding (Xu et al., 2020) is introduced to embed times in TKGs as it is fully compatible with attention to capture temporal dynamics, which is defined 1t  as: e t = d [cos(w 1 t + b ), ·d ·t · , cos(w d t t + b d t )]. √ [w 1 , · · · , w d t ] and [b 1 , · ·1 · , b ] are trainable parameters for transformation weights and biases. d t is the dimension of time embedding. Based on it, a temporal GCN is proposed by fusing neighbor information with the heterogeneous attention: where W denotes a transformation matrix. N is the neighbor set. m s,r,t k is the message information of neighbors that contains subject, relation and time representations, which is given by: h and g are the entity and relation embeddings, respectively. ⊙ is the element-wise product of two embedding vectors. α k s,r,o,t is a heterogeneous attention value to determine the importance of a current temporal edge. It is obtained by the correlation For decoding the answer for query (˜s,˜r,?,˜t), we introduce an iterative forward message-passing mechanism in a continuously expanding reasoning graph, regulated by propositional and first-order reasoning. In the reasoning graph, we set three learnable parameters for each node n i l to guide the computation: node embedding n i l , hidden FOTH embedding on i l  and reasoning attention β n i l . The start node n 0 =˜s is initialized as its embedding h ˜s . A hidden FOTH representation o n 0 for n 0 is initialized as a query relation embedding g ˜r . The attention weight βn 0  for n 0 is initialized as 1. The node n i =(e i , t i ) are firstly represented by the linear transformation of GCN embeddings: n i =W n [h e i ∥ e t i ]. Constant forward computation is required in the reasoning sequence of the target, whether conducting multi-hop propositional reasoning or first-order logic reasoning. Thus, forward message-passing is proposed to pass information (i.e., representations and attention weights) from the prior nodes to their posterior neighbor nodes. The computation of each node is contextualized with prior information that contains both entity-dependent and entity-independent parts, reflecting the continuous accumulation of knowledge and credibility in the reasoning process. Specifically, to update node embeddings in step l+1, its own feature and the information from its priors are integrated: This updating form superficially seems similar to the general message-passing in GCNs. However, they are actually different as ours is in a one-way and hierarchical manner, which is tailored for the tree-like structure of the reasoning graph. The attention weight β n i l , ¯r,n j l+1 for each edge in a reasoning graph contains two parts: propositional and first-order attention. As propositional attention is entity-dependent, we compute it by the semantic association of entity-dependent embeddings between the message and the query: among entity-independent relations, we first obtain the hidden FOTH embedding of an edge by fusing the hidden FOTH embedding of the prior node and current relation representation via a gated recurrent unit (GRU) (Chung et al., 2014). Then, the first-order attention is given by: Furthermore, the overall reasoning attention can be obtained by incorporating propositional and first-order parts to realize the complementarity of these two reasoning methods. Since the prior node with high credibility leads to faithful subsequent nodes, the attention of the prior flows to the current edge. Then, the softmax normalization is utilized to scale edge attentions on this iteration to [0,1]: where λ is the weight for balancing the two reasoning types. Finally, the FOTH representation and attention of a new node n j l+1 are aggregated from edges for the next iteration: Insights of FOTH Rule Learning and Reasoning. In general, the learning and reasoning of first-order logical rules on KGs or TKGs are usually in two-step fashion (Galárraga et al., 2013, 2015; Qu and Tang, 2019; Zhang et al., 2019; Qu et al., 2021; Vardhan et al., 2020; Liu et al., 2022; Cheng et al., 2022; Lin et al., 2023). First, it searches over whole data to mine rules and their confidences. Second, for a query, the model instantiates all variables to find all groundings of learned rules and then aggregates all confidences of eligible rules. For example, for a target entity o, its score can be the sum of learned rules with valid groundings and rule confidences can be modeled by a GRU. However, this is apparently not differentiable and cannot be optimized by an end-to-end manner. Thus, our model conducts the transformation of merging multiple rules by merging possible relations at each step, using first-order attention as: relation embeddings of head h and i-th body b i of this rule. ¯f l is for the attention calculation. In this way, the differentiable process is achieved. This is an extension and progression of Neural-LP (Yang et al., 2017) and DURM (Sadeghian et al., 2019) on TKGs. Figure 3 intuitively illustrates such transformation. Finally, the real FOTH rules can be easily induced to constantly perform attention calculation over the reasoning graph, which is summarized as FARI in Algorithm 1. After attention weights for nodes in the last decoding step L have been obtained, we can aggregate node attentions with the same entity to get the entity score: S o = ∑ n i L =(o,t i ) β n i L . All entity scores can be normalized into [0,1] by yo ˆ = S o S p . Compared ∑ p with the true label y o , the model can be optimized by a binary cross-entropy loss: The number of nodes may explode in the logical decoder as it shows an exponential increase to reach | N (n i ) | L by iterations. For computational efficiency, posterior neighbors of each node are sampled with a maximum of M nodes in each iteration. For sampling M node in the reasoning graph, we follow a time-aware weighted sampling strategy, considering that recent events may have a greater impact on the forecast target. Specifically, for a posterior neighbor node with time t ′ , we compute its sampling weight by exp(t ¯t exp( ′ − ¯t− ˜t) ˜t) for the query ∑(˜s,˜r,?,˜t), where ¯t denotes the time of all possible posterior neighbor nodes for a prior node. After computing attention weights for each edge in the same iteration, we select top-N among them with larger attention weights and prune others. As we add an extra self relation in the reasoning graph, the FARI algorithm can obtain all possible rules (no longer than length L) by deleting existing atoms with the self relation in induced FOTH rules. 