This section presents experiments conducted to evaluate QAMD EN, as well as the the ablations and baselines we used. For the intrinsic evaluation we evaluated the models over multi-document QA tasks. For extrinsic evaluations we considered the multi-document abstractive summarization task. Model Implementation Details Following Xiao et al. (2022), we use the large-sized LongformerEncoder-Decoder (LED) (Beltagy et al., 2020) for our model initialization. The length limits of input and output are 4096 and 1024, respectively.3  Following the Huggingface implementation (Wolf et al., 2020), we set the sliding window size to 1024 for local attention in the encoder part. Similar to the P RIMERA model (Xiao et al., 2022), when concatenating the documents and the question, we add a special document separator token (<doc-sep>) between the documents to signal to the model to be aware of the document boundaries. We also assign the global attention mode to these tokens which enables the model to share information across documents (Caciularu et al., 2021). For further hyperparameter and pre-training execution details, see App. B. Multi-document QA is the task of generating the correct answer, given a set of related multiple documents. For several multi-document QA benchmarks, models are often tasked to implicitly solve multiple sub-tasks or follow intermediate steps, such as comprehending the question, filtering out distracting documents in the context, and stitching pieces of information across the relevant documents (Geva et al., 2021; Caciularu et al., 2022). Recall that QAMD EN was pre-trained over a automatically generated multi-document QA dataset. Hence, as a preliminary assessment, we first investigate QAMD EN’s performance over two multi-document QA benchmarks, HopotQAdistractor (Yang et al., 2018) and WikiHop (Welbl et al., 2018) (see more details of the datasets in App. C.1), and compare to other models that were pre-trained using underling un-masking objectives. Fine-Tuning Format. To follow our pre-training scheme, we append the question to the context and fine-tune the model to generate the correct answer. We use the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) and P RIMERA (Xiao et al., 2022) as the baselines, for assesing the contribution of our pre-trainig format. Confirmed by Beltagy et al. (2020), we found out that appending the question: and context: prefixes before the question and the context tokens, respectively, resulted in better performance. Baselines. We compare QAMD EN (447M parameters) against a set of strong long-context transformer baselines, including LED (447M parameters) (Beltagy et al., 2020), P RIMERA (447M parameters) (Xiao et al., 2022), 4 and LongT5-xl (3B parameters) 5 (Guo et al., 2022) (see §2). The results on multi-document QA are shown in Table 2. We adopted the F1 and Exact Match (EM) evaluation metrics corresponding to the original works. Our QAMD EN outperforms both P RIMERA, LED, and LongT5, confirming that our pre-training data and input format are beneficial for both capturing cross-document relationships (QAMD EN ≻LED) as well as exploiting both context and question (QAMD EN ≻P RIMERA). This task aims at generating a summary for a given set of topically-related documents. Inherently, end-to-end MDS needs to implicitly address several subtasks including salience detection, redundancy removal, and text generation. Since dealing with multiple documents, MDS requires dealing with heterogeneous information and dispersed, while exhibiting substantial textual redundancy. We train and test QAMD EN with two challenging MDS benchmarks, each one dealing with a different domain: Multi-News (Fabbri et al., 2019), which is concerned on summarizing related news articles, and Multi-XScience (Lu et al., 2020), for scientific articles summarization (see more details of the datasets in App. C.2). Under this setting, we are provided sets of documents (without any query), and therefore we simply encode the documents using QAMD EN without appending additional text. Baselines. As in the previous experiment, we compare QAMD EN against LED, P RIMERA, LongT5-xl. Following Xiao et al. (2022) we report the results of the state-of-the-art models from Pasunuru et al. (2021b) and Lu et al. (2020), for MultiNews and Multi-XScience, respectively. Results. Tables 3 and 4 present the evaluation results over the Multi-News and Multi-XScience datasets, respectively. Following previous MDS works, we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.2 for details). For a fair comparison, we include the results of P RIMERA as well as the results of the previous state-of-the-art methods (Pasunuru et al. (2021b) and Lu et al. (2020), for Multi-News and for Multi-XScience, respectively), and LED (Beltagy et al., 2020). As shown in the results tables, QAMD EN exhibits the best performance across most of the examined models and benchmarks, especially on the Multi-News dataset, clearly demonstrating its consistent advan- tage. This excludes the results for Multi-XScience where QAMD EN slightly underperforms the prior work and LongT5. An explanation which Xiao et al. (2022) points refers to the fact that the clusters in Multi-XScience have less overlapping information compared to the corpus we used, attributed to the use of abstracts as the input documents in Multi-XScience. In addition, LongT5 advantage over QAMD EN is attributed to significantly larger number of parameters of LongT5-xl. The task of Query-focused Multi-Document Summarization (QMDS) aims at generating a summary from a set of documents, that answers a specific given query. Unlike MDS, QMDS tries to solve more realistic query-based scenarios, since it suggests summarizing only predefined salient information of interest that best answers the query. Since we proposed pre-trainng under the multi-document question answering setup, we posit that QAMD EN might be effective for QMDS. We consider the datasets constructed by Pasunuru et al. (2021a), Q MDS C NN and Q MDS I R (see more details of the datasets in App. C.3) as well as their strong baseline, and include also the results of P RIMERA and LED. Baselines. Similar to the previous experiments, we compare QAMD EN against LED, P RIMERA, LongT5-xl. In addition, we consider also the baseline from Pasunuru et al. (2021a). Results. Tables 5 and 6 present the evaluation results over the Q MDS C NN and Q MDS I R datasets, respectively. Following MDS tasks and Pasunuru et al. (2021a), we report the ROUGE R-1, -2, and -L scores, which are the standard MDS evaluation metrics (see App. C.3 for details). As shown in the tables, QAMD EN exhibits the best performance across most of the examined models and benchmarks, clearly demonstrating its consistent advantage over the baselines.