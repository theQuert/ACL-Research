We initialize the model with supervised data, and improve it by learning from user feedback through an offline contextual bandit learning process. We use a standard BERT-style architecture (Devlin et al., 2019). The input to the model is a concatenation of the question ¯q and the context text ¯c. We separately classify over the context tokens for the answer span start and end to compute the span distribution P s (Seo et al., 2017), and compute the binary answerability distribution P u with a classification head on the CLS token (Liu et al., 2019). We initialize the model parameters with DeBERTaV3 weights (He et al., 2023), 4 and fine-tune us-ing supervised data to get our initial model. This is critical to get a tolerable experience to early users. We usually use a small number of examples ( ≤ 512 examples), except when studying domain transfer. The training loss sums over the three cross-entropy classification losses, with a coefficient λ to weigh the binary answerable classification term. We learn through iterative deployment rounds. In each round ρ , we first deploy our model to interact with users (Section 3) and then fine-tune it using the data aggregated during the interactions. Each user interaction generates a tuple (¯q, ¯c, uˆ, ˆ i, ˆ j, f, θ ρ ), where ¯q is a question, ¯c is a context text, uˆ is the answerability classification decision, ˆ i and ˆ j are the returned span boundaries if a span was returned, and θ ρ are the model parameters when the interaction took place. Policy We formulate a policy that casts answer prediction as generating a sequence of one or two actions, given a question ¯q and a context ¯c. This sequential decision process formulation, together with the multi-head model architecture, allow to control the losses of the different classification heads by assigning separate rewards to the answerability classification and the span extraction. The policy action space includes classifying if the question is answerable (ANS) or not (UNANS) and actions for predicting any possible answer span [i, j] in ¯c. The set of possible action sequences is constrained. At the start of an episode, the policy first predicts if the question is answerable or not. The probability of the action a ∈ { ANS, UNANS } is P u (a | ¯q, ¯c; θ). Span prediction action are not possible, so their probabilities are set to 0. If the UNANS action is selected, the episode terminates. Otherwise, the second action selects a span [i, j] from ¯c as an answer, and the episode terminates. The probability of each span selection action is P s (i, j | ¯q, ¯c; θ). Answerability prediction actions are not possible, so their probabilities are set to 0. Reward Values We do not have access to a reward function. Instead, we map the user feedback f to a reward value depending on the action (Table 2), and cannot compute rewards for actions not observed during the interaction. The policy formulation, which casts the prediction problem as a sequence of up to two actions, allows to assign different rewards to answerability classification and span extraction. For example, if we get WRONG feedback when an answer is given, we cannot tell if the answerability classification was correct or not. Our formulation allows us to set the reward value of the first action to zero in such cases, thereby zeroing the answerability classification loss. The reward values were determined through pilot studies. For example, we observed that models overpredict unanswerable, so we set a relatively large penalty of -1 for wrongly predicting unanswerable. Learning Objective We use a policy gradient REINFORCE (Williams, 1992) objective with a clipped inverse propensity score coefficient (IPS; Horvitz and Thompson, 1952; Gao et al., 2022) and an entropy term for the answerability binary classification. IPS de-biases the offline data (Bietti et al., 2021), and also prevents unbounded negative loss terms (Kojima et al., 2021). The entropy term regularizes the learning (Williams, 1992; Mnih et al., 2016). If we substitute the policy terms with the predicted model distributions, the gradient for an answerable example with two actions with respect to the model parameters θ is: where the α 1 and α 2 are IPS coefficients for the first (answerability classification) and second (span extraction) actions, r 1 and r 2 are the corresponding reward values, γ is a hyperparameter, and H(·) is the entropy function. For examples the model predict as unanswerable, the second term is omitted. Deployment and Learning Process Algorithm 1 outlines our process. Each round (Line 2) includes interaction (Lines 4–14) and learning (Lines 1518) phases. During interaction, given a question and context (Line 5), we classify if it is answerable in the given context (Line 7) and extract the answer span (Line 8). Depending on the classification, we either display the answer (Line 10) or return that the question is not answerable in the given context (Line 12), and solicit feedback (Line 13). We aggregate the interaction data over time (Line 14). During learning, we use rehearsal (Rebuffi et al., 2017) for each update, creating a batch of size B by mixing examples from the most recent interactions (Line 16) and previous rounds (Line 17) to update the model parameters (Line 18).