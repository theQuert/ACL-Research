Datasets. We conduct our experiments on three publicly available datasets to examine the capacity and the generality of our model over various real-world AE settings: MEPAVE (Close-World Benchmark) 3 (Zhu et al., 2020) is a multimodal e-Commerce product attribute extraction dataset, which contains 87k product description texts (in Chinese) and images, involving 26 types of attributes. We follow the same dataset settings as Zhu et al. (2020), except that we leave the visual information and use the description texts only. AE-110K (Open-World Benchmark) 4 (Xu et al., 2019) is a collection of 110k product triples (in English) from AliExpress with 2,761 unique attributes. It can well measure the open extraction ability and generation performance of different models. We split this dataset via the cleaning script of Shinzato et al. (2022), and remove invalid and “NULL” value attributes following Roy et al. (2022). Re-CNShipNet (Semi-Open Benchmark) is a revised version of the functional attribute extraction dataset CNShipNet 5 (Zhang et al., 2021), where numerical attributes account for the majority to bring new challenges. We manually fix the incorrect annotations in the old version and rebalance the ratio of closed- to open-setting labels (Li et al., 2021). Now it contains about 5k entity-attribute instances (mostly in Chinese), among which 40% obtain attributes from the literal texts and others are within 9 pre-defined attribute types. Baselines. We compare the proposed model with several strong and typical baselines including: 1) Sequence Tagging-based methods, a kind commonly adopted in IE which typically uses semantic tags such as BIO to identify the extracted items: RNN-LSTM (Hakkani-Tür et al., 2016), Attn-BiRNN (Liu and Lane, 2016), and BiLSTMCRF (Huang et al., 2015) are all specially designed RNN-based models for modeling the intent of classification and extraction tasks. ScalingUp (Xu et al., 2019) is a BERT-based model to extract attribute values with BiLSTM to perform interaction attention between attribute names and values. 2) PLM-based methods: BERT (Devlin et al., 2019) is a well-known pre-trained language model (PLM) and we follow the vanilla setting of classification and sequence tagging tasks, JointBERT (Chen et al., 2019) is a variant of BERT to solve slot filling and classification jointly. 3) Joint IE-based (JE) methods, which originate from the entity-relation extraction task and typically extract entities and classify relations in a cascading fashion: ETL-Span (Yu et al., 2020) and CasRel (Wei et al., 2020) are two classic JE models for relation extraction and we adapt them to the AE task here. SOAE (Zhang et al., 2021) achieved SOTA on CNShipNet by merging the results of a JE model and a classification model. JAVE (Zhu et al., 2020) is an attention-based attribute joint extraction model and M-JAVE further takes advantage of multimodal information, and they were the best models for MEPAVE. 4) Sequence Generative Model: We also implement the latest word sequence generation method (Roy et al., 2022) based on the large-scale pre-trained BART (Lewis et al., 2020) model. We conduct the baselines and adapt them to the target datasets accordingly. See Appendix A for implementation details. Metrics. Following previous works (Zheng et al., 2018; Xu et al., 2019; Zhu et al., 2020; Zhang et al., 2021), we use F1 score as the metric and adopt Exact Match criteria (Wei et al., 2020), in which only the full match to the ground truth is considered correct. We report the results of attribute name and value extraction respectively as Zhu et al. (2020).