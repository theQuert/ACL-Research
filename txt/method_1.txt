We develop a method for ﬁne-grained moral norm inference across cultures. This method allows us to probe EPLMs with topic-country pairs, such as “getting a divorce in [Country]”.3  We build this method from the baseline method proposed by Schramowski et al. (2022) for homogeneous moral inference, where we probe EPLM’s moral knowledge about a topic without incorporating the cultural factor (i.e., the country names). Similar to that work, we use SBERT through bert-large-nli-mean-tokens sentence transformer model and use topic and topic-country pairs as our prompts. 4 This model is built on top of the BERT model, which is pre-trained on B OOKS C ORPUS (Zhu et al., 2015) and Wikipedia.Since the M ORAL D IRECTION is constructed from the semantic space of the BERT-based EPLMs (Schramowski et al., 2022), we develop a novel approach to probe autoregressive state-of-the-art EPLMs, GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020). For each topic or topiccountry pair, we construct the input s as “In [Country] [Topic]”. We then append a pair of opposing moral judgments to s and represent them formally as (s + , s − ). For example, for s = “In [Country] getting a divorce”, and (always justiﬁable, never justiﬁable) as the moral judgment pair, s + and s would be “In [Country] getting a divorce is alwaysjustiﬁable” and “In [Country] getting a divorce is never justiﬁable” respectively. 5 To make our probing robust to the choice of moral judgments, we use a set of K = 5 prompt pairs (i.e.,{(always justiﬁable, never justiﬁable), (morally good, morally bad), (right, wrong), (ethically right, ethically wrong), (ethical, unethical)}), and refer to appended input pairs as (s i + , s − ) where i ∈ [K]. i Since GPT2 and GPT3 are composed of decoder blocks in the transformer architecture (Vaswani et al., 2017), we use the probabilities of the last token in s i + , and s − as a moral score for each. The i moral score of the pair (s i + , s − ) is the difference i between the log probabilities of its positive and negative statements.Here s iT + and s − are the last tokens in s i + and s − reiT i spectively, and their probabilities can be estimated by the softmax layer in autoregressive EPLMs. We take an average of the estimated moral scores for all K pair statements to compute the moral score of the input.To construct the baseline, we compute the homogeneous moral score of a topic without specifying the country in the prompts. Using prompt pairs allows us to operationalize moral polarity: a positive moral score indicates that on average the EPLM is more likely to generate positive moral judgment for input s, compared to negative moral judgment. We use GPT2 (117M parameters), GPT2MEDIUM (345M parameters), GPT2-LARGE (774M parameters), and GPT3 (denoted as GPT3PROBS, 175B parameters) 6 . GPT2 is trained on W EB T EXT, which is a dataset of webpages and contains very few non-English samples. Around 82% of the pre-training data for GPT3 comes from Common Crawl data and W EB T EXT 2 (Kaplan et al., 2020), an extended version of W EB T EXT (Radford et al., 2019). Around 7% of the training corpusof GPT3 is non-English text. Considering such data shift from books and articles in BERT to webpages in GPT2 and GPT3 in astronomical sizes, it is interesting to observe how cultural moral norms would be captured by EPLMs trained on webpages, which cover a more heterogeneous set of contents and authors. We also design multiple-choice question prompts to leverage the question-answering capabilities of GPT3 (denoted as GPT3-QA). Similar to the wording used in our ground-truth survey datasets, questions are followed by three options each describing a degree of moral acceptability. We repeat this question-answering process 5 times for each topic-country pair and take the average of the model responses. Table 2 in the Appendix shows our prompts for all models.