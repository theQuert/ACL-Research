(Glass, 2012) first proposed the challenging task of ASR-U as a key step toward unsupervised speech processing, and framed it as a decipherment problem. (Liu et al., 2018) takes on the challenge by developing the first ASR-U system with groundtruth phoneme boundaries and quantized speech features as inputs, by training a GAN to match the speech-generated and real text distributions. (Chen et al., 2019) later replaced the ground truth boundaries with unsupervised ones refined iteratively by an HMM, which also incorporates language model information into the system. (Yeh et al., 2019) explored the cross entropy loss for matching the generated and real text distribution, but it is prone to mode collapse and needs the help of additional regularization losses such as smoothness weight. More recently, (Baevski et al., 2021; Liu et al., 2022) proposed another GAN-based model using continuous features from the last hidden layer of the wav2vec 2.0 (Baevski et al., 2020) model and additional regularization losses to stabilize training. Their approach achieves ASR error rates comparable to the supervised system on multiple languages, making it the current state-of-the-art system. To better understand the learning behavior of ASR-U systems, (Lin et al., 2022) analyze the robustness of wav2vec-U against empirical distribution mismatch between the speech and text, and found that N-gram language model is predictive of the success of ASR-U. Inspired by the original framework in (Glass, 2012), (Klejch et al., 2022) proposed a decipher-based cross-lingual ASR system by mapping IPA symbols extracted from a small amount of speech data with unpaired phonetic transcripts in the target language. Our analysis on the sufficient condition of ASRU is based on previous work on the asymptotic behaviour of GAN objective functions (Goodfellow et al., 2014; Arjovsky et al., 2017). Our finitesample analysis takes inspiration from later work extending the asymptotic analysis to the finitesample regimes (Arora et al., 2017; Bai et al., 2019). Such frameworks, however, do not account for the alternate gradient optimization method of GANs and inevitably lead to various inconsistencies between the theory and empirical observations of GAN training (Franceschi et al., 2021). Building upon prior works (Mescheder et al., 2017, 2018; Domingo-Enrich et al., 2020; Mroueh and Nguyen, 2021; Balaji et al., 2021), (Franceschi et al., 2021) proposed a unified framework called GANTK based on NTK (Jacot et al., 2018) to describe the training dynamic of any GAN objectives and network architectures. Our analysis on the training dynamic of ASR-U adopts and extends the GANTK framework to handle discrete, sequential data such as natural languages.