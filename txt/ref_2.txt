In this section, we first introduce the previous practices in binarization and ternarization. Then, we introduce a unified statistic-based weight binarization / ternarization method that can alleviate the gradient mismatch issue and enhance the quantized weights entropy. Lastly, we analyze the difference between weight quantization and activation quantization and propose an elastic ternarization method for activations. We abbreviate our method as TBT, short for “Ternary / Binary Transformer”. Ternary neural networks, where real values are quantized to three levels, are first introduced in (Li et al., 2016). Thus, these values can be repre-sented in 2 bits, leading to a 16× reduction in size and computation. Moreover, the computations can be calculated multiplication-free, leading to even further computation gains on suitable hardware. The recent work integrates the ternarization algorithm in natural language models for quantizing the weights and activations in classification tasks (Zhang et al., 2020) and ternarizing the weight (8bit activations are used) in generative models (Li et al., 2022; Tao et al., 2022). The general formula (Li et al., 2016) for ternarization is as follows: Here X T denotes the ternary weights/activations, and X R represents their real-valued counterparts. n X R denotes the total number of elements in the tensor. ∆ is the ternary threshold, and α T is the scaling factor that minimizes l2-loss between XT  and X R . The neural network binarization denotes representing the weights and/or activation with bi-level values. It is first proposed in BNN (Courbariaux et al., 2016) and has evolved in the follow-up works (Rastegari et al., 2016; Liu et al., 2018). Rastegari et al. (2016) formulates binarization as: Here X B can represent binary weights or binary activations. α B denotes the scaling-factor that minimize the l2 loss between X R and α B ·Sign(X ). The acceleration and compression effectR of ternary/binary neural networks is significant. By representing the weights and activations with { −1, 0, 1 } , the network enjoys ∼16× memory saving compared to its 32-bit floating-point counterpart. When further binarize the weights and activations to only 1-bit (i.e., { −1, 1 } ), up to 32× model-size reduction and 58× speedup on CPUs have been achieved (Rastegari et al., 2016), where the matrix multiplication operations are replaced with light-weighted bitwise XNOR operations. Despite its appealing characteristics, naively binarizing or ternarizing the transformer model for natural language generation results in several accuracy drops or even a total failure in training. It has been observed that the attention layers of the transformer network are difficult to quantize to low bits. Also, the auto-regressive decoding tends to accumulate errors due to quantization. Given the nature of generative language networks that require highprecision output, quantizing both the activations and weights in these models to extreme bit values is non-trivial and has not been explored before. We propose a statistics-based method for weight binarization/ternarization. Particularly, this novel quantization method considers maximizing the entropy of the quantized weights and reducing the gradient mismatch in the backward pass. Previous works (Courbariaux et al., 2016; Bai et al., 2021b; Zhang et al., 2020) are mainly focused on minimizing the l2 loss between the quantized weights and the real-valued weights to find the optimal quantization scheme, where W Q denotes binary/ternary weights and α ∗ denotes the optimal scaling factor calculated. Despite the broad application and great success of the classic quantization scheme, we found that merely minimizing the l2 loss neglects several critical but intractable issues in ultra-low-bit weight quantization: (1) The information entropy of the quantized weights is not considered. Eq. 1 and Eq. 4 calculate the quantized weights to minimize the distance to the real-valued weights, which could lead to imbalanced quantized weight distribution and harm the quantized weights representation capacity. (2) The quantization function Eq. 1 and Eq. 4 are not isometric, meaning that it does not consider the magnitude consistency between the quantized weights and real-valued weights, while we find that magnitude consistency contributes significantly to accurate gradient estimation. Considering the above two limitations in previous solutions, we are motivated to design a novel quantization function that enhances information entropy and reduces gradient mismatch. To boost the weights representation capability, in information theory, more information is preserved when the quantized weights contain higher entropy: with p i denoting the proportion of real-valued weights being quantized to i th quantization level in total N levels. Eq. 7 can be easily solved with a Lagrange multiplier, and the optimal p i ∗ = 1 N , i ∈ { 1, 2, . . . , N } , suggesting the best quantization scheme to preserve maximum information entropy is to distribute the real-valued weights in all quantization levels as evenly as possible. For reducing the gradient mismatch, as suggested by the previous binarization work (Liu et al., 2020b), the magnitude difference between the quantized weight and the real-valued weight will greatly influence the gradient scale and a mismatch in magnitude will be amplified in back-propagation and cause gradient vanishing or explosion during training. Thus it is important to ensure the magnitude of real-valued weights and quantized weights are consistent. Combining two requirements discussed above, we proposed max-entropy isometric weight quantization. In ternarization, it is formulated as Where W T and W R refer to the ternary weights and real-valued weights, respectively. The rounding function ⌊ · ⌉ and Clip(·) function quantize weights to { −1, 0, 1 } . µ T is the mean of realvalued weights and n W R denotes the number of weights in the weight matrix. Scaling factor α is calculated from the weight statistics and follows the entropy rule to scale the real-valued weight W R to be evenly distributed in quantization levels. In the ternary case, the weights are quantized to { −α T , 0, α T } . When the real-valued weights are initialized as uniformly and symmetrically distributed (He et al., 2015; Glorot and Bengio, WR i  2010), the scaling factor α T will distribute α T to [−1.5, 1.5], such that the output ternary weights will have near uniform distribution in three ternary levels. Meanwhile, Eq. 8 is an isometric mapping where the real-valued weights are scaled by α 1 to T near [-1, 1] and time α T to scale back after quantization. In this way, the magnitude is preserved. Correspondingly, in the binary case we have, Here W B denotes the binary weights, where substracting the average µ B makes the realvalued weight zero-centered before binarization and thus encourages an even distribution in binarized weights. Then the scaling factor α B matches the magnitude between real-valued and binary weights. Particularly, in Eq. 9, W B i = W R i i α B · Sign( α ) = α B · Sign(W R − µ B ), we B explicitly include the α B in the denominator to keep the binarization function isometric and the gradients w.r.t. weights can be calculated straight- STE is abbreviated for straight-through estimator (Bengio et al., 2013), which replaces the nondifferentiable Sign function with Clip function in the backward pass. We show that the proposed maxentropy isometric weight quantization improves the accuracy of weight binarization / ternarization by 6.0 / 11.53 RougeL scores on the CNN/DailyMail benchmark, respectively. More details can be found in Sec. 3.2. In contrast to neural network weights that are stored on the disk, activations are calculated on-the-fly. The distribution of activations in a particular layer depends on the network weights as well as the corresponding input sequence, and thus varies from batch to batch. In order to have the quantization function better capture the underlying activation distribution, we propose learning-based activation quantization. Inspired by BiT (Liu et al., 2022), we divide the activation layers into two categories: the activation layers with non-negative values (X R ∈ R + ), i.e., Softmax/ReLU layer outputs and the rest of the layers with both positive and negative activations (X R ∈ R). We binarize / ternarize the first activation category (X R ∈R + ) to { 0, α } / { 0, α, 2α } , and symmetrically quantize the later activation category (X R ∈ R) to { −α, α } and { −α, 0, α } in binary and ternary cases respectively. In this way, the activation distribution matches the original fullprecision activations and thus reduces the quantization error. Further, we learn to scale the real-valued activations to better fit quantization thresholds, and this learnable scaling factor can be updated end-to-end with the gradients from the network loss to better account for overall network optimization. In the ternary case, we propose the elastic ternarization function formulated as, where X R and X T denote real-valued and ternary activations, respectively. To keep the formula concise, we set X R ′ = X R − X R , denoting the zeromean real-valued activations. α T is the scaling factor. Different from the weight quantization, the scaling factor in Eq. 11 is learned with the gradient update. We follow the practice in (Zhou et al., 2016; Esser et al., 2019) to calculate the gradients with straight-through estimation (STE) bypassing the non-differentiable rounding function: The learnable scaling factor can dynamically adapt to different activation distributions and improve the ternarization accuracy. In the binary case, it is formulated as. Here X B denotes the binary activations. Correspondingly, the gradients w.r.t. the scaling factor α can be easily calculated as We demonstrate that with the learning-based activation quantization method and statistics-based weight quantization scheme, the proposed TBT for the first time is able to quantize the BART model for natural language generation tasks to ternary and even binary weights and activations, and achieve reasonable accuracy on summarization and translation benchmarks. 