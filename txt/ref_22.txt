We outline our experimental setup by describing the reasoning tasks and datasets (§3.1), followed by the task and evaluation models (§3.2), and the baseline metrics for comparison (§3.3). Additional details on the setup are provided in Appendix B. 3.1 Datasets We explore two reasoning tasks, namely CommonsenseQA (CQA) and Natural Language Inference (NLI) across four datasets, all containing humanannotated free-text rationales. For CQA task, we use ECQA (Aggarwal et al., 2021), CoS-E (v1.11; Rajani et al., 2019) and QuaRTz (Tafjord et al., 2019). For both ECQA and CoS-E, each commonsense question is paired with five candidate choices and the task is to select an answer from the candidates. ECQA contains higher quality humanwritten rationales compared to CoS-E (Aggarwal et al., 2021; Sun et al., 2022). QuaRTz is for opendomain reasoning about textual qualitative relationships, and the task is to select an answer from two options to the question based on the textual qualitative knowledge (rationale). For the NLI task, we use the e-SNLI (Camburu et al., 2018) dataset containing explanations for SNLI (Bowman et al., 2015), where the task is given a premise to predict if a hypothesis entails, contradicts or is neutral to it. More details on the datasets are in Appendix B.1. 3.2 Task and Evaluation Models Task models We choose T5 Large (Raffel et al., 2020) as the task model (finetuned on groundtruth labels and rationales) to produce generated rationale-label pairs under three settings: • XY ∗ →R: Given an input text and the groundtruth label, generate a rationale. • X→YR: Given an input text, generate a label followed by a rationale. Since T5 decodes tokens sequentially, each R is generated conditioned on the predicted Y. • X→RY: Given an input text, generate a rationale followed by a label. Here, we compute a likelihood for each candidate Y conditioned on R, and then select the most probable candidate. This operation can improve the model prediction accuracy, while weakening the consistency and relevance between the generated rationales and predicted labels. After training, we collect three types of rationalelabel pairs by applying the three task models on the test set of each dataset. In addition to these three settings, we also evaluate ground-truth labels paired with crowd-sourced rationales (Y ∗ ;R ∗ ). Constructing a Baseline with Vacuous Rationales Given an input x and a label y (groundtruth or model-generated), we construct a baseline rationale b by declaratively combining x and y into a sentence. For the CQA task, we adopt a T5-3B model fine-tuned on a set of (question, answer, declarative sentence) tuples (Demszky et al., 2018) following Chen et al. (2021b). 5 For the NLI task, we first use a template to convert (premise, hypothesis, label) tuple into a baseline rationale: “premise implies / contradicts / is not related to hypothesis”. Then we paraphrase these templated, vacuous NLI rationales using a pre-trained model6  in order to prevent the evaluators from learning the template patterns. Table 1 shows some examples of constructed vacuous baseline rationales. Training Evaluation Models, g and g ′ We train ′ two evaluation models, g and g , which take [r, b] and b as inputs, respectively (see Equation 5 in §2). Both evaluators are based on fine-tuning T5 Large (Raffel et al., 2020) models. We use the training set ∗ ∗ ∗ ∗ D train = {(x, y , r )}, where {y } and {r } are gold labels and human-annotated rationales, respec∗ tively. We construct baseline rationales {b } based ∗ on {(x, y )}. The objective is to maximize the log∗ ∗ ∗ ∗ likelihood of y given [r , b ] or b . After training, the evaluation models are applied to evaluate a rationale-label pair (y, r) w.r.t. an input x. The rationale-label pair (y, r) can be model-generated and the label may not be ground-truth (e.g., y 2 in Fig. 1), while R EV is able to provide an assessment on the rationale along the two dimensions (§1). We refer readers to the Appendix B.3 for results of using T5 Base, BART Large (Lewis et al., 2020), and GPT-2 Large (Radford et al., 2019) as evaluation model architectures. We compare with two existing automatic metrics for free-text rationale evaluation: LAS (Hase et al., 2020) and RQ (Wiegreffe et al., 2021). Analogous to our evaluation models, both approaches use proxy models; we use the same architecture (T5 Large) across metrics in our reported results. Leakage-Adjusted Simulatability (LAS) Hase et al. (2020) evaluate the quality of free-text rationales via a proxy model, trained with the task model outputs as labels and original input texts combined with rationales as input sequences. The metric computes the difference between its prediction accuracy on the predicted label when the rationale is included into the input vs. when it is not, 1[ˆy ∣ x, rˆ] − 1[ˆy ∣ x], averaged over examples grouped based on whether they leak labels or not. The final LAS score is given by the macro average across groups. Rationale Quality (RQ) Wiegreffe et al. (2021) propose a variant of the simulatability in Hase et al. (2020). The main difference is that gold labels are used to train the model proxy and evaluate rationale quality.