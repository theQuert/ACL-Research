Long-context efficient text generation transformers (Tay et al., 2021, 2022) extend earlier transformer models (Vaswani et al., 2017) for processing long sequences, often using a sparse self-attention architecture. Examples include the Longformer Encoder-Decoder (LED) (Beltagy et al., 2020), and LongT5 (Guo et al., 2022). These models demonstrated that single-text approaches be can adapted to multi-document tasks by concatenat-ing multiple documents into a single sequence and processing them using their sparse attention patterns. They sparsify the full self-attention matrix of transformers by using a combination of a localized sliding window (called local attention), as well as a global attention pattern on a few specific input locations. LED is build upon the BART model (Lewis et al., 2020) by using additional positional embeddings and global attention weights, and introduces the global attention mode that operates over pre-selected tokens. LongT5 extends the T5 model (Raffel et al., 2020) by using a similar technique introduced in the ETC and B IG B IRD models (Ainslie et al., 2020; Zaheer et al., 2020), relieving the requirement to manually select global tokens by automatically globalizing the aggregated representations of groups of tokens. Further strategies have been proposed for increasing these models’ abilities in multi-document tasks. The Cross-Document Language Model (CDLM) (Caciularu et al., 2021) suggested pretraining a Longformer-encoder (Beltagy et al., 2020) over sets of related documents, and showed superior performance results over several multidocument tasks. Following this methodology, the authors of LinkBERT (Yasunaga et al., 2022) used a similar approach, but utilized Wikipedia’s hyperlinks in order to curate informative pairs of linked documents for LM pre-training. In order to adopt the multi-document pretraining approach for sequence-to-sequence tasks, P RIMERA (Xiao et al., 2022), which is built on top of the Longformer encoder-decoder model (LED), selected salient sentences within clusters of related documents using a pyramid estimation approach, resembling the method presented for pre-training the single-document P EGASUS model (Zhang et al., 2020). While this work is the closest to ours, it was pre-trained to generate masked salient sentences without any control, which makes the model potentially hallucinate while generating text, while our model uses a controlled QA-based objective. Furthermore, unlike these works, our method generates significantly more data then used to pre-train P RIMERA, which is possible to obtain by the singledocument QA generation approach. Our QA pretraining formulation allows us to generate multiple contexts per document cluster. Another related line of work includes methods that incorporate large-scale QA-generated data for pre-training LMs (He et al., 2020; Jia et al., 2022; Huber et al., 2022). These works hypothesize and show that pre-training by utilizing generated QA data can encourage contextual representations to encode useful semantic information for other nonQA downstream tasks. Inspired by that, we conjecture that LMs can strongly benefit from infusing QA during pre-training in the multi-document setup, for adding an additional signal for modelling cross-text relationships.