We propose to regularize the model training in two aspects, as illustrated in Figure 1: representation consistency of tokens across different contexts (§3.1), and consistency of model prediction for a single sample (§3.2). The representation consistency encourages the contextualized representations of the same token across contexts to be more consistent in the embedding space. To this end, we introduce the popular contrastive learning (Chen et al., 2020a; He et al., 2020), especially the supervised variant (Khosla et al., 2020). Specifically, we collect representations that belong to the same token as positive samples, and representations of different tokens in the mini-batch as negative samples. For example, in Figure 1, for the token “book” in the sequence Y 1 , the positive sample is h 2 in Y 2 , and the negatives include the representations of other tokens. Following (Gao et al., 2021), the dropout augmentation is also considered as positive samples. For construction of positive samples, we can use a data sampling strategy which groups minibatches according to token types. When building a mini-batch, we first randomly sample a token from the vocabulary, then retrieve several sentence pairs (e.g., 8) containing the token. We repeat this process until reaching the batch size, and the sentence pairs that have been chosen will not be retrieved again in that training epoch. In practice, since the current focus on compositional generalization is the composition of high-frequency atoms, a relatively large batch size is able to ensure reasonable co-occurrence of positive samples. Formally, given a mini-batch of input pairs { (X, Y ) } , we define the contrastive objective as where N is the number of the total tokens that are chosen for regularization, considering that some tokens can be excluded from the consistency regularization, e.g., the token used for padding. P(i) is the set of indices of all the positive samples for h i , τ is a temperature hyper-parameter 2 . Moreover, s(·) denotes the cosine similarity between representations to: where h i is the representations of the top layer in the encoder or the decoder, projected by a multilayer perceptron with ReLU activation. Due to the training mechanism of neural models, predictions of the same instance can vary across forward passes. The internal stochastic perturbations in the model components accumulate layerby-layer, negatively affecting the efficiency of invariance learning (Ghiasi et al., 2018). To enforce the sample-level consistency, we feed the instance (X, Y ) to the model M multiple times during training, and obtain the final output distributions derived from different dropout perturbations. We minimize the difference between the output distributions for each target token: where | Y | is the number of tokens in the target sequence Y , d(·) is a metric function measuring the difference, and M denotes the number of perturbations. Empirical results show that Jensen-Shannon divergence between two perturbations are effective enough while maintaining efficiency We also experimented with more than two perturbations and other metrics such as sample variance, and found that it possibly lead to better performance but also more training cost. Therefore, we set M as 2 in all the experiments. By explicitly encouraging the model to generate consistent output during training, the model is able to capture global compositional patterns with more confidence. The overall loss function is defined as: L = L ce + αL r + β L p , (4) where L ce denotes cross-entropy loss for baseline models, and α and beta are the coefficients of the two regularization losses, respectively. Notably, our proposed regularization terms guide the model training from the aspects of representation and prediction, without changing the inference process, which means no additional decoding latency.