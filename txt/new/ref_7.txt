Using t5x and seqio (Roberts et al., 2022), we pretrain a T5 (Shazeer, 2020; Raffel et al., 2020) model with a subword-tokenizer of vocabulary size 150, 000. We pretrain for 524, 288 steps on the span-corruption objective using the Adafactor optimizer. Each training batch consists of 512 examples, each with an input of 512 tokens and an output of 114 tokens. Our new model is known as AfriTeVa V2, a 428M parameter model. We evaluated our models on the test set of AfriQA Ogundepo et al. (2023), a cross-lingual question answering dataset with questions in 10 African languages and gold passages in English or French. We evaluated in zero-shot generative cross-lingual QA settings using in-lang queries and the provided gold passages in English. 3.2.2 Machine Translation We evaluated using MAFAND-MT (Adelani et al., 2022) − a machine translation benchmark in the news domain. MAFAND-MT contains few thousand parallel training sentences (2, 500-30, 000 sentences) for 16 African languages, ideal for evaluating the effective adaptation of pretrained LMs to new languages and domains. 3.2.3 Summarization For summarization, we use XL-Sum (Hasan et al., 2021), an abstractive summarization dataset which covers 44 languages, including 9 African languages. The authors establish strong baselines on both low and high-resource languages in the dataset through multilingual finetuning of mT5. 3.2.4 Text Classification We use the news topic classification dataset recently introduced by Adelani et al. (2023) for 16 African languages, MasakhaNews. The authors establish multiple baselines on the dataset using both classical machine learning models and finetuning or prompting language models. 3.3 Baseline Models We compare our new model, AfriTeVa V2, with the base variants of existing multilingual T5 models: mT5 (Xue et al., 2021), ByT5 (Xue et al., 2022) and FlanT5 (Chung et al., 2022), as well as Africentric models: AfriTeVa (Ogundepo et al., 2022), AfriMT5 & AfriByT5 (Adelani et al., 2022). mT5 was pretrained on the mC4 corpus which is the starter point for this work while ByT5 is the byte-level adaptation of the mT5 model. FlanT5 is T5 instruction-finetuned for improved performance. AfriTeVa, AfriMT5 and AfriByT5 models provide a closer comparison given the nature and focus of our research. While AfriTeVa is a T5 model pretrained on a small corpus (∼1GB), AfriMT5 & AfriByT5 are adapted from mT5 and ByT5 models using continual pretraining. Apart from AfriTeVa, AfriTeVa V2 has ∼26% less parameters than the other baseline models.