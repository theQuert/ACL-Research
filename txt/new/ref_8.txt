We focus on the task of extractive QA, which creates an intuitive feedback solicitation scenario. It is relatively easy to visualize the model output (i.e., a span of text) in the context it is extracted from (i.e., the evidence paragraph), for the user to verify the answer. Automated evaluation is also well understood (Bulian et al., 2022), allowing reliable measurement of system performance over time. We deploy our QA system in rounds. Each round starts with an interaction phase, followed by a learning phase. At the interaction phase, users interact with a fixed, deployed model and provide feedback. We aggregate this interaction data until we collect a fixed amount of feedback data to enter a learning phase. The feedback is collected during natural user interactions (i.e., the model is deployed to fulfil its task of answering user questions). At the learning phase, we update the model parameters based on the aggregated feedback data. Because we observe no new feedback data during the learning phase, this creates an offline learning scenario (Levine et al., 2020), 3 which is practical for deployed systems. Except data aggregation, it requires no integration of the learning process into the deployed interactive system. The separation between deployment and training also enables sanity checks before deploying a new model, and for hyperparameter tuning as in supervised learning. Each interaction starts with a user posing a question. The model computes an answer, and returns it to the user alongside a visualization of it in the context text from which it was extracted. The user provides feedback, by selecting one of three options: “correct”, “partially correct” or “wrong”. Table 1 shows examples from our studies. Formally, let a question ¯q be a sequence of m tokens ⟨ q 1 , . . . , q m ⟩ and a context text ¯c be a sequence of n tokens ⟨ c 1 , . . . , c n ⟩ . A QA model at round ρ parameterized by θ ρ computes two probability distributions: a binary distribution indicating if the question is answerable or not by the context text P u (u | ¯q, ¯c; θ ρ ), where u ∈ { ANS, UNANS } ; and a distribution over answer spans in the context text P s (i, j | ¯q, ¯c; θ ρ ), where i, j ∈ [1, n] and i ≤ j. If arg max u ∈ { ANS,UNANS } P u (u | ¯q, ¯c; θ ρ ) = UNANS, the model returns to the user that the question is unanswerable. Otherwise, the model returns the highest probability span arg max i,j P s (i, j | ¯q, ¯c; θ ρ ). Given the model output, the user provides feedback f ∈ { CORRECT, PARTIALLY-CORRECT, WRONG } . Each user interaction generates a data tuple (¯q, ¯c, uˆ, i, j, f, θ ρ ), where uˆ is the binary answerability classification, and ˆ and ˆ are the start and i j end indices of the extracted answer, if the question is classified as answerable.