Our Subset kNN-MT (Figure 1) drastically accelerates vanilla kNN-MT by reducing the kNN search space by using sentence information (Section 3.1) and efﬁciently computing the distance between a query and key by performing table lookup (Section 3.2). 3.1 Subset Retrieval Sentence Datastore Construction In our method, we construct a sentence datastore that stores pairs comprising a source sentence vector and a target sentence. Concretely, a sentence datastore S is deﬁned as follows: S = { (h(x), y ) | (x, y ) ∈ D } , (4) x where h : V | X | → R D ′ represents a sentence encoder, which is a function that returns a D ′ dimensional vector representation of a source sentence Decoding At the beginning of decoding, the model retrieves the n-nearest-neighbor sentences of the given input sentence from the sentence dataˆ store S. Let S ⊂ S be the subset comprising nnearest-neighbor sentences. The nearest neighbor search space for target tokens in kNN-MT is then drastically reduced by constructing the datastore ˆ corresponding to S as follows: M = { (f(x, y <t ), y t ) | (5) (h(x), y ) ∈ S, 1 ≤ t ≤ | y |} , ˆ M ⊂ M where is the reduced datastore for the translation examples coming from the n-nearestneighbor sentences. During decoding, the model uses the same algorithm as kNN-MT except that M is used as the datastore instead of M . The proposed method reduces the size of the nearest neighbor search space for the target tokens from | D | to n ( ≪ | D | ) sentences. Subset kNN-MT retrieves the k-nearest-neighbor target tokens by an efﬁcient distance computation method that uses a look-up table. In the original kNN-MT, inverted ﬁle index (IVF) is used for retrieving kNN tokens. IVF divides the search space into N list clusters and retrieves tokens from the neighbor clusters. In contrast, in subset kNNMT, the search space varies dynamically depending on the input sentence. Therefore, clusteringbased search methods cannot be used; instead, it is necessary to calculate the distance for each key in the subset. For this purpose, we use asymmetric distance computation (ADC) (Jégou et al., 2011) instead of the usual distance computation between ﬂoating-point vectors. In ADC, the number of table lookup is linearly proportional to the number of keys N in the subset. Therefore, it is not suitable for searching in large datastore M , but in a ˆ small subset M , the search is faster than the direct calculation of the L2 distance. Product Quantization (PQ) The kNN-MT datastore M may become too large because it stores high-dimensional intermediate representations of all target tokens of parallel data. For instance, the WMT’19 German-to-English parallel data, which is used in our experiments, contains 862M tokens on the target side. Therefore, if vectors were stored directly, the datastore would occupy 3.2 TiB when a 1024-dimensional vector as a key 2 , and this would be hard to load into RAM. To solve this memory problem, product quantization (PQ) (Jégou et al., 2011) is used in both kNNMT and our subset kNN-MT, which includes both source sentence and target token search. PQ splits a D-dimensional vector into M subvectors and quantizes for each M D -dimensional sub-vector. Codebooks are learned by k-means clustering of key vectors in each subspace. It is computed iteratively by: (1) assigning the code of a key to its nearest neighbor centroid (2) and updating the centroid of keys assigned to the code. The m-th sub-space’s codebook C m is formulated as follows: D C m = { , . . . , c L m } , c l m ∈ R M . c1 m  (6) In this work, each codebook size is set to L = 256. A vector q ∈ R D is quantized and its code vector ¯q is calculated as follows: ¯q = [¯q 1 , . . . , ¯q M ] ⊤ ∈ { 1, . . . , L } M , (7) ¯q m = argmin qm  − c l m ∥ 2 2 , m q ∈ D ∥ RM . (8) Asymmetric Distance Computation (ADC) Our method efﬁciently computes the distance between a query vector and quantized key vectors using ADC (Jégou et al., 2011) (Figure 2). ADC computes the distance between a query vector ¯K ¯ki  q ∈ R D and N key codes = { } i=1 N ⊆ { 1, . . . , L } M . First, the distance look-up table A m ∈ R L is computed by calculating the distance between a query q m and the codes c l m ∈ C m in each sub-space m, as follows: Second, the distance between a query and each key ¯ki  d( q , ) is obtained by looking up the distance table as follows: A look-up table in each subspace, A m ∈ R L , consists of the distance between a query and codes. The number of codes in each subspace is L and a distance is a scalar; therefore, A m has L distances. And the table look-up key is the code of a key itself, i.e., if the m-th subspace’s code of a key is 5, ADC looks-up A 5 m . By using ADC, the distance is computed only once 3 (Equation 9) and does not decode PQ codes into D-dimensional key vectors; therefore, it can compute the distance while keeping the key in the quantization code, and the k-nearest-neighbor tokens are efﬁciently ˆ retrieved from M . In our subset kNN-MT, a variety of sentence encoder models can be employed. The more similar sentences extracted from M , the more likely the ˆ subset M comprises the target tokens that are useful for translation. Hence, we need sentence encoders that compute vector representations whose distances are close for similar sentences. In this work, we employ two types of representations: neural and non-neural. We can employ pre-trained neural sentence encoders. While they require to support the source language, we expect that the retrieved sentences are more similar than other encoders because we can use models that have been trained to minimize the vector distance between similar sentences (Reimers and Gurevych, 2019). An NMT encoder can also be used as a sentence encoder by applying average pooling to its intermediate representations. This does not require any external resources, but it is not trained from the supervision of sentence representations. Alternatively, we can also use nonneural models like TF-IDF. However, it is not clear whether TF-IDF based similarity is suitable for our method. This is because even if sentences with close surface expressions are retrieved, they do not necessarily have similar meanings and may not yield the candidate tokens needed for translation.