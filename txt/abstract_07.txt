We study continually improving an extractive question answering (QA) system via human user feedback. We design and deploy an iterative approach, where information-seeking users ask questions, receive model-predicted answers, and provide feedback. We conduct experiments involving thousands of user interactions under diverse setups to broaden the understanding of learning from feedback over time. Our experiments show effective improvement from user feedback of extractive QA models over time across different data regimes, including significant potential for domain adaptation. The deployment of natural language processing (NLP) systems creates ample opportunities to learn from interaction with users, who can often provide feedback on the quality of the system output. The combination of human feedback and continual learning presents exciting prospects, but is relatively understudied, partially because of the challenges it poses. Focusing on extractive question answering (QA), we study iteratively improving an NLP system by learning from human user feedback over time. Our setup is designed to study practical interaction with users. Following prior work on collecting information-seeking queries (Choi et al., 2018; Clark et al., 2020), we prompt users to ask ques-tions that they do not know the answers to. 