As language models have scaled up in size and multilingual capability in recent years, commensurate effort has followed to curate pretraining data (Raffel et al., 2020) to support this growth and improve the alignment of language models. Earlier multilingual models such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2019) were trained on monolingual data from Wikipedia and/or other large-scale web crawls which included only a few African languages. The introduction of mC4 (Xue et al., 2021), a document-level dataset spanning 101 languages helped alleviate this cover-age gap. 1 However, previous work (Kreutzer et al., 2022) has shown that mC4 and other existing largescale pretraining corpora have numerous quality issues, particularly for the low-resource African languages they contain. Against this backdrop, indigenous efforts to build language resources for African languages have converged to two approaches: (1) Small highquality data (e.g., 1GB) pretraining where most data are from the clean or verified sources like news domain (Ogueji et al., 2021). (2) Large aggregation of all available data (e.g., 15 − 42 GB) from noisy or unverified sources like CC-100 (Conneau et al., 2020), and mC4, combined with high-quality sources like news corpora (Adelani et al., 2022; Alabi et al., 2022; Adebara et al., 2022). This tradeoff between quantity and quality is forced by the unavailability of large, quality pretraining data for African languages. Motivated by this need, we introduce a new multilingual pretraining corpus in 20 African languages. We draw from Kreutzer et al. (2022)’s audit of existing pretraining corpora to understand prevailing quality issues. For mC4, they cite a high ratio both of sentences in incorrect languages (15.98% average) and nonlinguistic content (11.40% average). We trace these issues to the quality of data sources used in mC4 for the languages in our study and design heuristics to effectively extract clean monolingual text. More notably, we demonstrate how large-scale web crawls and document-level datasets, such as mC4, can be enhanced through meticulous auditing of their document sources i.e., base URLs (e.g., www.voahausa.com). Interestingly, for numerous credible sources, mC4 encompasses fewer documents than what is actually available. We conduct our own web crawl of these sources, collecting more documents than what is present in mC4 for the respective languages. We consolidate the result of our efforts (cleaning and crawling) with data from other sources, notably Wikipedia, and include four high-resource languages – Arabic, English, French & Portuguese. To evaluate the quality of our new corpus, we pretrain a new T5-based LM on the collected dataset and benchmark its performance on multiple downstream tasks. Our model demonstrates improved effectiveness over existing pretrained LMs further highlighting the importance of carefully curated datasets for pretraining language models in low-resource scenarios. Our model was significantly better than the baseline mT5 models across four different downstream tasks. Specifically, on cross-lingual QA evaluation, our new model achieves more than double the performance of multilingual T5.