We primarily study the proposed pretrained bidirectional distillation by conducting experiments on supervised, unsupervised, and zero-shot multilingual machine translation scenarios. 3.1.1 Language Model Pretraining Datasets We use the parallel dataset PC32 (Lin et al., 2020) and the monolingual dataset MC24 provided by Pan et al. (2021). PC32 contains 32 English-centric language pairs 1 , and MC24 consists of monolingual text in 24 languages 2 . We follow the original data preprocessing, data sampling, tokenization, and vocabulary by directly downloading the datasets 3 released by Pan et al. (2021), thus we can have a relatively fair comparison to our primary baselines, such as mRASP (Lin et al., 2020), mRASP2 (Pan et al., 2021) and CeMAT (Li et al., 2022). When pretraining, the source and target sentences are concatenated, and substituted synonyms are not masked. The masking ratio is 20%. Settings We adopt a 12-layer Transformer-based language model with 768 dimensions and 12 attention heads. The language model is trained on 8 Nvidia A100 GPUs for 1M steps using Adam optimizer. On each GPU, the number of tokens in each batch is at most 32K. The learning rate is set to 0.0001, and polynomial decay scheduling is used with a warm-up step of 10000. The hyperparameter λ in Equ 7 is 0.5, and the dropout rate is set to 0.1. See appendix for more details. Datasets For training multilingual translation models, we reuse the parallel dataset PC32 and monolingual dataset MC24, consistent with Pan et al. (2021). We follow the experimental settings in CeMAT (Li et al., 2022) for finetuning experiments. Language pairs of various data sizes from WMT are used for finetuning, and the dataset information is shown in Table 2. For evaluating unified multilingual models, we use the evaluation datasets from WMT, IWSLT, and OPUS-100 (Zhang et al., 2020) following mRASP2 (Pan et al., 2021). Settings We follow the model configurations used in CeMAT (Li et al., 2022) to train a Transformer-big (Vaswani et al., 2017) size NMT model, which will compare with models using the pretrain-finetune paradigm. And for a fair comparison, a larger NMT model with 12 encoder layers and 12 decoder layers is trained to compare with unified multilingual models. The contrastive loss is used in training a unified multilingual model due to its importance to zero-shot translation (Pan et al., 2021). Other training hyper-parameters are referred to from the open-source implementation of mRASP2. For pretrained bidirectional distillation losses, the intermediate layer to be distilled is set to the antepenultimate layer of the encoder and decoder. Note that global distillation doesn’t introduce extra parameters, and our model has the same size as the major baselines. We trained a unified multilingual NMT model with pretrained bidirectional distillation. As is shown in Table 1, our proposed PBD-MT clearly outperforms previously published approaches and achieves new state-of-the-art performances in most translation directions. It achieves +0.76 average BLEU improvement over mRASP2, which validates the effectiveness of the proposed pretrained bidirectional distillation. In addition, we investigate the effect of pretrained bidirectional distillation on the pretrainfinetune paradigm. Specifically, we adopt PBD losses on the encoder and decoder when finetuning. As we can see in Table 2, PBD-MT achieves better or competitive performance compared to previous pretrain-finetune models. It is noteworthy that no matter the unified model or the pretrain-finetune model, the improvement in X→En directions is more significant than that of En→X directions. We conjecture that English sentences are much more than other languages, thus the pretrained LM has a better understanding of English language. Table 3 summarizes the performance of unified multilingual models on a zero-shot translation scenario. Although the training data only consists of Englishcentric parallel sentences, multilingual NMT models show promising performance on zero-shot translation. Compared with mRASP2, PBD-MT further boosts the translation quality in most zero-shot di-rections, achieving a +1.24 average gain. Besides, we evaluate the unified multilingual models in unsupervised translation directions, and the results are shown in Table 4. For PBD-MT, positive results are observed in all translation directions but one direction, and the average BLEU score increases by a +0.73 point. These results validate the positive effects of the proposed pretrained bidirectional distillation not only on supervised scenario but also zero-shot and unsupervised scenarios. This section contains additional results for nonautoregressive translation (NAT) experiments. Specifically, we use a Transformer-big size fully NAT (Gu and Kong, 2021) as the base model. The model is initialized by a pretrained multilingual PBD-MT model and trained using a CTC loss as in Gu and Kong (2021). Because the decoder in the NAT model has upsampled length, for simplicity, we only adopt the encoder PBD loss when NAT training. Table 5 shows the performance of our model and other pretrained NAT models. Consistent BLEU gains are obtained by our PBD-NAT, validating its effectiveness. In order to evaluate the individual contribution of model components, we conduct an ablation study. We train a self-distilled LM and Transformer-base (Vaswani et al., 2017) size bilingual NMT models on the WMT14 English-German dataset, and report the results in Table 6. Compared with the standard bilingual Transformer and confidence-based KD (Zhou et al., 2022), PBD-MT significantly improves the performance, which verifies the effectiveness of pretrained bidirectional distillation on bilingual NMT. Without the PBD loss on the encoder or decoder, the BLEU scores degrade to some extent, and the decoder PBD loss has more impact than the encoder PBD loss. The results prove the necessity of both pretrained bidirectional distillation losses. To investigate the contribution of self-distillation on LM which generates globally defined distillation objectives in a single forward pass, a quantitative analysis is conducted here. Figure 4 illustrates the results. For execution efficiency, we compare marginalizing over multiple masks with the self- distillation on LM. For example, masking 10% tokens each time results in 10 LM forward passes to generate the full distillation objectives. As we can see, the design of self-distilled LM significantly accelerates the execution speed than multiple masks. For the distillation effect, we compare distillation on partial tokens with global distillation. The red lines show that 20% is a relatively reasonable proportion for partial distillation, and as the mask ratio increases, the performance degrades. Masking too many tokens increases the uncertainty for the LM. The best performance is achieved by global distillation, verifying the superiority of globally defined distillation objectives. We conduct a behavior analysis to understand which tokens are considered more certain in contexts by the self-distilled language model. In this experiment, instead of softmax, we use sigmoid to compute a scalar probability in the prediction head Ω. Figure 5 visualizes the predicted self-distilled token probabilities on randomly sampled sentences. In this experiment, no token is masked; thus, the token probabilities represent the tokens’ matching degree and certainty in the complete bidirectional context. As we can see, verbs, articles, conjunctions, and prepositions are roughly of higher probabilities, while nouns, adverbs, and adjectives are harder to be predicted. It can be concluded that the syntactic structure is more regular, and meaningful words are more changeable. 