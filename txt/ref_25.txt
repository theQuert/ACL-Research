To verify the effectiveness of our proposed framework, we conduct experiments to train a moral dialogue system and use the metrics proposed in §4 to evaluate. We use the popular open-source conversational models for our experiments: DialoGPT-medium (DGPT) (Zhang et al., 2019) and Blenderbot-400M (BBot) (Roller et al., 2020). We first pre-train (PT) them on RoTs, which is described in §3.2. Then as illustrated in §3.3, we do a multi-task training and train the conversational models on our constructed discussion dataset including MA, ME, MR, and RIL. Considering the catastrophic forgetting problem in deep learning (Kirkpatrick et al., 2017), we mix the discussion dataset with the general dialogue (GD) corpora including BST (Smith et al., 2020) and Daily Dialogue (Li et al., 2017). This is to confirm the general conversational ability other than morality. We name our proposed models trained on full tasks as Moral DGPT (BBot). We split train, dev, test sets based on meta dataset splits. There is no same question between train and dev/test sets and the overlap rate of RoTs in dev/test set to train set is 13%/12%. After training, we primarily use the metrics introduced in §4 to measure the moral performance of conversational models by interacting in real time. We take out the questions in dev and test sets as the discussion openings. Our experimental results are shown in Table 3. We compare the original conversational model with our proposed moral model (DGPT v.s. Moral DGPT, BBot v.s. Moral BBot). It is found that all the metrics get very significant improvement especially the most important metrics S MA and S ME . By training based on our proposed framework, DialoGPT and Blenderbot are thus equipped with much stronger power of moral answering, moral explanation, moral revision and moral inference. Besides, for controlling variables, we add experiments where we only train the models on GD. This proves (1) general dialogue corpora indeed helps morality performance, which indicates that morality is embodied in multiple scenarios (e.g. empathy in BST dataset) and could be enhanced implicitly; (2) The vast major improvement of scores of moral models is still attributed to the discussion datasets based on our framework, instead of GD. Meanwhile, we also notice that Moral DGPT and BBot perform poorly in the metric S △MR , which measures the agreement (to the user’s RoT) gap between the first and the second answers. The result is in line with our expectations. When the first answer gets a low score, it would be easier to get a high score of S △MR . However, training on MA and ME tasks makes the first answer of the models often good enough. The ablation study in the row “w/o MA” also verifies that from the other side. Therefore, we consider it acceptable that our proposed moral models have a low score of S △MR . At last, our experimental results also verify some findings by previous studies. For example, experimental results show that Blenderbot outperforms DialoGPT in all metrics, which is in accord with previous works (Roller et al., 2020; Xu et al., 2020). This also confirms that the proposed metrics are of practical significance.