Morality in Languages Morality in artificial intelligence draws great attention since many years ago (Moor, 2006; Savulescu and Maslen, 2015; Hendrycks et al., 2020). Language is one of the primary ways to express and embody morality (Hare and Hare, 1991). In NLP communities, to analyze morality in language, Forbes et al. (2020) propose and collect a well annotated Rules of Thumb corpora, which provides conceptual units to model morality for the follow-up studies such as MIC (Ziems et al., 2022). As another line of work, over the development of large-scale language models, some researchers find that language models contain inner morality (Schramowski et al., 2021) and is promising to judge morality in a specific situation (Jiang et al., 2021). Meanwhile, previous works discover some safety defects about morality in large language models (Brown et al., 2020; Perez et al., 2022), which leads us to further study morality modeling in languages. Multifacetedness of Morality Morality is multifaceted. The judgment of an action may change when the situation changes (Forbes et al., 2020). Beside situation, morality may also vary across cultures, parties (Ziems et al., 2022; Bang et al., 2022), history time (Joyce, 2007), and even individuals. Based on that, Talat et al. (2021) criticize that Delphi (Jiang et al., 2021) neglects the diversity of human values. For the multifacetedness of morality, the concurrent work Bang et al. (2022) studies how to answer ethical quandary questions. In our framework, We pay particular attention to the multifaceted nature of morality and design the moral conflict sub-module. Moreover, we specially distinguish between universal and dynamic RoTs when evaluating moral answer generation. Dialogue Safety and Morality With the great improvement of the open-domain dialogue system these years (Roller et al., 2020; Adiwardana et al., 2020; Rae et al., 2021), the safety bottleneck of dialogue system emerges gradually, hinders the deployment in real world. Numerous works study safety detection and safe generation in dialogue system (Xu et al., 2020; Dinan et al., 2021, 2019). Also, researchers discover morality is a core requirement in dialogue safety (Henderson et al., 2018; Sun et al., 2021; Bommasani et al., 2021). However, few works directly train a moral dialogue system for lack of relevant moral expression framework and corresponding evaluation methods. The concurrent work ProsocialDialog (Kim et al., 2022) applies RoTs into dialogue response generation to better detect and counter the unsafe context. Differently, we explore the communication mechanisms of morality and train moral dialogue system by constructing discussion dataset. Our method improves the comprehensive morality of dialogue system (from the four sub-modules in our framework). Also, our method does not require any extra plugins or parameters in conversational models.