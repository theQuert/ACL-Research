We have three goals for evaluation: 1) RR-based estimators of feature effect are more accurate than propensity-based estimators; 2) FEAG using RRbased estimators provides better overall accuracy while minimizing spurious correlation compared to existing baselines for removing spurious correlations; 3) Our feature effect estimator is a general method and can be used to detect annotator bias. Since the true feature effect is unknown for real-world data, we construct a semi-synthetic dataset based on the CiviComments dataset (Borkan et al., 2019). In addition, we evaluate on subsampled versions of the CivilComments and IMDB dataset. CivilComments Semi-Synthetic (SS). CivilComments is a toxicity detection dataset { (X, Y ) } , where X are input sentences and Y is the toxicity label (1 means toxic). To evaluate our methods, we need to construct a dataset generated from the causal graph in Fig. 2. Since the writer’s intent (confounder) is unknown, we construct it as a property of the input text, W = h(X) ∈ { 0, 1 } , leading to the modified causal graph in Fig. 3 (Supp G). To obtain h(X), we train a binary classifier using a DistilBERT model on (X, Y ) pairs. Finally we sample a new label as Y ′ ∼ Bernoulli((1 − τ)Y + τT), giving the true feature effect as τ. The complete text Z = (X, T) is constructed by prepending each covariate sentence X with the word Treated if T = 1 and Untreated if T = 0. CivilComments Subsampled. Rather than introducing a new treatment, here we subsample CivilComments to introduce a spurious correlation between an existing token kill and label Y . Here all sentences with token kill are considered as treated, while others untreated. To exacerbate the spurious correlation between T and Y , we subsample our data based on the learnt property W (from above), following the causal graph in Fig 3a. IMDB. From the IMDB reviews dataset (Maas et al., 2011), we consider reviews that contain a numerical rating—text string from either the set {7/,8/,9/} or {2/,3/,4/}. To construct a binary treatment variable, occurrences of these strings are replaced by Treated if the rating is 7, 8, or 9 and an empty string otherwise. The Treated token is predictive of the sentiment with 90% accuracy. For dataset and training details, see Supp B, Supp A respectively. All results are run for 3 seeds. We evaluate the performance of different estimators in Sec 3 on the CivilComments SS dataset (with different overlap ϵ and feature effects τ). Wecompare the Riesz-based DR estimator (Eqn 6) with the Direct (Eqn 4) and Propensity-based DR (Eqn 5) baselines. All estimators are finetuned using either BERT or DistilBERT as base model. See Supp ?? Quantitative Results. Table 1 shows the mean error in estimating feature effect across τ ∈ { 0.10, 0.30, 0.50 } and ϵ ∈ { 0.01, 0.05, 0.10 } . For hyperparameter selection, see Supp. D. Across all settings (barring 1% overlap with high τ), Riesz is able to estimate the effect with low error. Direct fails to do well in high τ and low ϵ ranges, failing for both τ =0.50 and ϵ=0.01. Due to its high variance, Propensity is unable to work well, often producing an estimate worse than Direct. For the two real-world datasets, true feature effect is unknown. But comparing the effect estimates of Direct and Riesz, Direct tends to overestimate the feature effect (due to spurious correlation), which is corrected to a lower value by Riesz. Qualitative Results. To understand how the Reisz estimator works, we show qualitative results for Civil Comments Subsampled dataset in Table 3. To counter the spurious correlation of token kill (T) with other parts of text (X) that cause toxicity (Y), the Riesz estimator provides a low weight to sentences having features X that commonly occur with T, and higher weight to sentences having X that rarely occur with T. Treated samples (T=1) have a positive Riesz value and vice versa. We can see that sentences with violent language (in addition to kill) are assigned a low score while other sentences with kill are assigned a high score, thus serving to extract the isolated feature effect of kill (without confounding due to other tokens). We now compare FEAG classifiers based on Riesz, FEAG(ate), and based on zero effect, FEAG(0), with prior debiasing algorithms. Groups. Classifiers that reduce spurious correlation are expected to decrease total accuracy but increase the accuracy of minority inputs that do not exhibit those correlations. To study such effects on accuracy, we divide our evaluation data into four groups: Group1 (Y = 0, T = 0), Group2 (Y = 0, T = 1), Group3 (Y = 1, T = 0), Group4 (Y =1, T =1). In addition, we report the average group accuracy across the four groups as a measure of debiasing/reduced spurious correlation. An ideal model should achieve both high overall accuracy and high average group accuracy, demonstrating its reduced reliance on spurious features. Baselines. We consider popular baselines from prior work (Joshi et al., 2022; He et al., 2022; Orgad and Belinkov, 2022): weighting methods like DFL, DFL-nodemog, Product of Experts (Mahabadi et al., 2019; Orgad and Belinkov, 2022) and latent space removal methods like INLP (Ravfogel et al., 2020). We also include worst-group accuracy methods like GroupDRO, Subsampling (Sagawa et al., 2019, 2020) from the machine learning literature, and a baseline RemoveToken that removes the treatment feature from input (see Supp C). Results. For the semi-synthetic dataset (CivilComments SS) in Table 4, FEAG(ate) increases the average group accuracy while retaining similar overall accuracy as Direct. FEAG(ate) also has bet-ter minority group accuracy (i.e. Group2,Group3) than Direct. In comparison, FEAG(0) leads to a decrease in overall accuracy and also average group accuracy compared to FEAG(ate). Other baselines like Subsample, GroupDRO or DFL achieve a higher average group accuracy as they improve accuracy on the minority groups, but they suffer a substantial reduction in overall accuracy, from 87 to 66-73, which hinders usability of the model. Methods like DFL-nodemog or POE have no impact or obtain worse results compared to Direct. These results show the fundamental tradeoff between total and average group accuracy and how FEAG(ate) provides a good tradeoff between the two. For the subsampled dataset (CivilComments Subsampled) in Table 5, we see a similar trend, where FEAG(ate) gives the best tradeoff between overall and average accuracy. FEAG(0) is substantially worse than FEAG(ate), showing the importance of not fully removing the effect of a spurious token. Except POE, Subsample and GroupDRO, all other methods obtain both lower total and average group accuracies compared to FEAG(ate). As before, POE is near identical to Direct while the weighting methods Subsample and GroupDRO lead to significant decreases in total accuracy. Finally, we show results for IMDB where the causal graph is unknown and our assumptions from Fig. 3a may not be valid. Nonetheless Table 6 shows that both FEAG(ate) and FEAG(0) achieve better average group accuracy with slightly better total accuracy than the Direct model. Other baselines follow their usual trend: ML weighting baselines (Subsample, GroupDRO) suffer reductions in total accuracy, DFL and POE methods are unable to improve average group accuracy substantially, and INLP is worse for both total and average group accuracy. Besides BERT, results using DistilBERT as a base model show a similar trend (Supp F). We also report FEAG(propen) numbers in Supp E. While we focused on the debiasing task for classifiers, our feature effect estimator is general: we apply it to detect annotator bias in the CivilComments dataset. If the true feature effect of a token is known, we can compare it to the estimated effect to detect any annotator bias in the dataset. For tokens like “racist” and “guys” where the true effect is likely to be high and zero respectively, the estimated effect confirms the prior (see Table 7). But for tokens like “gay” or “black”, our method shows a significant non-zero feature effect on the label which may indicate annotator bias, as it may be known that these tokens should have a zero effect on the toxicity label. Compared to the naive conditional probability (Y | T), our effect estimator can be used to provide a better sense of how impor-tant certain keywords are for generating the output label. (e.g., “guys” obtains a zero causal effect but P(Y | T) shows a substantial deviation from 0.5).