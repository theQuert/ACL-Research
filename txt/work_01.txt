In this section, we first introduce the previous practices in binarization and ternarization. Then, we introduce a unified statistic-based weight binarization / ternarization method that can alleviate the gradient mismatch issue and enhance the quantized weights entropy. Lastly, we analyze the difference between weight quantization and activation quantization and propose an elastic ternarization method for activations. We abbreviate our method as TBT, short for “Ternary / Binary Transformer”. Ternary neural networks, where real values are quantized to three levels, are first introduced in (Li et al., 2016). Thus, these values can be repre-sented in 2 bits, leading to a 16× reduction in size and computation. Moreover, the computations can be calculated multiplication-free, leading to even further computation gains on suitable hardware. The recent work integrates the ternarization algorithm in natural language models for quantizing the weights and activations in classification tasks (Zhang et al., 2020) and ternarizing the weight (8bit activations are used) in generative models (Li et al., 2022; Tao et al., 2022). The general formula (Li et al., 2016) for ternarization is as follows: Here X T denotes the ternary weights/activations, and X R represents their real-valued counterparts. n X R denotes the total number of elements in the tensor. ∆ is the ternary threshold, and α T is the scaling factor that minimizes l2-loss between XT  and X R . The neural network binarization denotes representing the weights and/or activation with bi-level values. It is first proposed in BNN (Courbariaux et al., 2016) and has evolved in the follow-up works (Rastegari et al., 2016; Liu et al., 2018). Rastegari et al. (2016) formulates binarization as: Here X B can represent binary weights or binary activations. α B denotes the scaling-factor that minimize the l2 loss between X R and α B ·Sign(X ). The acceleration and compression effectR of ternary/binary neural networks is significant. By representing the weights and activations with { −1, 0, 1 } , the network enjoys ∼16× memory saving compared to its 32-bit floating-point counterpart. When further binarize the weights and activations to only 1-bit (i.e., { −1, 1 } ), up to 32×model-size reduction and 58× speedup on CPUs have been achieved (Rastegari et al., 2016), where the matrix multiplication operations are replaced with light-weighted bitwise XNOR operations. Despite its appealing characteristics, naively binarizing or ternarizing the transformer model for natural language generation results in several accuracy drops or even a total failure in training. It has been observed that the attention layers of the transformer network are difficult to quantize to low bits. Also, the auto-regressive decoding tends to accumulate errors due to quantization. Given the nature of generative language networks that require highprecision output, quantizing both the activations and weights in these models to extreme bit values is non-trivial and has not been explored before.In contrast to neural network weights that are stored on the disk, activations are calculated on-the-fly. The distribution of activations in a particular layer depends on the network weights as well as the corresponding input sequence, and thus varies from batch to batch. In order to have the quantization function better capture the underlying activation distribution, we propose learning-based activation quantization. Inspired by BiT (Liu et al., 2022), we divide the activation layers into two categories: the activation layers with non-negative values (X R ∈ R + ), i.e., Softmax/ReLU layer outputs and the rest of the layers with both positive and negative activations (X R ∈ R). We binarize / ternarize the first activation category (X R ∈R + ) to { 0, α } / { 0, α, 2α } , and symmetrically quantize the later activation category (X R ∈ R) to { −α, α } and { −α, 0, α } in binary and ternary cases respectively. In this way, the activation distribution matches the original fullprecision activations and thus reduces the quantization error. Further, we learn to scale the real-valued activations to better fit quantization thresholds, and this learnable scaling factor can be updated end-to-end with the gradients from the network loss to better account for overall network optimization. In the ternary case, we propose the elastic ternarization function formulated as, where X R and X T denote real-valued and ternary activations, respectively. To keep the formula concise, we set X R ′ = X R − X R , denoting the zeromean real-valued activations. α T is the scaling factor. Different from the weight quantization, the scaling factor in Eq. 11 is learned with the gradient update. We follow the practice in (Zhou et al., 2016; Esser et al., 2019) to calculate the gradients with straight-through estimation (STE) bypassing the non-differentiable rounding function:The learnable scaling factor can dynamically adapt to different activation distributions and improve the ternarization accuracy. In the binary case, it is formulated as.