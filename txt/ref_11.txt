Synthetic language dataset To allow easy control of the eigenvalue spacings of the transition matrix T and thus observe the phase transition phenomena predicted by our theory, we design six synthetic languages with HMM language models as follows. First, we create the HMM transition graph by treating non-overlapping bigrams as hidden states of the HMM. The hidden state of the HMM will henceforth be referred to as the “speech unit”, while the observation emitted by the HMM will be referred to as the “text unit”. For the asymptotic ASR-U, we control the number of eigenvalues of the Markov transition graph by varying the number of disjoint, identical subgraphs. The number of distinct eigenvalues of the whole graph will then be equal to the number of eigenvalues of each subgraph. For the finite sample setting, we instead select only Hamiltonian graphs and either gradually decrease the degrees of the original graph to its Hamiltonian cycle or interpolate between the graph adjacency matrix and that of its Hamiltonian cycle. Thus, we can increase σ min (P X ) by increasing w. For both the subgraph in the former case and the Hamiltonian graph in the latter, we experiments with circulant, de Bruijn graphs (de Bruijn, 1946) and hypercubes, as illustrated in Figure 2. Next, we randomly permute the hidden state symbols to form the true generator mapping from the speech units to text units. To create matched speech-text data, we simply sample matched speech and text unit sequences using a single HMM. For unmatched datasets, we sample the speech and text data independently with two HMMs with the same parameters. Please refer to Appendix B for more details. Model architecture For finite-sample ASR-U, we use wav2vec-U (Baevski et al., 2021) with several modifications. In particular, we experiment with various training objectives other than the Jensen-Shannon (JS) GAN used in the original wav2vec-U, including the Wasserstein GAN (Liu et al., 2018) and the MMD GAN. All additional regularization losses are disabled. Moreover, we experimentally manipulate two hyperparameters: (1) the averaging strategy used by the generator, and (2) whether to reset the discriminator weights to zero at the beginning of each discriminator training loop. More details can be found in Appendix B. Phase transition of PER vs. eigenvalue gaps: asymptotic case The phoneme error rate (PER) as a function of the number of eigenvalues of A for the asymptotic ASR-U on the synthetic datasets are shown in Figure 3. For all three graphs, we observeclear phase transitions as the number of eigenvalues exceeds the number of speech units, and an increase of the number of distinct, nonzero eigenvalues required for perfect ASR-U as the number of speech units increases. Phase transition of PER vs. eigenvalue gaps: finite-sample case The PER as a function of the least singular value σ min (P X ) for the finite-sample ASR-U on the synthetic datasets are shown in Figure 4. As we can see, the ASR-U exhibit the phase transition phenomena in all three graphs, albeit with differences in the critical point and their rate of approaching the perfect ASR-U regime. While the PER generally decreases as σ min (P X ) gets larger, we found a dip in PER in the circulant graph case as σ min (P X ) moves from 10 −31 to 10 −15 . Though unexpected, this observation is not contradictory to our theory since our theory does not make explicit predictions about the rate of phase transition for ASR-U. Across different GAN models, we found that JSD generally approaches perfect ASRU at a faster rate than MMD in all three graphs, suggesting the use of nonlinear dynamic may be beneficial. Nevertheless, the overall trends for different GANs remain in large part homogeneous. Between Wasserstein and MMD, we observe very little difference in performance, suggesting the regularization effect of NTK is sufficient to control the Lipschitz coefficient of the network. Finally, for the MMD GAN in the matched setting, we found the network is able to achieve perfect ASR-U regardless of the spectral properties of the Markov transition graphs, which confirms our theory that a symmetric Markov random matrix tends to have simple eigenvalue spectrum suitable for ASR-U. Effect of discriminator reset As pointed out by (Franceschi et al., 2021), a discriminator may suffer from residual noise from previous updates and fail to approximate the target divergence measure. We analyze such effects for MMD and JSD as shown in Figure 5. We observed consistent trends that models whose weights are reset to the initial weights every discriminator loop outperform those without resetting. The effect is more pronounced for JSD GAN than MMD GAN and for smaller σ min (P X ). Effect of generator averaging strategy The original wav2vec-U (Baevski et al., 2021) directly feeds the text posterior probabilities O into the discriminator, which we refer to as the “soft input” approach. Alternatively, we can instead calculate a weighted average of the gradient form over the samples y ∈ Y L as in Eq. (13), which we refer to as the “outside cost” approach. The comparison between the two approaches are shown in Figure 6. We observed mixed results: for MMD GANs, the softinput approach outperforms the outside-cost approach and performs best among the models in the high-σ min (P X ) setting; for JSD GANs, we found that the outside-cost approach performs slightly better than the soft-input approach. Such inconsistencies may be another consequence of the regularization effect predicted by the GANTK. We leave the theoretical explanation as future work.