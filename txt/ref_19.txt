Following previous studies (Li et al., 2019c), we formulate ABSA and AE as a sequence labeling problem. Given a sentence with n words x = { w 1 , w 2 , ..., w n } , the goal is to predict its corresponding label sequence y = { y 1 , y 2 , ..., y n } , where y j ∈ { B-POS, I-POS, B-NEG, I-NEG, B-NEU, I-NEU, O } for ABSA and y j ∈ { B, I, O } for AE. In this work, we focus on the unsupervised domain adaptation setting, in which the source domain has enough labeled data and the target domain only has unlabeled data. Let D S = { (x i s , y i s ) }i=1 N s  denote a set of source-domain labeled data, and D T = { x i t } i=1 N t a set of target-domain unlabeled data. The goal is to leverage D S and D T to predict the label sequences of test data from the target domain. As illustrated in Figure 2, our Cross-Domain Data Augmentation framework contains three key stages, including 1) Domain-Adaptive Pseudo Labeling, 2) Domain-Adaptive Language Modeling, and 3) Target-Domain Data Generation. In the first stage, an aspect-aware domain adaptation model is trained to assign pseudo labels to unlabeled data in the target domain. In the second stage, the labeled source data and the pseudo-labeled target data are used to train a domain-adaptive language model, which integrates data generation and sequence labeling in a unified architecture to capture the transferable context and annotation across domains. After training the DALM, the last stage uses probabilitybased generation strategy to generate diverse targetdomain data with fine-grained annotations in an autoregressive manner. In this stage, our goal is to assign the pseudo labels to each unlabeled data in the target domain. Since the data distribution of the source domain is different from that of the target domain, directly training a classifier on the labeled source data to predict the pseudo labels of the unlabeled target data will bring much noise. Thus, it is necessary to alleviate the domain discrepancy to improve the quality of pseudo-labels. Since aspect terms are shown to play a crucial role in ABSA (Gong et al., 2020), we attempt to explicitly minimize the distance between source-domain and target-domain aspect term representations via Maximum Mean Discrepancy (MMD) (Gretton et al., 2012). Specifically, given the labeled source data DS  and the unlabeled target data D T , we first obtain the aspect terms in D S via the gold labels and extract the aspect terms in D T based on a rulebased algorithm named Double Propagation (Qiu et al., 2011). Let us use x d = { w 1 d , w 2 d , ..., w n d } to denote a source or target domain sentence and use a d = { w i d , ..., w j d } to denote one of the aspect terms in the sentence, where d ∈ { s, t } . We then employ a pre-trained BERT model to obtain the hidden representation of the sentence H d = { h 1 d , h 2 d , ..., h n d } and the aspect term representation a d = g(h i d , ..., h j d ), where h d ∈ R r , r refers to the hidden dimension, and g(·) denotes the meanpooling operation. Next, we propose an aspectlevel MMD loss to alleviate the distribution dis-crepancy across domains as follows: where D S a and D T a respectively denote the sets of aspect term representations in the source domain and the target domain, N a s and N a t refer to the number of aspect terms in the two domains, and k(·) denotes the Gaussian Kernel function. Meanwhile, for each source sample, the hidden representation H s is fed into a Conditional Random Field (CRF) layer to predict the label sequence for the ABSA or AE task p( y s | H s ). The goal is to minimize the negative log-probability of the correct label sequence of each source-domain sample: The CRF loss for the ABSA or AE task and the aspect-level MMD loss are combined to train the base model C b : where α is the hyper-parameter. Finally, we use C b to assign pseudo labels to each sample in D T , and obtain D PT = { (x i pt , y i pt ) } i=1 N t . shows an example of two input and two output sequences for a sample from the source domain. Next, for the input token sequence x in , we employ a decoder such as LSTM and the pre-trained GPT-2 model (Radford et al., 2019) to get its hidden representation as follows: e −1 w , e 0 w , ..., e n w = Decoder(w −1 , w 0 , w 1 , ..., w n ), where w −1 denotes ⟨ BOS ⟩ , e t w ∈ R d is the token representation, and d is the hidden dimension. For the input label sequence y in , a label embedding layer is used to get the label representation: e −2 y , ..., e n−1 y = LabelEmb(y −2 , y −1 , ..., y n−1 ), where y −2 d and y −1 denote ⟨ BOL ⟩ and y ⟨ BOS ⟩ , and e t y ∈ R . Next, at each time step t, we add et w  and e t−1 y to produce a token and label-aware representation (i.e., e t = e t w + e t−1 y ), which is then fed into two different full-connected softmax layers to predict the probabilities of the next token w t+1 and the label y t as follows: P(w t+1 | w ≤ t , y ≤ t−1 ) = σ(W w e t + b w ), (3) P(y t | w ≤ t , y ≤ t−1 ) = σ(W y e t + b w ), (4) where σ is the softmax function, and W x ∈ R | V x | ×d , W y ∈ R | V y | ×d , and | V x | and | V y | are the vocabulary size and the label size. For each sample (x, y ) ∈ D S ∪ D PT , we optimize the parameters for DALM by minimizing the combination of cross entropy losses for the output token sequence and label sequence as follows: L = L w + L y , (5) n L w = − ∑ logP(w t+1 | w ≤ t , y ≤ t−1 ), (6) t=−1 n L y = − ∑ logP(y t | w ≤ t , y ≤ t−1 ). (7) t=−1 3.5 Target-Domain Data Generation After training the DALM, we employ it to generate target-domain data with fine-grained annotations in an autoregressive manner. As shown in the bottom of Figure 2, the ⟨ BOS ⟩ token and the target-specific token [target] are fixed as the first two input tokens of the DALM, and ⟨ BOL ⟩ and O are fixed as the first two input labels. Next, we adopt a probabilitybased generation strategy to generate the following tokens and their corresponding labels. At each time step t, we first rank all the tokens in V x based on the probabilities computed by Eq. 3 and pick top-k tokens as a candidate set C t+1 . We then sample a token w t+1 from C t+1 as the next token. As the candidate tokens in C t+1 are predicted with higher probabilities, the generated data are generally fluent and close to the real target-domain data. Moreover, given the same context, the DALM can choose a synonym as the next token due to the randomness of sampling, which is conducive to diversifying the generated data. Next, for the label generation at each time step t, we directly select the label with the highest probability computed by Eq. 4 as the label of the current token y t , which can ensure the quality of the generated label sequence. The above process of token generation and labeling will be stopped when the next token is predicted as ⟨ EOS ⟩ . Because of the randomness brought by sampling, the trained DALM can be used to generate any amount of labeled data. However, generating more data may lead to significant vocabulary redundancy of generated data. Thus, once the size of generated data equals to N g , we will stop generating target-domain labeled data. 3.6 Generated Data Filtering To mitigate the presence of low-quality labels in the target data generated from the probability-based generation strategy, we introduce the following steps for generated data filtering: 1) Delete data with the illogical labels that violate the prefix order of the BIO tagging schema (e.g., having O before I in the AE task and having B-Positive before INeutral in the ABSA task); 2) Delete repetitive data whose token and label sequences are the same, and only keep one of the duplicate samples; 3) Use the base model C b in Section 3.3 to predict the label sequences of the generated sentences and delete data whose label sequences are different from those predicted by C b . Let us use D g = { (x i g , y i g ) } i=1 N g to denote the set of generated target-domain data. We then train a standard BERT-CRF model (Li et al., 2019b) on D g , and use it to predict the label sequences of test data from the target domain.To generate a large amount of target-domain labeled data with diverse syntactic structures, we propose a Domain-Adaptive Language Model (DALM), which leverages the labeled source data D S and the pseudo-labeled target data D PT to learn the shared distribution of words and labels across domains. Since our DALM unifies the process of word generation and sequence labeling, at each time step, we employ the current input token and the predicted label at the previous step to simultaneously maximize the probabilities of predicting the next token and the label of the current token. Specifically, for each sample (x, y ) ∈ D S ∪ D PT , we first construct an input token sequence, in which we insert a special token ⟨ BOS ⟩ to denote the sentence beginning, followed by a domain-specific token (i.e., [source] or [target]) to distinguish the domain that x belongs to. Let x in = {⟨ BOS ⟩ , w 0 , w 1 , w 2 , ..., w n } denote the expanded input sentence, where w 0 ∈ { [source], [target] } . Moreover, we construct another input label sequence, denoted by y in = {⟨ BOL ⟩ , y ⟨ BOS ⟩ , y 0 , y 1 , y 2 , ..., y n−1 } , where ⟨ BOL ⟩ denotes the initial state of the label sequence, y ⟨ BOS ⟩ is O, and y j refers to the label of w j . According to the input, the output token sequence is x out = { w 0 , w 1 , w 2 , ..., w n , ⟨ EOS ⟩} . The output label sequence is y out = { y ⟨ BOS ⟩ , y 0 , y 1 , y 2 , ..., y n } . The top of Figure 2