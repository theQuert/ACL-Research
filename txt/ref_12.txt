We first present some preliminaries ( § 3.1). Then, we introduce mapping dialogue texts to the desired latent space ( § 3.2), augmented data construction ( § 3.3), augmented data utilization ( § 3.4), and inference details ( § 3.5). Figure 2 shows the overview of DialoGPS. In open-domain dialogue generation, given a multiturn dialogue X = [x 0 , x 1 , ..., x T ], the goal is to predict the response x T based on the context X 0:T−1 . The number of tokens in x t is denoted as | x t | , t ∈ { 0, 1, . . . , T } . The i-th token in the xt  is denoted as x t i . A Brownian Bridge B defined on time range [0, T] is a special Gaussian process established on deterministic endpoints µ 0 and µ T . At time t, the latent variable z t follows a Gaussian distribution B(t |µ 0 , µ T ): Brownian Bridge, the covariance between t 1 and t 2 , with 0 < t 1 < t 2 < T is t 1 (T−t 2 ) , where the constant T positive covariance guarantees that B(t 1 |µ 0 , µ T ) and B(t 2 |µ 0 , µ T ) are temporally correlated. However, as defined in Eq. 1, a conventional Brownian Bridge B has deterministic endpoints, which prevents us from sampling for x T , the response, and x 0 , the first utterance in the context. To avoid degenerating to a many-to-one mode that impairs the generalization, we derive an extended Brownian Bridge β with samplable endpoints. Take the derivation of β (T |µ 0 , µ T ) as example: given a B, both the distance d δ between µ T and z T−δ and the summation of d δ and zT−δ  follow the Gaussian distribution, we can derive the distribution of z T as follows: To optimize the mapping function f θ , we follow (Wang et al., 2022) to adopt a contrastive learning framework where positive samples are ordered sentence triplets from the same conversation (x t 0 , x t 1 , x t 2 , t 0 < t 1 < t 2 ) and negative samples are constructed by randomly replacing the middle point x t 1 with other sentences x t 1 ′ from the mini-batch B. The objective is as below: where d(x t 0 , x t 1 , x t 2 ; f θ ) = − 1t 2 1  f θ (x t 1 )−(12σ ∥ )f (x t 0 ) − 1 2 f θ (x t 2 ) ∥ 2 2 . The essence of Eq. 4 is toθ optimize t t the outputs of f θ , i.e., µ t 0 , µ t 1 , and µ t 2 to the linear relationship as defined in Eq. 1. In DialoGPS, a 4-layer MLP serves as f θ . To embed utterance as inputs of f θ , there are many choices such as averaging token embeddings or encoding by a language model. We leave the embedding details in § 5.3. As shown in Figure 2(a), we take Transformer (Vaswani et al., 2017) as the bone architecture. With f θ , an extended Brownian Bridge β is established. We sample latent variables z t ∼ β (t |µ 0 , µ T ) and mix them with representations of corresponding x t . In the encoder, for each utterance x t in the context X 0:T−1 , we conduct: where e t i is the output corresponding to the i-th token in x t from the encoder, i ∈ [1, | x t | ]. Wz enc  and W x enc are trainable vectors of the same dimension as e and z. Finally, eˆ is sent to the decoder for cross-attention. We conduct the mixup every decoder layer: where N is the number of decoder layers, d j i is the self-attention output at position i in layer j. Also, ˆ W z dec j and W x dec j are trainable vectors. d j is used as Query, and eˆ are used as both Key and Value in the cross-attention. For a dialogue text X, we conduct sampling and mixup K times, which is equivalent to providing K extra discrete dialogues X k ˆ = [ x0 k ˆ , x1 k ˆ , ..., xT k ˆ ] , k ∈ [1, K] for training. Figure 2(b) shows mixup details. In general, given X to a dialogue generation model, parameters ϕ of model are optimized by minimizing the negative log-likelihood: However, as aforementioned, what we obtain are ˆ continuous representations of X whereas the corresponding discrete sentences are inaccessible, which makes Eq. 7 intractable. Hence, to utilize the augmented data, we make an assumption that: There is an inaccessible many-to-many dialogue dataset D MtoM . P MtoM describes the conditional distribution of responses given contexts in this dataset. The accessible one-to-one dataset D 1to1 is collected by sampling from D MtoM uniformly, and thus P1to1  can be viewed as an approximation of P MtoM . Based on this assumption, we propose a selfdistillation framework consisting of two steps: (1) It optimizes the model with the original discrete data following Eq. 7. (2) During training, as Pϕ  fits P 1to1 , which is an approximation of P MtoM , the model can use its output given X to teach itself when presented with augmented data, i.e., the ˆ where D KL [· || ·] is the KL-divergence (Kullback and Leibler, 1951). In Eq. 8, to remove the gap between utilizing the original discrete data X and ˆ the augmented continuous data X in the same architecture, we mix each utterance in X with the expectations µ 0:T . Formally, the overall training objective is to minimize: The inference goal is to predict x T based on context X 0:T−1 . First, f θ takes X 0:T−1 and outputs corresponding µ t for sampling and mixup in the encoder, where t ∈ { 0, 1, . . . , T − 1 } . Next, the decoder receives the encoder output and an inferred µ T to decode the response in an autoregressive manner. To obtain the value of µ T , we do not require additional prediction networks. Instead, we can directly derive its value based on the property of Brownian Bridge. Specifically, given the context, we know that for any t: If µ T is already known, a Brownian bridge established on µ T and µ 0 would yield the same µt  values. Consequently, we can establish an equality and derive the value of µ T as follows: We find that there is hardly a difference in evaluation results when conducting mixup operations with either expectations µ or sampled variables z. To reduce randomness for easier analyses, experiments in below use expectations µ to mixup. Nonetheless, sampling variables gives DialoGPS the ability to generate diverse responses to an arbitrary context and we will discuss it in § 5.4. 