In this section, we formally state the problem and present our method (see Figure 1 for an illustration) that produces a synthetic version of private text data with differential privacy. Let D be a database representing the collection of token sequences from a fixed dictionary V. We define a (randomized) mapping M : D → D such that for a given dataset D ∈ D, the goal is to generate a synthetic version M(D) = ˜D with privacy constraints and utility desiderata. Regarding privacy constraints, we require that M be (ϵ, δ)-DP with domain D. This requirement provides strong protection for the participants in the input dataset as this participation will be statistically indistinguishable to a certain degree through any adversary accessing the model or synthetic version of the dataset in the output. For the case of utility, ideally, the synthetic version ˜D should be able to replace D in providing a training resource for models on relevant downstream applications. In other words, on target downstream tasks, models trained on the synthetic dataset ˜D are expected to have performance similar to the models trained on the original dataset D. More generally, distributional properties of the dataset D should be captured as much as possible in the synthetic version ˜D without violating the aforementioned privacy requirement. These will be extensively explored in Section 4. Conventionally, to generate synthetic text, an autoregressive language model (e.g. GPT-2 (Radford et al., 2019)) is trained on the original dataset and subsequently sampled using a sampling mechanism (e.g., beam search, top-k sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2020), etc.) to produce synthetic sequences. To make this operation differentially private, we adopt DP-SGD to fine-tune a pre-trained generative LM. The post-processing property of DP ensures that once the LM has been fine-tuned with DP, sampling from the model incurs no extra privacy loss. It would be desirable to synthesize examples with labels. We achieve this by building a conditional generator introduced in (Keskar et al., 2019) to provide more explicit control over text generation. By using so-called control codes (Keskar et al., 2019), the probability distribution of a text sequence x = (x 1 , x 2 , . . . , x n ) is conditioned on a control code c and decomposed as: A neural network p θ (·) is then trained to model each conditional distribution. The model can later be used to generate new samples conditioned on a control code c by sequentially sampling p θ (x 1 | c), p θ (x 2 | ˜x 1 , c), . . . , p θ (x m | ˜x 1 , . . . ˜x m−1 , c). The advantage of this approach is that it provides flexibility in the text generation of the model by allowing the conditional control codes to specify a particular style, domain, sentiment, or category. For example, feedback data collected from users on a set of products may contain product types and review scores associated with each data sample. Control codes can be constructed as c p,r = "Product type: p | Review score: r" for different product type (p) and review score (r) pairs. In our method, we utilize control codes to prepend each sample with its corresponding categories as a simple preprocessing step. During the text generation, this allows us to use the control codes to generate as many samples as the original categorical distribution is preserved. We point out that the categorical distribution in the original dataset may also be a piece of private information itself. However, its estimation could easily be privatized (Dwork and Roth, 2014) and for simplicity, we ignore the low-cost privacy loss of this step and use the exact categorical distribution of the original dataset in this paper. 