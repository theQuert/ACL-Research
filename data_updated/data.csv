background,objective,reference
"In that case, the measurement error is related to the variability between persons. Consequently, reliability parameters are highly dependent on the heterogeneity of the study sample, while the agreement parameters, based on measurement error, are more a pure characteristic of the measurement instrument. #CITATION_TAG he Bland-Altman method provides insight into the distribution of differences in relation to mean values.","Agreement parameters assess how close the results of the repeated measurements are, by estimating the measurement error in repeated measurements. Reliability parameters assess whether study objects, often persons, can be distinguished from each other, despite measurement errors.","['But if the aim is to measure change in health status, which is often the case in clinical practice, parameters of agreement are preferred.']"
"Decision-making animals can use slow-but-accurate strategies, such as making multiple comparisons, or opt for simpler, faster strategies to find a 'good enough' option. Social animals make collective decisions about many group behaviours including foraging and migration. The key to the collective choice lies with individual behaviour. We present a case study of a collective decision-making process (house-hunting ants, Temnothorax albipennis), in which a previously proposed decision strategy involved both quality-dependent hesitancy and direct comparisons of nests by scouts. This highlights the need to carefully design experiments to detect individual comparison. This parsimonious mechanism could promote collective rationality in group decision-making. Many individual decisions are informed by direct comparison of the alternatives. In collective decisions, however, only certain group members may have the opportunity to compare options. How do they do this? Switching by ants that had the opportunity to compare nests had little effect on nest choice. Colony-level comparison and choice can emerge, without direct comparison by individuals. Although lone T. rugatulus ant workers are capable of comparing the attributes of nest-sites which are very close together [25], the evidence for individual ants making direct comparisons between nests during colony emigration is weak [#CITATION_TAG], and furthermore, ant colonies are able to choose a distant good nest over a nearby poor nest, when recruitment latency differences would be expected to be cancelled out by travel time (Fig. 1a) [26, 27].","We used radio-frequency identification-tagged ants to monitor individual behaviour. When ants switched quickly between the two nests, colonies chose the good nest. Previously proposed mechanisms, recruitment latency and nest comparison, can be explained as side effects of this simple rule.",['We suggest a new mechanism of collective nest choice: individuals respond to nest quality by the decision either to commit or to seek alternatives.']
"Decision-making animals can use slow-but-accurate strategies, such as making multiple comparisons, or opt for simpler, faster strategies to find a 'good enough' option. Social animals make collective decisions about many group behaviours including foraging and migration. The key to the collective choice lies with individual behaviour. We present a case study of a collective decision-making process (house-hunting ants, Temnothorax albipennis), in which a previously proposed decision strategy involved both quality-dependent hesitancy and direct comparisons of nests by scouts. This highlights the need to carefully design experiments to detect individual comparison. This parsimonious mechanism could promote collective rationality in group decision-making. When its nest is damaged, a colony of the ant Leptothorax albipennis skillfully emigrates to the best available new site. When the number of ants at a site reaches a 'quorum threshold', the ants switch to rapid transport behaviour to carry the brood, queen and remaining nest-mates to the new nest [#CITATION_TAG].","At first, they summon fellow recruiters via tandem runs, in which a single follower is physically led all the way to the new site. They later switch to recruiting the passive majority of the colony via transports, in which nestmates are simply carried to the site. After this switch, tandem runs continue sporadically but now run in the opposite direction, leading recruiters back to the old nest. The recruitment switch is triggered by population increase at the new site, such that ants lead tandem runs when the site is relatively empty, but change to transport once a quorum of nestmates is present. A model shows that the quorum requirement can help a colony choose the best available site, even when few ants have the opportunity to compare sites directly, because recruiters to a given site launch the rapid transport of the bulk of the colony only if enough active ants have been ""convinced"" of the worth of the site.","['We investigated how this ability emerges from the behaviors used by ants to recruit nestmates to potential homes.', 'This exemplifies how insect societies can achieve adaptive colony-level behaviors from the decentralized interactions of relatively poorly informed insects, each combining her own limited direct information with indirect cues about the experience of her nestmates.']"
"Decision-making animals can use slow-but-accurate strategies, such as making multiple comparisons, or opt for simpler, faster strategies to find a 'good enough' option. Social animals make collective decisions about many group behaviours including foraging and migration. The key to the collective choice lies with individual behaviour. We present a case study of a collective decision-making process (house-hunting ants, Temnothorax albipennis), in which a previously proposed decision strategy involved both quality-dependent hesitancy and direct comparisons of nests by scouts. This highlights the need to carefully design experiments to detect individual comparison. This parsimonious mechanism could promote collective rationality in group decision-making. Prior studies of this decision-making process indicate that swarms attempt to use the best-of-N decision rule: sample some number (N) of alternatives and then select the best one. Although a honey bee swarm has bounded rationality (e.g., it lacks complete knowledge of the possible nesting sites), through its capacity for parallel processing it can choose a nest site without greatly reducing either the breadth or depth of its consideration of the alternative sites. In contrast, bees use a graded process, whereby scouts initially discovering a new nest-site almost always recruit [41], but the duration and rate of the recruiting waggle-dances are dependent on nest quality [#CITATION_TAG].","We tested how well swarms implement this decision rule by presenting them with an array of five nest boxes, only one of which was a high-quality (desirable) nest site; the other four were medium-quality (acceptable) sites. A dancing bee tunes her dance strength by adjusting the number of waggle-runs/dance, and she adjusts the number of waggle-runs/dance by changing both the duration and the rate of her waggle-run production. Differences in return-phase duration underlie the impression that dances differ in liveliness.",['This study views a honey bee swarm as a supraorganismal entity which has been shaped by natural selection to be skilled at choosing a future home site.']
"Decision-making animals can use slow-but-accurate strategies, such as making multiple comparisons, or opt for simpler, faster strategies to find a 'good enough' option. Social animals make collective decisions about many group behaviours including foraging and migration. The key to the collective choice lies with individual behaviour. We present a case study of a collective decision-making process (house-hunting ants, Temnothorax albipennis), in which a previously proposed decision strategy involved both quality-dependent hesitancy and direct comparisons of nests by scouts. This highlights the need to carefully design experiments to detect individual comparison. This parsimonious mechanism could promote collective rationality in group decision-making. There was rapid decay in the dance response; the number of dance circuits produced by a bee after visiting a site decreased linearly over sequential visits, and eventually each bee ceased visiting her site. This decay, or ;leakage', in the accumulation of bees at a site improves a swarm's decision-making ability by helping a swarm avoid making fast-decision errors. In contrast, bees use a graded process, whereby scouts initially discovering a new nest-site almost always recruit [#CITATION_TAG], but the duration and rate of the recruiting waggle-dances are dependent on nest quality [42].","We presented honeybee swarms with a two-alternative choice between a high-value site and a medium-value site and recorded the behavior of individually identifiable scout bees as they reported on these two alternatives. The first bee to find a site had a high probability of reporting the site with a waggle dance, regardless of its value.",['This study investigates the first stage of the decision-making process of a honeybee swarm as it chooses a nest site: how a scout bee codes the value of a potential nest site in the waggle dances she produces to represent this site.']
"The effectiveness of alcohol brief intervention (ABI) has been established by a succession of meta-analyses but, because the effects of ABI are small, null findings from randomized controlled trials are often reported and can sometimes lead to skepticism regarding the benefits of ABI in routine practice. This article first explains why null findings are likely to occur under null hypothesis significance testing (NHST) due to the phenomenon known as ""the dance of the p-values."" From the standpoint of scientific progress, the chief problem about null findings under the conventional NHST approach is that it is not possible to distinguish ""evidence of absence"" from ""absence of evidence."" At the time of writing, only the results for the PHC trial have been published (#CITATION_TAG) and the other two trials will not be covered here.","The hypothesis was that more intensive intervention would result in a greater reduction in hazardous or harmful drinking.Pragmatic cluster randomised controlled trial.Primary care practices in the north east and south east of England and in London.3562 patients aged 18 or more routinely presenting in primary care, of whom 2991 (84.0%) were eligible to enter the trial: 900 (30.1%) screened positive for hazardous or harmful drinking and 756 (84.0%) received a brief intervention. The sample was predominantly male (62%) and white (92%), and 34% were current smokers.Practices were randomised to three interventions, each of which built on the previous one: a patient information leaflet control group, five minutes of structured brief advice, and 20 minutes of brief lifestyle counselling.",['To evaluate the effectiveness of different brief intervention strategies at reducing hazardous or harmful drinking in primary care.']
"The effectiveness of alcohol brief intervention (ABI) has been established by a succession of meta-analyses but, because the effects of ABI are small, null findings from randomized controlled trials are often reported and can sometimes lead to skepticism regarding the benefits of ABI in routine practice. This article first explains why null findings are likely to occur under null hypothesis significance testing (NHST) due to the phenomenon known as ""the dance of the p-values."" From the standpoint of scientific progress, the chief problem about null findings under the conventional NHST approach is that it is not possible to distinguish ""evidence of absence"" from ""absence of evidence."" Reactivity to assessment has attracted recent attention in the brief alcohol intervention literature. In a review of such trials, it was calculated that control group participants reduce their drinking by approximately 20% (#CITATION_TAG, 36).","Primary studies were identified from existing reviews published in English language, peer-reviewed journals between 1995 and 2005. Change in alcohol consumption and selected study-level characteristics for each primary study were extracted. Consumption change data were pooled in random effects models and meta-regression was used to explore predictors of change. Extreme heterogeneity was identified and the extent of observed reduction in consumption over time was greater in studies undertaken in Anglophone countries, with single gender study participants, and without special targeting by age. Heterogeneity was reduced but was still substantial in a sub-set of 15 general population studies undertaken in English language countries.",['This systematic review sought to examine the nature of change in alcohol consumption over time in control groups in brief intervention studies.']
"The effectiveness of alcohol brief intervention (ABI) has been established by a succession of meta-analyses but, because the effects of ABI are small, null findings from randomized controlled trials are often reported and can sometimes lead to skepticism regarding the benefits of ABI in routine practice. This article first explains why null findings are likely to occur under null hypothesis significance testing (NHST) due to the phenomenon known as ""the dance of the p-values."" From the standpoint of scientific progress, the chief problem about null findings under the conventional NHST approach is that it is not possible to distinguish ""evidence of absence"" from ""absence of evidence."" Practices of data analysis in psychology and related disciplines are changing. This is evident in the longstanding controversy about statistical tests in the behavioral sciences and the increasing number of journals requiring effect size information. (NHST as taught in textbooks today is a hybrid of the Fisher and the Neyman-Pearson approaches and no distinctions between these two approaches will be discussed here.) Opponents of NHST would no doubt attribute the misunderstandings of null findings that we will shortly consider to basic flaws in the logic of NHST (#CITATION_TAG, 18).",Readers will learn how to measure effect size on continuous or dichotomous outcomes in comparative studies with independent or dependent samples. They will also learn how to calculate and correctly interpret confidence intervals for effect sizes. Numerous research examples from a wide range of areas illustrate the application of these principles and how to estimate substantive significance instead of just statistical significance.,"['Written in a clear and accessible style, the book is intended for applied researchers and students who may not have strong quantitative backgrounds.']"
"The effectiveness of alcohol brief intervention (ABI) has been established by a succession of meta-analyses but, because the effects of ABI are small, null findings from randomized controlled trials are often reported and can sometimes lead to skepticism regarding the benefits of ABI in routine practice. This article first explains why null findings are likely to occur under null hypothesis significance testing (NHST) due to the phenomenon known as ""the dance of the p-values."" From the standpoint of scientific progress, the chief problem about null findings under the conventional NHST approach is that it is not possible to distinguish ""evidence of absence"" from ""absence of evidence."" In surveys of health professionals' attitudes to this work, one of the most commonly encountered obstacles is ""lack of time"" or ""too busy"" (#CITATION_TAG, 33).","Qualitative focus group discussion method study applying the deductive framework approach. Six focus groups involving 18 general practitioners and 19 nurses were recruited from primary health care of the City of Tampere, Finland. Possible obstacles are: (1) confusion regarding the content of early-phase heavy drinking, (2) lack of self-efficacy among primary health care professionals, (3) sense of lacking time needed for carrying out brief intervention, (4) not having simple guidelines for brief intervention, (5) sense of difficulty in identifying of early-phase heavy drinkers, and (6) uncertainty about the justification for initiating discussion on alcohol issues with patients.","['The objective of this study was to identify possible obstacles to carrying out competent early identification and brief intervention (EIBI) of heavy drinkers in primary health care.', 'The main actions to be taken to promote brief intervention are to educate professionals about the content of early-phase heavy drinking and to produce directing, but not excessively demanding guidelines for carrying out EIBI.']"
"The effectiveness of alcohol brief intervention (ABI) has been established by a succession of meta-analyses but, because the effects of ABI are small, null findings from randomized controlled trials are often reported and can sometimes lead to skepticism regarding the benefits of ABI in routine practice. This article first explains why null findings are likely to occur under null hypothesis significance testing (NHST) due to the phenomenon known as ""the dance of the p-values."" From the standpoint of scientific progress, the chief problem about null findings under the conventional NHST approach is that it is not possible to distinguish ""evidence of absence"" from ""absence of evidence."" The possible effects of regression to the mean on control group participants in brief intervention trials were studied empirically by McCambridge and colleagues (#CITATION_TAG).","METHODS: 967 participants in a cohort study of alcohol consumption in New Zealand provided data at baseline and again six months later. We use graphical methods and apply thresholds of 8, 12, 16 and 20 in AUDIT scores to explore RTM. When a threshold score of 8 was used to select a subgroup, the observed mean change was approximately half of that observed without a threshold.",['We sought to test the hypothesis that the statistical artefact regression to the mean (RTM) explains part of the reduction in such studies.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. CONTEXT The US Food and Drug Administration recently recommended that CYP2C19 genotyping be considered prior to prescribing clopidogrel, but the American Heart Association and American College of Cardiologists have argued evidence is insufficient to support CYP2C19 genotype testing. DATA SOURCES PubMed and EMBASE from their inception to October 2011. Other limitations included selective outcome reporting and potential for genotype misclassification due to problems with the * allele nomenclature for cytochrome enzymes. Patients with loss of function polymorphism in the CYP2C19 gene are less responsive to clopidogrel [5, 6], although the importance of this phenomenon remains controversial [#CITATION_TAG] [8] [9] [10] and may be limited to the risk of stent thrombosis [11].","STUDY SELECTION Studies that reported clopidogrel metabolism, platelet reactivity or clinically relevant outcomes (cardiovascular disease [CVD] events and bleeding), and information on CYP2C19 genotype were included. DATA EXTRACTION We extracted information on study design, genotyping, and disease outcomes and investigated sources of bias. Six studies were randomized trials (""effect-modification"" design) and the remaining 26 reported individuals exposed to clopidogrel (""treatment-only"" design).",['OBJECTIVE To appraise evidence on the association of CYP2C19 genotype and clopidogrel response through systematic review and meta-analysis.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Among them, CYP2C19, a CYP enzyme whose activity is determined genetically, contributes predominantly to this bioactivation [3, #CITATION_TAG] and modulates the antiplatelet and therapeutic response to clopidogrel.","In the in vitro experiments using cDNA-expressed human P450 isoforms, clopidogrel was metabolized to 2-oxo-clopidogrel, the immediate precursor of its pharmacologically active metabolite. CYP1A2, CYP2B6, and CYP2C19 catalyzed this reaction. In the same system using 2-oxo-clopidogrel as the substrate, detection of the active metabolite of clopidogrel required the addition of glutathione to the system. Secondly, the contribution of each P450 involved in both oxidative steps was estimated by using enzyme kinetic parameters.",['The aim of the current study is to identify the human cytochrome P450 (P450) isoforms involved in the two oxidative steps in the bioactivation of clopidogrel to its pharmacologically active metabolite.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. This test -also referred to as the VASP index -specifically assesses the activity of the P2Y12 receptor [31] (the target of clopidogrel antiplatelet action), and is widely used for monitoring the responsiveness to clopidogrel [32, #CITATION_TAG].","For pharmacokinetic analysis, blood was drawn at 0, 20, 40, 60, 90, 120, 180, 240 and 360 min after clopidogrel loading and peak plasma concentrations (C(max)) of the AMC were quantified with liquid chromatography-tandem mass spectrometry (LC-MS/MS). Platelet function testing was performed at baseline and 360 min after the clopidogrel loading.The VASP-assay, the VerifyNow P2Y12-assay and 20 micromol L(-1) adenosine diphosphate (ADP)-induced light transmittance aggregometry (LTA) showed strong correlations with C(max) of the AMC (VASP: R(2) = 0.56, P < 0.001; VerifyNow platelet reactivity units (PRU): R(2) = 0.48, P < 0.001; VerifyNow %inhibition: R(2) = 0.59, P < 0.001; 20 micromol L(-1) ADP-induced LTA: R(2) = 0.47, P < 0.001).",['Multiple platelet function tests claim to be P2Y12-pathway specific and capable of capturing the biological activity of clopidogrel.The aim of the present study was to determine which platelet function test provides the best reflection of the in vivo plasma levels of the active metabolite of clopidogrel (AMC).Clopidogrel-naive patients scheduled for elective percutaneous coronary intervention (PCI) received a 600 mg loading dose of clopidogrel and 100 mg of aspirin.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Aspirin-clopidogrel antiplatelet dual therapy is widely prescribed worldwide, with PPIs frequently associated to prevent gastrointestinal bleeding. Concerns about PPI and clopidogrel interaction were raised when omeprazole was found to inhibit the antiplatelet effect of clopidogrel in an in vivo study of 124 patients undergoing elective coronary stent implantation [#CITATION_TAG].","METHODS: In this double-blind placebo-controlled trial, all consecutive patients undergoing coronary artery stent implantation received aspirin (75 mg/day) and clopidogrel (loading dose, followed by 75 mg/day) and were randomized to receive either associated omeprazole (20 mg/day) or placebo for 7 days.",['International audienceOBJECTIVES: This trial sought to assess the influence of omeprazole on clopidogrel efficacy.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. High on-clopidogrel platelet reactivity (HCPR) and high on-aspirin platelet reactivity (HAPR) are associated with atherothrombotic events following coronary stenting. There are, however, few data concerning high on-treatment platelet reactivity to both aspirin and clopidogrel simultaneously. HCPR and HAPR were established by receiver-operator characteristic curve analysis. The VASP index is considered as a specific test for evaluating P2Y12 inhibition, while light-transmission aggregometry is used to predict outcome during dual antiplatelet therapy, although both tests have a predictive value [31, 33, 39, #CITATION_TAG].",,['The aim of the present study was to determine the incidence of dual high on-treatment platelet reactivity (DAPR) and its impact on clinical outcome.On-treatment platelet reactivity was measured in parallel by ADP- and arachidonic acid-induced light transmittance aggregometry (LTA) (n=921) and the point-of-care VerifyNow system (P2Y12 and aspirin) (n=422) in patients on dual antiplatelet therapy undergoing elective stent implantation.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. (A Study of the Effects of Multiple Doses of Dexlansoprazole, Lansoprazole, Omeprazole or Esomeprazole on the Pharmacokinetics and Pharmacodynamics of Clopidogrel in Healthy Participants; NCT00942175 However, it was recently demonstrated that generation of clopidogrel active metabolite and inhibition of platelet function are reduced less by the co-administration of dexlansoprazole or lansoprazole with clopidogrel than by the co-administration of esomeprazole or omeprazole [#CITATION_TAG].","However, PPIs may inhibit CYP2C19, potentially reducing the effectiveness of clopidogrel.MethodsA randomized, open-label, 2-period, crossover study of healthy subjects (n = 160, age 18 to 55 years, homozygous for CYP2C19 extensive metabolizer genotype, confined, standardized diet) was conducted.","['ObjectivesThe aim of this study was to assess the effects of different proton pump inhibitors (PPIs) on the steady-state pharmacokinetics and pharmacodynamics of clopidogrel.BackgroundMetabolism of clopidogrel requires cytochrome P450s (CYPs), including CYP2C19.']"
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Data Synthesis: Recent attention has been placed on a potential interaction observed between clopidogrel and the widely used PPIs. However, this may not be a class effect. Rabeprazole is mainly metabolized by nonenzymatic reduction to rabeprazole thioether [41] and is a less potent inhibitor of CYP2C19 than omeprazole [14, #CITATION_TAG, 43].","In addition, reference citations from publications identified in the search were reviewed. Articles were selected if they were published in English and focused on any of the key words or appeared to have substantial content addressing the drug interaction. In situations in which both clopidogrel and a PPI are indicated, pantoprazole should be used since it is the PPI least likely to interact with clopidogrel.",['Objective: To evaluate the interaction between clopidogrel and proton pump inhibitors (PPIs).']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Proton pump inhibitors are used extensively for the treatment of gastric acid-related disorders because they produce a greater degree and longer duration of gastric acid suppression and, thus, better healing rates, than histamine H2 receptor antagonists. The need for long-term treatment of these disorders raises the potential for clinically significant drug interactions in patients receiving proton pump inhibitors and other medications. Proton pump inhibitors can modify the intragastric release of other drugs from their dosage forms by elevating pH (e.g. Proton pump inhibitors also influence drug absorption and metabolism by interacting with adenosine triphosphate-dependent P-glycoprotein (e.g. inhibiting digoxin efflux) or with the cytochrome P450 (CYP) enzyme system (e.g. decreasing simvastatin metabolism), thereby affecting both intestinal first-pass metabolism and hepatic clearance.Although interactions based on the change of gastric pH are a group-specific effect and thus may occur with all proton pump inhibitors, individual proton pump inhibitors differ in their propensities to interact with other drugs and the extent to which their interaction profiles have been defined. The interaction profiles of omeprazole and pantoprazole have been studied most extensively. A number of studies have shown that omeprazole carries a considerable potential for drug interactions, since it has a high affinity for CYP2C19 and a somewhat lower affinity for CYP3A4. Although the interaction profiles of esomeprazole, lansoprazole and rabeprazole have been less extensively investigated, evidence suggests that lansoprazole and rabeprazole seem to have a weaker potential for interactions than omeprazole.Although only a few drug interactions involving proton pump inhibitors have been shown to be of clinical significance, the potential for drug interactions should be taken into account when choosing a therapy for gastric acid-related disorders, especially for elderly patients in whom polypharmacy is common, or in those receiving a concomitant medication with a narrow therapeutic index. Rabeprazole is mainly metabolized by nonenzymatic reduction to rabeprazole thioether [41] and is a less potent inhibitor of CYP2C19 than omeprazole [14, 42, #CITATION_TAG].",reducing the antifungal activity of ketoconazole).,"['Therefore, it is important to understand the mechanisms for drug interactions in this setting.']"
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. In the target population, clopidogrel is usually prescribed with aspirin, and it has been suggested that inhibition of antiplatelet effect may result from an interaction of PPIs with aspirin absorption [#CITATION_TAG, 45], independent of the interaction with clopidogrel [46, 47].",Platelet activation was assessed by soluble serum P-selectin.,['Objective To evaluate the effect of proton pump inhibitors (PPIs) on the platelet response to aspirin in patients with coronary artery disease (CAD).']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. BACKGROUND Controversy remains on whether the dual use of clopidogrel and proton-pump inhibitors (PPIs) affects clinical efficacy of clopidogrel. PATIENTS All patients discharged after first-time myocardial infarction from 2000 to 2006. In the target population, clopidogrel is usually prescribed with aspirin, and it has been suggested that inhibition of antiplatelet effect may result from an interaction of PPIs with aspirin absorption [44, 45], independent of the interaction with clopidogrel [#CITATION_TAG, 47].","DESIGN A nationwide cohort study based on linked administrative registry data. Patients were examined at several assembly time points, including 7, 14, 21, and 30 days after myocardial infarction.",['OBJECTIVE To examine the risk for adverse cardiovascular outcomes related to concomitant use of PPIs and clopidogrel compared with that of PPIs alone in adults hospitalized for myocardial infarction.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Emerging data suggests that several proton pump inhibitors (PPIs), including omeprazole, might interfere with the antiplatelet action of clopidogrel. However, there is a lack of data for rabeprazole. To our knowledge, only one study has compared the effects of omeprazole and rabeprazole on the antiplatelet action of clopidogrel in patients on dual antiplatelet therapy [#CITATION_TAG].","Forty three and 44 patients were randomized to receive omeprazole 20 mg and rabeprazole 20 mg once daily, respectively, for at least 2 weeks. Adenosine 5-diphosphate 20 umol/L-induced platelet aggregation was performed before and after PPIs treatment. Mean maximal platelet aggregation (MPA) before and after PPIs treatment of both groups were compared. Use of these agents resulted in a similar degree of interference on clopidogrel's action, as measured by ADP-induced platelet aggregation.","['This study aimed to investigate and compare the impact of omeprazole and rabeprazole on the antiplatelet action of clopidogrel among patients with coronary artery disease (CAD).A prospective, randomized, open-labeled study was conducted among 87 CAD patients receiving clopidogrel and aspirin.']"
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Context High on-treatment platelet reactivity is associated with atherothrombotic events following coronary stent implantation. Design, Setting, and Patients Prospective, observational, single-center cohort study of 1069 consecutive patients taking clopidogrel undergoing elective coronary stent implantation between December 2005 and December 2007. The VASP index is considered as a specific test for evaluating P2Y12 inhibition, while light-transmission aggregometry is used to predict outcome during dual antiplatelet therapy, although both tests have a predictive value [31, 33, #CITATION_TAG, 40].","On-treatment platelet reactivity was measured in parallel by light transmittance aggregometry, Verify Now P2Y12 and Platelet works assays, and the IMPACT-R and the platelet function analysis system (PFA-100) (with the Dade PFA collagen/adenosine diphosphate (ADP) cartridge and Innovance PFA P2Y). Cutoff values for high on-treatment platelet reactivity were established by receiver operating characteristic curve (ROC) analysis.",['Objective To evaluate the capability of multiple platelet function tests to predict clinical outcome.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. The concomitant use of proton pump inhibitors (PPIs) with clopidogrel is suspected to be associated with an adverse impact on clinical outcomes in patients with coronary artery disease. Whether this occurs with all PPIs or is even of significant amplitude with omeprazole remains a matter of debate [9, [24] [#CITATION_TAG] [26] [27] [28] [29].","Patients were categorized into 2 groups: those taking a PPI [PPI (+), n=751] and those not taking a PPI [PPI (-), n=1900] at discharge. In addition, propensity-matched analysis was performed in 685 pairs of patients. The PPI (+) group was older and had more comorbid conditions than the PPI (-) group.",['We sought to evaluate whether the use of PPIs with clopidogrel was associated with worse clinical outcomes after percutaneous coronary intervention (PCI) compared with the use of clopidogrel alone.We studied 2651 consecutive patients discharged alive after coronary stenting for stable or unstable coronary artery disease between 2001 and 2007.']
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Patients receiving dual antiplatelet treatment with aspirin and clopidogrel are commonly treated with proton pump inhibitors (PPIs). Attenuating effects on platelet response to clopidogrel have been reported solely for the PPI omeprazole. Whether this occurs with all PPIs or is even of significant amplitude with omeprazole remains a matter of debate [9, [24] [25] [26] [27] [28] [#CITATION_TAG].","PPIs differ in their metabolisation properties as well as their potential for drug-drug interactions. In a cross-sectional observational study, consecutive patients under clopidogrel maintenance treatment (n = 1,000) scheduled for a control coronary angiography were enrolled. Adenosine diphosphate (ADP)-induced platelet aggregation (in AU*min) was measured with multiple electrode platelet aggregometry (MEA). Attenuating effects of concomitant PPI treatment on platelet response to clopidogrel were restricted to the use of omeprazole. Specifically designed and randomized clinical studies are needed to define the impact of concomitant PPI treatment on adverse events after percutaneous coronary intervention.","['The aim of this study was to investigate the impact of different PPIs (pantoprazole, omeprazole, esomeprazole) on platelet response to clopidogrel in patients with previous coronary stent placement under chronic clopidogrel treatment.']"
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Dual antiplatelet therapy with aspirin and clopidogrel has been shown to reduce subsequent cardiac events in patients with acute coronary syndrome or coronary artery stenting. Clopidogrel, a thienopyridine, is a prodrug that is transformed in vivo to an active metabolite by the cytochrome P450 (CYP) enzyme system. The genes encoding CYP enzymes are polymorphic. Recent data demonstrated patients carrying a genetic variant of CYP enzymes (e.g. Furthermore, concomitant gastrointestinal ulcer prophylaxis with a proton pump inhibitor (PPI) is commonly prescribed to patients because of the increased risk of bleeding with dual antiplatelet therapy. Patients with loss of function polymorphism in the CYP2C19 gene are less responsive to clopidogrel [#CITATION_TAG, 6], although the importance of this phenomenon remains controversial [7] [8] [9] [10] and may be limited to the risk of stent thrombosis [11].",PPIs are extensively metabolized by the cytochrome P450 system and have been associated with decreased antiplatelet activity of clopidogrel.,"['In this review, we will discuss the impact of CYP450 enzymes genetic variation and CYP450 pathway drug-drug interactions in pharmacological and clinical response to clopidogrel.2009 Elsevier Inc. All rights reserved.']"
"-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Whether this occurs with all PPIs or is even of significant amplitude with omeprazole remains a matter of debate [9, [#CITATION_TAG] [25] [26] [27] [28] [29].",Comparisons were made between proton pump inhibitor use and non-use.,['OBJECTIVE: To measure the association between use of proton pump inhibitors and a range of harmful outcomes in patients using clopidogrel and aspirin.']
"When sensory input allows for multiple, competing perceptual interpretations, observers' perception can fluctuate over time, which is called bistable perception. Imaging studies in humans have revealed transient responses in a right-lateralized network in the frontal-parietal cortex (rFPC) around the time of perceptual transitions between interpretations, potentially reflecting the neural initiation of transitions. Carefully controlling for the character of perceptual transitions has been shown to dramatically reduce the number of brain regions that are viable candidates for determining alternations (#CITATION_TAG).","When replay, instead, depicts transitions with the actual durations reported during rivalry, yoked transitions and genuine rivalry transitions elicit equal activity.","['We investigated the role of this activity in male human observers, with specific interest in its relation to the temporal structure of transitions, which can be either instantaneous or prolonged by periods during which observers experience a mix of both perceptual interpretations.']"
"Multistable perception is the spontaneous alternation between two or more perceptual states that occurs when sensory information is ambiguous. Multistable phenomena permit dissociation of neural activity related to conscious perception from that related to sensory stimulation, and therefore have been used extensively to study the neural correlates of consciousness. The finding that activity in a given brain region (e.g. frontoparietal cortex Kleinschmidt et al., 1998; Lumer and Rees, 1999; #CITATION_TAG; Weilnhammer et al., 2013) correlates with perception does not reveal whether that region drives the alternations, or reflects a consequence of processes occurring elsewhere.",,"['Here, we review recent work on the neural mechanisms underlying multistable perception and how such work has contributed to understanding the neural correlates of consciousness.', 'Particular emphasis is put on the role of high-level brain mechanisms that are involved in actively selecting and interpreting sensory information, and their interactions with lower-level processes that are more directly concerned with the processing of sensory stimulus properties']"
"Unlike most previous research that focused on the relationship between rs-fcMRI and a single behavioral measure of EF, in the current study we examined the relationship of rs-fcMRI with individual differences in subcomponents of EF. Neuroimaging has revealed that almost all functional networks that support aspects of task related processing have a comparable resting state network (Smith et al., 2009), and the integrity of these networks varies across individuals in a manner that is predictive of complex forms of cognition such as meta cognitive accuracy (Baird et al., 2013), spontaneous thought (Gorgolewski et al., 2014), reading comprehension (Smallwood et al., 2013) and executive control (#CITATION_TAG).","From these three measures, we derived estimates of common aspects of EF, as well as abilities specific to working memory updating and task shifting. Using Independent Components Analysis (ICA), we identified across the group of participants several networks of regions (Resting State Networks, RSNs) with temporally correlated time courses. We then used dual regression to explore how these RSNs covaried with individual differences in EF.","['The goal of the present study was to examine relationships between individual differences in resting state functional connectivity as ascertained by fMRI (rs-fcMRI) and performance on tasks of executive function (EF), broadly defined as the ability to regulate thoughts and actions.']"
"Neural connections, providing the substrate for functional networks, exist whether or not they are functionally active at any given moment. However, it is not known to what extent brain regions are continuously interacting when the brain is ""at rest."" Neuroimaging has revealed that almost all functional networks that support aspects of task related processing have a comparable resting state network (#CITATION_TAG), and the integrity of these networks varies across individuals in a manner that is predictive of complex forms of cognition such as meta cognitive accuracy (Baird et al., 2013), spontaneous thought (Gorgolewski et al., 2014), reading comprehension (Smallwood et al., 2013) and executive control (Reineberg et al., 2015).","Independently, we extract the major covarying networks in the resting brain, as imaged with functional magnetic resonance imaging in 36 subjects at rest. The sets of major brain networks, and their decompositions into subnetworks, show close correspondence between the independent analyses of resting and activation brain dynamics.","['In this work, we identify the major explicit activation networks by carrying out an image-based activation network analysis of thousands of separate activation maps derived from the BrainMap database of functional imaging studies, involving nearly 30,000 human subjects.']"
"The rapid growth of the literature on neuroimaging in humans has led to major advances in our understanding of human brain function but has also made it increasingly difficult to aggregate and synthesize neuroimaging findings. We verified the distinctness of these two networks using a large meta-analytic resting state database (www.neurosynth.org; #CITATION_TAG), that produced similar (though less extensive) networks, shown in Supplementary Figure S1.",,"['Here we describe and validate an automated brain-mapping framework that uses text-mining, meta-analysis and machine-learning techniques to generate a large database of mappings between neural and cognitive states.']"
"The mind flows in a ""stream of consciousness,"" which often neglects immediate sensory input in favor of focusing on intrinsic, self-generated thoughts or images. Although considerable research has documented the disruptive influences of task-unrelated thought for perceptual processing and task performance, the brain dynamics associated with these phenomena are not well understood. We often lose ourselves in our thoughts, decoupling experience from the here and now (#CITATION_TAG).","Using an experience sampling paradigm coupled with continuous high-density electroencephalography, we observed that task-unrelated thought was associated with a reduction of the P1 ERP, replicating prior observations that mind-wandering is accompanied by a reduction of the brain-evoked response to sensory input.","['Here we investigate the possibility, suggested by several convergent lines of research, that task-unrelated thought is associated with a reduction in the trial-to-trial phase consistency of the oscillatory neural signal in response to perceptual input.']"
"Registration is an important component of medical image analysis and for analysing large amounts of data it is desirable to have fully automatic registration methods. Many different automatic registration methods have been proposed to date, and almost all share a common mathematical framework - one of optimising a cost function. To date little attention has been focused on the optimisation method itself, even though the success of most registration methods hinges on the quality of this optimisation. We extracted the brain from the skull using the BET toolbox for both the FLAIR and the structural T1 weighted images and these scans were registered to standard space using FLIRT (#CITATION_TAG).","It is demonstrated that the use of local optimisation methods together with the standard multi-resolution approach is not sufficient to reliably find the global minimum. To address this problem, a global optimisation method is proposed that is specifically tailored to this form of registration. A full discussion of all the necessary implementation details is included as this is an important part of any practical method.",['This paper examines the assumptions underlying the problem of registration for brain images using inter-modal voxel similarity measures.']
"Information quality, or InfoQ, is 'the potential of a data set to achieve a specific goal by using a given empirical analysis method'. Official statistics are extraordinary sources of information. However, because of temporal relevance and chronology of data and goals, these fundamental sources of information are often not properly leveraged resulting in a poor level of InfoQ in the use of official statistics. This leads to low valued statistical analyses and to the lack of sufficiently informative results. This work is about integrated analysis of data collected as official statistics with administrative data from operational systems in order to increase the quality of information. Vines are undirected graphs, representing pair copula constructions, which are used to model the dependence structure of a set of variables. A more sophisticated approach to data integration is illustrated by Dalla Valle, #CITATION_TAG where data from surveys of companies in the north of Italy is combined with official data from the Italian stock exchange so as to calibrate the survey data.","The approach is based on two types of graphical models: vines and non-parametric Bayesian belief nets (NPBBNs). NPBBNs are directed graphs, that use pair copulas to model the dependencies, and allow US for diagnosis and prediction via conditionalization. The illustrated methodologies are applied to two financial datasets, the first one containing data collected through a survey and the second one containing official statistics data.","['Abstract The aim of this paper is to propose a novel approach to integrate financial information, incorporating the dependence structure among the variables in the model.', 'This approach permits to aggregate information and to calibrate the results obtained with different sources of data.']"
"Information quality, or InfoQ, is 'the potential of a data set to achieve a specific goal by using a given empirical analysis method'. Official statistics are extraordinary sources of information. However, because of temporal relevance and chronology of data and goals, these fundamental sources of information are often not properly leveraged resulting in a poor level of InfoQ in the use of official statistics. This leads to low valued statistical analyses and to the lack of sufficiently informative results. This work is about integrated analysis of data collected as official statistics with administrative data from operational systems in order to increase the quality of information. From the Publisher:  Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. For an introduction and for more details about the definitions and main results see, for example, Cowell, 19 Jensen 20, 21 or Pearl #CITATION_TAG; for the use of BNs for problem solving and model building see Fenton and Neil.","Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation.","['The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic.', 'The author distinguishes syntactic and semantic approaches to uncertaintyand offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational.', 'Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences.']"
"The Bose-Hubbard model is the simplest model of interacting bosons on a lattice. It has recently been the focus of much attention due to the realization of this model with cold atoms in an optical lattice. The ability to tune parameters in the Hamiltonian as a function of time in cold atom systems has opened up the possibility of studying out-of-equilibrium dynamics, including crossing the quantum critical region of the model in a controlled way. In this paper, I give a brief introduction to the Bose Hubbard model, and its experimental realization and then give an account of theoretical and experimental efforts to understand out-of-equilibrium dynamics in this model, focusing on quantum quenches, both instantaneous and of finite duration. In recent years, quantum phase transitions have attracted the interest of both theorists and experimentalists in condensed matter physics. These transitions, which are accessed at zero temperature by variation of a non-thermal control parameter, can influence the behavior of electronic systems over a wide range of the phase diagram. Quantum phase transitions occur as a result of competing ground state phases. The cuprate superconductors which can be tuned from a Mott insulating to a d-wave superconducting phase by carrier doping are a paradigmatic example. An interesting separate class of transitions are boundary phase transitions where only degrees of freedom of a subsystem become critical; this will be illustrated in a few examples. It is relatively straightforward to calculate the mean field phase diagram as a function of / and / [1, [30] [#CITATION_TAG] [32] as described in Section 2.1 which leads to the well-known Mott insulator lobes illustrated in Figure 1.","Several classes of transitions will be briefly reviewed, pointing out, e.g., conceptual differences between ordering transitions in metallic and insulating systems.","['This review introduces important concepts of phase transitions and discusses the interplay of quantum and classical fluctuations near criticality.', 'The main part of the article is devoted to bulk quantum phase transitions in condensed matter systems.', 'The article is aimed on bridging the gap between high-level theoretical presentations and research papers specialized in certain classes of materials.']"
"The Bose-Hubbard model is the simplest model of interacting bosons on a lattice. It has recently been the focus of much attention due to the realization of this model with cold atoms in an optical lattice. The ability to tune parameters in the Hamiltonian as a function of time in cold atom systems has opened up the possibility of studying out-of-equilibrium dynamics, including crossing the quantum critical region of the model in a controlled way. In this paper, I give a brief introduction to the Bose Hubbard model, and its experimental realization and then give an account of theoretical and experimental efforts to understand out-of-equilibrium dynamics in this model, focusing on quantum quenches, both instantaneous and of finite duration. When atoms are loaded into an optical lattice, the process of gradually turning on the lattice is almost adiabatic. which is in accordance with calculations using QMC [#CITATION_TAG] and an analytic strong coupling approach [198] both of which suggest V ∼ ( /) −1.","To do so we calculate the entropy in the single-band Bose-Hubbard model for various densities, interaction strengths and temperatures in one and two dimensions for homogeneous and trapped systems. Our theory is able to reproduce the experimentally observed visibilities and therefore strongly supports that current experiments remain in the quantum regime for all considered lattice depths with low temperatures and minimal heating.Comment: 18 pages, 24 figur",['In this paper we investigate how the temperature changes when going from the gapless superfluid phase to the gapped Mott phase along isentropic lines.']
"Large river valleys have long been seen as important factors to shape the mobility, communication, and exchange of Pleistocene hunter-gatherers. However, rivers have been debated as either natural entities people adapt and react to or as cultural and meaningful entities people experience and interpret in different ways. Both ecological and cultural factors are crucial to explaining these patterns. Whereas the Earlier Upper Paleolithic record displays a general tendency toward conceiving rivers as mobility guidelines, the spatial consolidation process after the colonization of the European mainland is paralleled by a trend of conceptualizing river regimes as frontiers, separating archaeological entities, regional groups, or local networks. The Late Upper Paleolithic Magdalenian, however, is characterized again by a role of rivers as mobility and communication vectors. Here, we attempt to integrate both perspectives. Tracing changing patterns in the role of certain river regimes through time thus contributes to our growing Progressive environmental developments have left a strong imprint on the area's Palaeolithic record. The Middle Pleistocene record is divided into two palaeogeographical stages: the pre-Anglian/Elsterian stage, during which a wide land bridge existed between England and Belgium even during marine highstands; and the Anglian/Elsterian to Saalian interglacial, with a narrower land bridge, lowered by proglacial erosion but not yet fully eroded. The Late Pleistocene landscape was very different, with the land bridge fully dissected by an axial Rhine-Thames valley, eroded deep enough to fully connect the English Channel and the North Sea during periods of highstand. Moreover, the river occupied an extensive area in the landscape with multiple migratory channels, minor tributaries, and a massive floodplain, including swamp and marsh environments (e.g., #CITATION_TAG).","A detailed geological reconstruction of The Netherlands' south-west offshore area provides a stratigraphical context for archaeological and palaeontological finds. We highlight aspects of landscape evolution and related taphonomical changes, visualized in maps for critical periods of the Pleistocene in the wider southern North Sea region. This tripartite staging implies great differences in (i) possible migration routes of herds of herbivores as well as hominins preying upon them, (ii) the erosion base of axial and tributary rivers causing an increase in the availability of flint raw materials and (iii) conditions for loess accumulation in northern France and Belgium and the resulting preservation of Middle Palaeolithic sites.",['This paper links research questions in Quaternary geology with those in Palaeolithic archaeology.']
"Large river valleys have long been seen as important factors to shape the mobility, communication, and exchange of Pleistocene hunter-gatherers. However, rivers have been debated as either natural entities people adapt and react to or as cultural and meaningful entities people experience and interpret in different ways. Both ecological and cultural factors are crucial to explaining these patterns. Whereas the Earlier Upper Paleolithic record displays a general tendency toward conceiving rivers as mobility guidelines, the spatial consolidation process after the colonization of the European mainland is paralleled by a trend of conceptualizing river regimes as frontiers, separating archaeological entities, regional groups, or local networks. The Late Upper Paleolithic Magdalenian, however, is characterized again by a role of rivers as mobility and communication vectors. Here, we attempt to integrate both perspectives. Tracing changing patterns in the role of certain river regimes through time thus contributes to our growing During the recent ten years or so there has been a distinct shift in social sciences towards studying things as real objects in themselves instead of treating them simply as correlative (Meillassoux 2008) of human social order or, ultimately, thought. It is also Ian Hodder's view that things have often not received the attention they deserve in archaeology. In his definition of what a thing is, Hodder follows a somewhat Heideggerian line of thinking. For Hodder objects become things when they enter the human realm. Things are for humans while objects always remain partly withdrawn from relations, as GrahamHarman (2005) would argue. At the beginning of his book Hodder claims that most recent thing-oriented approaches in archaeology have concentrated on what things can do for people, while the objective should have been to study the things themselves. Hodder's entanglement theory is based on three 'axioms': 1) humans depend on things; 2) things depend on other things; 3) things depend on humans. The relationship between humans and things is of a dialectic kind, humans and things constantly moving closer and further away from each other. This movement is not simply movement between two entities, but always includes a third entity the presence of which means that things are always moving towards something and, at the same time, further away from something. Things and persons become identified in this process, but it is also what constitutes ownership, an important notion in social archaeology. Moving closer to a thing identifies me with it while moving away from a thing identifies me as an individual thing. The listing of things, something Ian Bogost (2012, p. 38) has labelled 'Latour litanies', is an effective way to remind us of how things are surrounded by other things and how intimately things depend on other things even in such a simple operation as fire making, not to mention the up to 20,000 parts needed for a modern car to function, both Hodder's examples of entanglement. Things are connected in various ways. In fact Hodder uses the term 'thing' as synonymous with 'drawing together'. Things not only draw together people, but other things as well. In explaining the ways things are connected, Hodder explores the term 'network'. In Hodder's view 'network' implies too much dependence (as in 'it depends'), and he replaces network with entanglement. Others have done the same. Tim Ingold (2008), for example, has replaced network with 'meshwork', a term similar to Timothy Morton's (2010) 'mesh' (implying infinite connections and infinitesimal differences). Ian Bogost (2012) has proposed the term 'mess'. The main reason Hodder seems to abandon networks is that things are not equal in their affordances. Some things are more central than others. While stating that things depend on humans may seem at first like tautology (after all humans are things), things' dependence on humans is not at all a trivial notion. As noted above, some things are more central than others. As humans have dispersed over the whole globe, they have become, on a global scale, what Levi Bryant (2012), one of the prominent object-oriented philosophers, would call a rogue object, an object that emerges out of nowhere and changes everything. Human dispersal on earth surely has been such an event. Each spatial feature has therefore to be understood as an entanglement of natural properties and sociocultural dimensions (e.g., Gamble 1993; Tilley 1994; Rockman 2003; Meskell and Preucel 2004; Edgeworth 2011; #CITATION_TAG).","Hodder provides an extensive array of examples of how things are connected to other things.Althoughhe never refers to any authors of the so-called objectoriented philosophies, his approach clearly shows some of the same characteristics. Unlike network, mess resists neat compartmentalization and order. Network for Hodder does not convey the 'stickiness' of dependence between things and humans.",['This is why he sets out to provide a study of entanglement from the viewpoint of things.']
"Large river valleys have long been seen as important factors to shape the mobility, communication, and exchange of Pleistocene hunter-gatherers. However, rivers have been debated as either natural entities people adapt and react to or as cultural and meaningful entities people experience and interpret in different ways. Both ecological and cultural factors are crucial to explaining these patterns. Whereas the Earlier Upper Paleolithic record displays a general tendency toward conceiving rivers as mobility guidelines, the spatial consolidation process after the colonization of the European mainland is paralleled by a trend of conceptualizing river regimes as frontiers, separating archaeological entities, regional groups, or local networks. The Late Upper Paleolithic Magdalenian, however, is characterized again by a role of rivers as mobility and communication vectors. Here, we attempt to integrate both perspectives. Tracing changing patterns in the role of certain river regimes through time thus contributes to our growing How can anyone be rational in a world where knowledge is limited, time is pressing, and deep thought is often an unattainable luxury? Traditional models of unbounded rationality and optimization in cognitive science, economics, and animal behavior have tended to view decision-makers as possessing supernatural powers of reason, limitless knowledge, and endless time. But understanding decisions in the real world requires a more psychologically plausible notion of bounded rationality. In Simple heuristics that make us smart (Gigerenzer et al. These simple heuristics perform comparably to more complex algorithms, particularly when generalizing to new data--that is, simplicity leads to robustness. Viewed in this light, heuristics can even be seen as integral components of the human Badaptive toolbox^ (#CITATION_TAG; Boudry et al. 2015; Polonioli 2015).","1999), we explore fast and frugal heuristics--simple rules in the mind's adaptive toolbox for making decisions with realistic mental resources. In this precis, we show how simple building blocks that control information search, stop search, and make decisions can be put together to form classes of heuristics, including: ignorance-based and one-reason decision making for choice, elimination models for categorization, and satisficing heuristics for sequential search.",['These heuristics can enable both living organisms and artificial systems to make smart choices quickly and with a minimum of information by exploiting the way that information is structured in particular environments.']
"Large river valleys have long been seen as important factors to shape the mobility, communication, and exchange of Pleistocene hunter-gatherers. However, rivers have been debated as either natural entities people adapt and react to or as cultural and meaningful entities people experience and interpret in different ways. Both ecological and cultural factors are crucial to explaining these patterns. Whereas the Earlier Upper Paleolithic record displays a general tendency toward conceiving rivers as mobility guidelines, the spatial consolidation process after the colonization of the European mainland is paralleled by a trend of conceptualizing river regimes as frontiers, separating archaeological entities, regional groups, or local networks. The Late Upper Paleolithic Magdalenian, however, is characterized again by a role of rivers as mobility and communication vectors. Here, we attempt to integrate both perspectives. Tracing changing patterns in the role of certain river regimes through time thus contributes to our growing A majority of laymen, politicians and scholars consciously or subconsciously understand settled living as the highest rung on the evolutionary ladder. Accounts of people surviving and even thriving in peripheral areas are often instrumental to construct and maintain the dichotomy between 'the desert and the sown.' It is sometimes stated that mobile peoples obtain their material culture from neighboring settled populations, rather than produce their own, and that they do not leave recognizable archaeological traces apart from 'ephemeral campsites.' From the 24 chapters in this volume, however, it is clear that there is indeed an 'archaeology of mobility. Such an archaeology of mobility encompasses much more than tracing ephemeral campsites. Much like any other group, mobile people produce, appear to use and discard a distinct material culture which includes functional objects, art and architecture. There have been edited books on the archaeology of nomadism in various regions, and there have been individual archaeological and anthropological monographs, but nothing with the kind of coverage provided in this volume. It presents many new ideas and thoughtful approaches, especially in the Central Asian region Archaeologists, consequently, have ever since attempted to tackle the Bspatiality^of past human social units from a whole range of different angles (e.g., Shott 1986; Kelly 1992; Close 2000; Brantingham 2006; #CITATION_TAG; Grove 2009 Grove, 2010 Turq et al. 2013; Cameron 2013; Van Dommelen 2014).","'By applying specific and well-defined methods, it is eminently possible to come to a better understanding of mobile people in archaeological contexts.",['Its strength and importance lie in the fact that it brings together a world-wide collection of studies of the archaeology of mobility.This book provides a ready-made reference to this world-wide phenomenon and is unique in that it tries to redefine pastoralism within a larger context by the term mobility.']
"Large river valleys have long been seen as important factors to shape the mobility, communication, and exchange of Pleistocene hunter-gatherers. However, rivers have been debated as either natural entities people adapt and react to or as cultural and meaningful entities people experience and interpret in different ways. Both ecological and cultural factors are crucial to explaining these patterns. Whereas the Earlier Upper Paleolithic record displays a general tendency toward conceiving rivers as mobility guidelines, the spatial consolidation process after the colonization of the European mainland is paralleled by a trend of conceptualizing river regimes as frontiers, separating archaeological entities, regional groups, or local networks. The Late Upper Paleolithic Magdalenian, however, is characterized again by a role of rivers as mobility and communication vectors. Here, we attempt to integrate both perspectives. Tracing changing patterns in the role of certain river regimes through time thus contributes to our growing Imperfections in encoding either relations can introduce imperfections in representations of environments in memory. This, together with individual differences in human spatial abilities, can result in data manipulations that produce error. When information stored in long term memory is brought into working memory for purposes of decision making and choice behavior (as in route selection), the result may be the selection of an inefficient or incorrect path. Blind, vision impaired, and sighted volunteers traveled and learned routes of approximately the same length (1.2miles) in their respective urban environments. To begin with, if we accept the two-phase dispersal model of Aurignacian technologies, with a pioneer and a consolidated phase reflected in different sociocultural signatures (Davies 2001 (Davies, 2007, the conditions of landscape knowledge and learning deserve special attention (e.g., #CITATION_TAG; Kelly 2003).","An initial trial was experimenter guided; three following trials were regarded as ""test"" trials where the participants learned the route and performed route fixing tasks including pointing between designated places, verbally describing the route after each completion, and building a model of the route using metallic strips on a magnetic board.","['In this paper we discuss the relations between cognitive maps, spatial abilities and human wayfinding, particularly in the context of traveling without the use of sight.', 'Initially we discuss the nature of cognitive maps and the process of cognitive mapping as mechanisms for developing person to object (egocentric) and object to object (allocentric) internal representations.', 'We explore the connection between environmental learning and cognitive maps in the context of learning a route in two different cultural environments-Belfast (Northern Ireland) and Santa Barbara (California).']"
"Biological theories of sexual orientation, typically presented in human sexuality classes, are considered by many social psychologists to cause reductions in students' sexual prejudice. Yet when biological theories were not presented to 36 psychology students in a 10-week seminar on lesbian, gay, bisexual and transgender (LGBT) psychology, both sexual prejudice and two forms of essentialist thinking reduced significantly. Enrolled students reported increased exposure to issues of homosexuality since entering college, and many had sexual minority friends. #CITATION_TAG assessed students' prejudice and their interest in 26 topics at the beginning and end of a course titled 'The Psychology of Homosexuality'.",We investigated who enrolled in a class about sexual diversity and what they most wanted to learn. Students left the class with significantly decreased homophobia.,"[""In this study, we examined the impact of a course, the Psychology of Homosexuality, on heterosexual students' attitudes toward and knowledge about sexual minorities (i.e., lesbians, gay men, bisexual men and women, and transgendered persons).""]"
"Biological theories of sexual orientation, typically presented in human sexuality classes, are considered by many social psychologists to cause reductions in students' sexual prejudice. Yet when biological theories were not presented to 36 psychology students in a 10-week seminar on lesbian, gay, bisexual and transgender (LGBT) psychology, both sexual prejudice and two forms of essentialist thinking reduced significantly. Psychological essentialism is an ordinary mode of category representation that has powerful social-psychological consequences. Why and when people engage in this mode of thinking remain open questions. Variability in essentialism across cultures, categories, and contexts suggests that this mode of representing human categories is rooted in a naturalistic theory of category origins, combined with a need to explain differences that cross category boundaries. Although belief in the biological determination of sexual orientation is correlated with tolerance towards lesbians and gay men, beliefs in biological determination are also correlated with prejudice and stereotyping of other minority groups (c.f., Bastian & Haslam, 2006; Keller, 2005; Martin & Parker, 1995; Morton, Postmes, Haslam, & Hornsey, 2009; #CITATION_TAG; Williams & Eberhardt, 2008; Yzerbyt, Rocher, & Schadron, 1997).",,"['This article reviews those consequences, with a focus on the distinctive ways people perceive, evaluate, and interact with members of human categories they essentialize.']"
"Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. 2] [23] Evidence from four well-designed trials demonstrated that arthroscopic interventions 10 24 and meniscectomy [25] [26] [#CITATION_TAG] were no better or provided no additional effect, than the comparator (ie, sham surgery, physical therapy or a combination of physical and medical therapy) to relieve pain and improve function in the middle-aged patients with knee OA or early signs of knee OA.",Radiographic examination was done before randomization and after 5 years. The patients were randomly assigned to either arthroscopic treatment followed by exercise therapy for 2 months or to the same exercise therapy alone.,"['The aim of this prospective randomized intervention study was to evaluate the outcome at a 2 and 5 year follow-up whether combined arthroscopic surgery followed by exercise therapy was superior to the same exercise therapy alone when treating non-traumatic, degenerative medial meniscal tears.Ninety-six middle-aged patients with MRI-verified degenerative medial meniscus tear and radiographic osteoarthritis grade <=1 (Ahlback) participated in the study.']"
"Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. 2] [23] Evidence from four well-designed trials demonstrated that arthroscopic interventions 10 24 and meniscectomy [#CITATION_TAG] [26] [27] were no better or provided no additional effect, than the comparator (ie, sham surgery, physical therapy or a combination of physical and medical therapy) to relieve pain and improve function in the middle-aged patients with knee OA or early signs of knee OA.",,['The aim was to evaluate knee function and physical activity.']
"Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. #CITATION_TAG[4][5] More importantly, however, recent studies have shown substantial patient-reported disability and pain in patients up to 4 years after surgery","Knee extensor and flexor strength was evaluated at four different velocities (60, 120, 180, and 240 degrees/sec) preoperatively and every 2 weeks from weeks 2-12 postsurgery. Eight subjects were evaluated on a Cybex II+ and 14 subjects were evaluated on a Cybex II isokinetic device. A repeated measures analysis of variance was used to determine possible side (involved and uninvolved), speed (60, 120, 180, and 240 degrees/sec), or time (preoperative, 2, 4, 6, 8, 10, and 12 weeks postoperatively) effects as well as possible interactions between these factors.",['The purpose of this study was to investigate the time course of spontaneous recovery (no supervised training) of muscle torques in the first 3 months postarthroscopic partial meniscectomy.']
"Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. Total work and average power developed by the quadriceps and hamstrings during the fatigue protocol changed with time in a similar manner to torque. [3][4]#CITATION_TAG More importantly, however, recent studies have shown substantial patient-reported disability and pain in patients up to 4 years after surgery","Training was done on the same device (three times a week for 1-2 months), beginning either 2 or 6 weeks post-op.",['SummaryThe aim of this study was to assess the effects in humans of early (2 weeks) and delayed (6 weeks) isokinetic strength training in the recovery of muscle strength following an arthroscopic partial meniscectomy.']
"Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. Meniscectomy patients have an increased risk of developing knee OA. 2 7] [#CITATION_TAG] One explanation for the poor selfreported outcomes may be that the loss of meniscal function triggers other events that may cause knee pain.","Reduced muscle strength is suggested as a risk factor for knee osteoarthritis (OA). The Knee Injury and Osteoarthritis Outcome Score (KOOS) was used to determine self-reported outcomes.No differences were detected in any muscle strength variables between the operated and nonoperated leg (mean +- SD quadriceps maximum voluntary contraction of 2.80 +- 0.10 for the operated leg and 2.88 +- 0.10 for the nonoperated leg), between patients and controls (mean +- SD torque of 2.70 +- 0.09 Nm x kg(-1) for the controls; P = 0.26 for main effect leg), or in objectively measured function (P >= 0.27).","['The aim of this study was to identify reductions in different aspects of muscle strength as well as objectively measured and self-reported lower extremity function in middle-aged patients who had undergone a meniscectomy compared with controls.Thirty-one patients who had undergone surgery in 2006 and 2007 (mean +- SD age 46 +- 6 years, mean +- SD body mass index [BMI] 26 +- 4 kg/m(2), and mean +- SD postsurgery 21 +- 6 months) and 31 population-based controls (mean +- SD age 46 +- 6 years and mean +- SD BMI 26 +- 4 kg/m(2)) were examined for maximal muscle strength and rapid force capacity, distance achieved during the one-leg hop test, and the maximum number of knee bends performed in 30 seconds.']"
"Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. 10 11 critical limitation of previous studies [#CITATION_TAG] [13] [14] [15] is their failure to account for the type of symptom onset (ie, injury mechanism).",MATERIALS AND METHODS: 435 patients of both sexes and different age groups underwent meniscectomy after their isolated meniscal injuries were clinically diagnosed and confirmed by nuclear magnetic resonance imaging.,['PURPOSE: To evaluate the results of the treatment of patients with isolated meniscal injuries of different etiologies.']
"The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. These genetic markers have applications in the study and management of both species. In order to elucidate and analyze the two biogeographic hypotheses about the connections or disconnections between the Zirahuén, Pátzcuaro and Cupatitzio basins, and the possibility of a species translocation, we examined the evolutionary history of Allotoca diazi complex using two different molecular markers described for the Goodeinae Subfamily [9, #CITATION_TAG, 26]: a conserved molecular marker of mitochondrial DNA (Cytochrome b, Cytb gene) and a less conserved microsatellite nuclear markers.",We also present a preliminary survey of the variability of nine of these microsatellites in both Z. tequila and the congeneric wild species Z. quitzeoensis.,"['This report describes the characterization of 13 microsatellites in Zoogoneticus tequila, a goodeid fish that is extinct in nature and exists only in captivity.']"
"The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. There has long been speculation as to the relationship between climate, humans and the environment. Until recently, however, it has proved difficult to establish the degree to which these factors are interlinked. Fluctuations in the level of Lake Pátzcuaro during the Holocene associated with climate change, human activity [90, 91, [94] [95] [#CITATION_TAG] [97] [98] [99], and occurrence of tsunamis [100] could be involved to the isolation and demographics changes of A. diazi and A. meeki.",,['Here we draw on evidence that has recently emerged from a series of investigations in central Mexico to evaluate the long-term human impact on the environment and to establish the impact that late Holocene changes in the climate have had on the indigenous populations that lived on the arid frontier of Mesoamerica.']
"The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. There is a growing number of studies that use historical sources to reconstruct recent environmental change. In Mexico there are abundant sources of information that can be used to determine variations in the climate over the historical period. The Aztecs are believed to have entered the Basin of Mexico during a period of severe drought. Wet conditions, however, characterized much of the Aztec (AD 1345-1521) and early colonial period prior to a return to drier conditions in the 1640s. Between 1640 and 1915, a series of severe droughts caused widespread devastation throughout central Mexico, particularly during the mid- to late 1700s and late 1800s. Since the early 1900s this region has experienced slightly wetter conditions. Although the palaeoclimatic records from central Mexico are similar, there is greater divergence between the records from central and northern Mexico. In general, fluctuations in the climate over the last 600 years can be explained by changes in the relative strength of the summer 'monsoon', but it is possible that ENSO events can cause considerable differences in prevailing climate conditions in the northern and central parts of the country. Fluctuations in the level of Lake Pátzcuaro during the Holocene associated with climate change, human activity [90, 91, [94] [95] [96] [#CITATION_TAG] [98] [99], and occurrence of tsunamis [100] could be involved to the isolation and demographics changes of A. diazi and A. meeki.",The climatic record from the Basin of Mexico is compared with proxy climatic data from two other areas in Mexico: a detailed lake-level curve from Lake Patzcuaro in central Mexico and the few tree-ring records from northern Mexico.,['In this paper we draw together information from a variety of primary and secondary historical sources to reconstruct fluctuations in the climate of central Mexico over the last 600 years.']
"The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. Land degradation is frequently cited as a factor in the collapse of ancient complex societies. Implicit in these tales of ecological suicide is the assumption that land degradation is an ecological rather than a social problem. Tecto-volcanic events giving rise to the separation of Pátzcuaro and Zirahuén lakes began 8000 years ago with the formation and activity of the La Tasa volcano southwest of Lake Pátzcuaro and climate fluctuations causing decline in water level and drying of streams [21, [#CITATION_TAG] [91] [92] [93].","I then discuss the implications of this perspective using evidence from a recent landscape project exploring diachronic relationships between environmental and social transformations in the development of the Precolumbian Tarascan (Purepecha) empire, centered in the Lake Patzcuaro Basin, Mexico.","['Here, I discuss how land degradation can be reconceptualized as a social-environmental dialectic.']"
"The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. Last year marked the 10th anniversary of the birth of phylogeography as a formal discipline. However, the field's gestation began in the mid-1970s with the introduction of mitochondrial (mt) DNA analyses to population genetics, and to the profound shift toward genealogical thought at the intraspecific level (now formalized as coalescent theory) that these methods prompted. The genetic distances among the three species in Cytb are smaller than reported to sister Goodeinae species (1.7-11%) [9] and sister species, congeneric species, and cofamilial genera within and across the major vertebrate taxonomic classes (~2%) [76] [77] [#CITATION_TAG] [79].",,"['This paper traces the early history and explosive growth of phylogeography, and closes with predictions about future challenges for the field that centre on several facets of genealogical concordance.']"
"The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. # Abstract Molecular clocks have profoundly influenced modern views on the timing of important events in evolutionary history. On the population genetic scale, we review advances in the incorporation of ancestral population processes into the estimation of divergence times between recently separated species. The isolation of A. diazi, A. meeki, and A. catarinae is reflected in the low genetic differences, non-monophyletic patterns, shared haplotypes, and genetic groups assignment, and we considered that the species complex consist in an incomplete lineage sorting pattern [72] [#CITATION_TAG] [74] [75].","On the phylogenetic scale, we address the complexities of DNA sequence evolution as they relate to estimating divergences, focusing on models of nucleotide substitution and problems associated with among-site and among-lineage rate variation. Throughout the review we emphasize new statistical methods and the importance of model testing during the process of divergence time estimation.","['We review recent advances in estimating divergence times from molecular data, emphasizing the continuum between processes at the phylogenetic and population genetic scales.']"
"The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. In this paleoenvironment, the presence of the terrestrial mammals was favored by the availability of relatively abundant mesophytic vegetation, which was seemingly perturbed by the volcanic exhalations associated to the monogenetic volcanism of the Michoacan-Guanajuato volcanic field. The isolation of the Allotoca diazi complex from its common ancestor A. zacapuensis, does not contradict the hypothesis of Álvarez (1972) [21] with respect to the connection between the Zacapu and Pátzcuaro basins during the Pleistocene, resulted from the formation of the El Zirate mountain and the northern Pátzcuaro Lake shoreline during the Late Pleistocene [#CITATION_TAG].","This column contains vegetation remnants and bone fragments, including a well preserved jaw of a gomphothere, which was recovered from a volcanic lahar deposit intercalated with fluvial deposits and a pyroclastic succession associated to the basaltic monogenetic volcanism of the Cerro Catio. The complete volcanic sequence was emplaced within a fluvial endorreic basin restricted to the northern portion of the Patzcuaro lake. This late Pleistocene age can be assigned to the gomphothere as well as to the basaltic volcanic event of the Cerro Catio. Moist climatic conditions favored the devitrification process of the volcanic ash and the weathering of other minerals, which provided good conditions for recovering of the mesophillic vegetation: Fraxinus, Acer, Corylus, Ulmus, Betula, and Juglans.","['The stratigraphic, petrographic and pollinic description of the fluvial volcanic-sedimentary column of the Barranca Rancho Viejo, Tzintzuntzan and north of the Patzcuaro lake is presented.']"
"The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. Recent studies have focused on the relationship between the marine fauna of the Eastern Atlantic and the Mediterranean Sea, but within the Atlantic, little is known about genetic relationships between populations of the Macaronesian islands. The genetic, biological, and ecological information obtained in this investigation, along with relevant published information [64, #CITATION_TAG], was used for identification of conservation units.",We combined phylogeographic and coalescent approaches using the fast evolving mitochondrial control region gene. Migration across the three archipelagos was estimated and a prevailing northwest trend was detected.,['We propose that this difference reflects differences in glaciating extents in the Northeastern Atlantic and the Mediterranean.']
"Solar radiation and ambient temperature have acted as selective physical forces among populations and thereby guided species distributions in the globe. Circadian clocks are universal and evolve when subjected to selection, and their properties contribute to variations in fitness within specific environments. Because of their position in the hierarchy and repressive actions, cryptochromes are the key components of the feedback loops on which circadian clocks are built. It has been shown that warming patients prior to and during (18)F-FDG uptake by controlling the room temperature can decrease uptake by brown adipose tissue (BAT). This effect is greatest in the summer and winter. Brown adipose tissue may in fact be an active pacemaker tissue, having activity in a range of ultradian (Ootsuka et al., 2009) to infradian (#CITATION_TAG) oscillations.",Patients over 22 years of age and those who received pre-medication known to reduce FDG uptake by BAT were excluded. One hundred and three patients were warmed to 24 degrees C prior to scanning.,['The aim of this study is to determine if this effect is subject to seasonal variation.A retrospective review was conducted of all patients referred for whole-body (18)F-FDG PET between December 2006 and December 2008.']
"Solar radiation and ambient temperature have acted as selective physical forces among populations and thereby guided species distributions in the globe. Circadian clocks are universal and evolve when subjected to selection, and their properties contribute to variations in fitness within specific environments. Because of their position in the hierarchy and repressive actions, cryptochromes are the key components of the feedback loops on which circadian clocks are built. CONTEXT Positron emission tomography (PET)-computed tomography (CT) has identified metabolically active supraclavicular fat in adult humans based on uptake of labeled glucose and confirmed to be brown adipose tissue (BAT) histologically. However, PET-CT has estimated a prevalence of BAT as low as 5% in adult humans, casting doubt on its significance. PET-positive fat harbored multilobulated lipid droplets and stained strongly for uncoupling protein 1 (UCP1). BAT is highly prevalent in adult humans, and its abundance determines PET status. It is currently evident that brown adipose tissue is a highly active tissue, rather than only ""a form of embryonic adipose tissue"" (Sheldon, 1924) or the so-called hibernating gland, and it is highly prevalent in adult humans (#CITATION_TAG).","SETTING The study was conducted at a tertiary referral hospital. PATIENTS Seventeen patients who underwent preoperative PET-CT for staging of head and neck malignancy participated in the study. By contrast, PET-negative fat contained a predominance of cells with unilobulated lipid droplets, with scattered cells containing multilobulated lipid droplets and variable UCP1 staining.",['OBJECTIVE The objective of the study was to determine whether BAT is present in PET-negative supraclavicular fat.']
"Solar radiation and ambient temperature have acted as selective physical forces among populations and thereby guided species distributions in the globe. Circadian clocks are universal and evolve when subjected to selection, and their properties contribute to variations in fitness within specific environments. Because of their position in the hierarchy and repressive actions, cryptochromes are the key components of the feedback loops on which circadian clocks are built. Sleep deprivation (SD) results in increased electroencephalographic (EEG) delta power during subsequent non-rapid eye movement sleep (NREMS) and is associated with changes in the expression of circadian clock-related genes in the cerebral cortex. The increase of NREMS delta power as a function of previous wake duration varies among inbred mouse strains. It is strengthened further by those which show that the CRY2 gene expression is abnormal when inbred-strain mice with the intrinsic level of high anxiety are deprived of sleep (#CITATION_TAG), and when humans with bipolar type 1 disorder do remain depressed after the antidepressant sleep deprivation (Lavebratt et al., 2010).","Cortical expression of clock genes subsequent to SD was proportional to the increase in delta power that occurs in inbred strains: the strain that exhibits the most robust EEG response to SD (AKR/J) exhibited dramatic increases in expression of bmal1, clock, cry2, csnkIepsilon, and npas2, whereas the strain with the least robust response to SD (DBA/2) exhibited either no change or a decrease in expression of these genes and cry1.",['We sought to determine whether SD-dependent changes in circadian clock gene expression parallel this strain difference described previously at the EEG level.']
"Solar radiation and ambient temperature have acted as selective physical forces among populations and thereby guided species distributions in the globe. Circadian clocks are universal and evolve when subjected to selection, and their properties contribute to variations in fitness within specific environments. Because of their position in the hierarchy and repressive actions, cryptochromes are the key components of the feedback loops on which circadian clocks are built. Plasma levels of corticosterone exhibit both circadian and ultradian rhythms. The circadian component of these rhythms is regulated by the suprachiasmatic nucleus (SCN). Furthermore, there is an ultradian rhythm of e.g., free corticosterone in the blood (Qian et al., 2012; #CITATION_TAG) that translates into synchronized rhythms of free glucocorticoid hormone in peripheral (the subcutaneous tissue) and central (the hippocampus) tissues (Qian et al., 2012).",Two approaches were used to dissociate the hypothalamic-pituitary-adrenal (HPA) axis from normal circadian input in rats: (i) exposure to a constant light (LL) environment and (ii) electrolytic lesioning of the SCN. Blood was sampled using an automated sampling system.,['Our studies investigate the importance of the SCN in regulating ultradian rhythmicity.']
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. The Case Register Interactive Search (CRIS) system enabled searching and retrieval of anonymised information since 2008. Deaths were identified by regular national tracing returns after 2006. The main limitation was the setting of secondary mental health care provider in SLAM.Substantially higher mortality persists in people with serious mental illness, substance use disorders and depressive disorders. Increased mortality is particularly associated with people who have substance misuse disorders and severe mental illness (psychosis) (#CITATION_TAG).",Standardized mortality ratios (SMRs) were calculated for the period 2007 to 2009 using SLAM records for this period and the expected number of deaths from age-specific mortality statistics for the England and Wales population in 2008.,"['Our aim was to characterize vulnerable groups for excess mortality among people with SMI, substance use disorders, depressive episode, and recurrent depressive disorder.A case register was developed at the South London and Maudsley National Health Services Foundation Trust (NHS SLAM), accessing full electronic clinical records on over 150,000 mental health service users as a well-defined cohort since 2006.']"
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. CONTEXT In 2002, an estimated 877,000 lives were lost worldwide through suicide. Some developed nations have implemented national suicide prevention plans. Although these plans generally propose multiple interventions, their effectiveness is rarely evaluated. Studies, published between 1966 and June 2005, included those that evaluated preventative interventions in major domains; education and awareness for the general public and for professionals; screening tools for at-risk individuals; treatment of psychiatric disorders; restricting access to lethal means; and responsible media reporting of suicide. Ascertaining which components of suicide prevention programs are effective in reducing rates of suicide and suicide attempt is essential in order to optimize use of limited resources. Several health problems are associated with increased risk of suicide, and depression appears to be the most important mental disorder for suicidal ideation and behaviour among all age groups (#CITATION_TAG).","DATA SOURCES AND STUDY SELECTION Relevant publications were identified via electronic searches of MEDLINE, the Cochrane Library, and PsychINFO databases using multiple search terms related to suicide prevention. DATA EXTRACTION Data were extracted on primary outcomes of interest: suicidal behavior (completion, attempt, ideation), intermediary or secondary outcomes (treatment seeking, identification of at-risk individuals, antidepressant prescription/use rates, referrals), or both. Included articles were those that reported on completed and attempted suicide and suicidal ideation; or, where applicable, intermediate outcomes, including help-seeking behavior, identification of at-risk individuals, entry into treatment, and antidepressant prescription rates. We included 3 major types of studies for which the research question was clearly defined: systematic reviews and meta-analyses (n = 10); quantitative studies, either randomized controlled trials (n = 18) or cohort studies (n = 24); and ecological, or population- based studies (n = 41). Heterogeneity of study populations and methodology did not permit formal meta-analysis; thus, a narrative synthesis is presented. Other methods including public education, screening programs, and media education need more testing.",['OBJECTIVES To examine evidence for the effectiveness of specific suicide-preventive interventions and to make recommendations for future prevention programs and research.']
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. BACKGROUND There is a growing body of research in the United States to suggest that men are less likely than women to seek help from health professionals for problems as diverse as depression, substance abuse, physical disabilities and stressful life events. Previous research has revealed that the principle health related issue facing men in the UK is their reluctance to seek access to health services. However, the growing body of gender-specific studies highlights a trend of delayed help seeking when they become ill. A prominent theme among white middle class men implicates ""traditional masculine behaviour"" as an explanation for delays in seeking help among men who experience illness. The reasons and processes behind this issue, however, have received limited attention. The effects of health problems may be compounded by different approaches to seeking help between men and women, with men seeking help less frequently than women (#CITATION_TAG), particularly for psychological problems (Smith et al 2006).","METHOD The investigation of men's health-related help seeking behaviour has great potential for improving both men and women's lives and reducing national health costs through the development of responsive and effective interventions. A search of the literature was conducted using CINAHL, MEDLINE, EMBASE, PsychINFO and the Cochrane Library databases.","[""AIM This paper reviews the key research literature regarding men's health-related help seeking behaviour.""]"
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. RESULTS Contact with primary care providers in the time leading up to suicide is common. About one in five suicide victims had contact with mental health services within a month before their suicide. On average, 45% of suicide victims had contact with primary care providers within 1 month of suicide. Ideas of suicide, acts of self-harm and completed suicide are associated with mental health problems, with approximately 90% of people who complete suicide having a diagnosable mental disorder, although only half of these individuals will have had a history of involvement with mental health services (#CITATION_TAG).",METHOD The authors reviewed 40 studies for which there was information available on rates of health care contact and examined age and gender differences among the subjects.,['OBJECTIVE This study examined rates of contact with primary care and mental health care professionals by individuals before they died by suicide.']
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. An increased risk of death from all causes is not restricted to the most severe mental illnesses, but is also associated with conditions such as depression and anxiety disorders (#CITATION_TAG).","I also discuss the health care that psychiatric patients receive, both in terms of recognition of physical illness and subsequent intervention, with particular reference to cardiovascular disease.","['In this review, I examine the physical health of psychiatric patients, especially those with schizophrenia or depression and some possible explanations for any inequities in their health status.']"
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. BACKGROUND Little is known about the general population prevalence or severity of DSM-IV mental disorders. For example, the teenage years are associated increasingly with incidence of mental health problems, with half of all lifetime cases of mental illness commencing by the age of 14 (#CITATION_TAG), exacerbated by increased incidence of relapse and persistence.","DESIGN AND SETTING Nationally representative face-to-face household survey conducted between February 2001 and April 2003 using a fully structured diagnostic interview, the World Health Organization World Mental Health Survey Initiative version of the Composite International Diagnostic Interview. PARTICIPANTS Nine thousand two hundred eighty-two English-speaking respondents 18 years and older.","['OBJECTIVE To estimate 12-month prevalence, severity, and comorbidity of DSM-IV anxiety, mood, impulse control, and substance disorders in the recently completed US National Comorbidity Survey Replication.']"
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. No recent attempt has been made to synthesize information on mortality and depression despite the theoretical and practical interest in the topic. Depression is associated with a near doubling of all-cause mortality rates (#CITATION_TAG Dewey 2001, Cuijpers and Smit 2002).",,['Our objective was to estimate in the older population the influence on mortality of depression and depressive symptoms.']
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. In women, unemployment increases the risk of suicide regardless of the number of follow-up years. There is an increased risk of suicide and deliberate self-harm in men and women who are unemployed (#CITATION_TAG), although findings suggest that risks may be increased for unemployed men (Ying and Chang 2009).",,"['The purpose of the study was to examine the effect of employment status measured at baseline on the risk of suicide by years of follow-up, using a large nationally representative sample of the US population.Cox regression models were applied to data from the National Longitudinal Mortality Study, based on the 1979-1989 follow-up.']"
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. The topic of suicide has long been an important socioeconomic issue studied in many countries. Suicides inject an atmosphere of unrest into society, and media attention furthers that social uneasiness. From the viewpoint of economics and management, suicide is a waste of human resource: it decreases the labor force in society and deteriorates human capital. As a result, a low income family with an unemployed man and an employed woman is at high risk for adult male suicide. There is an increased risk of suicide and deliberate self-harm in men and women who are unemployed (Kposowa 2001), although findings suggest that risks may be increased for unemployed men (#CITATION_TAG).","Aggregate data from G7 countries are obtained and stacked into panel data for analysis. Even though suicide issues have been extensively discussed in the past, newly developed econometric tools are applied to her.",['This paper provides a series of analyses of suicide rate based on theoretical reasoning and empirical approaches.']
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. Although less data exist for nonaffective psychosis, available evidence suggests that median age-of-onset is in the range late teens through early 20s. Roughly half of all lifetime mental disorders in most studies start by the mid-teens and three quarters by the mid-20s. Later onsets are mostly secondary conditions. Severe disorders are typically preceded by less severe disorders that are seldom brought to clinical attention. SUMMARY: First onset of mental disorders usually occur in childhood or adolescence, although treatment typically does not occur until a number of years later. Anxiety disorders other than phobiaswhich include generalised anxiety disorder and panic disorder, depression, alcohol and substance misuse disorders, and schizophreniabegin most commonly between the late teens and early adulthood (#CITATION_TAG).",,"['PURPOSE OF REVIEW: The aim of this article is to review recent epidemiological research on age-of-onset of mental disorders, focusing on the WHO World Mental Health surveys.']"
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. The NHS National Institute for Health and Clinical Excellence (NICE, 2009) publication - Depression in adults with a chronic physical health problem: treatment and management - is a clinical practice guideline for the UK which partially updates and extends the earlier depression management in primary and secondary care guideline (NICE, 2004). There are variations in the suicide rates for different parts of the world, with the highest rates in the Russian Federation, Baltic States, Sri Lanka and Japan, and the lowest rates in Latin America (#CITATION_TAG).","Like other clinical guidelines, it has been systematically developed from the best available research evidence to assist clinicians and patients, and service commissioners and providers in making decisions about the most appropriate treatment and service organisation for this important area of health care need. The National Collaborating Centre for Mental Health (one of a number of centres established by NICE for the purpose of clinical guideline development) together with a guideline development group, comprising health and social care professionals, lay and patient representatives, and technical experts, worked on the guidance. The process from initial scope preparation to the production of the final guideline took over two years.",['The aim of this editorial is to bring this important document to the attention of nurses and midwives worldwide who work with depression sufferers']
"Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. BACKGROUND Excessive drinking is a significant cause of mortality, morbidity and social problems in many countries. Interventions usually take the form of a conversation with a primary care provider and may include feedback on the person's alcohol use, information about potential harms and benefits of reducing intake, and advice on how to reduce consumption. Brief interventions can also include behaviour change or motivationally-focused counselling.This is an update of a Cochrane Review published in 2007. 'Brief intervention' was defined as a conversation comprising five or fewer sessions of brief advice or brief lifestyle counselling and a total duration of less than 60 minutes. Most studies (61 studies, 88%) compared brief intervention to minimal or no intervention. Few studies targeted particular age groups: adolescents or young adults (6 studies, 9%) and older adults (4 studies, 6%). Around 10-15% of people respond to these interventions, with men appearing more likely than women to reduce alcohol use following advice about behaviour change (#CITATION_TAG).","SEARCH METHODS We searched the Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE, and 12 other bibliographic databases to September 2017. We searched Alcohol and Alcohol Problems Science Database (to December 2003, after which the database was discontinued), trials registries, and websites. We carried out handsearching and checked reference lists of included studies and relevant reviews. SELECTION CRITERIA We included randomised controlled trials (RCTs) of brief interventions to reduce hazardous or harmful alcohol consumption in people attending general practice, emergency care or other primary care settings for reasons other than alcohol treatment. DATA COLLECTION AND ANALYSIS We used standard methodological procedures expected by Cochrane. We carried out subgroup analyses where possible to investigate the impact of factors such as gender, age, setting (general practice versus emergency care), treatment exposure and baseline consumption. Most interventions were delivered in general practice (38 studies, 55%) or emergency care (27 studies, 39%) settings. Extended interventions were compared with brief (4 studies, 6%), minimal or no intervention (7 studies, 10%). Main sources of bias were attrition and lack of provider or participant blinding. With two exceptions, studies were funded by government institutes, research bodies or charitable foundations. One study was partly funded by a pharmaceutical company and a brewers association, another by a company developing diagnostic testing equipment.","['Brief interventions aim to reduce alcohol consumption and related harm in hazardous and harmful drinkers who are not actively seeking help for alcohol problems.', 'OBJECTIVES To assess the effectiveness of screening and brief alcohol intervention to reduce excessive alcohol consumption in hazardous or harmful drinkers in general practice or emergency care settings.']"
"Especially in northern Wisconsin, bogs are relatively unaffected by humans, but naturally comprise \1% of the landscape. Bog specialist species composition varied by bog type (muskeg, kettlehole, coastal peatland). A number of bog specialists frequently occurred in numerous examples of bogs, including all three types. But virtually no specialist individuals occurred in nearby upland roadsides. Bogs have different vegetation types superimposed on each other, including bog, heath, forest, sedge meadow, and wet meadow associates in the same spots. Conservation management needs to avoid simplifying the vegetation to one layer, reducing specialist fauna. A fundamental lesson may be that aiming to conserve typical ecosystems, even if native, and their average processes, leads to average (generalist) butterflies. Looking for qualified reading sources? We have the vegetation of wisconsin an ordination of plant communities to review, not just check out, yet likewise download them and even read online. Once more, never miss out on to review online and download this publication in our website below. We have got a considerable collection of totally free of expense Book for people from every single stroll of life. Searching for most offered book or reading resource worldwide? Well, merely read online or download by registering in our site here. Although often viewed as a long-lived successional stage between open water and forest in glaciated landscapes, peatlands can get reset to an earlier successional stage (#CITATION_TAG).","Obtain the documents in the types of txt, zip, kindle, word, ppt, pdf, and also rar. GO TO THE TECHNICAL WRITING FOR AN EXPANDED TYPE OF THIS THE VEGETATION OF WISCONSIN AN ORDINATION OF PLANT COMMUNITIES, ALONG WITH A CORRECTLY FORMATTED VERSION OF THE INSTANCE MANUAL PAGE ABOVE.",['Our goal is always to offer you an assortment of cost-free ebooks too as aid resolve your troubles.']
"This article discusses the possible response of the large-scale atmospheric structure to a warmer climate. Regarding the circulation, most models show a slight expansion and weakening of the Hadley Cell, depending on season and hemisphere. The expansion is small and largely confined to winter but with some expansion in Southern Hemisphere summer. The weakening occurs principally in the Northern Hemisphere but the intermodel scatter is large. There is also a general polewards shift in surface westerlies, but the changes are small and again are little larger than the intermodel variability in the change. There is a robust strengthening in the Southern Hemisphere surface winds across seasons. In the Northern Hemisphere there is a slight strengthening in the westerlies in most models in winter but a consistent weakening of the westerlies in summer. In that sense the dynamical changes are less robust given the current state of knowledge and simulation, although one cannot conclude that they are, in principle, unknowable or less predictable. Water vapor is not only Earth's dominant greenhouse gas. Through the release of latent heat when it condenses, it also plays an active role in dynamic processes that shape the global circulation of the atmosphere and thus climate. Here we present an overview of how latent heat release affects atmosphere dynamics in a broad range of climates, ranging from extremely cold to extremely warm. Contrary to widely held beliefs, atmospheric circulation statistics can change non-monotonically with global-mean surface temperature, in part because of dynamic effects of water vapor. For example, the strengths of the tropical Hadley circulation and of zonally asymmetric tropical circulations, as well as the kinetic energy of extratropical baroclinic eddies, can be lower than they presently are both in much warmer climates and in much colder climates. In addition, we try to assess whether the response is consistent with or explainable by simple physical arguments, some of which have been presented in the literature before (see also the review by #CITATION_TAG).","We discuss how latent heat release is implicated in such circulation changes, particularly through its effect on the atmospheric static stability, and we illustrate the circulation changes through simulations with an idealized general circulation model.","['This allows us to explore a continuum of climates, constrain macroscopic laws governing this climatic continuum, and place past and possible future climate changes in a broader context.Comment: 22 pages, 11 figure']"
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Forest structure and dynamics vary across the Amazon Basin in an east-west gradient coincident with variations in soil fertility and geology. Basin-wide differences in stand-level turnover rates are mostly influenced by soil physical properties with variations in rates of coarse wood production mostly related to soil phosphorus status. A hypothesis of self-maintaining forest dynamic feedback mechanisms initiated by edaphic conditions is proposed. However, allometric equations that relate physical attributes of trees to their above-ground biomass normally rely on three parameters: in addition to tree height (H), tree diameter at 1.3 m (D) and wood density (ρ) are very important (Chave et al., 2005), and mean values and ratios between these parameters vary significantly between regions (Chave et al., 2005; Feldpausch et al., 2012; #CITATION_TAG), associated with different species communities (ter Steege et al., 2006).","Soil samples were collected in a total of 59 different forest plots across the Amazon Basin and analysed for exchangeable cations, carbon, nitrogen and pH, with several phosphorus fractions of likely different plant availability also quantified. Physical properties were additionally examined and an index of soil physical quality developed. Bivariate relationships of soil and climatic properties with above-ground wood productivity, stand-level tree turnover rates, above-ground wood biomass and wood density were first examined with multivariate regression models then applied. Taking this into account, otherwise enigmatic variations in stand-level biomass across the Basin were then accounted for through the interacting effects of soil physical and chemical properties with climate.","['This has resulted in the hypothesis that soil fertility may play an important role in explaining Basin-wide variations in forest biomass, growth and stem turnover rates.']"
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Tropical tree height-diameter (H:D) relationships may vary by forest type and region making large-scale estimates of above-ground biomass subject to bias if they ignore these differences in stem allometry. Annual precipitation coefficient of variation (PV), dry season length (SD), and mean annual air temperature (TA) emerged as key drivers of variation in H:D relationships at the pantropical and region scales. Forests in Asia, Africa and the Guyana Shield all have, on average, similar H:D relationships, but with trees in the forests of much of the Amazon Basin and tropical Australia typically being shorter at any given D than their counterparts elsewhere. Some of the plot-to-plot variability in H:D relationships not accounted for by this model could be attributed to variations in soil physical conditions. The relationship between diameter and height also varies across the basin, but with more complexity than wood density, mostly related to climatic factors (#CITATION_TAG).",2. to ascertain if the H:D relationship is modulated by climate and/or forest structural characteristics (e.g. 3. to develop H:D allometric equations and evaluate biases to reduce error in future local-to-global estimates of tropical forest biomass.,"['Utilising this database, our objectives were: 1. to determine if H:D relationships differ by geographic region and forest type (wet to dry forests, including zones of tension where forest and savanna overlap).']"
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Protection of large natural forest landscapes is a highly important task to help fulfill different international strategic initiatives to protect forest biodiversity, to reduce carbon emissions from deforestation and forest degradation, and to stimulate sustainable forest management practices. The vast majority of IFL are found in two biomes: Dense Tropical and Subtropical Forests (45.3%) and Boreal Forests (43.8%). The IFL exist in 66 of the 149 countries that together make up the forest zone. Therefore the maps are most comparable in undisturbed forest areas, so all comparisons were performed in Intact Forest Landscape (IFL) (#CITATION_TAG) areas only, with the exception of the analysis of recent deforestation.",We have created a global IFL map using existing fine-scale maps and a global coverage of high spatial resolution satellite imagery.,"['This paper introduces a new approach for mapping large intact forest landscapes (IFL), defined as an unbroken expanse of natural ecosystems within areas of current forest extent, without signs of significant human activity, and having an area of at least 500 km 2.', 'The world IFL map presented here is intended to underpin the development of a general strategy for nature conservation at the global and regional scales.']"
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Large-scale patterns of Amazonian biodiversity have until now been obscured by a sparse and scattered inventory record. The most diverse forests for any given DSL are concentrated in a narrow latitudinal band just south of the equator, while the least diverse forests for any given DSL are found in the Guayana Shield and Amazonian Bolivia. In order to test this, we use a unique dataset of 413 field plots located throughout tropical South America, compiled as part of RAINFOR (Red Amazónica de Inventarios Forestales; Amazon Forest Inventory Network; Malhi et al., 2002), the Amazon Tree Diversity Network (#CITATION_TAG), TEAM (Tropical Ecology Assessment and Monitoring) and PPBio (Brazilian Program for Biodiversity Research) (Fig. 1).","Here we present the first comprehensive spatial model of tree a-diversity and tree density in Amazonian rainforests, based on the largest-yet compilation of forest inventories and bolstered by a spatial interpolation technique that allows us to estimate diversity and density in areas that have never been inventoried. These data were then compared to continent-wide patterns of rainfall seasonality. Denser forests are more diverse than sparser forests, even when we used a measure of diversity that corrects for sample size.",['We propose that rainfall seasonality regulates tree a-diversity and tree density by affecting shade tolerance and subsequently the number of different functional types of trees that can persist in an area.']
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Yet, even after accounting for these, tropical forest architecture varies significantly from continent to continent. The greater stature of tropical forests in Asia is not directly determined by the dominance of the family Dipterocarpaceae, as on average non-dipterocarps are equally tall. The key ecological parameters associated with differing AGB are basal area, wood density and D:H ratios, all of which vary across the basin (Baker et al., 2004; ter Steege et al., 2006; #CITATION_TAG; Feldpausch et al., 2012; Quesada et al., 2012), but none of which is directly detected by RS.","Methods: Using height and diameter data from 20,497 trees in 112 non-contiguous plots, asymptotic maximum height (H AM) and height-diameter relationships were computed with nonlinear mixed effects (NLME) models to: (1) test for environmental and structural causes of differences among plots, and (2) test if there were continental differences once environment and structure were accounted for; persistence of differences may imply the importance of biogeography for vertical forest structure. NLME analyses for floristic subsets of data (only/excluding Fabaceae and only/excluding Dipterocarpaceae individuals) were used to examine whether family-level patterns revealed biogeographical explanations of cross-continental differences. We hypothesise that dominant large-statured families create conditions in which only tall species can compete, thus perpetuating a forest dominated by tall individuals from diverse families.","['Aim: To test the extent to which the vertical structure of tropical forests is determined by environment, forest structure or biogeographical history.']"
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Uncertainty in biomass estimates is one of the greatest limitations to models of carbon flux in tropical forests. Previous comparisons of field-based estimates of the aboveground biomass (AGB) of trees greater than 10 cm diameter within Amazonia have been limited by the paucity of data for western Amazon forests, and the use of site-specific methods to estimate biomass from inventory data. In addition, the role of regional variation in stand-level wood specific gravity has not previously been considered. We know that wood density increases from west to east across Amazonia (#CITATION_TAG; ter Steege et al., 2006), inversely correlated to stem turnover rate (Quesada et al., 2012).","Using data from 56 mature forest plots across Amazonia, we consider the relative roles of species composition (wood specific gravity) and forest structure (basal area) in determining variation in AGB.","['This pattern is due to the higher diversity and abundance of taxa with high specific gravity values in central and eastern Amazonia, and the greater diversity and abundance of taxa with low specific gravity values in western Amazonia.', 'The variation in specific gravity is important because it determines the regional scale, spatial pattern of AGB.']"
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. While wood density has been historically measured at 12% moisture, it is convenient for ecological purposes to convert this measure to basic wood density, i.e., the ratio of dry mass over green volume. individually for each stem using the region-specific Weibull models from Feldpausch et al. (2012), and wood density values estimated for each stem using the mean value for the species in the Global Wood Density Database (Chave et al., 2009; #CITATION_TAG), or the mean for the genus using congeneric taxa from Mexico, Central America and tropical South America if no data were available for that species (K DHρ).","It is used to characterize species performance and fitness in community ecology and to compute tree and forest biomass in carbon cycle studies. Basic wood density can then be used to compute tree dry biomass from living tree volume. Methods: Here, we derive a new exact formula to compute the basic wood density Db from the density at moisture content w denoted Dw, the fiber saturation point S, and the volumetric shrinkage coefficient R. We estimated a new conversion factor using a global wood technology database where values to use this formula are available for 4022 trees collected in 64 countries (mostly tropical) and representing 872 species.",['Premise of the Study : Basic wood density is an important ecological trait for woody plants.']
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Abstract The Amazon basin is likely to be increasingly affected by environmental changes: higher temperatures, changes in precipitation, CO2 fertilization and habitat fragmentation. In order to test this, we use a unique dataset of 413 field plots located throughout tropical South America, compiled as part of RAINFOR (Red Amazónica de Inventarios Forestales; Amazon Forest Inventory Network; #CITATION_TAG), the Amazon Tree Diversity Network (ter Steege et al., 2003), TEAM (Tropical Ecology Assessment and Monitoring) and PPBio (Brazilian Program for Biodiversity Research) (Fig. 1).","The network will focus on sample plots established by independent researchers, some providing data extending back several decades. We will also conduct rapid transect studies of poorly monitored regions. Field expeditions analysed local soil and plant properties in the first phase (2001-2002). The network will also serve as a forum for discussion between researchers, with the aim of standardising sampling techniques and methodologies that will enable Amazonian forests to be monitored in a coherent manner in the coming decades.","['To examine the important ecological and biogeochemical consequences of these changes, we are developing an international network, RAINFOR, which aims to monitor forest biomass and dynamics across Amazonia in a co-ordinated fashion in order to understand their relationship to soil and climate.']"
"Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. At current status of negotiation five forest-related activities have been listed to be implemented as mitigation actions by developing countries, namely: reducing emissions from deforestation (which implies a land-use change) and reducing emissions from forest degradation, conservation of forest carbon stocks, sustainable management of forest, Enhancement of forest carbon stocks (all relating to carbon stock changes and GHG emissions within managed forest land use). The UNFCCC negotiations and related country submissions on REDD+ have advocated that methodologies and tools become available for estimating emissions and removals from deforestation and forest land management with an acceptable level of certainty. Amazonia contains half of all remaining tropical moist forest (#CITATION_TAG).",,['It compliments the Intergovernmental Panel on Climate Change (IPCC) 2006 Guidelines for National Greenhouse Gas Inventories and it is aimed at being fully consistent with this IPCC Guidelines and with the UNFCCC reporting guidelines on annual inventories (FCCC/SBSTA/2006/9).']
"Most baseline demographic and clinical features did not differ between subjects with or without SFMD. All patients received a full neurological, neuropsychological, neuroimaging and neuropsychiatric evaluation, as reported in Online Resource 1, and as described in previous studies on the same cohort [29] [#CITATION_TAG] [31] [32].","Methods: SFMD were assessed by direct observation of symptoms in the year coincident (+-6 months) with definite diagnosis of PD, DLB, Alzheimer disease, multiple system atrophy, progressive supranuclear palsy, or frontotemporal dementia, and by interviews with patients, caregivers, and general practitioners, and reviews of prior hospital admissions, in a cohort of 942 patients with neurodegenerative disorders. Matched groups of patients with PD and patients with DLB without vs with SFMD were selected for comparisons and followed up over 4 years. SFMD consisted of conversion motor or sensory disorders, often accompanied by delusional thought content; in one patient catatonic symptoms were observed concomitantly with PD diagnosis.",['Objective: To assess somatoform disorder (SFMD) prevalence and impact in Parkinson disease (PD) and dementia with Lewy bodies (DLB).']
"EEG abnormalities have been reported for both dementia with Lewy bodies (DLB) and Alzheimer's disease (AD). Although it has been suggested that variations in mean EEG frequency are greater in the former, the existence of meaningful differences remains controversial. No evidence is as yet available for Parkinson's disease with dementia (PDD). If revised consensus criteria for DLB diagnosis are properly applied (i.e. All patients received a full neurological, neuropsychological, neuroimaging and neuropsychiatric evaluation, as reported in Online Resource 1, and as described in previous studies on the same cohort [#CITATION_TAG] [30] [31] [32].","To improve clinical diagnostic accuracy, special emphasis was placed on identifying cognitive fluctuations and REM-sleep behaviour disorder. EEG variability was assessed by mean frequency analysis and compressed spectral arrays (CSA) in order to detect changes over time from different scalp derivations. Graded according to the presence of alpha activity, five different patterns were identified on EEG CSA from posterior derivations. A pattern with dominant alpha bands was observed in patients with AD alone while, in those with DLB and PDD, the degree to which residual alpha and 5.6-7.9 bands appeared was related to the presence and severity of cognitive fluctuations. Of interest, in four patients initially labelled as having AD, in whom the occurrence of fluctuations and/or REM-sleep behaviour disorder during the 2-year follow-up had made the diagnosis of AD questionable, the initial EEG was characterized by the features observed in the DLB group. emphasizing the diagnostic weight of fluctuations and REM sleep behaviour disorder), EEG recording may act to support discrimination between AD and DLB at the earliest stages of dementia, since characteristic abnormalities may even precede the appearance of distinctive clinical features.","['The aim of this study was to evaluate whether EEG abnormalities can discriminate between DLB, AD and PDD in the earliest stages of dementia and to do this 50 DLB, 50 AD and 40 PDD patients with slight cognitive impairment at first visit (MMSE > or = 20) were studied.']"
"In his essay on the Shaking Palsy,1 James Parkinson noted the presence of tremor as a cardinal feature: ""Involuntary tremulous motion, with lessened muscular power, in parts not in action and even when supported."" While superficially, tremor might seem to be a simple aspect of Parkinson's disease, it has actually turned out to be rather difficult to understand. One issue is that there is clearly more than one type of tremor. The classic"" tremor is tremor-at-rest; as Parkinson put it: ""in parts not in action"". This has been called re-emergent tremor. Many patients, however, have a distinct postural tremor clearly different from re-emergent tremor. The distinction can come from the frequency which is often faster than tremor-at-rest or it can be present in some patients who lack tremor-at-rest. To be fair, however, sometimes it is difficult to tell whether it is truly different. A third postural tremor can be an essential tremor. There is sometimes a co-existence of essential tremor and Parkinson's disease, and, in this regard, it is now established that patients with essential tremor have a slightly higher chance of developing Parkinson's disease. Parkinson ""postural"" tremor and essential tremor can appear clinically very similar. Given that there are sometimes dystonic elements in Parkinson's disease, yet another possibility is a dystonic tremor, and, if present, it would also look similar to ""postural"" tremor and essential tremor. In this regard, recently it has been suggested that the SWEDD (scans without evidence of dopamine deficiency) patients have dystonic tremor.2 Moreover, any patient can have an exaggerated physiological tremor. The clarification of the different tremor types and response to treatment in DLB could improve clinical recognition of DLB, but mostly, understanding tremor in DLB, would provide clarity to recent controversial debates [16] [17] [18] [19] [20] [21] [22] [#CITATION_TAG] [24] on the long term outcome of patients putatively affected by ET.",,"[""In order to understand the pathogenesis of postural tremor in Parkinson's disease, the first step should be a clear clinical characterization.""]"
"The last consensus on diagnostic criteria for dementia with Lewy Bodies (DLB), states that ''tremor is less frequent than in Parkinson's disease (PD)'' [1] but does not detail the types and relative prevalence of tremor, despite earlier studies reporting a prevalence of 40 % for rest and 30 % for action (kinetic-postural) tremor [#CITATION_TAG, 3].","Parkinsonian symptoms were assessed in 73 dementia patients using the UPDRS and staged using the Hoehn & Yahr system. DLB (n=42) was diagnosed using the McKeith et al. criteria, Alzheimer's disease (n=30) was diagnosed using the NINCDS ADRDA criteria. A Principal Components Analysis derived a sub-scale including the items tremor at rest, action tremor, bradykinesia, facial expression and rigidity.","[""The study aimed to evaluate the merits of the Unified Parkinson's Disease Rating Scale (UPDRS) in the assessment of parkinsonism in patients suffering from Dementia with Lewy Bodies (DLB).""]"
"Background: Mild cognitive deficits, mainly in frontal-executive function and memory, have been reported in patients with essential tremor (ET). The clarification of the different tremor types and response to treatment in DLB could improve clinical recognition of DLB, but mostly, understanding tremor in DLB, would provide clarity to recent controversial debates [16] [17] [18] [#CITATION_TAG] [20] [21] [22] [23] [24] on the long term outcome of patients putatively affected by ET.","Methods: Community-dwelling elders in northern Manhattan were enrolled in a prospective cohort study. Baseline ET diagnoses were assigned from handwriting samples. Dementia was diagnosed at baseline and follow-up using DSM-III-R criteria. Rather than attributing cognitive complaints in patients with ET to old age, assessment and possible treatment of dementia should be routinely incorporated into the treatment plan","['Objective: To determine whether baseline ET is associated with prevalent and incident dementia in an ethnically diverse, community-based sample of elders.']"
"Conditions such as asterixis, epilepsia partialis continua, clonus, and rhythmic myoclonus can be misinterpreted as tremor. Generally rest tremor is considered specific for PD, while postural and action (or intentional) tremor are attributed to essential tremor (ET), although exceptions to this rule are clearly reported [#CITATION_TAG] [5] [6].","The classification is based on the distinction between rest, postural, simple kinetic, and intention tremor (tremor during target-directed movements). Additional data from a medical history and the results of a neurologic examination can be combined into one of the following clinical syndromes defined in this statement: enhanced physiologic tremor, classical essential tremor (ET), primary orthostatic tremor, task- and position-specific tremors, dystonic tremor, tremor in Parkinson's disease (PD), cerebellar tremor, Holmes' tremor, palatal tremor, drug-induced and toxic tremor, tremor in peripheral neuropathies, or psychogenic tremor. The features distinguishing these conditions from tremor are described. Controversial issues are outlined in a comment section for each item and thus reflect the open questions that at present cannot be answered on a scientific basis.",['This is a proposal of the Movement Disorder Society for a clinical classification of tremors.']
"Tremor in Parkinson's disease has several mysterious features. Clinically, tremor is seen in only three out of four patients with Parkinson's disease, and tremor-dominant patients generally follow a more benign disease course than non-tremor patients. Pathophysiologically, tremor is linked to altered activity in not one, but two distinct circuits: the basal ganglia, which are primarily affected by dopamine depletion in Parkinson's disease, and the cerebello-thalamo-cortical circuit, which is also involved in many other tremors. While recent studies [39, #CITATION_TAG] might further help to clarify this controversy, by adding new concepts to the debate, our concluding remark would be focused to a simple take home message: the appropriate examination and investigation of patients with tremor should not be simply addressed to motor aspects but should also consider non-motor features and specifically the core and supportive features of DLB [1] [2] [3] i.e., cognitive, visuo-spatial and dysexecutive abnormalities, RBD, and EEG abnormalities, before reaching definite conclusions.","We first describe clinical and pathological differences between tremor-dominant and non-tremor Parkinson's disease subtypes, and then summarize recent studies on the pathophysiology of tremor. We also discuss a newly proposed 'dimmer-switch model' that explains tremor as resulting from the combined actions of two circuits: the basal ganglia that trigger tremor episodes and the cerebello-thalamo-cortical circuit that produces the tremor.","[""The purpose of this review is to integrate these clinical and pathophysiological features of tremor in Parkinson's disease.""]"
"Supernova theory, numerical and analytic, has made remarkable progress in the past decade. Violent, large-scale nonradial mass motions are generic in supernova cores. The neutrino-heating mechanism, aided by nonradial flows, drives explosions, albeit low-energy ones, of O-Ne-Mg-core and some Fe-core progenitors. Now that modeling is being advanced from two to three dimensions, more realism, new perspectives, and hopefully answers to long-standing questions are coming into reach. Each year, roughly 30,000 tons of extraterrestrial solid material, liberated from larger parent bodies within our Solar System, is captured by the Earth [1]. A significant fraction of this material are submillimetre-sized spherical to teardrop-shaped particles, termed micrometeorites. They represent signatures of asteroidal collisions and cometary sublimation [2], hence, the determination of their origin gives valuable information on recent cosmic events and processes. Their cosmic ray exposure age can be derived by measuring cosmic ray-induced spallation products such as the long-lived radionuclides Al and Be that accumulate within the particles during their journey through space (e.g. Surface composition analyses with EDX-measurements can lead to misidentifications, since these particles have melted during entry, and do not represent the total composition. Quantitative differences occur only on the modest level of 5% to 25% in quantities characterizing collapse, bounce, and early postbounce evolution, for instance, in the central lepton fraction at neutrino trapping, the position of shock formation, the peak luminosity of the ν e burst, and the maximum radius to which the shock expands before it retreats again (17, (129) (#CITATION_TAG) (131).","The low concentrations of Al and Be within micrometeorites are close to the detection limits of current accelerator mass spectrometry (AMS), hence, any loss of material for their identification and classification as micrometeorites needs to be minimized. For identifying micrometeorites with high confidence, it is necessary to analyze the interior of the particle [4], commonly through epoxy embedded cross-section EDX-analysis, which, however, leads to substantial material loss [5]. Using a focused ion beam (FIB) on the non-coated particle, we are able to cut off a thin slice of the surface. Subsequently, with field emission electron probe microanalysis (FE-EPMA), its inner composition (and textures) can be determined quantitatively at lower detection limits down to a few tens of ppm with wavelength dispersive X-Ray spectrometers.","['We present a new methodology on how to identify micrometeorites without a substantial material loss, nor impediments such as coating and epoxy embedding.']"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Some of these possibilities might require the support of IS and communication technologies. Many researchers have questioned whether IT adoption decisions are made in the interest of executives or other stakeholders [#CITATION_TAG, 28].","The framework emphasizes continuous identification of concerns from stakeholders, and facilitates critical reflection in the exploration of possibilities for improvement. To define the framework, two systems theories are used: boundary critique and autopoiesis. The first one enables critical reflection on values and assumptions about potential situations or marginalization. The second one fosters continuous dialogue, listening and mutual collaboration between participants. With these theories, the framework enables people to reflect on issues of inclusion, exclusion and marginalization, and to participate in the design of plans for improvement.","['This paper presents a methodological framework to support the process of information systems (IS) planning in organizations.', ""It draws on the ideas of critical systems thinking (CST), a research perspective that encourages the analysis of stakeholders' understandings prior to the selection and implementation of planning methods.""]"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. This perspective also underlies many ICT policies. Diffusion in turn is the decision to implement such technology after adoption [#CITATION_TAG].","The human environmental model is presented as a useful alternative which, if embraced, can inform more holistic information and communication technology (ICT) policies. Design/methodology/approach        - Through a review of diffusion of innovations models an alternative diffusion framework is described and applied to an interpretive open source case study in South Africa.","['Purpose        - Through an evaluation of the information technology (IT) adoption and diffusion models and the free and/or open source policy of the South African Government, the underlying assumption is that the developmental divide between those with and those without access to technology is purely technical.', 'This paper aims to illustrate that if Free and/or Open Source Software is to be used as a building block to bridge the ""digital divide"" a more social and environmental perspective, which embraces the philosophy behind the software, needs to complement the technical perspective.', 'Such an approach to ICT policy formulation will assist with broadening the perspective of policy makers from IT as a technical solution to IT as part of a socio-technical solution and recognise the duality of the innovation process.']"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. In the current knowledge society, adoption of information technology (IT) innovation initiatives has become a necessity for the success of most organizations. The decision to adopt information technology solutions however must be made on welldefined user requirements, and not on mere high-expectations. Even after the deployment of the information system, the shape of organizational operations was still transforming. Naturally any IT adoption ought to be done for the benefit of the organization [#CITATION_TAG].",,['In this paper we present a case study of an ambitious Saudi Arabian charitable organization that decided to start the development of its information system based on anticipated modes of operation and not actual operational and user requirements.']
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Moving from mainly techni cal issues in procurement to corporate IS governance presents OSS with new challenges beyond outlining a business case for a particular OSS application. Amongst other things, IT governance is tasked with deciding on how decision rights and accountability are distributed in organizations to avoid ad hoc decision making [#CITATION_TAG].",We draw parallels to the business case for commercial software products (COTS).,['In this paper we focus on managerial and technical decisions for acquisi tion of OSS and discuss potential approaches to a widespread adoption of OSS.']
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. The conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted. However, while their criteria are useful in evaluating case study research conducted according to the natural science model of social science, the positivist criteria they suggest are inappropriate for interpretive research.The use of interpretive approach is relatively new to information systems field, the approach has emerged as a valid and important strand in information systems research and most mainstream IS journals now welcome interpretive research and significant groups of authors are working within the interpretive tradition (Walsham, 1995). The interpretive research does not subscribe to the idea that a pre-determined set of criteria can be applied in a mechanistic way, it does not follow that there are no standards at all by which interpretive research can be judged.Striving and ensuring rigor in interpretive study requires different criteria through which one views and judges the quality and completeness of the research process. Many researchers (Orlikowski et al, 1991; Walsham, 1993, 1995; Klein and Myers, 1999) have addressed qualitative research and they have shown how the nature and purpose of interpretive research differs from positivist research. At present, there are no agreed criteria for evaluating research of this kind. Nonetheless, there must be some criteria by which the quality of interpretive research can be evaluated. Myers (1997) and Klein and Myers (1999) have proposed a set of criteria for the conduct and evaluation of interpretive research in information systems.This study is not concerned with adhering to the scientific tenets of precision and replication, instead the study is concerned in seeking a theory that is compatible with evidence that is both rigorous and relevant and generally useful to other areas. The remainder of the paper is structured as follows. The importance of understanding the IT adoption decision making process in organizations has been highlighted by many researchers [2, #CITATION_TAG].","Benbasat et al (1987), Lee (1989) and Yin (1994) formulated a set of methodological principles for case studies that were consistent with the conventions of positivism. It begins with an overview of case study method and a discourse on the use of case study in Information Systems. This will be followed by a description of the procedures involved in collecting and analyzing data in grounded theory method. Then the criteria proposed by Myers (1997) for evaluating interpretive research will be discussed in relation to this particular study.","['IntroductionThis paper presents an evaluation of an interpretive in-depth case study.', 'One of the main aims of interpretive research is seeking meaning in context.', 'It has the potential to produce deep insights into information systems phenomena including the use and the management of information systems.', 'Methodology2.1 Case studyThe purpose of using case study was to provide an understanding of the factors that influence Small to Medium-sized Enterprises (SMEs) decision to adopt and use Internet in business.', 'The aims of using case study are: (1) to elicit qualitative information (2) to produce an in-depth and holistic study (Yin, 1994), that gives the reader sufficient contextual and environmental descriptions to allow them to transfer the case studies based on conceptual applicability.']"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. The organization was one of the few that migrated to open source software as part of a South African government initiative. From a managerial perspective, business leaders must understand the value of stakeholder collaboration in open source software migration. Boundary critique can be an important tool for achieving broader collaboration of stakeholders. Many researchers have highlighted the importance of stakeholder participation in the success of IT adoption [32], hence the involvement and participation of stakeholders have been found desirable in IT adoption decision making [#CITATION_TAG].",The case study consisted of semi-structured interviews with the participants involved in the migration. The interviews centered on the contribution of stakeholder collaboration during the software migration using a boundary critique.,['This paper investigates the contribution of stakeholder collaboration during an open source software migration using a case study.']
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Top-performing enterprises succeed in obtaining value from IT where others fail, in part, by implementing effective IT governance to support their strategies and institutionalize good practices. IT governance involves specifying decision rights and accountabilities for important IT decisions. In studying the IT governance of more than 250 enterprises in 23 countries, we found a wide array of IT governance arrangements. Enterprises assign ""decision rights"" to different ""archetypes"" (Business or IT Monarchy, Federal, Duopoly, Feudal, or Anarchy) to govern five key IT decisions (IT investment, architecture, principles, application needs, and infrastructure). Top-performing enterprises govern IT differently from each other and from average enterprises. Firms leading on growth decentralize more of their IT decision rights and place IT capabilities in the business units. Those leading on profit centralize more decision rights and senior business leaders make the major IT decisions. In order to improve IT governance in an organization, Weill [#CITATION_TAG] proposes the assignment of decision rights to five IT decision areas (architecture, infrastructure, principle, applications and investment) in an organization.",Top performers design their IT governance to reinforce their performance goals and link IT governance to the governance of their other key enterprise assets and desired behaviors. A case study of State Street Corporation illustrates IT governance evolution and a method to diagrammatically represent IT governance.,"['The goal is to encourage ""desirable behaviors"" in the use of IT.']"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Systems approaches are focused on understanding problem situations in order to improve the situations not solve the problem [#CITATION_TAG].",The five approaches are: System Dynamics (SD) developed originally in the late 1950s by Jay Forrester Viable Systems Model (VSM) developed originally in the late 1960s by Stafford Beer Strategic Options Development and Analysis (SODA: with cognitive mapping) developed originally in the 1970s by Colin Eden Soft Systems Methodology (SSM) developed originally in the 1970s by Peter Checkland Critical Systems Heuristics (CSH) developed originally in the late 1970s by Werner Ulrich,"['Systems Approaches to Managing Change brings together five systems approaches to managing complex issues, each having a proven track record of over 25 years.']"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Irrigation farmers in the lower reaches of the Vaal and Riet Rivers are experiencing substantial yield reductions in certain crops and more profitable crops have been withdrawn from production, hypothesised, as a result of generally poor but especially fluctuating water quality. Leaching is justified financially and there is a strong motivation for a change in the current water pricing system. Organizations have been urged to view IT adoption decision making as a social phenomenon which needs systems approaches to reveal competing interests among stakeholders [#CITATION_TAG].","In this paper secondary data is used in a linear programming model to test this hypothesis by calculating the potential loss in farm level optimal returns. Linear crop-water quality production functions (Ayers & Westcot, 1983; adapted from Maas & Hoffmann, 1977) are used to calculate net returns for the eight most common crops grown.","['The ultimate aim for SALMOD is a mathematical model using dynamic optimisation, simulation and risk modelling techniques to aid in whole farm and system level management decisions to ensure sustainable irrigation agriculture under stochastic river water quality conditions.International Development,']"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Many researchers have highlighted the importance of stakeholder participation in the success of IT adoption [#CITATION_TAG], hence the involvement and participation of stakeholders have been found desirable in IT adoption decision making [20].","To do so, we adapted from Freeman's (1984) work and developed a theoretical assessment framework to organize and interpret data, and define avenues (five propositions were proposed) for further research into information technology project cancellation.","[""This study adopts a stakeholder analysis to examine stakeholders' roles in influencing organizational decisions to abandon information technology projects.""]"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Increasingly, information technology governance is being considered an integral part of corporate governance. There has been a rapid increase in awareness and adoption of IT governance as well as the desire to conform to national governance requirements to ensure that IT is aligned with the objectives of the organization. Although IT governance as a framework may improve controls with respect to the alignment of IT and business objectives, it pays less attention to how IT adoption decisions are made [#CITATION_TAG].",,"['Information Technology Governance and Service Management: Frameworks and Adaptations provides an in-depth view into the critical contribution of IT service management to IT governance, and the strategic and tactical value provided by effective service management.', 'A must-have resource for academics, students, and practitioners in fields affected by IT in organizations, this work gathers authoritative perspectives on the state of research on organizational challenges and benefits in current IT governance frameworks, adoption, and incorporation.']"
"Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. In order for individuals to change their ways of thinking they first need to abandon their old worldview [#CITATION_TAG].","A theoretical and methodological pluralism which allows the development of new perspectives for organizational analysis is suggested. While orthodoxy is based upon a few metaphors characteristic of the functionalist paradigm, metaphors characteristic of other paradigms, which challenge the ground assumptions of orthodoxy, are shown to have much to offer.","['The paper explores the relationship among paradigms, metaphors, and puzzle solving showing how organization theory and research is constructed upon a network of assumptions that are taken-for-granted.', 'The metaphorical nature of theory and the implications of metaphor for theory construction are examined.']"
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Understanding value stability and change is essential for understanding values of both individuals and cultures.Yet theoretical thinking and empirical evidence on this topic have been scarce. Hooghe (2003, p. 93) implies a similar idea when arguing that association membership is unlikely to 'introduce qualitatively new values, but enforces already existing values' (see also #CITATION_TAG; Katz & Lazersfeld, 1955).","In this article, the authors suggest a model outlining processes of individual value change. They identify five facilitators of value change (priming, adaptation, identification, consistency maintenance, and direct persuasion) and consider the moderating role of culture in each. In addition, the authors discuss the roles of culture, personal values, and traits as general moderators of value change.",['This model proposes that value change can occur through automatic and effortful routes.']
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. The empirical analysis thereby builds on recent work by #CITATION_TAG to allow easy reference to existing findings.","I pay particular attention to the theory and measurement of voluntary associations in promoting trust, hypothesizing that voluntary associations connected to other voluntary associations are more beneficial for the creation of generalized trust than associations isolated from other associations. The theory is tested with a multi-level, cross-national model, including both individual-level and country-level variables to predict the placement of trust.","['This paper presents a large-scale, comprehensive test of generalized trust across 31 nations.']"
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. In his 1790 address to the Academie Francaise in Paris, Condorcet noted that every new generation has a tendency to accuse itself of being less civic-minded than previous cohorts. Two centuries later, this argument has once again regained front-page status. The debate is currently focused on the question of whether or not social capital and civic engagement are declining in Western societies. In his academic best-seller Bowling Alone , Robert Putnam argues that younger age cohorts, socialized in the prosperous economic conditions of the 1960s and onwards, are less inclined to engage in community life and in politics, and also less likely to trust their fellow citizens. By contrast, the 'long civic generation', born roughly between 1910 and 1940, is portrayed as much more motivated in these respects. They readily volunteer in community projects, read newspapers and take on more social responsibilities. As the long civic generation is replaced by younger age cohorts, the social capital stock of American communities slowly diminishes. All of these attitudes and behaviours, it is argued, depict a significant downward trend. Although Putnam is by far the most vocal of all scholars in the 'decline of social capital' choir, he certainly is not the only author describing an erosion of traditional societal relations. Over the past decade, a lively debate has developed regarding the decline in civic participation observed by some, but not by others, across a number of Western democracies (Dekker & van den Broek, 2005; Listhaug & Grønflaten, 2007; Paxton, 1999; Putnam, 2000; #CITATION_TAG).","The indicators used to substantiate this claim are numerous and diverse: measures for voter turnout, attendance of club meetings, generalized trust, the number of common family dinners, the number of card games played together, and even respect for traffic rules.","['In this view, a process of generational replacement is responsible for a steady decline of social capital and civic engagement in American society.']"
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Wars, terrorist activity or natural disasters therefore have the potential to induce sudden, although possibly temporary, shifts in people's attitudes and value patterns (e.g., Ladd & Cairns, 1996; #CITATION_TAG).","The respondents' reactions to the two events were compared as a function of gender and political orientation. The intensity of emotional reactions was affected by political orientation: Although Rabin's supporters and his opponents reacted with equal intensity to the terror attacks, supporters reacted with the same intensity to Rabin's assassination, whereas opponents' reactions to the assassination were less intense.",['This study examined the cognitive and emotional reactions of 477 Israeli high school students to the assassination of Prime Minister Yitzhak Rabin (immediately after the event as well as 5 months later) and to a series of terror attacks.']
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Second, the relation between ethnic and religious diversity, on the one hand, and social capital, civic engagement and trust, on the other hand, has attracted significant scholarly discussion in recent years (e.g., #CITATION_TAG, 2002 Coffé & Geys, 2006; Delhey & Newton, 2005; Gijsberts, van der Meer, & Dagevos, forthcoming; Hallberg & Lund, 2005; Putnam, 2007).",,"['This paper studies what determines group formation and the degree of participation when the population is heterogeneous, both in terms of income and race or ethnicity.', 'We are especially interested in whether and how much the degree of heterogeneity in communities influences the amount of participation in different types of groups.']"
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Debates surrounding institutional change have become increasingly central to Political Science, Management Studies, and Sociology, opposing the role of globalization in bringing about a convergence of national economies and institutions on one model to theories about 'Varieties of Capitalism'. Stability and Change in German and Japanese Capitalism  Wolfgang Streeck and Kozo Yamamura  Preface Wolfgang Streeck and Kathleen Thelen  1   Introduction: Institutional Change in Advanced Political Economies Wolfgang Streeck and Kathleen Thelen   2   Policy Drift: The Hidden Politics of US Welfare State Retrenchment Jacob S. Hacker  3   Changing Dominant Practice: Making Use of Institutional Diversity in Hungary and the UK Colin Crouch and Maarten Keune  4   Redeploying the State: Liberalization and Social Policy in France Jonah D. Levy  5   Ambiguous Agreement, Cumulative Change: French Social Policy in the 1990s Bruno Palier  6   Routine Adjustment and Bounded Innovation: The Changing Political Economy of Japan Steven K. Vogel  7   Change from Within: German and Italian Finance in the 1990s Richard Deeg  8   Institutional Resettlement: The Case of Early Retirement in Germany Christine Trampusch  9   Contested Boundaries: Ambiguity and Creativity in the Evolution of German Codetermination Gregory Jackson  10   Adaptation, Recombination, and Reinforcement: The Story of Antitrust and Competition Law in Germany and Europe Sigrid Quack and Marie-Laure Djeli Although such endogenous incremental changes can take different forms (see Boas, 2007; Mahoney & Thelen, 2010; #CITATION_TAG), they all have in common that 'the effect of the institution is transformed' in a gradual process (Boas, 2007, p. 34, italics in original).","The chapters highlight the limitations of these theories, finding them lacking in the analytic tools necessary to identify the changes occurring at a national level, and therefore tend to explain many changes and innovation as simply another version of previous situations. Instead a model emerges of contemporary political economies developing in incremental but cumulatively transformative processes. They offer an empirically grounded typology of modes of institutional change that offer important insights on mechanisms of social and political stability, and evolution generally.",['This book brings together a distinguished set of contributors from a variety of disciplines to examine current theories of institutional change.']
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Most quantitative research in social capital focuses on civic engagement in formal organisations. Data on social capital in informal social networks are harder to obtain and there has also been insufficient means for investigating this. Informal social networks, especially having good neighbourly relations, tend to foster greater trust than does formal civic engagement. Similarly, the 9/11 attack on the New York World Trade Centre, and the subsequent 2004 Madrid and 2005 London bombings had a significant impact on 'the thoughts, feelings and behaviors of ind ividuals' (Woods, 2011, p. 214; see also Best, Krueger, & Ladewig, 2006; Colás, 2010; Huddy, Khatib, & Capelos, 2002; #CITATION_TAG; Panagopoulos, 2006; Verkasalo, Goodwin, & Bezmenova, 2006; Yum & Schenck-Hamlin, 2005).",The first two refer to informal social networks and the last to formal social networks. We use gllamm (Generalized Linear Latent and Mixed Models) to construct the latent variable scores from the categorical component variables. We also analyse the socio-cultural determinants of the three types of social capital and their impacts on social trust.,"['In this paper, we use the British Household Panel Survey (BHPS) to conceptualise and measure three types of social capital: neighbourhood attachment, social network and civic participation.']"
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Electoral turnout in Norway has been declining over a long period for local elections and, at the four most recent Storting elections, turnout has been at a lower level than in the preceding 25 years. The rise in political involvement and activism is quite widespread, covering dimensions like political interest, political discussion and political action. Education is strongly associated with most forms of civic participation and the rise in educational levels normally leads to an increase in participation rates. There is no general civic decline. Over the past decade, a lively debate has developed regarding the decline in civic participation observed by some, but not by others, across a number of Western democracies (Dekker & van den Broek, 2005; #CITATION_TAG; Paxton, 1999; Putnam, 2000; Stolle & Hooghe, 2003).",Data from the Norwegian Election Studies 1965-2001 and the Norwegian Values Studies 1982-1996 are analysed. The increase includes forms of participation where political parties play a strong role and in direct action where parties are supposed to be less important.,['This article investigates whether the fall in turnout generalises to other forms of political participation and political involvement.']
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Building on social identity theory (e.g., Tajfel, 1978) and social network analysis (e.g., #CITATION_TAG), Paxton (2007, p. 51, italics in original) argues that the generalization of trust beyond a given voluntary association 'is critically dependent on whether an individual belongs to an association that is connected to other associations or one that is isolated'.","We will then move on to Durkheim's organic view of society, to Marx's dialectical materialism, finishing with Weber's Verstehen sociology and ideal types of authority. We'll try to understand their theories not just as historical relics, but as living sets of ideas relevant to contemporary social issues.","['This course will deal with the foundations of social theory, starting with the French and Scottish Enlightenments and the beginnings of a specifically sociological worldview.']"
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Mismatches between institutions and social problems trigger reforms, but do not determine the options that policy makers finally choose. Frictions caused by emerging social risks interact with difficulties of established welfare regimes to cope with old risks to facilitate access to public agendas for reformist projects. Ultimately, however, reforms depend on the construction of pro-and anti-reform coalitions, shaped by two main forces: 1) lines of discrimination in the distribution of benefits by existing welfare regimes; 2) strategies of parties, interest groups, and bureaucracies, competing to activate those cleavages according to their interests. Selective pay-offs to appease privileged groups constitute the most direct determinants of the architecture of reforms. However, exogenous shocks may facilitate changes away from expected paths. Based on the idea that institutions get 'locked in' as a result of self-reinforcement, self-reproduction and path dependence (e.g., #CITATION_TAG; Mahoney, 2000; Pierson, 2000), institutions have long been viewed as stable and resistant to change until 'exogenous shocks (...) bring about radical institutional reconfigurations' (Mahoney & Thelen, 2010, p. 2).","It theorizes the reforms by comparing pensions, health care, and social assistance policies. First, it provides an explanation of recent transformations of welfare regimes as resulting from the combined effects of gradual institutional change and exogenous socioeconomic transformations. Second, it explores the potentialities and limitations of historical institutionalism. Third, it identifies emerging patterns of governance. This requires combining strategies of blame-avoidance and credit-claiming that variably mix persuasion, exclusion, and division targeting potential opposition. In explaining the reforms, I discuss endogenous institutional change and how this results in fragmented social protection policies.","['This dissertation examines the processes and outcomes of welfare regimes reforms in eleven Latin American countries, between 1980 and 2010.', 'In so doing, it confronts three theoretical goals.', 'Socioeconomic change, fiscal strain, and transnational factors, interact to make the expansion of social protection contingent upon redistributions of burdens and benefits guaranteed to trigger resistance from groups privileged by existing schemes.', 'The strategic challenge for reformist politicians is the crafting of formulas aimed at simultaneously neutralizing potential veto coalitions and mobilizing unprotected populations.']"
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Putnam (2007) claims that in ethnically diverse neighbourhoods, residents of all ethnic groups tend to 'hunker down'. Various studies in the United States found a clear correlation between diversity and cohesion, and also for many different dimensions of social cohesion. Whether this finding also holds in other (European) settings is the subject of hot and unresolved debate. Previous studies on the Netherlands remained inconclusive. This article examines the relationship between ethnic diversity (in socio-graphically defined neighbourhoods) and four dimensions of social cohesion (trust, informal help, voluntary work, and neighbourhood contacts) for the 50 largest cities in the Netherlands. Second, the relation between ethnic and religious diversity, on the one hand, and social capital, civic engagement and trust, on the other hand, has attracted significant scholarly discussion in recent years (e.g., Alesina & La Ferrara, 2000, 2002Coffé & Geys, 2006;Delhey & Newton, 2005; #CITATION_TAG, van der Meer, & Dagevos, forthcoming; Hallberg & Lund, 2005;Putnam, 2007).",We examine how this lack of consensus can be explained.,"['Specifically, this article addresses the question of whether living in an ethnically diverse setting has negative consequences for social cohesion in the Netherlands as well.', 'To further the debate, this article pulls apart various contexts and various dimensions of social cohesion.']"
"Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Practically everywhere one looks these days the concept of ""civil society"" is in vogue. Neo-Tocquevillean scholars argue that civil society plays a role in driving political, social, and even economic outcomes. This new conventional wisdom, however, is flawed. It is simply not true that democratic government is always strengthened, not weakened, when it faces a vigorous civil society. To know when civil society activity will take on oppositional or even antidemocratic tendencies, one needs to ground one's analyses in concrete examinations of political reality. 2 Given the importance of formal and informal institutions for governing people's behaviour (e.g., North, 1990; Thelen, 1999) and earlier findings linking the institutional environment to the development of specific types of voluntary associations (e.g., #CITATION_TAG; Kääriäinen & Lehtonen, 2006; Schofer & Fourcade-Gourinchas, 2001), it is surprising that both lines of argument ignore the socio-political and institutional environment within which the individual and the association exist.",,"[""This essay shows how a robust civil society helped scuttle the twentieth century's most critical democratic experiment, Weimar Germany.""]"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. As people have spread around the world, they have taken with them a broad range of other species to satisfy a variety of human needs. Some of these species went on to establish wild populations well outside their native ranges. These biological invaders are a major component of current global change, and often represent threats to the maintenance of global biodiversity, human health, and the success of human economic enterprises. The continuing globalization of our society ensures that the need to understand the process of biological invasion will only increase in the future. There is also a growing recognition that the study of biological invaders provides significant insight into basic questions in ecology and evolution. Exotic birds provide a particularly good opportunity to study the causes and consequences of biological invasions. Therefore, in Europe, we buffered each locality where parakeets have been introduced with a distance equal to the minimum invasion speed recorded for birds (i.e. 4.59 km year À1, derived from #CITATION_TAG) multiplied by the number of years since introduction (see Strubbe et al., 2013 for details).","Chapters cover causes of non-randomness in which species get transported and released into novel environments, the stochastic (relating to numbers released) and deterministic (relating to species and location) effects that influence establishment success, patterns and processes in range expansion, and the ecology, genetics, and evolution of exotic birds in their new environment.","['By combining good historical records of bird introductions with the detailed information available on many other aspects of avian biology, this book advances understanding of the invasion process while also exploring avian conservation biology, and basic principles of ecology and evolution.']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Despite this, concerns remain that the invasion process is too complex for accurate predictions to be made. To identify potentially invasive species, risk assessment protocols based on species traits associated with invasiveness have been developed (#CITATION_TAG).",,"['Aim  Trait-based risk assessment for invasive species is becoming an important tool for identifying non-indigenous species that are likely to cause harm.', 'Our goal was to test risk assessment performance across a range of taxonomic and geographical scales, at different points in the invasion process, with a range of statistical and machine learning algorithms.']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. The niches of non-native plants and birds have recently been assessed in large-scale multispecies studies, but such large-scale tests are lacking for non-native reptiles and amphibians (herpetofauna). Furthermore, little is known about the factors that contribute to niche shifts when they occur. Niche conservatism was the norm for non-native vertebrates introduced to Europe and North America (Strubbe et al., 2013 (Strubbe et al.,, 2014, whereas a global study on amphibians and reptiles found widespread evidence for niche expansion (#CITATION_TAG).","Based on the occurrence of 71 reptile and amphibian species, we compared native and non-native realized niches in 101 invaded ranges at a global scale and identified the factors that affect niche shifts. Methods    We assessed climatic niche dynamics in a gridded environmental space that allowed niche overlap and expansion into climatic conditions not colonized by the species in their native range to be quantified. We analysed the factors that affect niche shifts using a model-averaging approach, based on generalized linear mixed-effects models. The 'climate-matching hypothesis' should be used with caution for species undergoing niche shifts, because it could underestimate the risk of their establishment.",['Aim    Identifying climatic niche shifts and their drivers is important for the accurate prediction of the risk of biological invasions.']
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Non-native birds in Europe occupy a subset of the environments they inhabit in their native ranges. Niche expansion into novel environments is rare for most species, allowing species distribution models to accurately predict invasion risk. Data on parakeet introduction success were taken from #CITATION_TAG (n = 123 introduction events).","Based on the occurrence of 28 non-native birds in Europe, we assess to what extent Grinnellian realized niches are conserved during invasion, formulate hypotheses to explain the variation in observed niche changes and test how well species distribution models can predict non-native bird occurrence in Europe. Methods To quantify niche changes, a recent method that applies kernel smoothers to densities of species occurrence in a gridded environmental space was used. This corrects for differences in the availability of environments between study areas and allows discrimination between 'niche expansion' into environments new to the species and 'niche unfilling', whereby the species only partially fills its niche in the invaded range. Predictions of non-native bird distribution in Europe were generated using several distribution modelling techniques.","['Aim Niche conservatism, or the extent to which niches are conserved across space and time, is of special concern for the study of non-native species as it underlies predictions of invasion risk.']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. It is appropriate to study niche differences between species, subspecies or intraspecific lineages that differ in their geographical distributions. To assess niche differences between phylogroups and between native and invasive parakeet populations, we used the #CITATION_TAG framework.","Methods - The framework applies kernel smoothers to densities of species occurrence in gridded environmental space to calculate metrics of niche overlap and test hypotheses regarding niche conservatism. We use this framework and simulated species with predefined distributions and amounts of niche overlap to evaluate several ordination and species distribution modeling techniques for quantifying niche overlap. The method is robust to known and previously undocumented biases related to the dependence of species occurrences on the frequency of environmental conditions that occur across geographic space. The use of a kernel smoother makes the process of moving from geographical space to multivariate environmental space independent of both sampling effort and arbitrary choice of resolution in environmental space. Alternatively, it can be used to measure the degree to which the environmental niche of a species or intraspecific lineage has changed over time","['Aim - Concerns over how global change will influence species distributions, in conjunction with increased emphasis on understanding niche dynamics in evolutionary and community contexts, highlight the growing need for robust methods to quantify niche differences between or within taxa.', 'We propose a statistical framework to describe and compare environmental niches from occurrence and spatial environmental data.|2.']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Chrysanthemoides monilifera, native to Southern Africa, has two subspecies invasive in Australia, which has led to an importation ban on all six subspecies. Spatially explicit predictions of invasion risk derived from bioclimatic envelope models [also referred to as species distribution models (SDM) or ecological niche models (ENM)] calibrated with native species distributions are increasingly incorporated into such invasive species risk assessments (#CITATION_TAG).","These issues are not addressed in Weed Risk Assessments (WRAs), which have been developed to identify potentially invasive species and prevent their importation. Location: Southern Africa and Australia Methods: Realized climatic niches of native and alien populations of two invasive subspecies (Bitou Bush and Boneseed) were compared using niche identity tests. The distribution of climatically suitable habitat within Australia for all subspecies was modelled using MaxEnt, under current and future climate scenarios. For invasive subspecies, models were calibrated using (1) native or (2) alien range data.","['Aim: Climate change and the ability of alien populations to realize different climatic niches compared to native populations pose challenges for pre-empting invasion risk.', 'We call for greater dialogue to identify and standardize a comprehensive system for incorporating these challenging issues into WRA systems to ensure that they remain effective in reducing the weed risk into the future.13 page(s']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Modelling strategies for predicting the potential impacts of climate change on the natural distribution of species have often focused on the characterization of a species' bioclimate envelope. A number of recent critiques have questioned the validity of this approach by pointing to the many factors other than climate that play an important part in determining species distributions and the dynamics of distribution changes. Such factors include biotic interactions, evolutionary change and dispersal ability. However, it is stressed that the spatial scale at which these models are applied is of fundamental importance, and that model results should not be interpreted without due consideration of the limitations involved. #CITATION_TAG suggested a hierarchical approach to modelling environment-biota relationships whereby bioclimatic envelope models should form the first step, identifying the broad outlines of species' distributions.","A hierarchical modelling framework is proposed through which some of these limitations can be addressed within a broader, scale-dependent context",['This paper reviews and evaluates criticisms of bioclimate envelope models and discusses the implications of these criticisms for the different modelling strategies employed.']
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. The taxonomic rank of subspecies remains highly contentious, largely because traditional subspecies boundaries have sometimes been contradicted by molecular phylogenetic data. However, the global generality of this phenomenon remains unclear due to this previous study's narrow geographic focus on continental Nearctic and Palearctic subspecies. The broader picture is that avian subspecies often provide an effective short-cut for estimating patterns of intraspecific genetic diversity, thereby providing a useful tool for the study of evolutionary divergence and conservation. Subspecies are generally based on discontinuities in the geographical distribution of phenotypic traits instead of molecular phylogenies, but can generally be considered useful proxies of patterns of divergence among populations (#CITATION_TAG).","We suggest that the widespread impression that avian subspecies are not real arises from a predominance of studies focusing on continental subspecies in North America and Eurasia, regions which show unusually low levels of genetic differentiation.","['Here, we present a new global analysis of avian subspecies and show that 36% of avian subspecies are, in fact, phylogenetically distinct.']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Size and shape are important determinants of a species' niche but their causal role is often difficult to interpret. Laboratory measures of metabolic rate used to describe TNZs cannot be generalized to infer the capacity for terrestrial animals to find their TNZ in complex natural environments. The combination of spatially explicit data with biophysical models of heat exchange provides a powerful means for studying the thermal niches of endotherms across climatic gradients. Indeed, endotherms such as birds are often able to tolerate a wide range of environmental conditions, but this comes at a potentially high energetic cost (#CITATION_TAG).","For endotherms, size and shape define the thermal niche through their interaction with core temperature, insulation, and environmental conditions, determining the thermoneutral zone (TNZ) where energy and water costs are minimized. Here, we derive an analytical model of the thermal niche of an ellipsoid furred endotherm that accurately predicts field and laboratory data. We use the model to illustrate the relative importance of size and shape on the location of the TNZ under different environmental conditions. We show how such functional traits models can be integrated with spatial environmental datasets to calculate null expectations for body size clines from a thermal perspective, aiding mechanistic interpretation of empirical clines such as Bergmann's Rule.","[""A key challenge in ecology is to define species' niches on the basis of functional traits.""]"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Understanding how biotic and environmental factors facilitate their invasion success remains a challenge. Indeed, variance in laying dates between European and native (Asian) parakeet populations suggests that in Europe, parakeets are delaying their breeding in response to colder temperatures (#CITATION_TAG).","Here, we assess the role of two major hypotheses explaining invasion success: (1) enemy-release, which argues that invasive species are freed from their native predators and parasites in the new areas; and (2) climate-matching, which argues that the climatic similarity between the exotic and native range determines the success of invasive populations.",['Aim  Some invasive species succeed particularly well and manage to establish populations across a wide variety of regions and climatic conditions.']
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. In the recent past, availability of large data sets of species presences has increased by orders of magnitude. This, together with developments in geographical information systems and statistical methods, has enabled scientists to calculate, for thousands of species, the environmental conditions of their distributional areas. Climate is generally recognized as a chief driver of species' distributions at large spatial scales (Ara ujo & Peterson, 2012), although the broad distributional limits governed by climate may be modified by factors such as habitat availability, biotic interactions and dispersal limitations (#CITATION_TAG).",I use set theory notation and analogies derived from population ecology theory to obtain formal definitions of areas of distribution and several types of niches.,"['I argue that it is useful to define Grinnellian and Eltonian niches on the basis of the types of variables used to calculate them, the natural spatial scale at which they can be measured, and the dispersal of the individuals over the environment.']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. There is a large and growing number of alien species in ecosystems all over the world. However, action against invasives is often hindered by a lack of relevant ecological information such as the expected distribution and impact of the invader. In Europe, radio-tracking (Clergeau & Vergnes, 2011; Strubbe & Matthysen, 2011) and habitat selection studies (#CITATION_TAG; Newson et al., 2010) indicate that parakeets prefer to forage in city parks and gardens, where bird feeders and ornamental vegetation present parakeets with abundant food.","We determined the abundance of parakeets and native hole-nesters in 44 study sites using point counts. We examined the relationship between parakeet numbers and a set of habitat and landscape variables and to assess the effect of competition, we studied the relationships between the number of parakeets and the number of native hole-nesters.","['In view of this, management efforts should concentrate on invasives that have detrimental effects on native biota.', 'Our aims were to identify the habitat characteristics that influence ring-necked parakeet abundance in Belgium and to assess the effects of competition for nesting cavities with native hole-nesters.']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Biotic interactions and their dynamics influence species' relationships to climate, and this also has important implications for predicting future distributions of species. It is already well accepted that biotic interactions shape species' spatial distributions at local spatial extents, but the role of these interactions beyond local extents (e.g. 10 km(2)  to global extents) are usually dismissed as unimportant. Simplified ecosystems where there are relatively few interacting species and sometimes a wealth of existing ecosystem monitoring data (e.g. arctic, alpine or island habitats) offer settings where the development of modelling tools that account for biotic interactions may be less difficult than elsewhere. Climate influences species distributions directly through species' physiological tolerances or indirectly through its effect on available habitats, food resources and biotic interactions such as the presence of competitors (Ara ujo & Peterson, 2012, #CITATION_TAG.","In this review we consolidate evidence for how biotic interactions shape species distributions beyond local extents and review methods for integrating biotic interactions into species distribution modelling tools. A range of species distribution modelling tools is available to quantify species environmental relationships and predict species occurrence, such as: (i) integrating pairwise dependencies, (ii) using integrative predictors, and (iii) hybridising species distribution models (SDMs) with dynamic models. These methods have typically only been applied to interacting pairs of species at a single time, require a priori ecological knowledge about which species interact, and due to data paucity must assume that biotic interactions are constant in space and time. To better inform the future development of these models across spatial scales, we call for accelerated collection of spatially and temporally explicit species data. Ideally, these data should be sampled to reflect variation in the underlying environment across large spatial extents, and at fine spatial resolution.","['Predicting which species will occur together in the future, and where, remains one of the greatest challenges in ecology, and requires a sound understanding of how the abiotic and biotic environments interact with dispersal processes and history across scales.']"
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Abstract The Rose-ringed parakeet Psittacula krameri, a bird species of subtropical origin, has established feral populations in temperate Europe. In Europe, radio-tracking (#CITATION_TAG; Strubbe & Matthysen, 2011) and habitat selection studies (Strubbe & Matthysen, 2007; Newson et al., 2010) indicate that parakeets prefer to forage in city parks and gardens, where bird feeders and ornamental vegetation present parakeets with abundant food.","We analysed the feeding habits of Rose-ringed parakeets near Paris, France, in order to assess if food provided by humans might contribute to the success of this invasive bird species. We recorded the consumption of seeds, buds and fruits from many native and introduced shrubs and trees, and the use of bird feeders in private and public gardens throughout the year. We followed four radio-equipped birds for 150 hours during September - December 2008, which spent about half of their feeding time at bird feeders.",['Our study underlines the opportunistic granivorous-frugivorous character of this parakeet species and the possible role of human food sources in the success of its establishment in many cities across temperate Europe.']
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Niche-based models calibrated in the native range by relating species observations to climatic variables are commonly used to predict the potential spatial extent of species' invasion. Climate matching is thus a useful approach to identify areas at risk of introduction and establishment of newly or not-yet-introduced neophytes, but may not predict the full extent of invasions. Therefore, we assess three key assumptions underlying bioclimatic envelope models: (1) that species' distributions are largely governed by climate (Ara ujo & Peterson, 2012), (2) that a species' current native distribution corresponds with the total set of climate conditions under which it can persist (Peterson, 2003) and (3) that the climatic niche remains conserved across time and space (#CITATION_TAG).","We test this assumption by analysing the climatic niche spaces of Spotted Knapweed in western North America and Europe. The models fail to predict the current invaded distribution, but correctly predict areas of introduction.",['This climate matching approach relies on the assumption that invasive species conserve their climatic niche in the invaded ranges.']
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Numbers of non-indigenous species--species introduced from elsewhere--are increasing rapidly worldwide, causing both environmental and economic damage. Rigorous quantitative risk-analysis frameworks, however, for invasive species are lacking. allocation of resources between prevention and control). By contrast, the US Fish and Wildlife Service spent US$825 000 in 2001 to manage all aquatic invaders in all US lakes. As eradication is frequently costly and sometimes impossible, attempting to limit the further introduction and spread of invasive species is the most effective and cost-efficient management strategy (#CITATION_TAG).","We need to evaluate the risks posed by invasive species and quantify the relative merits of different management strategies (e.g. The model identifies the optimal allocation of resources to prevention versus control, acceptable invasion risks and consequences of invasion to optimal investments (e.g. We apply the model to zebra mussels (Dreissena polymorpha), and show that society could benefit by spending up to US$324 000 year-1 to prevent invasions into a single lake with a power plant.",['We present a quantitative bioeconomic modelling framework to analyse risks from non-indigenous species to economic activity and the environment.']
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Models predicting species spatial distribution are increasingly applied to wildlife management issues, emphasising the need for reliable methods to evaluate the accuracy of their predictions. As many available datasets (e.g. museums, herbariums, atlas) do not provide reliable information about species absences, several presence-only based analyses have been developed. However, methods to evaluate the accuracy of their predictions are few and have never been validated. Model transferability was assessed using European parakeet occurrence data (n = 513), applying the full range of evaluation statistics available in biomod2, plus two statistics specifically designed for presence-only models (the 10-fold and the continuous Boyce index, #CITATION_TAG).","We use a reliable, diverse, presence/absence dataset of 114 plant species to test how common presence/absence indices (Kappa, MaxKappa, AUC, adjusted D-2) compare to presenceonly measures (AVI, CVI, Boyce index) for evaluating generalised linear models (GLM). Moreover we propose a new, threshold-independent evaluator, which we call ""continuous Boyce index"". All indices were implemented in the B10MAPPER software. The continuous Boyce index is thus both a complement to usual evaluation of presence/absence models and a reliable measure of presence-only based predictions",['The aim of this paper is to compare existing and new presenceonly evaluators to usual presence/absence measures.']
"Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Adaptive evolution is currently accepted as playing a significant role in biological invasions. Adaptations relevant to invasions are typically thought to occur either recently within the introduced range, as an evolutionary response to novel selection regimes, or within the native range, because of long-term adaptation to the local environment. However, within the native range, species may also evolve pre-adaptations to invasiveness; strong selection imposed by human modification of habitats within the native range is likely to lead to adaptation prior to introduction elsewhere (#CITATION_TAG).","We label this scenario 'Anthropogenically Induced Adaptation to Invade'. We illustrate how it differs from other evolutionary processes that may occur during invasions, and how it can help explain accelerating rates of invasions.","['We propose that recent adaptation within the native range, in particular adaptations to human-altered habitat, could also contribute to the evolution of invasive populations.']"
"NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the ""Western diet"" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The ""Selfish Brain"" is a term coined by Robert L. DuPont in the title of his book where he wrote: ""With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: ""The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources"" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the ""push"" principle borrowed from the economic ""push-pull"" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: ""According to the 'push' principle, the environment pushes excess amounts of energy into the organism"" (Hill and Peters, 1998). According to DuPond, ""What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward"" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both ""pull"" and ""push"" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this ""push"" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called ""unique"" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the ""puSh"" component of energy metaBolISm The environment in Western-type societies can be characterized as ""pushing"" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental ""Western Diet"" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: ""antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other"" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental ""push."" In a recent article entitled ""Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy,"" J. Zimmerman wrote: ""In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories"" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. ""Control"" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. The ability to store energy enables organisms to deal with temporarily harsh and uncertain conditions. Empirical studies have demonstrated that organisms adapted to fluctuating energy availability plastically adjust their storage strategies. So far, however, theoretical studies have investigated general storage strategies only in constant or deterministically varying environments. In environments with low variability and low predictability of energy availability, it is not optimal to store energy. As environments become more variable or more predictable, energy allocation to storage is increasingly favoured. The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (#CITATION_TAG).","By varying environmental variability, environmental predictability, and the cost of survival, we obtain a variety of different optimal life-history strategies, from highly iteroparous to semelparous, which differ significantly in their storage patterns.","['In this study, we analyze how the ability to store energy influences optimal energy allocation to storage, reproduction, and maintenance in environments in which energy availability varies stochastically.']"
"NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the ""Western diet"" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The ""Selfish Brain"" is a term coined by Robert L. DuPont in the title of his book where he wrote: ""With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: ""The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources"" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the ""push"" principle borrowed from the economic ""push-pull"" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: ""According to the 'push' principle, the environment pushes excess amounts of energy into the organism"" (Hill and Peters, 1998). According to DuPond, ""What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward"" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both ""pull"" and ""push"" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this ""push"" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called ""unique"" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the ""puSh"" component of energy metaBolISm The environment in Western-type societies can be characterized as ""pushing"" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental ""Western Diet"" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: ""antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other"" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental ""push."" In a recent article entitled ""Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy,"" J. Zimmerman wrote: ""In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories"" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. ""Control"" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. The brain occupies a special hierarchical position in the organism. It is separated from the general circulation by the blood-brain barrier, has high energy consumption and a low energy storage capacity, uses only specific substrates, and it can record information from the peripheral organs and control them. The brain gives priority to regulating its own adenosine triphosphate (ATP) concentration. In that postulate, the peripheral energy supply is only of secondary importance. The brain has two possibilities to ensure its energy supply: allocation or intake of nutrients. The term 'allocation' refers to the allocation of energy resources between the brain and the periphery. This signal affects the brain ATP concentration by locally (via astrocytes) stimulating glucose uptake across the blood-brain barrier and by systemically (via the LHPA system) inhibiting glucose uptake into the muscular and adipose tissue. This setpoint can permanently and pathologically be displaced by extreme stress situations (chronic metabolic and psychological stress, traumatization, etc. ), by starvation, exercise, infectious diseases, hormones, drugs, substances of abuse, or chemicals disrupting the endocrine system. Disorders in the 'energy on demand' process or the LHPA-system can influence the allocation of energy and in so doing alter the body mass of the organism. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources"" (#CITATION_TAG).","Neocortex and the limbic-hypothalamus-pituitary-adrenal (LHPA) system control the allocation and intake. In order to keep the energy concentrations constant, the following mechanisms are available to the brain: (1) high and low-affinity ATP-sensitive potassium channels measure the ATP concentration in neurons of the neocortex and generate a 'glutamate command' signal. (2) High-affinity mineralocorticoid and low-affinity glucocorticoid receptors determine the state of balance, i.e. the setpoint, of the LHPA system. In summary, the presented model includes a newly discovered 'principle of balance' of how pairs of high and low-affinity receptors can originate setpoints in biological systems.","['Here we present a new paradigm for the regulation of energy supply within the organism.', ""In this 'Selfish Brain Theory', the neocortex and limbic system play a central role in the pathogenesis of diseases such as anorexia nervosa and obesity""]"
"NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the ""Western diet"" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The ""Selfish Brain"" is a term coined by Robert L. DuPont in the title of his book where he wrote: ""With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: ""The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources"" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the ""push"" principle borrowed from the economic ""push-pull"" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: ""According to the 'push' principle, the environment pushes excess amounts of energy into the organism"" (Hill and Peters, 1998). According to DuPond, ""What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward"" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both ""pull"" and ""push"" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this ""push"" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called ""unique"" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the ""puSh"" component of energy metaBolISm The environment in Western-type societies can be characterized as ""pushing"" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental ""Western Diet"" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: ""antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other"" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental ""push."" In a recent article entitled ""Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy,"" J. Zimmerman wrote: ""In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories"" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. ""Control"" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. The alarming increase in the incidence of obesity and obesity-associated disorders makes the etiology of obesity a widely studied topic today. As opposed to 'homeostatic feeding', where food intake is restricted to satisfy one's biological needs, the term 'non-homeostatic' feeding refers to eating for pleasure or the trend to over-consume (palatable) food. Overconsumption is considered a crucial factor in the development of obesity. At a molecular level, insulin and leptin resistance are hallmarks of obesity. The inter-relationship between neuronal populations in the arcuate nucleus and other areas regulating energy homeostasis (lateral hypothalamus, paraventricular nucleus, ventromedial hypothalamus etc.) Their traits notwithstanding, these behaviors are highly evolutionary significant: ""Although at first glance, hijacking of the homeostatic regulatory mechanisms by its hedonic counterpart may seem conflicting, it should be borne in mind that during evolution, humans have lived in an environment where food availability was restricted and uncertain (e.g., hunter-gatherers) and the biological system has been 'hard-wired' to maximize energy stores"" (#CITATION_TAG).",,"['In this review, we specifically address the question how leptin resistance contributes to enhanced craving for (palatable) food.', 'Since dopamine is a key player in the motivation for food, the interconnection between dopamine, leptin and neuropeptides related to feeding will be discussed.', 'Understanding the mechanisms by which these neuropeptidergic systems hijack the homeostatic feeding mechanisms, thus leading to overeating and obesity is the primary aim of this review.', 'The melanocortin system, one of the crucial neuropeptidergic systems modulating feeding behavior will be extensively discussed.']"
"NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the ""Western diet"" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The ""Selfish Brain"" is a term coined by Robert L. DuPont in the title of his book where he wrote: ""With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: ""The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources"" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the ""push"" principle borrowed from the economic ""push-pull"" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: ""According to the 'push' principle, the environment pushes excess amounts of energy into the organism"" (Hill and Peters, 1998). According to DuPond, ""What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward"" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both ""pull"" and ""push"" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this ""push"" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called ""unique"" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the ""puSh"" component of energy metaBolISm The environment in Western-type societies can be characterized as ""pushing"" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental ""Western Diet"" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: ""antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other"" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental ""push."" In a recent article entitled ""Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy,"" J. Zimmerman wrote: ""In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories"" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. ""Control"" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. Alzheimer's disease is a devastating disease whose recent increase in incidence rates has broad implications for rising health care costs. Huge amounts of research money are currently being invested in seeking the underlying cause, with corresponding progress in understanding the disease progression. This leads to cholesterol deficiency in neurons, which significantly impairs their ability to function. Over time, a cascade response leads to impaired glutamate signaling, increased oxidative damage, mitochondrial and lysosomal dysfunction, increased risk to microbial infection, and, ultimately, apoptosis. Other neurodegenerative diseases share many properties with Alzheimer's disease, and may also be due in large part to this same underlying cause.Copyright (c) 2011 European Federation of Internal Medicine. This metabolic mode, typical for consumers of the ""Western diet"" (Cordain et al., 2005; #CITATION_TAG), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011).","A first step in the pathophysiology of the disease is represented by advanced glycation end-products in crucial plasma proteins concerned with fat, cholesterol, and oxygen transport.","[""In this paper, we highlight how an excess of dietary carbohydrates, particularly fructose, alongside a relative deficiency in dietary fats and cholesterol, may lead to the development of Alzheimer's disease.""]"
"NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the ""Western diet"" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The ""Selfish Brain"" is a term coined by Robert L. DuPont in the title of his book where he wrote: ""With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: ""The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources"" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the ""push"" principle borrowed from the economic ""push-pull"" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: ""According to the 'push' principle, the environment pushes excess amounts of energy into the organism"" (Hill and Peters, 1998). According to DuPond, ""What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward"" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both ""pull"" and ""push"" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this ""push"" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called ""unique"" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the ""puSh"" component of energy metaBolISm The environment in Western-type societies can be characterized as ""pushing"" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental ""Western Diet"" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: ""antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other"" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental ""push."" In a recent article entitled ""Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy,"" J. Zimmerman wrote: ""In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories"" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. ""Control"" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. The young rat adjusts its food intake so precisely to its energy needs that its fat stores remain almost constant. Considerable variation in food intake is brought about in response to change in heat loss to the environment, or in loss of food through the mammary gland in lactation, without appreciable change of weight. Hypothalamic damage permits excessive intake and causes obesity. The degree of obesity and in general its rate of development, is a function of the degree of dam age to the region of the tuber cinereum , and is independent of changes of intake with environmental temperature. There is no disturbance of temperature regulation or acclimatization to changed environmental temperature in obese rats. In this system, either the size of fat depot (#CITATION_TAG; Woods and Ramsay, 2011) or glucose levels (Mayer, 1953) are being controlled.","Reasons are advanced for regarding this as the limiting rate at which absorbed foodstuffs can be removed from the circulation, that is as some aspect of the synthesis or transport of fat.","['It is suggested that the hypothalamic satiety mechanism is concerned only in the prevention of an overall surplus of energy intake over expenditure, which would cause the deposition of fat in the depots.']"
"NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the ""Western diet"" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The ""Selfish Brain"" is a term coined by Robert L. DuPont in the title of his book where he wrote: ""With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: ""The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources"" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the ""push"" principle borrowed from the economic ""push-pull"" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: ""According to the 'push' principle, the environment pushes excess amounts of energy into the organism"" (Hill and Peters, 1998). According to DuPond, ""What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward"" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both ""pull"" and ""push"" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this ""push"" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called ""unique"" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the ""puSh"" component of energy metaBolISm The environment in Western-type societies can be characterized as ""pushing"" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental ""Western Diet"" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: ""antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other"" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental ""push."" In a recent article entitled ""Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy,"" J. Zimmerman wrote: ""In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories"" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. ""Control"" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. The success of treatment, the average of results in all types of cases, depends on the truth of our concept of the relationship existing between symptoms or signs and the food supply. During the last few years the average of results obtained in the dietetic management of diabetes has been improved greatly through the work of Allen and Joslin, and the system they have developed is in some respects more logical and less empirical than any we have had heretofore. Yet the literature of the subject is still confused by a lack of unanimity among all writers as to the best manner of handling all cases. In a recent monograph, Falta1has again told the merits of his ""cereal cure"" Later, #CITATION_TAG Where KR is ""ketogenic ratio,"" g is grams, P is protein, F is fat, and C is CHO.",,['In the dietetic management of diabetes we are engaged in the effort to correlate symptoms and signs shown by the patient with the kinds and quantities of food he consumes.']
"NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the ""Western diet"" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The ""Selfish Brain"" is a term coined by Robert L. DuPont in the title of his book where he wrote: ""With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: ""The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources"" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the ""push"" principle borrowed from the economic ""push-pull"" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: ""According to the 'push' principle, the environment pushes excess amounts of energy into the organism"" (Hill and Peters, 1998). According to DuPond, ""What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward"" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both ""pull"" and ""push"" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this ""push"" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called ""unique"" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the ""puSh"" component of energy metaBolISm The environment in Western-type societies can be characterized as ""pushing"" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental ""Western Diet"" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: ""antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other"" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental ""push."" In a recent article entitled ""Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy,"" J. Zimmerman wrote: ""In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories"" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. ""Control"" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. It has become clear that the underlying mechanisms for the ergogenic effect during this type of activity are not metabolic but may reside in the central nervous system. Recent findingsCarbohydrate mouth rinses have been shown to result in similar performance improvements. These effects are specific to carbohydrate and are independent of taste. The receptors in the oral cavity have not (yet) been identified and the exact role of various brain areas is not clearly understood. SummaryCarbohydrate is detected in oral cavity by unidentified receptors and this can be linked to improvements in exercise performance. These regions were unresponsive to sweet, non-CHO stimulation with saccharin (#CITATION_TAG).",,['Purpose of reviewCarbohydrate during exercise has been demonstrated to improve exercise performance even when the exercise is of high intensity (>75% VO2max) and relatively short duration (~1 h).']
"The topic of discussion was their views on the implementation of inclusive education. In seeking this feedback, we were interested in seeing if they interpreted the sessions as being learning experiences -that is, as sessions that enabled the participants to learn from each other as well as from facilitators with a view to promoting mutual learning. Furthermore, our form of pragmatism veers in the direction of including 1 features associated with the transformative paradigm as elucidated by #CITATION_TAG. Mertens (2010) indicates that, within the transformative paradigmatic outlook, it is recognised that it is part of the researcher's responsibility to consider the uses that might be made of their work, and to take into consideration the way in which research outcomes can be linked to social justice.","Abstract As teachers of mixed methods, we have a responsibility to nurture our students' abilities to think through their choices in terms of mixed methods research based on a critically examined understanding of their philosophical assumptions. The belief systems associated with the transformative paradigm are used to illustrate the importance of teaching philosophical frameworks as a part of mixed methods instruction.",['This is an important area of exploration for mixed methods researchers who seek to improve the validity of their findings.']
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. In addition, basal respiratory frequency has a non-linear relationship with spectral power as breathing rate falls below approximately 0.15 Hz (as it occasionally does in athletes; #CITATION_TAG).","Fifteen male athletes were subjected to HRV measurements under six randomised breathing conditions: spontaneous breathing frequency (SBF) and five others at controlled breathing frequencies (CBF) (0.20; 0.175; 0.15; 0.125 and 0.10 Hz). The subjects were divided in two groups: the first group included athletes with SBF <0.15 Hz (infSBF) and the second athletes with SBF higher than 0.15 Hz (supSBF). In this study, BF was the main modulator of the LF/HF ratio in both controlled breathing and spontaneous breathing. During each CBF, all athletes presented spectral energy mainly concentrated around their BF.",['The purpose of this study was to measure the influence of breathing frequency (BF) on heart rate variability (HRV) and specifically on the Low Frequency/High Frequency (LF/HF) ratio in athletes.']
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. Frequency analysis of the electrocardiographic RR interval is a common method of quantifying autonomic outflow by measuring the beat-to-beat modulation of the heart (heart rate variability; HRV). Few papers ideally control for medication, food, and water consumption, bladder filling, time of day, and other extraneous factors (Tak et al., 2009; #CITATION_TAG).",,"['This review identifies a series of problems with the methods of doing so-the interpretation of low-frequency spectral power, the multiple use of equivalent normalized low frequency (LFnu), high frequency (HFnu) and ratio (LF/HF) terms, and the lack of control over extraneous variables, and reviews research in the calendar year 2012 to determine their prevalence and severity.']"
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. Heart rate variability (HRV), the beat-to-beat variation in either heart rate or the duration of the R-R interval - the heart period, has become a popular clinical and investigational tool. The temporal fluctuations in heart rate exhibit a marked synchrony with respiration (increasing during inspiration and decreasing during expiration - the so called respiratory sinus arrhythmia, RSA) and are widely believed to reflect changes in cardiac autonomic regulation. Although the exact contributions of the parasympathetic and the sympathetic divisions of the autonomic nervous system to this variability are controversial and remain the subject of active investigation and debate, a number of time and frequency domain techniques have been developed to provide insight into cardiac autonomic regulation in both health and disease. Briefly, pulse rate was first measured by ancient Greek physicians and scientists. However, it was not until the invention of the ""Physician's Pulse Watch"" (a watch with a second hand that could be stopped) in 1707 that changes in pulse rate could be accurately assessed. Stephen Hales (1733) was the first to note that pulse varied with respiration and in 1847 Carl Ludwig was the first to record RSA. With the measurement of the ECG (1895) and advent of digital signal processing techniques in the 1960s, investigation of HRV and its relationship to health and disease has exploded. While it may be the case that HRV can be used as a neurobiological index of interpersonal interaction, significant caveats exist due to the complicated nature of HRV and consequently uncertainty regarding what information is actually provided by common HRV indices (Berntson et al., 1997; Malpas, 2002; #CITATION_TAG).",,['It is the purpose of this essay to provide an historical overview of the evolution in the concept of HRV.']
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. Random error, however, represents a limited part of the between-subject variability; therefore observed differences between individuals mostly reflect differences in the subjects' error-free value rather than random error. Indeed, to appropriately detect a difference between groups, a sample size between 30 and 77, depending on the HRV metric used, is needed (#CITATION_TAG).","From the RR intervals we computed standard indexes of HRV: SDNN (S.D. of RR interval values), RMSSD (root-mean-square of successive RR interval differences), LF (low frequency) and HF (high frequency) power (absolute and normalized units) and LF/HF. Absolute reliability was assessed by 95% limits of random variation; relative reliability was assessed by the ICC (intraclass correlation coefficient).",['The main aim of the present study was to perform an in-depth assessment of absolute and relative reliability of standard indexes of HRV from short-term laboratory recordings.']
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. Normal breathing consists of considerable correlated variability (parameters of subsequent breaths are correlated) and some random variability. Emotional and attentive states alter normal breathing variability, which can be restored by a sigh. Moreover, the mental stress that usually accompanies these tasks can also disorder general respiratory coordination (#CITATION_TAG).","Mental arithmetic and sustained attention were characterized by decreased correlated and total breathing variability, respectively. A spontaneous sigh restored correlated variability. An instructed sigh restored correlated variability following mental arithmetic, and increased total variability following sustained attention.",['The present study aimed to investigate the effects of mental arithmetic and sustained attention on respiratory variability.']
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. Intriguingly, the degree of coupling may be higher when HRV is increased and at lower breathing frequencies (#CITATION_TAG; Tzeng et al., 2003), suggesting that unhealthy populations or experiments that are designed to reduce HRV may be more prone to decoupling of cardiorespiratory oscillations.","The beat-to-beat RR interval time series of 98 anaesthetized, spontaneously breathing subjects were represented graphically as (1) raw RR interval time series, (2) RR consecutive difference time series and (3) a phase portrait of the RR consecutive difference time series. We then examined the relationships between the presence of cardioventilatory coupling in these epochs and the plot appearance and entropy measures derived from these plots.",['In this study we sought to develop quantitative methods for determining the presence of cardioventilatory coupling in raw heart rate time series.']
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. In lieu of direct respiratory measures, established algorithms (Moody et al., 1985 (Moody et al.,, 1986) that have been successively improved (e.g., #CITATION_TAG; Langley et al., 2010) can also provide an appropriate surrogate measure of respiration from based on ECG signal morphology.","The whole system consists of two-lead electrocardiogram acquisition using conductive textile electrodes located in bed, baseline fluctuation elimination, R-wave detection, adjustment of sudden change in R-wave area using moving average, and optimal lead selection. In order to solve the problems of previous algorithms for the ECG-derived respiration (EDR) signal acquisition, we are proposing a method for the optimal lead selection. An optimal EDR signal among the three EDR signals derived from each lead (and arctangent of their ratio) is selected by estimating the instantaneous frequency using the Hilbert transform, and then choosing the signal with minimum variation of the instantaneous frequency.","['In this paper, an improved algorithm for the extraction of respiration signal from the electrocardiogram (ECG) in home healthcare is proposed.']"
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. The diagnosis of autonomic neuropathy frequently depends on results of tests which elicit reflex changes in heart rate. Few well-documented normal ranges are available for these tests. In view of the decline in heart rate variation with increasing age, normal ranges for tests of autonomic function must be related to the age of the subject. physical activity levels (Britton et al., 2007; Soares-Miranda et al., 2014), and age (#CITATION_TAG).",A computerised method of measurement of R-R interval variation was used to study heart rate responses in 310 healthy subjects aged 18-85 years. Normal ranges (90% and 95% confidence limits) for subjects aged 20-75 years were calculated for heart rate difference (max-min) and ratio (max/min) and standard deviation (SD).,"['The present study was designed to investigate the effect of age upon heart rate variability at rest and in response to a single deep breath, the Valsalva manoeuvre, and standing.']"
"Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. An academic scientist's professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. Likewise, recent interest in data uploading and retention (e.g., #CITATION_TAG) has received little systematic attention in cardiac psychophysiology so far, even though a) data retention is a American Psychological Association requirement (American psychological association [APA], 2001) and b) the ability to broadly access raw data is a potentially excellent control for the methodological and analytical issues outlined here, as well as a test bed for the development of future HRV metrics and meta-analysis.",,"['This enshrines the low status of the journal and its content.', 'This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.']"
"During the stationary growth phase, the stoichiometric relationship depended on the limiting nutrient, but with generally increasing C:N:P ratio. Understanding these dynamics will be important for improving models of aquatic primary production and biogeochemical cycles in a warming climate. A global assessment of carbon flux in the world ocean is one of the major undertakings of the Joint Global Ocean Flux Study (JGOFS). As required by the temporal and spatial scales involved in a global study, it can be conveniently done by combining, through appropriate models, remotely sensed information (chlorophyll a, temperature) with basic information about the parameters related to the carbon uptake by phytoplanktonic algae. When modelling primary productivity, some of the key parameters in measurements of photosynthetic production are: the maximum light utilization coefficient (α Ã), which is the initial slope, α, of the photosynthesis-irradiance (PE) curve normalized to Chl a; the maximum photosynthetic rate (P m Ã); the irradiance where production equals consumption i.e. the compensation light intensity (E c); and finally the light saturation parameter (E k) [#CITATION_TAG].","This has to be undertaken using historical in situ data of primary productivity. The measurement of the photosynthetic response of algae [the photosynthesis (P) versus in-adiance (PS) curves], besides being less shiptime consuming than in situ primary production experiments, allows the needed parameters to be derived and systematically studied as a function of the physical, chemical and ecological conditions.","['The aim of the present paper is to review the significance of these parameters, especially in view of their introduction into models, to analyze the causes of their variations in the light of physiological considerations, and finally to provide methodological recommendations for meaningful determinations, and interpretation, of the data resulting from /""versus PS determinations.']"
"During the stationary growth phase, the stoichiometric relationship depended on the limiting nutrient, but with generally increasing C:N:P ratio. Understanding these dynamics will be important for improving models of aquatic primary production and biogeochemical cycles in a warming climate. Increased anthropogenic loading of nitrogen (N) and phosphorus (P) has led to an eutrophication problem in the Baltic Sea, and the spring bloom is a key component in the biological uptake of increased nutrient concentrations. The spring bloom in the Baltic Sea is dominated by both diatoms and dinoflagellates. However, the sedimentation of these groups is different: diatoms tend to sink to the sea floor at the end of the bloom, while dinoflagellates to a large degree are been remineralized in the euphotic zone. The main phytoplankton groups, diatoms and dinoflagellates, have different physiological properties, which clearly separate their life strategies. De fysiologiska egenskaperna hos kiselalgerna och dinoflagellaterna blev anvanda i en modell av tidpunkten for varblomningen. In this study, we present growth, element stoichiometry and primary production of a coldwater adapted, model organism, Chaetoceros wighamii; a common bloom forming diatom in the Baltic Sea [#CITATION_TAG], subjected to a range of growth conditions around its known optimum.","This was achieved by specific studies of: (1) seasonal vertical positioning, (2) dinoflagellate life cycle, (3) mixotrophy, (4) primary production, respiration and growth and (5) diatom silicate uptake, using cultures of common cold-water diatoms: Chaetoceros wighamii, C. gracilis, Pauliella taeniata, Thalassiosira baltica, T. levanderi, Melosira arctica, Diatoma tenuis, Nitzschia frigida, and dinoflagellates: Peridiniella catenata, Woloszynskia halophila and Scrippsiella hangoei. The physiological properties of diatoms and dinoflagellates were used in a model of the onset of the spring bloom: for the diatoms the model could predict the initiation of the spring bloom; S. hangoei, on the other hand, could not compete successfully and did not obtain positive growth in the model. The dinoflagellates do, however, have competitive advantages that were not included in the model: motility and mixotrophy. In mesocosm experiments dinoflagellates could not compete with diatoms when their initial numbers were low.","['Understanding phytoplankton competition and species specific ecological strategies is thus of importance for assessing indirect effects of phytoplankton community composition on eutrophication problems.', 'The main objective of this thesis was to describe some basic physiological and ecological characteristics of the main cold-water diatoms and dinoflagellates in the Baltic Sea.']"
"During the stationary growth phase, the stoichiometric relationship depended on the limiting nutrient, but with generally increasing C:N:P ratio. Understanding these dynamics will be important for improving models of aquatic primary production and biogeochemical cycles in a warming climate. Description: This landmark publication takes the 50th anniversary of the publication of the seminal paper by the Danish scientist, Einer Steemann Nielsen, as an occasion to assess the development, present state and future of the major aspects in freshwater and marine plankton productivity. Each chapter of this important work has been written by internationally-acknowledged experts in the subject, and the whole has been carefully drawn together and edited to provide a book that is an essential tool and reference for all aquatic scientists. Phytoplankton Productivity provides, in one book, cutting edge reviews and key facts on the subject, making it a vital information source for marine and freshwater biologists, oceanographers, ecologists, environmental scientists and plant scientists. Copies should also be available in libraries of any research establishment and university as a reference for students, wherever these subjects are studied and taught. Another process that will affect the PQ is photorespiration, which is a process consuming the O 2 produced during photosynthesis and thereby lowering the PQ [#CITATION_TAG].","The book takes ascending temporal and spatial size scale as its framework - covering molecular to geological scales. Chapters include reviews of physiology and biochemistry, measurement of phytoplankton productivity, the supply and uptake of nutrients, variability in processes and production, the evolution of the carbon cycle, and ecosystems. Historical aspects are discussed together with thought-provoking assessments of modern technological approaches and where future research emphasis should be focussed.","['The subject is set in context with a chapter covering the work of Steemann Nielsen, whose work inspired the last 50 years of aquatic productivity studies.']"
"During the stationary growth phase, the stoichiometric relationship depended on the limiting nutrient, but with generally increasing C:N:P ratio. Understanding these dynamics will be important for improving models of aquatic primary production and biogeochemical cycles in a warming climate. Biological stoichiometry provides a mechanistic theory linking cellular and biochemical features of co-evolving biota with constraints imposed by ecosystem energy and nutrient inputs. Among various factors affecting organism stoichiometry, differences in C : P and N : P stoichiometry have been hypothesized to reflect organismal P-content because of altered allocation to P-rich ribosomal RNA at different growth rates (the growth rate hypothesis, GRH). Increasing rRNA, coupled with increasing growth rates, have been shown to decrease the N:P ratio over a range of different organisms and biotopes [#CITATION_TAG].",,"['Thus, understanding variation in biomass carbon : nitrogen : phosphorus (C : N : P) stoichiometry is a major priority for integrative biology.']"
"During the stationary growth phase, the stoichiometric relationship depended on the limiting nutrient, but with generally increasing C:N:P ratio. Understanding these dynamics will be important for improving models of aquatic primary production and biogeochemical cycles in a warming climate. The elemental ratios of marine phytoplankton emerge from complex interactions between the biotic and abiotic components of the ocean, and reflect the plastic response of individuals to changes in their environment. The stoichiometry of phytoplankton is, thus, dynamic and dependent on the physiological state of the cell. We present a theoretical model for the dynamics of the carbon, nitrogen and phosphorus contents of a phytoplankton population. Traditionally factors such as the growth-limiting nutrient have been used to model nutrient uptake and growth [27], and recent advances have started to incorporate uptake-protein regulation into this equation [#CITATION_TAG] [29].","By representing the regulatory processes controlling nutrient uptake, and focusing on the relation between nutrient content and protein synthesis, our model qualitatively replicates existing experimental observations for nutrient content and ratios. The population described by our model takes up nutrients in proportions that match the input ratios for a broad range of growth conditions. In addition, there are two zones of single-nutrient limitation separated by a wide zone of co-limitation. Within the co-limitation zone, a single point can be identified where nutrients are supplied in an optimal ratio.","['Our model contributes to the understanding of the global cycles of oceanic nitrogen and phosphorus, as well as the elemental ratios of these nutrients in phytoplankton populations.']"
"During the stationary growth phase, the stoichiometric relationship depended on the limiting nutrient, but with generally increasing C:N:P ratio. Understanding these dynamics will be important for improving models of aquatic primary production and biogeochemical cycles in a warming climate. Most oceanic biogeochemical models include dynamic C:Chl a ratios with photoacclimation parameterization [1, [7] [8], and it is important to understand interaction effects of several environmental parameters for improved parameterization [9] [#CITATION_TAG].","Photosynthetic rates, growth rates, cell carbon, cell protein, and chlorophyll a content of two diatom and two dinoflagellate species were measured. The microalgae were chosen to have one small and one large species from each phylogenetic group; the two size categories differed from each other by 1.5 orders of magnitude in terms of cell carbon or cell protein. The cultures for the experiments were grown under continuous light at an irradiance high enough for the light-saturation of growth for all four species. The four species were found to have similar maximum photosynthetic rates per unit chlorophyll a. The higher growth rates of the diatoms were shown to be related to their higher photosynthetic rates per unit carbon.",['The ecological significance of the physiological difference between these two groups of microalgae is discussed.']
"During the stationary growth phase, the stoichiometric relationship depended on the limiting nutrient, but with generally increasing C:N:P ratio. Understanding these dynamics will be important for improving models of aquatic primary production and biogeochemical cycles in a warming climate. Phytoplanktons are globally important organisms, responsible for up to 50 % of the world&apos;s primary production. 1) Spatial patterns: the vertical distribution of phytoplankton in poorly-mixed water columns. For example, there tend to be greater allocation of resources to P rich RNA during exponential growth (reducing the N:P ratio), and the N:P ratio has different optima for different growth conditions; the canonical N:P of 16 represents rather an average of a whole community than the optimum for individual species [#CITATION_TAG].",3) Stoichiometric patterns: nitrogen to phosphorus content of phytoplankton.,"['In this talk I will discuss three patterns found in phytoplankton communities.', '2) Temporal patterns: the seasonal succession of species.']"
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Although the associations between oral biologic variables such as malocclusion and oral-health-related quality of life (OHRQOL) have been explored, little research has been done to address the influence of psychological characteristics on perceived OHRQOL. It has also been found that psychological factors might explain more about the impact of dental disorders upon individuals than their clinical symptoms [39] [40] [41] [#CITATION_TAG] [43].","The child perception questionnaire (CPQ11-14) and the PWB subscale of the child health questionnaire were administered at baseline and follow-up. Occlusal changes were assessed by using the dental aesthetic index. A waiting-list comparison group was used to account for age-related effects.Although the treatment subjects had significantly better OHRQOL scores at follow-up, the results were significantly modified by each subject's PWB status (P <0.01).",['The aim of this study was to assess OHRQOL outcomes in orthodontics while controlling for individual psychological characteristics.']
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. #CITATION_TAG, 90, 91 O her studies have found no difference in the OHQoL between treated and untreated groups.","METHODS A cross-sectional study was conducted in Bauru, SP, Brazil, on 1675 randomly selected adolescents aged between 15 and 16 years. Adolescents were clinically examined using the Index of Orthodontic Treatment Need (IOTN). Two oral health-related quality of life measures, namely the Oral Impacts on Daily Performance (OIDP) and the shortened version of the Oral Health Impacts Profile (OHIP-14) were used to assess the adolescents' oral health-related impacts. Multiple logistic regression was used in the data analysis.","['OBJECTIVES The first objective was to assess whether having had orthodontic treatment affected the levels of oral health-related quality of life impacts in Brazilian adolescents.', 'A second objective was to assess the relationship between a normative clinical measure of orthodontic treatment need and two measures of oral health-related quality of life.']"
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Using a technique of altering the arrangements of the teeth on standardised photographs of young people smiling, Shaw and colleagues showed that the appearance of teeth could influence the social judgements made by their peers about the person in the photograph #CITATION_TAG, 48; however dental appearance did not affect the judgements made by teachers.","Black and white photographs of an attractive boy and girl and an unattractive boy and girl were obtained and modified so that, for each face, five different photographic versions were available. In each version, the child's face was standardized except that a different dentofacial arrangement was demonstrated. These were normal incisors, prominent incisors, a missing lateral incisor, severely crowded incisors, and unilateral cleft lip. Each photograph was viewed by a different group of forty-two children and forty-two adults, equally divided as to sex. Their impressions of the depicted child's social attractiveness were recorded on visual analogue scales. The experimental procedure was such that the effect and interaction of different levels of facial attractiveness, different dentofacial arrangements, sex of the photographed child, and sex of the judge could be analyzed.",['The purpose of this study was to determine whether the social attractiveness of a child would be influenced by his or her dentofacial appearance.']
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Few studies have examined, comprehensively and prospectively, determinants of oral-health-related quality of life. Sense of coherence was the most important psychosocial predictor. It has also been found that psychological factors might explain more about the impact of dental disorders upon individuals than their clinical symptoms [#CITATION_TAG] [40] [41] [42] [43].","Measures of symptom and functional status, health perceptions, quality of life, oral health beliefs, and psychological (sense of coherence, self-esteem, health locus of control) and social factors (parents' income and education) were collected from 439 12- and 13-year-olds at baseline and six-month follow-up, together with a clinical examination at baseline.","['The aim of this study was to examine the relationships between psychosocial factors and oral health status, health perceptions, and quality of life.', ""These factors are important in understanding how oral health affects young people's daily lives.""]"
Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Sense of coherence (SOC) has been related to oral health behaviors and oral-health-related quality of life (OHRQoL) in observational studies. It has also been found that psychological factors might explain more about the impact of dental disorders upon individuals than their clinical symptoms [39] [#CITATION_TAG] [41] [42] [43].,"Twelve primary schools were randomly allocated to intervention and control groups. The intervention was comprised of 7 sessions over 2 mos, focusing on child participation and empowerment. The first 4 sessions were classroom activities, and the last 3 involved working on healthy school projects. Trained teachers who received a one-day course delivered the intervention.",['This cluster-randomized trial aimed to test the effect of an intervention to enhance SOC on OHRQoL in children.']
Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. This has been found in non-clinical populations (mainly schoolchildren) [65][66][67]#CITATION_TAG[69][70][71][72][73][74] as well as young people referred for orthodontic treatment,These were related to the child's emotional and social well-being.,"['The purpose of this study was to validate the child perception questionnaire (CPQ(11-14)) with a sample of schoolchildren in Greater Manchester, United Kingdom.We made a longitudinal survey of children from 1999 to 2002, using the index of orthodontic treatment need (IOTN) at baseline when the children were 11 to 12 years old, the CPQ(11-14), and their uptake of orthodontic treatment 3 years later.CPQ(11-14) scores corresponded to differences in IOTN scores.']"
Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Different anterior occlusal traits affected the quality of life differently. #CITATION_TAG] [100] Marshman and colleagues 101 have expressed concern that generic measures of OHQoL might not fully capture the impact that malocclusion has suggested the development of a malocclusion specific measure of OHQoL for young people with malocclusion.,"For that, 1,318 15-16-yr-old adolescents were randomly selected from children attending all secondary schools in Bauru, Brazil. Participants were first interviewed and then dentally examined. Condition-specific impacts (CSI) attributed to malocclusion were calculated using the Oral Impacts on Daily Performances (OIDP index). Nine anterior occlusal traits were evaluated: maxillary and mandibular overjet; openbite and overbite; centreline deviation; dental crowding and spacing; and maxillary and mandibular irregularity. Statistical associations were tested using binary logistic regression analysis. No interaction term was identified.","['The aim of this study was to assess the association between different anterior occlusal traits and the presence of condition-specific sociodental impacts on the quality of life attributed to malocclusion.', 'This study, designed to overcome the limitations found in the existing literature, supports the idea that malocclusion can affect social interaction and psychosocial wellbeing.']"
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. However, there was a linear trend for the prevalence of CS-OIDP attributed to malocclusion, by level of normative orthodontic treatment need (P = 0.042). #CITATION_TAG] [100] Marshman and colleagues 101 have expressed concern that generic measures of OHQoL might not fully capture the impact that malocclusion has suggested the development of a malocclusion specific measure of OHQoL for young people with malocclusion.","Two hundred 16- to 17-year-old adolescents were randomly selected from 957 children attending a public college in London, UK. During interviews, participants provided information about demographic variables and socio-dental impacts on quality of life attributed to malocclusions, using the Condition-Specific form of the Oral Impacts on Daily Performances (CS-OIDP) index. Statistical comparison by covariates was performed using chi-squared test and chi-squared test for trends.",['The aim of this study was to assess the prevalence of condition-specific impacts on daily performances attributed to malocclusion in British adolescents.']
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. #CITATION_TAG This will also affect muscle function, which in turn will reduce masticatory efficiency.","The oral impact of malocclusion was assessed using the Oral Impact on Daily Performance (OIDP), whereas clinical criteria were assessed using the Dental Aesthetic Index (DAI). Self-perception of dental aesthetics was assessed using the Oral Aesthetic Subjective Impact Scale (OASIS) and self-esteem was assessed using the Global Negative Self-Evaluation (GSE) scale. Other variables were assessed using questionnaires. The chi-square test, simple and multiple logistic regression analyses were used for the statistical analysis.Ninety five adolescents (24%) reported feeling embarrassed to smile (aesthetic impact).","['The aim of the present study was to determine the biopsychosocial impact of malocclusion on the daily living of Brazilian adolescents (14 to 18 years of age) through normative and subjective records and identify factors directly involved in the self-perception of malocclusions.Cross-sectional.Public and private schools in the city of Belo Horizonte, Brazil.The sample was made up of 403 adolescents, with no prior history of orthodontic treatment, who were selected randomly from a population of 182,291 students in the same age range.']"
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. A number of studies focusing on pre-school children, often aged 5 years or below, failed to demonstrate an association between the presence of a malocclusion and OHQoL, 87, 88 whilst other studies based on adolescents between the approximate ages of 11 to 15 years, have found the presence of a malocclusion to have a negative impact on OHQoL. #CITATION_TAG, 89 arious factors may account for the disparity observed in the different age groups.",Adolescents were examined for malocclusion using the Dental Aesthetic Index (DAI) and for dental caries.,"['The objective of this study was to examine, using structural equation modelling, the relationships among clinical characteristics (such as caries experience and malocclusion), oral health-related quality of life (OHRQoL), and psychological characteristics (mental health, self-esteem, somatisation, and social perception of body image) in adolescents in New Zealand.']"
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Despite its relatively recent emergence over the past few decades, oral health-related quality of life (OHRQoL) has important implications for the clinical practice of dentistry and dental research. It has wide-reaching applications in survey and clinical research. OHRQoL is an integral part of general health and well-being. In fact, it is recognized by the World Health Organization (WHO) as an important segment of the Global Oral Health Program (2003). [98]#CITATION_TAG[100] Marshman and colleagues 101 have expressed concern that generic measures of OHQoL might not fully capture the impact that malocclusion has suggested the development of a malocclusion specific measure of OHQoL for young people with malocclusion","OHRQoL is a multidimensional construct that includes a subjective evaluation of the individual's oral health, functional well-being, emotional well-being, expectations and satisfaction with care, and sense of self. A supplemental Appendix contains a Medline and ProQuest literature search regarding OHRQoL research from 1990-2010 by discipline and research design (e.g., descriptive, longitudinal, clinical trial, etc.). The search identified 300 articles with a notable surge in OHRQoL research in pediatrics and orthodontics in recent years.","['This paper identifies the what, why, and how of OHRQoL and presents an oral health theoretical model.', 'The relevance of OHRQoL for dental practitioners and patients in community-based dental practices is presented.']"
"Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. A number of studies focusing on pre-school children, often aged 5 years or below, failed to demonstrate an association between the presence of a malocclusion and OHQoL, 87, 88 whilst other studies based on adolescents between the approximate ages of 11 to 15 years, have found the presence of a malocclusion to have a negative impact on OHQoL. #CITATION_TAG, 89 arious factors may account for the disparity observed in the different age groups.","METHODS Data from two cross-sectional epidemiological studies of adolescents in Taranaki and Otago were used. Each participant completed a self-administered questionnaire and underwent a clinical examination. Information collected included sociodemographic characteristics (sex, ethnicity and household deprivation), and clinical measures (caries and malocclusion, the latter measured with the Dental Aesthetic Index, or DAI). OHRQoL was measured using the validated 16-item impact short-form Child Perceptions Questionnaire (CPQ11-14). Linear regression was used to model the CPQ11-14 score.",['OBJECTIVES To determine whether malocclusion is associated with oral-health-related quality of life (OHRQoL) in New Zealand adolescents.']
"This paper describes the scenario matrix architecture that underlies a framework for developing new scenarios for climate change research. Conventional atmosphere-ocean global climate models and Earth system models of intermediate complexity are for the first time being joined by more recently developed Earth system models under an experiment design that allows both types of models to be compared to observations on an equal footing. As the first step, the IAM community 1 developed a set of concentration pathways (the Representative Concentration Pathways or RCPs) that were used by the ESM community to project the magnitude and extent of climate change (#CITATION_TAG; Van Vuuren et al. 2011a).","Unprecedented in scale and attracting interest from all major climate modeling groups, CMIP5 includes ""long term"" simulations of twentieth-century climate and projections for the twenty-first century and beyond. Besides the longterm experiments, CMIP5 calls for an entirely new suite of ""near term"" simulations focusing on recent decades...",['The fifth phase of the Coupled Model Intercomparison Project (CMIP5) will produce a state-of-the- art multimodel dataset designed to advance our knowledge of climate variability and climate change.']
"This paper describes the scenario matrix architecture that underlies a framework for developing new scenarios for climate change research. Advances in the science and observation of climate change are providing a clearer understanding of the inherent variability of Earth's climate system and its likely response to human and natural influences. The implications of climate change for the environment and society will depend not only on the response of the Earth system to changes in radiative forcings, but also on how humankind responds through changes in technology, economies, lifestyle and policy. To date, such scenarios have not adequately examined crucial possibilities, such as climate change mitigation and adaptation, and have relied on research processes that slowed the exchange of information among physical, biological and social scientists. Elements of an integrated framework were elaborated earlier by #CITATION_TAG and Kriegler et al. (2012).","Extensive uncertainties exist in future forcings of and responses to climate change, necessitating the use of scenarios of the future to explore the potential consequences of different response options.",['Here we describe a new process for creating plausible scenarios to investigate some of the most challenging and important questions about climate change confronting the global community.']
"This paper describes the scenario matrix architecture that underlies a framework for developing new scenarios for climate change research. Although the use of climate scenarios for impact assessment has grown steadily since the 1990s, uptake of such information for adaptation is lagging by nearly a decade in terms of scientific output. Nonetheless, integration of climate risk information in development planning is now a priority for donor agencies because of the need to prepare for climate change impacts across different sectors and countries. Up to this time the human signal, though detectable and growing, will be a relatively small component of climate variability and change. Such methods are commonly applied in IAVanalysis, although multiple criteria are normally applied in determining the final selection of representative climate scenarios to be used (IPCC-TGICA 2007; #CITATION_TAG).","This implies the need for a twin-track approach: on the one hand, vulnerability assessments of social and economic strategies for coping with present climate extremes and variability, and, on the other hand, development of climate forecast tools and scenarios to evaluate sector-specific, incremental changes in risk over the next few decades. This review starts by describing the climate outlook for the next couple of decades and the implications for adaptation assessments. We then review ways in which climate risk information is already being used in adaptation assessments and evaluate the strengths and weaknesses of three groups of techniques. We assert that climate change scenarios can meet some, but not all, of the needs of adaptation planning. Even then, the choice of scenario technique must be matched to the intended application, taking into account local constraints of time, resources, human capacity and supporting infrastructure.","['This urgency stems from concerns that progress made against Millennium Development Goals (MDGs) could be threatened by anthropogenic climate change beyond 2015.', 'Although the focus of the review is on information provision and uptake in developing regions, it is clear that many developed countries are facing the same challenges.']"
"There is evidence that breastfeeding reduces the risk of type 2 diabetes. To evaluate the effect of breastfeeding on long-term (breast carcinoma, ovarian carcinoma, osteoporosis and type 2 diabetes mellitus) and short-term (lactational amenorrhoea, postpartumdepression, postpartum weight change) maternal health outcomes. The interplay of factors that affect post-partum loss or retention of weight gained during pregnancy is not fully understood. Weight change patterns varied significantly among sites. In Brazil, India, Norway and USA, mothers on average lost weight during the first year followed by stabilization in the second year. Lactation intensity and duration explained little of the variation in weight change patterns. In most sites, obese mothers tended to lose less weight than normal-weight mothers. Culturally defined mother-care practices probably play a role in weight change patterns among lactating women. We updated the systematic review by Neville et al. (33) by including 5 additional studies (Table 5) (191) (192) (#CITATION_TAG) (194) (195).","Mothers of 1743 breastfed children enrolled in the MGRS had weights measured at days 7, 14, 28 and 42 post-partum, monthly from 2 to 12 months and bimonthly thereafter until 24 months post-partum. Height, maternal age, parity and employment status were recorded and breastfeeding was monitored throughout the follow-up.",['The objective of this paper is to describe patterns of weight change in the six sites of the World Health Organization (WHO) Multicentre Growth Reference Study (MGRS) and explore variables that explain variation in weight change within and between sites.']
"Research published in this series may include views on policy, but the institute itself takes no institutional policy positions. The Institute for the Study of Labor (IZA) in Bonn is a local and virtual international research center and a place of communication between science, politics and business. IZA is an independent nonprofit organization supported by Deutsche Post Foundation. IZA Discussion Papers often represent preliminary work and are circulated to encourage discussion. Citation of such a paper should account for its provisional character. A revised version may be available directly from the author. The IZA research network is committed to the IZA Guiding Principles of Research Integrity. IZA engages in (i) original and internationally competitive research in all fields of labor economics, (ii) development of policy concepts, and (iii) dissemination of research results and concepts to the interested public. Is the way that people make risky choices, or tradeoffs over time, related to cognitive ability? This interpretation suggests that the positive correlation between risk aversion and IQ emphasized, among others, by #CITATION_TAG, could be an artifact of the format of the price list, as already argued by Andersson et al. (2013).","We conduct choice experiments measuring risk aversion, and impatience over an annual time horizon, for a randomly drawn sample of roughly 1,000 German adults. Subjects also take part in two different tests of cognitive ability, which correspond to sub-modules of one of the most widely used IQ tests. Interviews are conducted in subjects' own homes. We perform a series of additional robustness checks, which help rule out other possible confounds.","['This paper investigates whether there is a link between cognitive ability, risk aversion, and impatience, using a representative sample of the population and incentive compatible measures.']"
"Historical archival data is interpreted through the lens of the rhetoric of rationality. Queensland legislation permitted the employment of indentured Pacific islanders to assist in the development of its sugar industry. Accounting practices employed at the Colonial Sugar Refinery (CSR) Company's Goondi Plantation and Mill focused on recording and controlling labour costs to maximize profits and maintain a healthy dividend to shareholders. It examines the technical and persuasive rhetorical dimensions of calculations employed at a nineteenth-century Queensland sugar plantation and mill in relation to the employment of indentured labour. This article addresses claims made by Weber, Schumpeter, and Sombart concerning the importance of double-entry bookkeeping. They argue that accounting played a key technical role in enhancing rationality and furthering the development of capitalism methods of production. This paper examines the technical and persuasive rhetorical dimensions (#CITATION_TAG) of a specific genealogy of calculation (Miller & Napier, 1993) ""local in both space and time"" (Carnegie & Napier, 1996, p.7).",The history of accounting methods and practices from the Middle Ages to the 19th century is surveyed in order to evaluate these arguments. Two important dimensions of accounting are discussed: the rhetorical and the technical. Goody's analysis of writing and literacy is applied to the development of accounting as a technique.,"['The argument is that, as rethoric, accounting must be understood as an attempt to convince some audience of the legitimacy of business ventures.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. 1 and 21, on a global scale urban land expansion will be much more rapid than urban population growth-in some places resulting in large, complex, urbanizing regions comprised of aggregations of interconnected cities and interspersed rural landscapes with multiple impacts, dependence and feedbacks (#CITATION_TAG; Seitzinger et al. 2012).","We illustrate how three key themes that are currently addressed separately in the urban sustainability and land change literatures can lead to incorrect conclusions and misleading results when they are not examined jointly: the traditional system of land classification that is based on discrete categories and reinforces the false idea of a rural-urban dichotomy; the spatial quantification of land change that is based on place-based relationships, ignoring the connections between distant places, especially between urban functions and rural land uses; and the implicit assumptions about path dependency and sequential land changes that underlie current conceptualizations of land transitions. We then examine several environmental ""grand challenges"" and discuss how urban land teleconnections could help research communities frame scientific inquiries.",['This paper introduces urban land teleconnections as a conceptual framework that explicitly links land changes to underlying urbanization dynamics.']
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Threats to biodiversity, food shortages, urban sprawl ...lessons for environmental problems that confront us today may well be found in the past. The archaeological record contains hundreds of situations in which societies developed long-term sustainable relationships with their environments and thousands in which the relationships were destructive. Whereas most books on this topic tend to treat human societies as mere reactors to environmental stimuli, Redman's volume shows them to be active participants in complex and evolving ecological relationships. History offers many lessons relevant to sustainability by exhibiting how humans and their societies have recognized and responded to challenges and opportunities of their natural environment (#CITATION_TAG; Diamond 2005; Costanza et al. 2007a; Sinclair et al. 2010).",By discussing archaeological case studies from around the world from the deforestation of the Mayan lowlands to soil erosion in ancient Greece to the almost total depletion of resources on Easter Island Redman reveals the long-range coevolution of culture and environment and clearly shows the impact that ancient peoples had on their world. They show that humankind's commitment to agriculture has had cultural consequences that have conditioned our perception of the environment and reveal that societies before European contact did not necessarily live the utopian existences that have been popularly supposed.,"['These case studies focus on four themes: habitat transformation and animal extinctions, agricultural practices, urban growth, and the forces that accompany complex society.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Urban green spaces provide socially valuable ecosystem services. External pressures resulting in urban sprawl in the Stockholm metropolitan region increasingly challenge the capacity of the NUP to continue to generate valuable ecosystem services. Setting aside protected areas, without accounting for the role of human stewardship of the cultural landscape, will most likely fail. In Stockholm, for instance, ordinary citizens were not allowed to enter such parks and gardens until the mid-1700s (#CITATION_TAG).","Through an historical analysis of the development of the National Urban Park (NUP) of Stockholm, we illustrate how the co- evolutionary process of humans and nature has resulted in the high level of biological diversity and associated recreational services found in the park. The ecological values of the area are generated in the cultural landscape. In a social inventory of the area, we identify 69 local user and interest groups currently involved in the NUP area.","['We propose that incentives should be created to widen the current biodiversity management paradigm, and actively engage local stewardship associations in adaptive co-management processes of the park and surrounding green spaces.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Abstract The legacy of the Maya forest is entwined with the Maya people who have lived and worked across this landscape over the past five millennia or more. The signatures of their land use, the complexity of their strategies, and the diversity of their adaptation is only recently being investigated, let alone understood. Underscoring all the papers is the evidence that the Maya tropics are resilient as a result of the land use strategies that have become a part of its essence. The infi elds were used as household farmstead gardens, which concentrated agricultural knowledge and stewardship of the agricultural biodiversity that was the ultimate survival strategy for the populace (#CITATION_TAG).",The papers herein combine archaeological and ethnographic data on the Maya and the long history of their relationship with the forest.,['This special issue of the Journal of Ethnobiology takes a close look at the region and brings into focus the intricacies that must be considered in the explanations of the rise and fall of the Maya as well as the conservation of the Maya forest today.']
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Constantinople is a city whose origin can be traced back to the establishment of Greek cities and colonies in early antiquity. Eventually it became the capital of the East Roman Empire, and since then its major role in the region has not diminished, whether under the rule of Byzantine emperors or Ottoman sultans. For more than 2000 years the city and its inhabitants have endured numerous changes and crises. Plague, war and economic regression have at times reduced its population to only a fraction of the previous size. The city has been subject to numerous sieges, the longest lasting eight years! Conquered only once prior to the major transformation in 1453, the city flourished again after each crisis and today it is still an important centre in this part of the world, on the border between the Mediterranean and the Black Sea. How could Constantinople maintain its leading position for such a long time, after suffering so many crises? The authors explore how the inhabitants of the ancient city of Constantinople managed to maintain a resilient food supply system. Constantinople differs in many ways from our modern cities, which are dependent on resources from a global hinterland that are transported using fossil fuels, and thus it can serve as an educational example for our time. At its first peak during the 6th century it was dependent on a complex grain transport system with ships travelling all the way to North Africa. This system collapsed in conjunction with the Arab expansion in the 7th century, and the collapse became a major part of a long recession that profoundly affected the city. That the city nonetheless survived cannot be explained by any single factor. A particularly interesting aspect, related to today's global transport system, is the urban agriculture system within and just outside the city walls. In our society, where the supply of food is considered as something obvious, one can question whether we lack memory as well as preparations for similar crises despite the fact that the food supply crisis of the Second World War is only 65 years behind us.Urban mind The most diffi cult blockade on the food supply lines, at the end of the fourteenth century AD, lasted an astonishing 8 years, but it did not succeed in starving out the urban population (#CITATION_TAG).","They rather delimited an area with dispersed ""sub-communities"" and numerous acres of, for example, orchards and vineyards. This system was continuous and was maintained by the inhabitants' living memory as well as by important institutions.","['In this chapter, the authors emphasize that the ability of a city to survive under stress has its fundamental origins in how the city was organized and maintained.', 'Special focus is put on the organizational and ecosystem services aspects of urban agriculture in the city.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. The Fortune at the Bottom of the Pyramid: Eradicating Poverty through Profits Pearson Education Inc., Wharton School Publishing, Upper Saddle River, New Jersey, 2004; Pages: 432; Price: US $ 29.99; ISBN: 0-13-146750-6. The treasure lying at the bottom of the world's economic pyramid, attracts companies with unimaginable prospects and profits. The economic pyramid of the world contains more than 400 crores of population at the bottom whose earnings and spendings per capita are low, but as an aggregate, are much larger than they are perceived to be. Hence, it is high time for the corporate managers to make an effort to locate the Alibaba's treasure lying hidden at the Bottom of the Pyramid (BOP). Broadly, the characteristics and spending patterns of the BOP segment are different as compared to the other segments of the society. In the high and medium range markets, consumers are able to buy bigger packages of consumables such as a 10 kg packet of detergents so that they need not shop frequently. But low-end BOP consumers do not have so much disposable cash to buy in bulk and store. Already in India, 30% of personal care and other consumables such as shampoo, tea, and cold medicines are sold in single serve packages and most single serve packages are priced at one rupee. Hindustan Lever Limited offers Clinic Plus shampoo priced at 50 Paisa. The major differences between the Bottom of the Pyramid and the Top of the Pyramid markets are differences in buying habits and cultural differences. The Bottom of the Pyramid is characterized by a low brand consciousness due to low economic position, and is mostly need driven. Major emerging BOP markets in the world, including China, India, Mexico, Russia, Brazil, South Africa, Thailand, Turkey and Indonesia, with a combined population of about 300 crores, represent 70% of the world BOP population. Such a huge jumbo size BOP market promises intense marketing opportunities for the companies. Even with NGOs working tirelessly and government support and international aid being circulated heavily, the problem of poverty still persists. According to the author, the main reason for this situation is that the private sector, which is huge and potentially capable of solving any problem in the world, is practically not involved. The World's Economic Pyramid contains wealthy people at the top with high spending capacities and habits. But the size of this segment is very small i.e., only about 7.5 to 10 crores of 600 crores plus population, whereas more than 400 crores of population live at the bottom of the pyramid earning less than $2 per day and in India alone around 40 crores of people earn less than Rs. THE BOOK PRESENTS CASES OF COMPANIES THAT ARE INVOLVED IN BOP MARKETING * The Solar Electric Light Fund (SELF): It established the Solar Electric Light Company (SELCO) in 1995 to market, install, and serve the Solar Home Systems (SHS) in south India. Strength, agility, and intelligence certainly were important, but which family, clan, and class one was born into set the limits on one's future potential in the age of early cities; to some extent, these constraints continue to operate today (Adams 1966; #CITATION_TAG; Scott 1998).","Hence, it is suggested that this segment be provided with ""single serve"" packages. Part 2 describes 12 cases, in a variety of businesses where BOP is becoming an active market and is bringing benefits to its customers far beyond offering just products. Part 3 contains stories in video form-an attempt to present the prospects and prosperity underlying the BOP markets, and to bring the BOP markets into limelight.","[""The book specifies workable solutions for serving huge population lying at the bottom of the world's economic pyramid."", 'The concept of \'Bottom of the Pyramid"" is explained in this book.', 'The author opines that, to effectively serve this BOP market, instead of old and tried solutions, continuous innovation and a paradigm shift in the thinking process of the private sector is needed.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Contemporary urbanization differs from historical patterns of urban growth in terms of scale, rate, location, form, and function. Some have also argued that we have already moved beyond the ""Anthropocene"" into the new urban era (#CITATION_TAG; Ljungqvist et al. 2010).",,"['This review discusses the characteristics of contemporary urbanization and the roles of urban planning, governance, agglomeration, and globalization forces in driving and shaping the relationship between urbanization and the environment.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Advanced communication and transport technologies allow food sequestration from the farthest reaches of the planet, but have markedly increasing urban dependence on global food systems over the past 50 years. Simultaneously, such advances have eroded collective memory of food production, while suitable spaces for urban gardening have been lost. Urban gardening and urban social movements can build local ecological and social response capacity against major collapses in urban food supplies. Urban governance for resilience should be historically informed about major food crises and allow for redundant food production solutions as a response to uncertain futures.SUPER, ''Sustainable Urban Planning for Ecosystem Services and Resilience However, clear delineations between urban and rural areas and use of urban green spaces for purely recreational purposes did not emerge until the nineteenth and twentieth centuries, and were reinforced by the development of a globalized economy, the fossil fuel energy regime, and technological innovations such as the steam engine and the railway (McNeill 2000; Barthel and Isendahl 2012; #CITATION_TAG).","Hence, they should be incorporated as central elements of sustainable urban development.","['This article examines the role played by urban gardens during historical collapses in urban food supply lines and identifies the social processes required to protect two crit- ical elements of urban food production during times of crisis--open green spaces and the collective memory of how to grow food.', 'These factors combine to heighten the potential for food shortages when--as occurred in the 20th century-- major economic, political or environmental crises sever supply lines to urban areas.', 'This paper considers how to govern urban areas sustainably in order to ensure food security in times of crisis by: evincing the effectiveness of urban gardening during crises; showing how allotment gardens serve as conduits for transmitting collective social-ecological memories of food production; and, discussing roles and strategies of urban environmental movements for protecting urban green space.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Human history, as written traditionally, leaves out the important ecological and climate context of historical events. But the capability to integrate the history of human beings with the natural history of the Earth now exists, and we are finding that human-environmental systems are intimately linked in ways we are only beginning to appreciate. In Sustainability or Collapse?, researchers from a range of scholarly disciplines develop an integrated human and environmental history over millennial, centennial, and decadal time scales and make projections for the future. History offers many lessons relevant to sustainability by exhibiting how humans and their societies have recognized and responded to challenges and opportunities of their natural environment (Redman 1999; Diamond 2005; #CITATION_TAG; Sinclair et al. 2010).",The contributors focus on the human-environment interactions that have shaped historical forces since ancient times and discuss such key methodological issues as data quality.,"['Topics highlighted include the political ecology of the Mayans; the effect of climate on the Roman Empire; the ""revolutionary weather"" of El Nino from 1788 to 1795; twentieth-century social, economic, and political forces in environmental change; scenarios for the future; and the accuracy of such past forecasts as The Limits to Growth.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Simple, deterministic relationships between environmental stress and social change are inadequate. Extreme drought, for instance, triggered both social collapse and ingenious management of water through irrigation. Human responses to change, in turn, feed into climate and ecological systems, producing a complex web of multidirectional connections in time and space. Humans cannot predict the future. However, since the beginning of the industrial revolution, and especially after the start of the ""great acceleration"" following the end of WWII, there has been rapid economic expansion coupled with rapid urban growth-all driven by rapid expansion of fossil fuel use, especially oil (#CITATION_TAG).",,"['Integrated records of the co-evolving human-environment system over millennia are needed to provide a basis for a deeper understanding of the present and for forecasting the future.', 'This requires the major task of assembling and integrating regional and global historical, archaeological, and paleoenvironmental records.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Abstract There is growing interest in the ecology of the Maya Forest past, present, and future, as well as in the role of humans in the transformation of this ecosystem. During the Archaic period, a time of stable climatic conditions 8,000-4,000 years ago, we propose that the ancestral Maya established an intimate relationship with an expanding tropical forest, modifying the landscape to meet their subsistence needs. This highly productive and sustainable system of resource management formed the foundation for the development of the Maya civilization, from 3,000 to 1,000 years ago, and was intensified during the latter millennia of a stable climatic regime as population grew and the civilization developed. These strategies of living in the forest evolved into the milpa cycle--the axis of the Maya Forest garden resource management system that created the extraordinary economic value recognized in the Maya Forest today. Remnant urban ecosystems and the rich levels of biodiversity found in the urban Yucatan today are hence viewed to be the products of a millennia-long co-evolution in cultural landscapes (Ford and Emery 2008; #CITATION_TAG).","In particular, we consider the paleoenvironmental data from the Maya Forest area in light of interpretations of the precipitation record from the Cariaco Basin. This new adaptation, we suggest, was based on a resource management strategy that grew out of earlier landscape modification practices.","['In this paper, we bring together and re-evaluate paleoenvironmental, ethnobiological, and archaeological data to reconstruct the related effects of climatic shifts and human adaptations to and alterations of the lowland Maya Forest.', 'We propose that the succeeding period of climatic chaos during the Preclassic period, 4,000-1,750 years ago, provoked the adaptation to settled agrarian life.']"
"Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Abstract Recognition of the importance of land-use history and its legacies in most ecological systems has been a major factor driving the recent focus on human activity as a legitimate and essential subject of environmental science. Ecologists, conservationists, and natural resource policymakers now recognize that the legacies of land-use activities continue to influence ecosystem structure and function for decades or centuries--or even longer--after those activities have ceased. As a result, environmental history emerges as an integral part of ecological science and conservation planning. Current biodiversity and ecosystem services are conditioned by history, regional context and continuity (#CITATION_TAG).",,"['By considering diverse ecological phenomena, ranging from biodiversity and biogeochemical cycles to ecosystem resilience to anthropogenic stress, and by examining terrestrial and aquatic ecosystems in temperate to tropical biomes, this article demonstrates the ubiquity and importance of land-use legacies to environmental science and management.']"
"A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. A realizability-notion akin to Kleene's slash [18, 19] was extended to various intuitionistic set theories by Myhill [27, #CITATION_TAG], whereby he also drew on work by Moschovakis [24].","The upshot is that CZF+CC+FT possesses the same strength as CZF, or more precisely, that CZF+CC+FTis conservative over CZF for 02 statements of arithmetic, whereas the addition of a restricted version of bar induction to CZF (called decidable bar induction, BID) leads to greater proof-theoretic strength in that CZF+BID proves the consistency of CZF","['The paper furnishes realizability models of constructive Zermelo-Fraenkel set theory, CZF, which also validate Brouwerian principles such as the axiom of continuous choice (CC), the fan theorem (FT), and monotone bar induction (BIM), and thereby determines the proof-theoretic strength of CZF augmented by these principles.']"
"A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. For instance from a proof of a statement n  o m  o ph(n, m) one can effectively construct an index e of a recursive function such that n  o ph(n, {e}(n)) is provable. In [33, 36, #CITATION_TAG] the author of the present paper developed a different machinery for showing the DP and the NEP (and several other properties) directly for extensional set theories.","It is proved that the disjunction property, the numerical existence property, Church's rule, and several other metamathematical properties hold true for Constructive Zermelo-Fraenkel Set Theory and full Intuitionistic Zermelo-Fraenkel augmented by any combination of the principles of Countable Choice, Dependent Choices and the Presentation Axiom. Also Markov's principle may be added. Thus we have an explicit method of witness and program extraction from proofs involving choice principles. As for the proof technique, this paper is a continuation of [32]. [32] introduced a selfvalidating semantics for CZF that combines realizability for extensional set theory and truth",['This paper is concerned with metamathematical properties of intuitionistic set theories with choice principles.']
"A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. Let P be some property of natural numbers. Consider the existential statement, ""There exists a number n having the property P."" To explain the meaning which this has for a constructivist or intuitionist, it has been described as a partial judgement, or incomplete communication of a more specific statement which says that a certain given number n, or the number n obtainable by a certain given method, has the property P.2 The meaning of the existential statement thus resides in a reference to certain information, which it implies could be stated in detail, though the trouble is not taken to do so. Perhaps the detail is suppressed in order to convey a general view of some fact. They were first proposed by Kleene [#CITATION_TAG] in 1945.","The information to which reference is made should be thought of as possibly comprising other items besides the value of n or method for obtaining it, namely such items as may be necessary to complete the communication that that n has the property P. Consider next the generality statement, ""All numbers n have the property P."" The accompanying explanation which has been given for this is that it is a hypothetical assertion about whatever particular n might be given. We now propose, without excluding this motif, likewise to regard the generality statement as an incomplete communication of a more specific statement, namely of one which gives an effective general method for obtaining, to any particular value of n, the information implicit in the assertion that that n has the property P. As a third example, consider the implication, ""A implies B.""",['This we now propose to interpret intuitionistically as an incomplete communication of']
"A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. There has been increasing interest in intuitionistic methods over the years. Still, there has been relatively little work on intuitionistic set theory, and most of that has been on intuitionistic ZF. Those papers that have dealt with IZF usually were more proof-theoretic in nature, and did not provide models. Furthermore, the inspirations for many of the constructions here are classical forcing arguments. Along different lines, the subject of least and greatest fixed points of inductive definitions, while of interest to computer scientists, has yet to be studied constructively, and probably holds some surprises. Admissibility is of course the proper set-theoretic context for this study. A class X is said to be -closed if A  X implies a  X for every pair a, A  .Theorem 2.7 (IKP) For any  inductive definition  there is a smallest -closed class I(); moreover, I() is a  class as well.Proof : [2, Theorem 11.4] and #CITATION_TAG","The next section develops the basics of IKP, including some remarks on fixed points of inductive definitions.","['This investigation is about intuitionistic admissibility and theories of similar strength.', 'There are several more particular goals for this paper.', 'One is just to get some more Kripke models of various set theories out there.']"
"A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. It is possible to carry out an ordinal analysis of IKP just as for KP as in [#CITATION_TAG].",The strength of such  fragments will be characterized in terms of the smallest ordinal ff such  that L ff is a model of every \Pi 2 sentence which is provable in the theory,"['In this paper we shall investigate fragments of Kripke--Platek set  theory with Infinity which arise from the full theory by restricting  Foundation to \\Pi n Foundation, where n  2.']"
"A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. Constructive Zermelo-Fraenkel Set Theory, CZF, has emerged as a standard reference theory that relates to constructive predicative mathematics as ZFC relates to classical Cantorian mathematics. A hallmark of this theory is that it possesses a type-theoretic model. In [#CITATION_TAG, 36, 37] the author of the present paper developed a different machinery for showing the DP and the NEP (and several other properties) directly for extensional set theories.","Aczel showed that it has a formulae-as-types interpretation in Martin-Lof's intuitionist theory of types [14, 15]. It is shown that Kleene realizability provides a self-validating semantics for CZF, viz. this notion of realizability can be formalized in CZF and demonstrably in CZF it can be verified that every theorem of CZF is realized. This semantics, then, is put to use in establishing several equiconsistency results. Specifically, augmenting CZF by well-known principles germane to Russian constructivism and Brouwer's intuitionism turns out to engender theories of equal proof-theoretic strength with the same stock of provably recursive functions","['This paper, though, is concerned with a rather different interpretation.']"
"A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. Realizability based on indices of general set recursive functions was introduced in [#CITATION_TAG] and employed to prove, inter alia, metamathematical properties for CZF augmented by strong forms of the axiom of choice in [35, Theorems 8.3, 8.4].","It is argued that this interpretation plays a similar role for CZF as the constructible hierarchy for classical set theory, in that it can be employed to show that CZF is expandable by several forms of the axiom of choice without adding more consistency strength.","['The main objective of this paper is to show that a certain formulae-asclasses interpretation based on generalized set recursive functions provides a selfvalidating semantics for Constructive Zermelo-Fraenkel Set theory, CZF.']"
"The fungicides used to control diseases in cereal production can have adverse effects on non-target fungi, with possible consequences for plant health and productivity. The fungal community on wheat leaves consisted mainly of basidiomycete yeasts, saprotrophic ascomycetes and plant pathogens. This study examined fungicide effects on fungal communities on winter wheat leaves in two areas of Sweden. With recent methodological advances, molecular markers are increasingly used for semi-quantitative analyses of fungal communities. The internal transcribed spacer (ITS) region of the ribosome encoding genes is a commonly used marker for many fungal groups. The ITS2 region was amplified on a 2720 Thermal Cycler (Life Technologies, CA, USA) using the forward primer fITS7 (GTGARTCATCGAATCTTTG; [#CITATION_TAG] and the reverse primer ITS4 (TCCTCCGCTTATTGATATGC; [26].","Here, we describe three new primers - fITS7, gITS7 and fITS9, which may be used to amplify the fungal ITS2 region by targeting sites in the 5.8S encoding gene. We evaluated the primers and compared their performance with the commonly used ITS1f primer by 454-sequencing of both artificially assembled templates and field samples. When the entire ITS region was amplified using the ITS1f/ITS4 primer combination, we found strong bias against species with longer amplicons. This problem could be overcome by using the new primers, which produce shorter amplicons and better preserve the quantitative composition of the template.",['The aim to preserve quantitative relationships between genotypes through PCR places new demands on primers to accurately match target sites and provide short amplicons.']
"The fungicides used to control diseases in cereal production can have adverse effects on non-target fungi, with possible consequences for plant health and productivity. The fungal community on wheat leaves consisted mainly of basidiomycete yeasts, saprotrophic ascomycetes and plant pathogens. This study examined fungicide effects on fungal communities on winter wheat leaves in two areas of Sweden. The phyllosphere is a rich and varied microbial community comprising organisms with diverse functional types. Its composition is strongly influenced by both genotypic and environmental factors, many of which can be manipulated by breeding, agronomy and crop protection strategies in an agricultural context. The phyllosphere, defined as the total above-ground parts of plants, provides a habitat for many microorganisms [#CITATION_TAG].",,"['Understanding the population dynamic balance between the organisms of the phyllosphere as an ecological system should lead to new approaches in agronomy, crop protection and breeding that enhance sustainability, where the previously presumed requirement to eliminate putative pathogens is replaced by management that favours dominance of beneficial organisms and contains putative pathogens in asymptomatic or stable states.']"
"The fungicides used to control diseases in cereal production can have adverse effects on non-target fungi, with possible consequences for plant health and productivity. The fungal community on wheat leaves consisted mainly of basidiomycete yeasts, saprotrophic ascomycetes and plant pathogens. This study examined fungicide effects on fungal communities on winter wheat leaves in two areas of Sweden. Foliar fungal communities of plants are diverse and ubiquitous. In grasses endophytes may increase host fitness; in trees, their ecological roles are poorly understood. At the time of the sampling the poplars had been growing in a common garden for two years. Leucosporidium golubevii is a yeast discovered in freshwater [46], and has been reported from the phyllosphere of balsam poplar [#CITATION_TAG].","We sampled leaves from genotyped balsam poplars from across the species' range, and applied 454 amplicon sequencing to characterize foliar fungal communities. We found diverse fungal communities associated with the poplar leaves. The observed patterns may be explained by a filtering mechanism which allows the trees to selectively recruit fungal strains from the environment. Alternatively, host genotype-specific fungal communities may be present in the tree systemically, and persist in the host even after two clonal reproductions.",['We investigated whether the genotype of the host tree influences community structure of foliar fungi.']
"The fungicides used to control diseases in cereal production can have adverse effects on non-target fungi, with possible consequences for plant health and productivity. The fungal community on wheat leaves consisted mainly of basidiomycete yeasts, saprotrophic ascomycetes and plant pathogens. This study examined fungicide effects on fungal communities on winter wheat leaves in two areas of Sweden. Seed is among the most key input for improving crop production and productivity. Increasing the quality of seeds can increase the yield potential of the crop by significant folds. In recent years seed has become an international commodity used to exchange germplasm around the world. Seed is, however, also an efficient means of introducing plant pathogens into a new area as well as providing a means of their survival from one cropping season to another. Seed borne mycroflora are significant destroyers of food stuffs and grains during storage rendering them unfit for human consumption by retarding their nutritive value and often by producing mycotoxins. Seed-borne pathogens have been involved in seed rots during germination and seedling mortality leading to poor crop stand reduction in plant growth and productivity of crops. The seed-borne pathogens associated with seeds externally or internally may cause seed abortion, seed rot, seed necrosis, reduction or elimination of germination capacity, as well as seedling damage resulting in development of disease at later stages of plant growth by systemic or local infection. Infected seeds play considerable role in the establishment of economically important plant diseases in the field resulting in heavy reduction of crop yields. The ascomycete A. pullulans is one of the most common inhabitants of the phyllosphere of many crops [48] and is also present in many other habitats [#CITATION_TAG].",It also discusses the detection mechanism and implies some management strategies that are implemented to reduce the loss due to seed borne fungi.,['This paper presents the negative impact of seed borne fungi and their implication in food safety.']
"It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. By online practicing, children were more involved in the storytelling and language acquisition process, chances rarely available in the regular class. In a similar vein, it has been argued that story genres are considered some of the most suitable for students learning a second language because of their emphasis on action and events, their strong tradition of oral, embodied performance, and their concern with common themes (#CITATION_TAG; Pennington, 2009; Tsou, Wang, & Tzeng, 2006).","VoiceThread is used to present the story with audio and visual aids to help children review keywords and read aloud simple summarizing sentences. Twenty Taiwanese EFL children determined learning-at-risk took part in this one-year project; survey questionnaires, storytellers' ethnographic notes, and teacher interviews were collected to examine the progress in terms of their changes in attitude, motivation, and responsiveness to storytelling and English learning.","['This action research aims to investigate how technology improves the conditions of storytelling to help enhance the learning attitude and motivation of EFL children with learning difficulty using power point designs and an online recording system--VoiceThread (http://voicethread.com/).', 'The use of power point designs is to assure children of clear illustrations and print during storytelling.']"
"It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. Teacher knowledge, as an important cognitive basis of teaching, has attracted tremendous attention in educational research in the last few decades. Given the different learning situations, such as limited time allocated to English lessons, large class size, students' low motivation, and form-focused exams (#CITATION_TAG; Ramon-Plo & Pilarmur-Duenas, 2014), the use of stories in EFL learning contexts accordingly needs modifications.",A sample of 527 teachers from 56 universities in 29 cities across the country responded to a self-report questionnaire. Thirty teachers were interviewed. Teachers' self-perceived knowledge was assessed by two factors: pedagogical content knowledge about oral English teaching (PCK) and knowledge of students' oral English characteristics (KOS).,"[""This study examines whether teachers' self-perceived knowledge about oral English teaching differs with regard to their professional profiles in the English as a Foreign Language (EFL) context in China.""]"
"It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. English language teaching (ELT) has been investigated from various angles including how English language teachers perceive what happens in an ELT classroom. How primary school English language learners perceive their experiences of ELT is rarely reported in the published literature, particularly from developing countries such as Bangladesh. This article reports on a study that examined Bangladeshi primary school learners' experience of English language classroom practices in which technology-enhanced communicative language teaching activities were promoted through a project called English in Action (EIA). EIA is a large-scale 9-year long international English language development project in Bangladesh, funded by the UK government. This contradicts studies by #CITATION_TAG in Bangladesh and Asafeh, Khwaile, and Alshbou (2012) in Jordan that report many EFL learners preferred to have more communicative activities to practice their English, although they showed positive attitudes to traditional activities such as drilling of grammar rules and vocabulary.",A semi-structured group interview was conducted with 600 Grade 3 students from different regions of Bangladesh.,"[""The paper critiques EIA and argues that any major language development project needs to consider the local context and learners' views on language learning for its success.""]"
"It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. In this respect, I collaboratively worked with the teacher participants in designing and executing the learning experiences for the students throughout the iteration of action research cycle (#CITATION_TAG).","Over two years, 14 case studies were generated involving six university researchers and 61 teacher researchers. As teacher researchers collected their topic-specific data, university researchers worked in parallel on case studies at the 14 sites in an attempt to gain insights into the types and value of collaboration in collaborative action research. University researchers gathered 20-30 points of data for each case-study site (including teacher researcher final reports as well as journal entries, focus group interviews, field notes, video footage and questionnaires). Through pattern matching, hypothesised events were compared with observations to test the claim that a triad of relationships between the team, the team lead and the researchers functioned as a positive collaborative force that propelled teacher team success in a generative ethos, even when faced with substantial challenges.","[""The purpose of this paper is to report on the nature of collaboration in a multi-year, large-scale collaborative action research project in which a teachers' federation (in Ontario, Canada), university researchers and teachers partnered to investigate teacher-selected topics for inquiry.""]"
"It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. Communicative language teaching (CLT) applicability to English as a foreign language (EFL) contexts has recently been debated extensively. Too, whereas EFL instruction met learners' preferences associated with FFI, it rarely responded to learners' MOI needs. This contradicts studies by Shrestha (2013) in Bangladesh and #CITATION_TAG in Jordan that report many EFL learners preferred to have more communicative activities to practice their English, although they showed positive attitudes to traditional activities such as drilling of grammar rules and vocabulary.",The data were collected using a 41-item questionnaire and analyzed using descriptive and referential statistics.,"[""This study addressed 1525 Jordanian EFL school learners' attitudes and perceived implementation of traditional form-focused (FFI) instruction and communicative meaning-oriented instruction (MOI) of English.""]"
"Time Series Forecasting (TSF) uses past patterns of an event in order to predict its future values and is a key tool to support decision making. In the last decades, Computational Intelligence (CI) techniques, such as Artificial Neural Networks (ANN) and more recently Support Vector Machines (SVM), have been proposed for TSF. In this work, we propose a novel Evolutionary SVM (ESVM) approach for TSF based on the Estimation Distribution Algorithm to search for the best number of inputs and SVM hyperparameters. Barcelona, Spain.Accurate time series forecasting are important for displaying the manner in which the past continues to affect the future and for planning our day to-day activities. In recent years, a large literature has evolved on the use of evolving artificial neural networks (EANNs) in many forecasting applications. Evolving neural networks are particularly appealing because of their ability to model an unspecified nonlinear relationship between time series variables. In [#CITATION_TAG], EDA was used as the search engine of an EANN, outperforming a GA based EANN.","A comparative study between these two methods, with a set of referenced time series will be shown.","['This paper evaluates two methods to evolve neural networks architectures, one carried out with genetic algorithm and a second one carry out with differential evolution algorithm.', 'The object of this study is to try to improve the final forecasting getting an accurate system.The research reported here has been supported by the Spanish Ministry of Science and Innovation under project TRA2007-67374-C02-02']"
"Time Series Forecasting (TSF) uses past patterns of an event in order to predict its future values and is a key tool to support decision making. In the last decades, Computational Intelligence (CI) techniques, such as Artificial Neural Networks (ANN) and more recently Support Vector Machines (SVM), have been proposed for TSF. In this work, we propose a novel Evolutionary SVM (ESVM) approach for TSF based on the Estimation Distribution Algorithm to search for the best number of inputs and SVM hyperparameters. LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. LIBSVM has gained wide popularity in machine learning and many other areas. Since TSF is a particular regression case, for the SVM type and kernel, we selected the popular ε-insensitive loss function (known as ε-SVR) and Gaussian kernel combination, as implemented in the LIBSVM tool [#CITATION_TAG].","In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.",['The goal is to help users to easily apply SVM to their applications.']
"Time Series Forecasting (TSF) uses past patterns of an event in order to predict its future values and is a key tool to support decision making. In the last decades, Computational Intelligence (CI) techniques, such as Artificial Neural Networks (ANN) and more recently Support Vector Machines (SVM), have been proposed for TSF. In this work, we propose a novel Evolutionary SVM (ESVM) approach for TSF based on the Estimation Distribution Algorithm to search for the best number of inputs and SVM hyperparameters. TSF has become increasingly used in distinct areas such as Agriculture, Finance, Production or Sales [#CITATION_TAG].","A broad outline  follows. We begin with an introduction to smoothing in one dimension, followed by  a discussion of multi-dimensional smoothing methods. We then move on to review  and develop the array methods of Currie et al. (2006), and show how these methods  can be applied in additive models even when the data do not have a standard array  structure. Our main contributions are: firstly we extend the array methods of Currie et al. (2006) to cope with more general covariance structures; secondly we describe an additive  model of mortality which decomposes the mortality surface into a smooth twodimensional  surface and a series of smooth age dependent shocks within years; thirdly  we describe an additive model of mortality for data with a Lexis triangle structure",['In this thesis we investigate the application of array methods for the smoothing of  multi-dimensional arrays with particular reference to mortality data.']
"Time Series Forecasting (TSF) uses past patterns of an event in order to predict its future values and is a key tool to support decision making. In the last decades, Computational Intelligence (CI) techniques, such as Artificial Neural Networks (ANN) and more recently Support Vector Machines (SVM), have been proposed for TSF. In this work, we propose a novel Evolutionary SVM (ESVM) approach for TSF based on the Estimation Distribution Algorithm to search for the best number of inputs and SVM hyperparameters. Feature-based time series representations have attracted substantial attention in a wide range of time series analysis methods. Recently, the use of time series features for forecast model averaging has been an emerging research focus in the forecasting community. Nonetheless, most of the existing approaches depend on the manual choice of an appropriate set of features. Exploiting machine learning methods to extract features from time series automatically becomes crucial in state-of-the-art time series analysis. are useful to aid tactical decisions, such as planning production resources or evaluating alternative economic strategies [#CITATION_TAG].","We first transform time series into recurrence plots, from which local features can be extracted using computer vision algorithms. The extracted features are used for forecast model averaging.","['In this paper, we introduce an automated approach to extract time series features based on time series imaging.']"
"Time Series Forecasting (TSF) uses past patterns of an event in order to predict its future values and is a key tool to support decision making. In the last decades, Computational Intelligence (CI) techniques, such as Artificial Neural Networks (ANN) and more recently Support Vector Machines (SVM), have been proposed for TSF. In this work, we propose a novel Evolutionary SVM (ESVM) approach for TSF based on the Estimation Distribution Algorithm to search for the best number of inputs and SVM hyperparameters. The authors have been quite successful in achieving this objective, and their work is a welcome addition to the statistics and learning literatures. Statistics has always been interdisciplinary, borrowing ideas from diverse  elds and repaying the debt with contributions, both theoretical and practical, to the other intellectual disciplines. For statistical learning, this cross-fertilization is especially noticeable. This book is a valuable resource, both for the statistician needing an introduction to machine learning and related  elds and for the computer scientist wishing to learn more about statistics. Statisticians will especially appreciate that it is written in their own language. In a stimulating article, Breiman (2001) argued that statistics has been focused too much on a ""data modeling culture,"" where the model is paramount. Breiman's article is controversial, and in his discussion, Efron objects that ""prediction is certainly an interesting subject, but Leo's paper overstates both its role and our profession's lack of interest in it."" Although I mostly agree with Efron, I worry that the courses offered by most statistics departments include little, if any, treatment of statistical learning and prediction. Graduate students in statistics certainly need to know more than they do now about prediction, machine learning, statistical learning, and data mining (not disjoint subjects). Most of the book is focused on supervised learning, where one has inputs and outputs from some system and wishes to predict unknown outputs corresponding to known inputs. There is a  nal chapter on unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components and curves, and independent component analysis. Many statisticians will be unfamiliar with at least some of these algorithms. Association rules are popular for mining commercial data in what is called ""market basket analysis."" Self-organizing maps (SOMs) involve essentially constrained k-means clustering, where prototypes are mapped to a two-dimensional curved coordinate system. Independent components analysis is similar to principal components analysis and factor analysis, but it uses higher-order moments to achieve independence, not merely zero correlation between components. I know of no other book that covers so much ground. Fortunately, each chapter includes bibliographic notes surveying the recent literature. On the other hand, CI models such as ANN and SVM have hyperparameters that need to be adjusted (e.g., number of ANN hidden nodes or kernel parameter) [#CITATION_TAG].","Breiman argued instead for an ""algorithmic modeling culture,"" with emphasis on black-box types of prediction. The methods discussed for supervised learning include linear and logistic regression; basis expansion, such as splines and wavelets; kernel techniques, such as local regression, local likelihood, and radial basis functions; neural networks; additive models; decision trees based on recursive partitioning, such as CART; and support vector machines. The relationships among the methods are emphasized.","['In the words of the authors, the goal of this book was to ""bring together many of the important new ideas in learning, and explain them in a statistical framework.""', 'The aim is to discover types of products often purchased together.', 'A strength of the book is the attempt to organize a plethora of methods into a coherent whole.']"
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. In recent years there has been widespread acceptance that cognitive behavior therapy (CBT) is the treatment of choice for bulimia nervosa. The cognitive behavioral treatment of bulimia nervosa (CBT-BN) was first described in 1981. Over the past decades the theory and treatment have evolved in response to a variety of challenges. Although several randomised controlled trials have shown that CBT is more effective than a wide range of alternative treatments [1, 2, 5, 11], amongst BN treatment completers, only 40-50 % have a full and lasting response [#CITATION_TAG] [13] [14].","The treatment has been adapted to make it suitable for all forms of eating disorder-thereby making it ""transdiagnostic"" in its scope- and treatment procedures have been refined to improve outcome. The new version of the treatment, termed enhanced CBT (CBT-E) also addresses psychopathological processes ""external"" to the eating disorder, which, in certain subgroups of patients, interact with the disorder itself.",['In this paper we discuss how the development of this broader theory and treatment arose from focusing on those patients who did not respond well to earlier versions of the treatment.']
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Diagnosis and Epidemiology of Binge-Eating Disorder. Binge-Eating Disorder and Obesity. Eating Behavior, Psychobiology, Medical Risks, and Pharmacotherapy of Binge-Eating Disorder. Binge-Eating Disorder and Bariatric Surgery. Psychotherapy for Binge-Eating Disorder. Part II: A Cognitive-Behavioral Treatment Program for Binge-Eating Disorder. Prior research comparing individuals with provisional DSM-IV BED to either DSM-IV BN purging or non-purging subtypes found significant differences in both current (age, BMI, and dietary restraint at the time of the assessment) and age-historical variables (age of onset) [#CITATION_TAG] [74] [75].","Clinical Features, Longitudinal Course, and Psychopathology of Binge-Eating Disorder.",['Part I: What We Know About Binge-Eating Disorder and its Treatment.']
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Stice's (1994, 2001) dual pathway model proposed a mediational sequence that links body dissatisfaction to lack of control over eating through dieting and negative affect. Van Strien et al. Although there is evidence that initial dietary restraint levels predict future onset of binge eating among asymptomatic individuals [24, 56, #CITATION_TAG], as in this study, prior research using clinical interviews or ecological momentary assessment failed to support the dietary restraint-binge eating relationship among bulimic-type ED patients [9, [58] [59] [60].","(2005) extended the negative affect pathway of the original dual pathway model by adding two additional intervening variables: interoceptive deficits and emotional eating. Data collected from 361 adolescent girls, who were interviewed and completed self-report measures annually over a 2-year period, were analysed using structural equation modeling.",['The purpose of this study was to test and compare the original and extended model using prospective data.']
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. OBJECTIVE Negative affect precedes binge eating and purging in bulimia nervosa (BN), but little is known about factors that precipitate negative affect in relation to these behaviors. The final maintenance factor of mood intolerance (i.e. inability to appropriately cope with adverse affective states followed by dysfunctional impulsive behaviours) [13] is believed to directly affect and maintain binge eating [12, 17, [#CITATION_TAG] [22] [23] [24].","METHOD A total of 133 women with current BN recorded their mood, eating behavior, and the occurrence of stressful events every day for 2 weeks. Multilevel structural equation mediation models evaluated the relations among Time 1 stress measures (i.e., interpersonal stressors, work/environment stressors, general daily hassles, and stress appraisal), Time 2 negative affect, and Time 2 binge eating and purging, controlling for Time 1 negative affect.","['We aimed to assess the temporal relation among stressful events, negative affect, and bulimic events in the natural environment using ecological momentary assessment.']"
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Finally, as testing the significance of the mediation or indirect effects using bootstrap procedure has been recommended [#CITATION_TAG], Mplus [49] was specified to (a) create 5,000 bootstrap samples from the data set by random sampling with replacement and (b) generate indirect effects and bias-corrected confidence intervals (95 % CIs) around the indirect effects when analysing the (final) structural models (Fig. 2).",Separate sections describe examples of moderating and mediating variables and the simplest statistical model for investigating each variable. The strengths and limitations of incorporating mediating and moderating variables in a research study are discussed as well as approaches to routinely including these variables in outcome research. The routine inclusion of mediating and moderating variables holds the promise of increasing the amount of information from outcome studies by generating practical information about interventions as well as testing theory.,"['The purpose of this article is to describe mediating variables and moderating variables and provide reasons for integrating them in outcome studies.', 'The primary focus is on mediating and moderating variables for intervention research but many issues apply to nonintervention research as well.']"
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. This finding, as well as the evidence that interventions for binge eating that do not focus on reducing OSW and/or dietary restraint (i.e. interpersonal therapy, dialectical-behaviour therapy) decrease binge eating relative to assessment-only control conditions [1, 2, 11, 54, #CITATION_TAG], seems incompatible with the theoretical assertion of both CB models, i.e. OSW affects binging indirectly through increasing the likelihood of dietary restraint [6, 13].","Individuals (88.3% female; mean 42.8 years) were randomized to DBTgsh (n=30) or wait-list (WL; n=30). DBTgsh participants received an orientation, DBT manual, and six 20-min support calls over 13 weeks.",['This study examined the efficacy of guided self-help based on dialectical behaviour therapy (DBTgsh) for binge eating disorder (BED).']
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. Banks and other lenders often transfer credit risk to liberate capital for further loan intermediation. 57 This difficulty is also stressed by #CITATION_TAG.","After an overview of recent credit risk transfer activity, the following points are discussed: motivations for CRT by banks; risk retention; theories of CDO design; specialty finance companies. As an illustration of CLO design, an example is provided showing how the credit quality of the borrowers can deteriorate if efforts to control their default risks are costly for issuers.","['This paper aims to explore the design, prevalence and effectiveness of credit risk transfer (CRT).', 'The focus is on the costs and benefits for the efficiency and stability of the financial system.']"
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. In a typical CDO transaction a bank retains through a first loss piece a very high proportion of the expected default losses, and transfers only the extreme losses to other market participants. The size of the first loss piece is largely driven by the average default probability of the securitized assets. If the bank sells loans in a true sale transaction, it may use the proceeds to to expand its loan business, thereby incurring more systematic risk. 21 For a more detailed account of the argument, see #CITATION_TAG.",,"['This paper contributes to the economics of financial institutions risk management by exploring how loan securitization a.ects their default risk, their systematic risk, and their stock prices.']"
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. This supplement to the Swiss Journal of Economics and Statistics presents material from a conference on Capital Adequacy Rules as Instruments for the Regulation of Banks, which took place in Basle on July 5, 1996. As such it complements the December 1995 supplement, which presented papers from a conference on the same subject a year earlier. The area of banking regulation has not always been marked by intense interaction between academics and practitioners. Indeed, from the times of Regulation Q to the enactment of the 1988 Basle Accord on Capital Adequacy Requirements for Credit Risks, it has been an area where people were sure of their views, with little need to listen to each other. At the same time, not much academic research was actually done in the area; this has only changed with the various banking crises of the eighties and early nineties and the tightening of capital adequacy regulation of banks following the Basle Accord. However the certainties of yesteryear are disappearing. This is most obvious in the development of proposals to extend the 1988 Basle Accord to market risks. When the Basle Committee on Banking Supervision presented the first such proposal in April 1993, it seemed that the regulatory train was moving full speed ahead on a track built by traditional practice, with little analysis of the likely economic effects of the proposed measures. However the industry's reactions then made it clear that conceptually and procedurally this proposal was lagging behind some of the approaches to risk measurement and risk management that had been developed by financial institutions themselves. Because of these reactions, the Basle Committee's revised proposal of April 1995 allowed for the possibility that capital adequacy requirements for banks bearing market risks be based on the banks' own risk models rather than any exogenously set, rigid regulatory standard. #CITATION_TAG compares ""the speed with which the regulatory community moved from the April 1993 and April 1995 proposals to the actual Amendment to the Capital Accord to Incorporate Market Risks of January 1996 to the time and expenses it takes for a private company to get a new drug approved for sale"" and notes that ""both the 1988 Accord and the 1996 Amendment to the 1988 Accord were enacted with hardly any evidence about the economic effects of capital requirements for banks.""","Both conferences were organized by NlKLAUS BLATTNER, of the University of Basle, and myself.","['Their purpose was partly to present results from an ongoing joint research project and partly to stimulate some interaction of discussion of banking regulation between academics, bankers, and regulators (in alphabetical order).']"
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. Increased regulatory competition has sharpened the comparative awareness of advantages or disadvantages of different national models of political economy, economic organization, governance and regulation. Although institutional change is slow and subject to functional complementarities as well as social and cultural entrenchment, at least some features of successful modern market economies have been in the process of converging over the last decades. Furthermore, at least to some extent, public enforcement is being reduced in favor of private enforcement by way of disclosure, enhanced liability, and correspondent litigation for damages. Corporatist approaches to governance are giving way to market approaches, and outsider and market-oriented corporate governance models seem to be replacing insider-based regimes. This transition is far from smooth and poses a daunting challenge to regulators and academics trying to redefine the fundamental governance and regulatory setting. They are confronted with the task of making or keeping the national regulatory structure attractive to investors in the face of competitive pressures from other jurisdictions to adopt state-of-the-art solutions. At the same time, however, they must establish a coherent institutional framework that accommodates the efficient, modern rules with the existing and hard-to-change institutional setting. 62 For a detailed, critical discussion of ""market discipline"", see #CITATION_TAG.","As bureaucratic ex-ante control is replaced by judicial ex-post control, administrative discretion is replaced by the rule of law as guidelines for the economy. As a reflection of the transnationality of the issues addressed, the world's three leading economies and their legal systems are included on an equal basis: the EU, the U.S., and Japan across each of the subtopics of corporations, bureaucracy and regulation, markets, and intermediaries.","['The most important change is a shift in governance from state to the market.', 'These challenges - put in a comparative and interdisciplinary perspective - are the subject of the book.']"
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. While the wealth effects are quite strong--between 4 and 6 cents--in countries with more developed mortgage markets and in market-based, Anglo-Saxon and non euro area economies, consumption only barely reacts to wealth elsewhere. The effect of housing wealth is somewhat smaller than that of financial wealth for most countries, but not for the U.S. and the UK. The housing wealth effect has risen substantially after 1988 as it has become easier to borrow against housing wealth. 73 Comparisons are made on the basis of numbers given in International Monetary Fund (2007) and #CITATION_TAG.",The baseline estimation method based on the sluggishness of consumption growth implies that the eventual (long-run) marginal propensity to consume out of total wealth is 5 cents (averaged across countries).,['I investigate the effect of wealth on consumption in a new dataset with financial and housing wealth from 16 countries.']
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. Diversification within an intermediary serves to reduce these costs, even in a risk neutral economy. 24 In the theory of financial institutions, therefore, the paradigmatic model of viable financial intermediation, due to #CITATION_TAG, postulates an intermediary holding a fully diversified portfolio of assets, with outside finance taking entirely the form of debt, with claims that are independent of the returns which the intermediary earns on his portfolio: If the claims on the financial intermediary are independent of returns on the intermediary's assets and if diversification ensures that the probability of default is zero, any benefits of taking greater effort in managing assets, e.g., more thorough monitoring of loans clients, accrue entirely to the intermediary.",It presents a characterization of the costs of providing incentives for delegated monitoring by a financial intermediary.,"['This paper develops a theory of financial intermediation based on minimizing the cost of monitoring information which is useful for resolving incentive problems between borrowers and lenders.', 'The paper presents some more general analysis of the effect of diversification on resolving incentive problems.']"
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. #CITATION_TAG 's famous model of the used-car market is paradigmatic for the problem.",,['This paper examines the allocation of credit in a market in which borrowers have greater information concerning their own riskiness than do lenders.']
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. The global financial crisis (GFC) caused catastrophic losses in the highly regulated banking sector. In contrast, the largely unregulated global hedge fund industry navigated through the crisis relatively unscathed. As a consequence of the GFC, there is the tidal wave of opinion calling for reform of the global financial architecture, with a specific emphasis on tightened oversight of hedge funds. 88 For theoretical treatments of the problem, see Carletti (2006, 2008), #CITATION_TAG (2006). 89 International Monetary Fund (2008a, pp. 65f).",,"['In this article, we consider the debate regarding the future of hedge fund regulation, building the case that regulatory reform to constrain excessive leverage must be applied in equal measure to all financial market participants, not just hedge funds.', 'The challenge for regulators is to carefully craft a regime of transparency and disclosure which minimises the potential for systemic risk without jeopardising the financial innovation and entrepreneurship that is emblematic of the hedge fund sector.Griffith Business School, Department of Accounting, Finance and EconomicsFull Tex']"
"First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. There hardly ever is enough time for an exchange of views in conferences. In this particular case, this was all the more advisable as it offered the opportunity to receive more information from the practioners of banking and of regulation. These statements are reprinted here. To the extent that different assets should carry different weights at all, the different weights should probably be attuned to differences in marketability of these assets, because these differences affect the difficulty 115 For evidence of the heterogeneity of rationales, see the discussion documented in #CITATION_TAG.","In the first part of the Panel the participants were asked to give their personal views and priorities with respect capital adequacy regulation of banks. The second part of the Panel was devoted to a general discussion between the participants, including MARTIN HELLWIG and myself as the Conference organisers. An attempt is made to summarise this discussion.",['One way of dealing with this is to end a conference in a Panel discussion.']
"In Czech, German, and many other languages, part of the semantic focus of the utterance can be moved to the left periphery of the clause. Given elements may later receive (topic or contrastive) accents, which accounts for fronting in multiple focus/contrastive topic constructions. We propose that movement to the left periphery is generally triggered by an unspecific edge feature of C (Chomsky 2008) and its restrictions can be attributed to requirements of cyclic linearization, modifying the theory of cyclic linearization developed by Fox and Pesetsky (2005) . In the picture that we had in mind, the syntax is autonomous - ''it does what it does'' - but sometimes the result maps to an unusable phonological representation. In this sense, linearization acts logically as a filter on derivations. We know of no evidence that the syntax can predict which syntactic objects will be usable by the phonology, and we know of no clear evidence that the phonology communicates this information to the syntax. Accentuation comes into play indirectly only, its relevance for fronting stems from the fact that accentuation is a side-effect of cyclic linearization in the sense of #CITATION_TAG and Müller (2007).","We thus attempted to identify certain deviant configurations that are not plausibly excluded for syntax-internal reasons, but are filtered out in the linearization process.",['Abstract Our proposal is concerned with the relation between an aspect of phonology (linearization) and syntax.']
"In Czech, German, and many other languages, part of the semantic focus of the utterance can be moved to the left periphery of the clause. Given elements may later receive (topic or contrastive) accents, which accounts for fronting in multiple focus/contrastive topic constructions. We propose that movement to the left periphery is generally triggered by an unspecific edge feature of C (Chomsky 2008) and its restrictions can be attributed to requirements of cyclic linearization, modifying the theory of cyclic linearization developed by Fox and Pesetsky (2005) . In his foundational book, The Minimalist Program, published in 1995, Noam Chomsky offered a significant contribution to the generative tradition in linguistics. This twentieth-anniversary edition reissues this classic work with a new preface by the author. In the preface to this edition, Chomsky emphasizes that the minimalist approach developed in the book and in subsequent work ""is a program, not a theory."" The use of focus/topic features in the syntactic derivation violates the inclusiveness condition (#CITATION_TAG), according to which only those features can figure in syntactic computations that represent properties of lexical items.","Building on the theory of principles and parameters and, in particular, on principles of economy of derivation and representation, the minimalist framework takes Universal Grammar as providing a unique computational system, with derivations driven by morphological properties, to which the syntactic variation of languages is also restricted. Within this theoretical framework, linguistic expressions are generated by optimally efficient derivations that must satisfy the conditions that hold on interface levels, the only levels of linguistic representation. The interface levels provide instructions to two types of performance systems, articulatory-perceptual and conceptual-intentional. All syntactic conditions, then, express properties of these interface levels, reflecting the interpretive requirements of language and keeping to very restricted conceptual resources.","['In four essays, Chomsky attempts to situate linguistic theory in the broader cognitive sciences, with the essays formulating and progressively developing the minimalist approach to linguistic theory.', 'With this book, Chomsky built on pursuits from the earliest days of generative grammar to formulate a new research program that had far-reaching implications for the field.']"
"lable at ScienceDirect Contents lists avai Journal of Clinical Gerontology & Geriatrics journal homepage: www.e- jcgg.com Letter to the Editor Evidence on the role of prebiotics, probiotics, and synbiotics in gut health and disease prevention in the elderly To the Editor In their review article, Patel et al1 elegantly described the process of ageing of the small bowel under normal conditions. In order to properly introduce the topic, it is reasonable to add information about the aging stomach and colon, as evidence shows age-related declines in these structures. However, four general benefits have been described: (1) suppression of growth or epithelial binding/invasion by pathogenic bacteria, (2) improvement of intestinal barrier function, (3) modulation of the immune system, and (4) modulation of pain perception.1,3,4 The majority of bacteria in the colon are anaerobes that can ferment carbohydrates that escape digestion, to form short chain fatty acids (SCFAs) and anions that have a distinct role in promoting gut health (e.g., acetate, propionate, and butyrate).3 Studies in the elderly described a shift in the composition of intestinal microbiota, with a lower number of beneficial organisms such as bifidobacteria and lactobacilli1 and an increase in Enterobacteriaceae and certain Proteobacteria.3 Compared to younger adult controls, aged persons seem to have a lower number of Firmicutes and more abundant Bacteroidetes.3 Butyrate is the major http://dx.doi.org/10.1016/j.jcgg.2014.01.001 2210-8335/Copyright 2014, Asia Pacific League of Clinical Gerontology & Geriatrics. Conflicts of interest The authors have no conflicts of interest relevant to this article : Antibiotic-associated diarrhoea (AAD) occurs most frequently in older (>=65 years) inpatients exposed to broad-spectrum antibiotics. When caused by Clostridium difficile, AAD can result in life-threatening illness. Although underlying disease mechanisms are not well understood, microbial preparations have been assessed in the prevention of AAD. However, studies have been mostly small single-centre trials with varying quality, providing insufficient data to reliably assess effectiveness. #CITATION_TAG nother recent large trial evaluated the evidence of the preventive effect of probiotics on upper respiratory tract infections (URTI) in an aged sample, and was unable to find differences between groups.",": We did a multicentre, randomised, double-blind, placebo-controlled, pragmatic, efficacy trial of inpatients aged 65 years and older and exposed to one or more oral or parenteral antibiotics. A computer-generated randomisation scheme was used to allocate participants (in a 1:1 ratio) to receive either a multistrain preparation of lactobacilli and bifidobacteria, with a total of 6 x 10(10) organisms, one per day for 21 days, or an identical placebo. Patients, study staff, and specimen and data analysts were masked to assignment. AAD (including CDD) occurred in 159 (10*8%) participants in the microbial preparation group and 153 (10*4%) participants in the placebo group (relative risk [RR] 1*04; 95% CI 0*84-1*28; p=0*71). CDD was an uncommon cause of AAD and occurred in 12 (0*8%) participants in the microbial preparation group and 17 (1*2%) participants in the placebo group (RR 0*71; 95% CI 0*34-1*47; p=0*35).",['We aimed to do a pragmatic efficacy trial in older inpatients who would be representative of those admitted to National Health Service (NHS) and similar secondary care institutions and to recruit a sufficient number of patients to generate a definitive result.']
"The 1958 birth cohort or the National Child Development Study (NCDS) began as a study of Perinatal Mortality focussing on just over 17 000 births in a single week in 1958. #CITATION_TAG We adapted the method used by Strachan et al 22 and created a variable to indicate presence of each respiratory symptom ( phlegm or cough), which cohort participants might have experienced at any time of day.","1 The initial survey was not planned as a longitudinal study, but subsequently the National Children's Bureau was commissioned by the Central Advisory Council for Education (The Plowden Committee) to retrace the cohort at age 7 and monitor their educational, physical, and social development.","['To address concerns regarding the stillbirth rate not falling, the original study aimed to identify social and obstetric factors linked to stillbirth and neonatal death.']"
"In recent years there has been a huge growth in using statistical and machine learning methods to find useful prediction systems for software engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though results are often promising no single technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our research is of necessity experimental. Minimally, as scientists, we need reproducible studies. Jin Nian ,sohutouearipozitorinodetamaininguwoLi Yong shitaBu Ju He Yu Ce noYan Jiu gaHuo Fa niXing wareteiru.Bu Ju He Yu Ce niGuan Lian shitedonoyounametorikusugaYan Jiu sareteirukawoMing rakanisurutame,Ben Gao dehaXi Tong De rebiyuwoXing tsuta.2tsunoLun Wen Zhi to5tsunoGuo Ji Hui Yi nioite,2000Nian kara2010Nian niFa Biao saretaWen Xian nouchi63Bian woDiao Cha shita.Yan Jiu saretametorikusuwoFen Xi surutame,Ce Ding Dui Xiang toCe Ding niBi Yao naLu Li Qing Bao niJi duki8tsunoLing Yu niFen Lei shita.rebiyunoJie Guo ,Zhu ni2005Nian Yi Jiang kodoyapurosesunoLu Li niJi dukumetorikusugaYan Jiu sareruyouninatsuteori,mata2008Nian Yi Jiang nihaXin taniKai Fa Zu Zhi yaDi Li De Wei Zhi Guan Xi niGuan Lian shitametorikusugaTi An sareteirukotogaMing rakaninatsuta.This paper provides a systematic review of studies related to software fault prediction with a specifec focus on types of metrics. Useful systematic literature reviews of research progress may be found in [#CITATION_TAG, 2].","The review uses 63 papers in 2 journals related and process-related historical metrics had been used mainly since 2005, and organization-related and geographic metrics had been used since 2008","['The research questions on this paper are: what types of metrics are used for studies related to faults, and is there trend in proposal of fault-related metrics.']"
"In recent years there has been a huge growth in using statistical and machine learning methods to find useful prediction systems for software engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though results are often promising no single technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our research is of necessity experimental. Minimally, as scientists, we need reproducible studies. Sadly, there are all too few examples of stable conclusions in software engineering (SE). Below are four examples of this type of problem which we believe to be endemic within SE. A challenge has been that whilst there has been much research activity and ingenuity in devising new techniques there has been rather less agreement upon the relative effectiveness of these different techniques and clearly no one technique dominates [#CITATION_TAG].",to discover some effect X that holds in multiple situations. We can find numerous studies of the following form: there is as much evidence for as against the argument that some aspect X adds value to a software project.,"['The goal of science is conclusion stability, i.e.']"
"Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. A great many tools have been developed for supervised clas- sification, ranging from early methods such as linear discriminant anal- ysis through to modern developments such as neural networks and sup- port vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the di!erence in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm. These and related issues are discussed further in [3, #CITATION_TAG, 5, 6].",,"['This paper argues that these comparisons often fail to take into account important aspects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illu- sion.']"
"Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. Different measures will yield different results, and it follows that it is crucial to match the measure to the true objectives. These and related issues are discussed further in [3, 4, #CITATION_TAG, 6].","In predictive data mining, algorithms will be both optimized and compared using a measure of predictive performance. We define two measures, one based on minimizing the overall cost to the card company, and the other based on minimizing the amount of fraud given the maximum number of investigations the card company can afford to make. We also describe a plot, analogous to the standard ROC, for displaying the performance trace of an algorithm as the relative costs of the two different kinds of misclassification--classing a fraudulent transaction as legitimate or vice versa--are varied.","['In this paper, we explore the desirable characteristics of measures for constructing and evaluating tools for mining plastic card data to detect fraud.']"
"Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. Perhaps paradoxically, however, there is still little understanding of the circumstances under which certain methods will perform better than others. and is reported in [#CITATION_TAG], in a meta-analysis of classification studies, as being used in 'the vast majority' of comparative studies of classification rules (p19).","Numerous methods have been designed in the past twenty years or so, but by different communities who had different approaches and interests. Comparative studies have thus been carried out to assess the relative merits of the methods.","['Aim of the project Supervised classification has a substantial history, but has recently progressed dramatically as a result of the advent of the computer, and is now considered as one of the main components of the relatively new field of data-mining.']"
"Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. The ROC curve and its properties have been extensively studied and are well understood -see [#CITATION_TAG, 11] t represent two values of the threshold.","It provides illustrative examples of the major methodological developments and includes as much of the mathematical theory as necessary without making the treatment too dense. The authors survey the uses made of the methodology across a range of different areas, from atmospheric science and geoscience to experimental psychology and sociology. They also list a number of websites from which software implementing the various techniques can be downloaded.","['Bringing together all the relevant material to impart a clear understanding of how to analyze ROC curves, this book covers the fundamental theory as well as various special topics.']"
"Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. Identification of key factors associated with the risk of developing cardiovascular disease and quantification of this risk using multivariable prediction algorithms are among the major advances made in preventive cardiology and cardiovascular epidemiology in the 20th century. The ongoing discovery of new risk markers by scientists presents opportunities and challenges for statisticians and clinicians to evaluate these biomarkers and to develop new risk formulations that incorporate them. Some researchers have advanced that the improvement in the area under the receiver-operating-characteristic curve (AUC) should be the main criterion, whereas others argue that better measures of performance of prediction models are needed. Although the AUC has been criticised on various methodological grounds (see, for example, [15, #CITATION_TAG]) this interpretation suggests that it also has a core theoretical weakness, at least when viewed from some perspectives.","In this paper, we address this question by introducing two new measures, one based on integrated sensitivity and specificity and the other on reclassification tables. We discuss the properties of these new measures and contrast them with the AUC. We also develop simple asymptotic tests of significance. We illustrate the use of these measures with an example from the Framingham Heart Study. We propose that scientists consider these types of measures in addition to the AUC when assessing the performance of newer biomarkers.",['One of the key questions is how best to assess and quantify the improvement in risk prediction offered by these new models.']
"Despite increased research on the various effects of Corporate Social Responsibility (CSR), the question of whether CSR is  worthwhile for firms still remains to be addressed. Prior work suggests that CSR offers firms insurance-like protection against  negative publicity due to greater levels of goodwill with various stakeholders. Yet, we still miss an answer to the following question:  How effective, if at all, is CSR in insulating firms from scrutiny compared to other important marketing measures, such as  customer orientation and service quality orientation? Therefore, managers should consider concentrating efforts on understanding their customers, closely monitoring their perceptions about the company, and devoting attention to accommodating their individual requirements (#CITATION_TAG).",,['This study develops and empirically tests a theoretical framework that  demonstrates the relative impact of CSR on consumer resistance to negative information when confronted with negative information about a firm.']
"The editorial reviews a number of insights by prominent scholars including Gerd Gigerenzer's treatise that ""Scientists' tools are not neutral."" The present study builds on the existing literature that underscores the value of fuzzy-set qualitative comparative analysis (fsQCA) (e.g., Fiss, 2011; #CITATION_TAG; Woodside & Zhang, 2013) and shows that the proposed methodological tool offers much in terms of understanding causal relationships, by virtue of providing information that is unique in comparison with the information that conventional correlational methods provide.",The editorial includes an example of testing an MRA model for fit and predictive validity. The same data used for the MRA is used to conduct a fuzzy-set qualitative comparative analysis (fsQCA).,"['This editorial suggests moving beyond relying on the dominant logic of multiple regression analysis (MRA) toward thinking and using algorithms in advancing and testing theory in accounting, consumer research, finance, management, and marketing.']"
"Typologies are an important way of organizing the complex cause-effect relationships that are key building blocks of the strategy and organization literatures. Types and typologies are ubiquitous, both in every-day social life and in the language of the social sciences. Everybody uses them, but almost no one pays any attention to the nature of their construction.-McKinney (1969: 4) The notion of causality plays a key role in both the strategy and organization literatures. For in-stance, cause-effect relationships are the central way in which strategic decisions and organization-al structures are understood and communicated in organizations (Ford, 1985; Huff, 1990; Huff &amp; Jen-kins, 2001). Building on this insight, the cognitive strategy literature has aimed to map and explain the causal reasoning of managers regarding both organizational performance and competitive envi-ronments (e.g., Barr, Stimpert,  &amp; Huff, 1992; Nad-karni &amp; Narayanan, 2007a, 2007b; Reger &amp; Huff, 1993). Similarly, cause-effect relationships are the main building blocks of the organizational design literature and have recently received increasing at-tention (e.g., Burton &amp; Obel, 2004; Grandori &amp; Fur The present study builds on the existing literature that underscores the value of fuzzy-set qualitative comparative analysis (fsQCA) (e.g., #CITATION_TAG; Woodside, 2013; Woodside & Zhang, 2013) and shows that the proposed methodological tool offers much in terms of understanding causal relationships, by virtue of providing information that is unique in comparison with the information that conventional correlational methods provide.","Using data on high-technology firms, I empirically investigate configurations based on the Miles and Snow typology using fuzzy set qualitative comparative analysis (fsQCA).","['Here, I develop a novel theoretical perspective on causal core and periphery, which is based on how elements of a configuration are connected to outcomes.']"
"Building on the varieties-of-capitalism approach, it is argued that competitive advantage in high-tech industries with radical innovation may be supported by combinations of certain institutional conditions: lax employment protection, weak collective bargaining coverage, extensive university training, little occupational training, and a large stock market. Furthermore, multinational enterprises engage in ""institutional arbitrage"": they allocate their activities so as to benefit from available institutional capital. A high share of university graduates and a large stock market are complementary institutions leading to strong export performance in high-tech. Employment protection is neither conducive nor harmful to export performance in high-tech. A high volume of cross-border mergers and acquisitions, as a form of institutional arbitrage leading to knowledge flows, acts as a functional equivalent to institutions that support knowledge production in the home economy. For example, in contrast with correlational techniques, which attempt to estimate the net effect of an independent variable on an outcome variable, fsQCA attempts to identify the conditions that lead to a given outcome (#CITATION_TAG).",These hypotheses are tested on country-level data for 19 OECD economies in the period 1990 to 2003.,"['We examine how institutional configurations, not single institutions, provide companies with institutional capital.']"
"However, there is only one avenue to successful product innovation for low-agreement cases. Applications of the technique in business and management are fewer, though representative examples are available in areas such as international business (e.g., Pajunen, 2008; Schneider et al., 2010), innovation (e.g., #CITATION_TAG; Ganter & Hecker, 2013; Stanko & Olleros, 2013), organizational behavior and strategic management (e.g., Fiss, 2011; Greckhamer et al., 2008; Stokke, 2007), inter-organizational alliances (e.g., Leischnig et al., 2013), tourism management (e.g., Woodside et al., 2011), socially responsible practices (e.g., Crilly et al., 2012), and labor relations (e.g., Coverdill et al., 1994).","[[abstract]]This study offers an integrated framework involving antecedent paths to successful product innovation. The study explores conditional models leading to high product-innovation performance using fuzzy-set qualitative comparative analysis (fsQCA). Based on relevant literature, this study categorizes relevant antecedents including organization-related, project-related, process-related, product-related, and market-related categories, and the newness of product innovation, into causal recipes. To assess the applicability of this conceptual model, researchers collected data from R&D managers and members of high-tech firms in Taiwan. The study includes dividing sampled firms into three groups according to the extent of within-firm agreement on responses by executives to questions about antecedents (in surveys that they answered independently).",['The strategy implication is to think through alternative paths and not key success factors for achieving high product-innovation performance']
"The scholarly literature on health care politics has generated a series of hypotheses to explain U.S. exceptionalism in health policy and to explain the adoption of national health insurance (NHI) more generally. The literature is dominated by national and comparative case studies that illustrate the theoretical logic of these hypotheses but do not provide a framework for examining the hypotheses cross-nationally. Representative contributions from various sub-fields include policy analysis (e.g., #CITATION_TAG), political parties (e.g., Gordin, 2001), social and political change (e.g., Berg-Schlosser & De Meur, 1994), social movements (e.g., Nomiya, 2001), welfare states (e.g., Peillon, 1996), law and criminology (e.g., Tarohmaru, 2001), linguistics (e.g., Mendel & Korjani, 2012), psychology (e.g., Theuns, 1994), and addictive behavior (e.g., Eng & Woodside, 2012).","Various cultural, institutional, and political conditions are held to make the establishment of some form of national health insurance policy more (or less) likely to occur.",['This article is an initial attempt to address that void by using Boolean analysis to examine systematically several of the major propositions that emerge from the case study literature on the larger universe of twenty advanced industrial democracies.']
"type of analysis to explore social responsibility phenomena (for exceptions, see #CITATION_TAG; Crilly et al., 2012).","To identify how factors at different levels of analysis combine to shape attitudes toward social responsibility, I apply fuzzy-set qualitative comparative analysis (fsQCA) to survey and archival data from 335 managers of overseas subsidiaries of three Dutch corporations. Attention to the simultaneous effects of individual psychological factors, the organizational context, and the broader social context offers a configurational perspective on the micro and macrofoundations of social responsibility.",['This chapter explores the integrative effects of individual psychology and social context in explaining why managers would behave in socially responsible ways.']
"Because of its inherently asymmetric nature, set-theoretic analysis offers many interesting contrasts with analysis based on correlations. Until recently, however, social scientists have been slow to embrace set-theoretic approaches. The perception was that this type of analysis is restricted to primitive, binary variables and that it has little or no tolerance for error. With the advent of ''fuzzy' ' sets and the recognition that even rough set-theoretic relations are relevant to theory, these old barriers have crumbled. The higher the consistency cutoff point the researcher sets for selecting the best combinations, the higher the final consistency will be, but the lower the respective coverage (Elliott, 2013; #CITATION_TAG).","The first mea-sure, ''consistency,' ' assesses the degree to which a subset relation has been approxi-mated, whereas the second measure, ''coverage,' ' assesses the empirical relevance of a consistent subset.","['This paper advances the set-theoretic approach by presenting simple descriptive measures that can be used to evaluate set-theoretic relationships, especially relations between fuzzy sets.']"
"Research in business examines skepticism in the areas of advertising, promotion, and public relations (e.g., Boush et al., 1994; Obermiller et al., 2005); corporate social marketing (Forehand & Grier, 2003); environmental claims (Mohr et al., 1998); cause-related claims (#CITATION_TAG); CSR communication during crises (Vanhamme & Grobben, 2009); and CSR programs (Pirsch et al., 2007).","It also seeks to test whether the relationship between familiarity and skepticism may be moderated by skepticism towards advertising in general.Design/methodology/approach - A single factor experimental design with four levels of claim repetition was utilized to test the hypothesized effects between claim repetition, familiarity, skepticism towards advertising and skepticism towards CRM claim.Findings - The findings support the hypothesized effects.","[""Purpose - This study, conducted in Norway, aims to investigate whether increasing consumers' familiarity by repeating cause related marketing (CRM) claims helps in reducing their skepticism towards CRM campaigns.""]"
"Corporate social responsibility (CSR) is a hot topic in management today. More than ever before, companies engage in CSR initiatives to make a positive contribution to society or support their strategic goals. Yet, in the face of a plethora of CSR claims and numerous reported incidents of corporate misconduct, many people doubt the extent to which companies live up to their professed standards, and consumer skepticism toward corporate social involvement is on the rise. Using structural equation modeling (SEM), #CITATION_TAG (hereinafter SL) find that attributions of egoistic-and stakeholder-driven motives provoke consumer skepticism about corporate social responsibility (CSR) while values-driven attributions alleviate skepticism.",,"['Drawing on attribution theory, this study proposes and tests a model that explains both how consumer skepticism toward the CSR of grocery retailers develops and its influence on important consumer-related outcomes.']"
"When reading pro and con arguments, participants (Ps) counterargue the contrary arguments and uncritically accept supporting arguments, evidence of a disconfirmation bias. Many disciplines discuss skepticism, including politics (e.g., #CITATION_TAG), philosophy (e.g., McGrath, 2011), sociology (e.g., Freudenburg et al., 2008), and psychology (e.g., Lilienfeld, 2012).","Two experimental studies explore how citizens evaluate arguments about affirmative action and gun control, finding strong evidence of a prior attitude effect such that attitudinally congruent arguments are evaluated as stronger than attitudinally incongruent arguments. We also find a confirmation bias--the seeking out of confirmatory evidence--when Ps are free to self-select the source of the arguments they read.",['We propose a model of motivated skepticism that helps explain when and why citizens are biased-information processors.']
"Reports of firms ' behaviors with regard to corporate social responsibility (CSR) are often contrary to their stated standards of social responsibility. Corporate wrongdoing attracts the attention of the media and watchdog organizations, triggering questions about why companies engage in CSR and how they contribute to social well-being (Bielak et al., 2007; #CITATION_TAG).",,"[""This research examines the effects of communication strategies a firm can use to mitigate the impact of these inconsistencies on consumer perceptions of corporate hypocrisy and subsequent beliefs about the firm's social responsibility and attitudes toward the firm.""]"
"In their formulation, the causal conjuncture is a sequence of conditions or events. The main aim of the technique is to identify all necessary and sufficient conditions that lead to a specific outcome condition (#CITATION_TAG).","This comment clarifies and corrects aspects of their analysis and present methods for assessing temporality that are more amenable to truth table analysis and the use of existing software, fsQCA. The methods presented utilize codings that indicate event order in addition to codings that indicate whether specific events occurred. They also demonstrate how to use ``don't care'' codings to bypass consideration of event sequences when they are not relevant (e.g., as when only a single event occurs).",['Caren and Panofsky (2005) seek to advance qualitative comparative analysis (QCA) by demonstrating that it can be used to study causal conditions that occur in sequences and introduce a technique they call TQCA (temporal QCA).']
"At least since the time of Popper, scientists have understood that science provides falsification, but not ""proof."" In the world of environmental and technological controversies, however, many observers continue to call precisely for ""proof,"" often under the guise of ""scientific certainty."" Given that most scientific findings are inherently probabilistic and ambiguous, if agencies can be prevented from imposing any regulations until they are unambiguously ""justified,"" most regulations can be defeated or postponed, often for decades, allowing profitable but potentially risky activities to continue unabated. Many disciplines discuss skepticism, including politics (e.g., Taber & Lodge, 2006), philosophy (e.g., McGrath, 2011), sociology (e.g., #CITATION_TAG), and psychology (e.g., Lilienfeld, 2012).","An exploratory examination of previously documented controversies suggests that SCAMs are more widespread than has been recognized in the past, and that they deserve greater attention in the future. In the first section of the article, we draw on science and technology studies to underscore the point that science is often characterized not by certainty, but by uncertainty--meaning that the outcomes of scientific/technological controversies may depend less on which side has the ""best science"" than on which side enjoys the benefit of the doubt in the face of scientific ambiguity. In the second section, we note that the benefits of doubts may be distributed in ways that are not merely random: Although there is clearly a need for more extensive research, a series of riskrelated controversies, over a period of nearly a century, indicate that industrial interests have often managed to delay or prevent legislative and/or regulatory actions even in ""tough"" cases--those where the preponderance of scientific evidence had indicated significant reasons for concern.","['This article will identify a pattern of argument that is sufficiently common in regulatory debates that it appears to deserve its own name--""Scientific Certainty"" Argumentation Methods, or SCAMs.']"
"Notwithstanding the significance to organizations of external reactions to bad behavior, the corporate social responsibility literature tends to focus on the meaning of and expectations for responsible behavior, rather than on the meaning of irresponsible behavior. We draw on attribution theory to describe how attributions of irresponsibility stem from the observer's subjective assessments of effect undesirabili... While increasingly more companies undertake CSR initiatives in an attempt to contribute to society or pursue their strategic goals, examples of corporate social irresponsibility abound (e.g., Carson, 2003; #CITATION_TAG; Murphy & Schlegelmilch, 2013).",,"[""Here we develop a theoretical perspective that explicitly focuses on irresponsibility and that particularly helps explain attributions of social irresponsibility in the minds of the firm's observers."", 'In contrast to approaches in the corporate social responsibility literature that tend to deemphasize the role of the individual perceiver of firm behavior in favor of emphasizing such broader social structures as value systems, institutions, and stakeholder relations, our focus is on how the social reality of external expectations for social responsibility is rooted in the perceptions of the beholder.']"
"Recent challenges in information retrieval are related to cross media information in social networks including rich media and web based content. In those cases, the cross media content includes classical file and their metadata plus web pages, events, blog, discussion forums, comments in multilingual. This heterogeneity creates large complex problems in cross media indexing and retrieval for services that integrate qualified documents and user generated content together. This paper presents a model and an indexing and searching solution for cross media contents, addressing the above issues, developed for the ECLAP Social Network, in the domain of Performing Arts. The research aimed to cope with the complexity of a heterogeneous indexing semantic model, using stochastic optimization techniques, with tuning and discrimination of relevant metadata terms. Other techniques make use of Fuzzy algorithms [33, 47], local context analysis [56], clustering [59], and ranking improvement [#CITATION_TAG].","It is argued that the optimal ranking problem should be factorized into two distinct yet interrelated stages: the relevance prediction stage and ranking decision stage. During retrieval the relevance of documents is not known a priori, and the joint probability of relevance is used to measure the uncertainty of documents' relevance in the collection as a whole. The resulting optimization objective function in the latter stage is, thus, the expected value of the IR metric with respect to this probability measure of relevance. Through statistically analyzing the expected values of IR metrics under such uncertainty, we discover and explain some interesting properties of IR metrics that have not been known before. Our analysis and optimization framework do not assume a particular (relevance) retrieval model and metric, making it applicable to many existing IR models and metrics.",['This paper presents a new way of thinking for IR metric optimization.']
"Recent challenges in information retrieval are related to cross media information in social networks including rich media and web based content. In those cases, the cross media content includes classical file and their metadata plus web pages, events, blog, discussion forums, comments in multilingual. This heterogeneity creates large complex problems in cross media indexing and retrieval for services that integrate qualified documents and user generated content together. This paper presents a model and an indexing and searching solution for cross media contents, addressing the above issues, developed for the ECLAP Social Network, in the domain of Performing Arts. The research aimed to cope with the complexity of a heterogeneous indexing semantic model, using stochastic optimization techniques, with tuning and discrimination of relevant metadata terms. Classical Information Retrieval (IR) is the sifting out of the documents most relevant to a user's information requirement (expressed as a ""query""), from a large electronic store of documents. Rather than regarding foreign-language documents simply as unwanted ""noise"", Cross Language Information Retrieval allows the user to state their query in one language, and retrieve documents in another. Some CLIR systems use language resources such as bilingual dictionaries to translate the user's original query, while other systems use machine translation to translate the foreign-language documents beforehand, enabling them to be retrieved by the original query. Problems arise due to ambiguity in language, the use of synonyms to express a single idea, and the lack of context available in translating a short query. Indeed, the automatic query translation process could create word ambiguity, poly-semy, inflection and homonymy issues [#CITATION_TAG], especially in the case of short queries [25].",A search engine performs IR by retrieving relevant web pages from the internet.,"['This paper will discuss previous work in CLIR, current problems in CLIR, and make recommendations for future work.']"
"Recent challenges in information retrieval are related to cross media information in social networks including rich media and web based content. In those cases, the cross media content includes classical file and their metadata plus web pages, events, blog, discussion forums, comments in multilingual. This heterogeneity creates large complex problems in cross media indexing and retrieval for services that integrate qualified documents and user generated content together. This paper presents a model and an indexing and searching solution for cross media contents, addressing the above issues, developed for the ECLAP Social Network, in the domain of Performing Arts. The research aimed to cope with the complexity of a heterogeneous indexing semantic model, using stochastic optimization techniques, with tuning and discrimination of relevant metadata terms. In recent years, the growth of Social Network communities has posed new challenges for content providers and distributors. Digital contents and their rich multilingual metadata sets need improved solutions for an efficient content management. There is an increasing need of services for scaling digital services, searching and indexing. Despite the huge amount of contents, users want to easily find relevant unstructured documents in large repositories, on the basis of multilingual queries, with a limited waiting time. At the same time, digital archives have to be fully accessible even if a major restructuring is in progress, or without a significant downtime. The ECLAP information model for cross-media integrates sources coming from 35 different international institutions [9, #CITATION_TAG].",Effectiveness and optimization analysis of the retrieval solution are presented with relevant metrics.,"['This paper presents an indexing and searching solution for cross media content, developed for a Social Network in the domain of Performing Arts.', 'The research aims to cope with the complexity of a heterogeneous indexing semantic model, with tuning techniques for discrimination of relevant metadata terms.']"
"Recent challenges in information retrieval are related to cross media information in social networks including rich media and web based content. In those cases, the cross media content includes classical file and their metadata plus web pages, events, blog, discussion forums, comments in multilingual. This heterogeneity creates large complex problems in cross media indexing and retrieval for services that integrate qualified documents and user generated content together. This paper presents a model and an indexing and searching solution for cross media contents, addressing the above issues, developed for the ECLAP Social Network, in the domain of Performing Arts. The research aimed to cope with the complexity of a heterogeneous indexing semantic model, using stochastic optimization techniques, with tuning and discrimination of relevant metadata terms. Knowledge intensive organizations have vast array of information contained in large document repositories. With the advent of E-commerce and corporate intranets/extranets, these repositories are expected to grow at a fast pace. This explosive growth has led to huge, fragmented, and unstructured document collections. Although it has become easier to collect and store information in document collections, it has become increasingly difficult to retrieve relevant information from these large document collections. There are three important paradigms of research in the area of information retrieval (IR): Probabilistic IR, Knowledge-based IR, and, Artificial Intelligence based techniques like neural networks and symbolic learning. Very few researchers have tried to use evolutionary algorithms like genetic algorithms (GA&apos;s). Previous attempts at using GA&apos;s have concentrated on modifying document representations or modifying query representations. In the context of IR optimization, stochastic approaches have been exploited to improve the IR effectiveness; for example genetic algorithms have been used for improving the effectiveness of IR systems [#CITATION_TAG, 37, 41], for query reformulation [38], for query selection [17] and improving [57].",An overall matching function is treated as a weighted combination of scores produced by individual matching functions. This overall score is used to rank and retrieve documents. Weights associated with individual functions are searched using Genetic Algorithm. The idea is tested on a real document collection called the Cranfield collection.,"['This paper addresses the issue of improving retrieval performance (in terms of precision and recall) for retrieval from document collections.', 'This work looks at the possibility of applying GA&apos;s to adapt various matching functions.']"
"Recent challenges in information retrieval are related to cross media information in social networks including rich media and web based content. In those cases, the cross media content includes classical file and their metadata plus web pages, events, blog, discussion forums, comments in multilingual. This heterogeneity creates large complex problems in cross media indexing and retrieval for services that integrate qualified documents and user generated content together. This paper presents a model and an indexing and searching solution for cross media contents, addressing the above issues, developed for the ECLAP Social Network, in the domain of Performing Arts. The research aimed to cope with the complexity of a heterogeneous indexing semantic model, using stochastic optimization techniques, with tuning and discrimination of relevant metadata terms. Nowadays, personalization of search engines as the only web search tools plays important role in increasing the speed of access to web information. Relevant examples of fuzzy techniques application include semantic search [27], ontologies [#CITATION_TAG], Cloud Computing [30], image text analysis [12], query expansion [51], clustering [34] and popular search platforms such as Apache Lucene.",In this paper we personalize the search engine results using the automatic fuzzy concept networks.,"[""Our main idea is to employ the concepts of ontology in order to enrich the common fuzzy concept networks built based on user's profile.""]"
"We propose the use of finite mixtures of continuous distributions in modelling the process by which new individuals, that arrive in groups, become part of a wildlife population. Accurate estimates of demographic parameters are key for understanding and predicting population dynamics and for providing insights for effective wildlife management. Up until recently, no suitable methodology has been available to estimate survival probabilities of species with asynchronous reproduction and a high level of individual variation in capture probabilities. Specifically, it has been frequently reported that unmodelled heterogeneity in capture probabilities leads to biased estimates of the population size (Pollock et al. 1990), but it can also affect estimation of survival probabilities (#CITATION_TAG; Fletcher et al. 2012; Matechou et al. 2013b).",,"['The present work develops a capture-mark-recapture model for cheetahs in the Serengeti National Park, Tanzania, which (a) deals with continuous reproduction, (b) takes into account the high level of individual heterogeneity in capture probabilities and (c) is spatially explicit.']"
"Several variant RARA translocations have been reported in acute promyelocytic leukemia (APL) of which the t(11;17)(q23;q21), which results in a ZBTB16-RARA fusion, is the most widely identified and is largely resistant to therapy with all-trans retinoic acid (ATRA). Acute promyelocytic leukemia (APL) is typified by the t(15;17) translocation, which leads to the formation of the PML/RARA fusion gene and predicts a beneficial response to retinoids. However, approximately 10% of all APL cases lack the classic t(15;17). To address this issue, a European workshop was held in Monza, Italy, during June 1997, and a morphologic, immunophenotypic, cytogenetic, and molecular review was undertaken in 60 cases of APL lacking t(15;17). This process led to the development of a novel morphologic classification system that takes into account the major nuclear and cytoplasmic features of APL. The bone marrow morphology of patients with ZBTB16-RARA APL tends to be distinct from those patients with either classical or the hypogranular variant of APL [#CITATION_TAG].","This group includes (1) cases with cryptic PML/RARA gene rearrangements and t(5;17) that leads to the NPM/RARA fusion gene, which are retinoid-responsive, and (2) cases with t(11;17)(q23;q21) that are associated with the PLZF/RARA fusion gene, which are retinoid-resistant.",['A key issue is how to rapidly distinguish subtypes of APL that demand distinct treatment approaches.']
"Several variant RARA translocations have been reported in acute promyelocytic leukemia (APL) of which the t(11;17)(q23;q21), which results in a ZBTB16-RARA fusion, is the most widely identified and is largely resistant to therapy with all-trans retinoic acid (ATRA). All-trans retinoic acid (ATRA) is a potent inducer of differentiation and cell death in malignant cells. Paradoxically, most AML3 cells harbor an abnormal retinoic acid receptor (PML/RAR alpha) resulting from the t(15;17) translocation. Though few AML3 patients do not respond to ATRA therapy, individualization of these cases is of practical importance. Recently the RAR alpha gene has been demonstrated to be involved in a novel fusion transcript (PLZF/RAR alpha) through a t(11;17) translocation. Identification of the ZBTB16-RARA fusion is critical for therapeutic purposes as these patients are generally resistant to differentiating retinoid therapy, specifically alltrans retinoic acid (ATRA) [#CITATION_TAG], and treatment should be with standard AML regimens according to current recommendations [8].",We describe here the second case of such a patient with a t(11;17)-PLZF/RAR alpha leukemic clone.,['Its effect is known to be mediated through binding to specific nuclear (RARs and RXRs) or cytoplasmic (CRABP) proteins.']
Increasing life span and lack of medication for prevention or treatment of progressive dementias will significantly increase the number of individuals with advanced dementia worldwide. Providing optimal care for them will stretch health care resources and will require evaluation of different treatment strategies. This paper is presenting measures that may be used in this patient population. Only very few residents who understood others and were not depressed were abusive. Behavioral interventions preventing escalation of resistiveness to care into combative behavior and the treatment of depression can be expected to decrease or prevent abusive behavior of most nursing home residents with dementi Recent work developed also from sharing newly collected [3] and existing data [#CITATION_TAG] -the Dutch author analyzing the U.S. data and vice versa.,"DESIGN: Analysis of Minimum Data Set (MDS) of the Resident Assessment Instrument (RAI) information. SETTING: We used MDS-RAI data from 8 Dutch nursing homes and 10 residential homes that volunteered to collect data for care planning. We included the data of residents within a 12-month time window for each facility separately, resulting in a range from April 4, 2007, to December 1, 2008. PARTICIPANTS: We selected 929 residents older than 65 with Alzheimer's disease or other dementia who were dependent in decision making and not comatose. MEASUREMENTS: Cognitive Performance Scale, MDS Depression Scale and several individual items from the MDS-RAI (ability to understand others, verbally and physically abusive behavioral symptoms, resist care, diagnosis of Alzheimer's disease and of dementia other than Alzheimer's disease, diagnosis of depression, presence of delusions, hallucinations, pain frequency and constipation, and number of days receiving medications).",['OBJECTIVES: To determine modifiable factors related to abusive behaviors in nursing home residents with dementia.']
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. As expected, major depression was more common in women than in men, but the difference became smaller with advancing age. An increasing prevalence with age in single (never-married) men was an unexpected finding. For example, the Canadian Community Health Survey (CCHS) recently reported a one-year prevalence rate of 4.5% for MDD, indicating that over 1.2 million Canadians suffer significant distress and impairment in functioning due to mood disorders [#CITATION_TAG].",Method: All estimates used appropriate sampling weights and bootstrap variance estimation procedures. The analysis consisted of estimating proportions supplemented by logistic regression modelling. Logistic regression analysis suggested that the annual prevalence may increase with age in men who never married.,"['Objective: The Canadian Community Health Survey: Mental Health and Well-Being (CCHS 1.2) is the first national study to use a full version of the Composite International Diagnostic Interview.', 'Using the CCHS 1.2 data, our study aimed to describe the epidemiology of major depression in Canada.']"
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. Systemic lupus erythematous (SLE) is an autoimmune and inflammatory disease that can affect all organs of the body. For example, one study reported that workers with MDD missed an average of 32 days of work in a 12-month assessment period [#CITATION_TAG], while another found that about 30% of work disability claims in Canada were attributed to mental illness, predominant depression, and other mood disorders [6].",Participants were 257 residents of the state of Georgia in the United States with SLE and were recruited from the Georgians Organized Against Lupus study.,"['The purpose of this quantitative cross-sectional study was to examine SLE-related issues associated with depression and work-productivity impairment, and to assess if depression mediated the relationship between SLE disease activity and damage and work-productivity impairment.']"
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. For example, Sanderson and colleagues [10] studied a nonclinical sample of 431 employees at 10 Australian call centres and examined depressive symptoms endorsed on a rating scale (the Patient Health Questionnaire, PHQ-9) and effects on work productivity, as measured by the work limitations questionnaire (WLQ) [11, #CITATION_TAG].","Research Design.Three pilot studies (focus groups, cognitive interviews, and an alternate forms test) generated candidate items, dimensions, and response scales. To test recall error, study 1 subjects were randomly assigned to 2 different questionnaire groups, a questionnaire with a 4-week reporting period completed once or a 2-week version completed twice. Responses were compared with data from concurrent work limitation diaries (the gold standard). To test construct validity, we compared questionnaire scores of patients with those of healthy job-matched control subjects. Study 2 was a cross-sectional mail survey testing scale reliability and construct validity. Subjects.The study subjects were employed individuals (18-64 years of age) from several chronic condition groups (study 1, n = 48; study 2, n = 121) and, in study 1, 17 healthy matched control subjects. Measures.Study 1 included the assigned questionnaires and weekly diaries. Study 2 included the new questionnaire, SF-36, and work productivity loss items.","['Objective.The objective of this work was to develop a psychometrically sound questionnaire for measuring the on-the-job impact of chronic health problems and/or treatment (""work limitations"").']"
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. However, brief measures of mental health-related functional impairment are not commonly applied in primary care settings. At their initial assessment, all patients completed the QIDS-SR [18], a validated self-rated scale to assess severity and type of depressive symptoms, and the Sheehan Disability Scale [#CITATION_TAG].","The Sheehan Disability Scale (SDS), a three-item instrument for assessing such impairment, is evaluated in this study. Method: A psychometric analysis of the SDS was conducted with a sample of 1001 primary care patients at Kaiser Permanente in Oakland, California. The construct validity was substantiated in two ways.",['Objective: Several recent studies have documented that substantial functional impairment is associated with many of the mental disorders seen in primary care.']
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. For example, women have been found to have more work absence days than men [#CITATION_TAG].","METHODS The study population was drawn from the Canadian Community Health Survey 1.2, a national population-based survey that gathered cross-sectional data on health status from 22,118 working respondents. The relationship between chronic work stress, chronic physical conditions, and psychiatric disorders and disability in the past 14 days was examined for working respondents by using logistic regressions controlling for sociodemographic characteristics, region, and occupation.","['OBJECTIVE There appear to be links between psychiatric disorders and work-related stress as well as between psychiatric disorders and physical conditions.', 'This study explores the relationships between chronic work stress, psychiatric disorders, and chronic physical conditions and disability among workers.', 'By doing so, this study sought to understand how these factors are associated with worker disability when they are experienced alone versus in combination with one another.']"
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. Several data limitations suggest that these are underestimates. The estimated total burden of $14.4 billion places mental health problems among the costliest conditions in Canada. In Canada, the economic costs of depression-related 2 Depression Research and Treatment presenteeism alone are estimated at over $5 billion annually [#CITATION_TAG].","In particular, it estimates the cost of non-medical services that have not been previously published and the value of short-term disability associated with mental health problems that were previously underestimated according to the definitions used here.",['This study provides a comprehensive estimate of the economic burden of mental health problems in Canada in 1998.']
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. The most common were allergies, arthritis/joint pain or stiffness, and back or neck disorders. However, the greater proportion of the total economic burden of MDD lies in reduced productivity, or presenteeism, in which the depressed individual remains in the work setting but with productivity suffering both in quality and quantity [7, #CITATION_TAG].","Methods: Using the Stanford Presenteeism Scale, information was collected from workers at five locations on work impairment and absenteeism based on self-reported ""primary"" chronic health conditions. Survey data were merged with employee demographics, medical and pharmaceutical claims, smoking status, biometric health risk factors, payroll records, and job type.",['Objective: The objective of this study was to determine the prevalence and estimate total costs for chronic health conditions in the U.S. workforce for the Dow Chemical Company (Dow).']
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. Having a health problem is a strong determinant of sickness presenteeism (odds ratio = 3.32). For any given health status, there are certain other factors (personally and work-related demands) that impact on the risk of sickness presence, such as difficulties in staff replacement, time pressure, insufficient resources, and poor personal financial situation. However, the greater proportion of the total economic burden of MDD lies in reduced productivity, or presenteeism, in which the depressed individual remains in the work setting but with productivity suffering both in quality and quantity [#CITATION_TAG, 8].",Methods: The study group comprised a random sample of 3136 persons who responded to a questionnaire administered in conjunction with Statistics Sweden's labor market survey. Logistic regressions were used in the analyses.,"[""Objective: Sickness presence, that is, going to work despite judging one's current state of health as such that sick leave should be taken, was investigated in relation to different work and background factors.""]"
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. Annually, 12% of Canadians from 15 to 64 years suffer from a mental disorder or substance dependence. Few studies have examined the prevalence of mental disorders among Canadian workers. About one-third of society's depression-related productivity losses can be attributed to work disruptions. The impact of mental illness on the workplace has been examined in terms of its effect on presenteeism, absenteeism and disability days. The presence of any of these has been used to indicate decreased productivity, the largest burden arising from presenteeism. In total, Canada annually loses about $4.5 billion from this decreased productivity. Mental illness is also associated with short-term and long-term disability, which in turn is often related to insurance coverage. Mental illness related disability claims have doubled and mental illness accounts for 30% of disability claims, at a cost of $15 to $33 billion annually. The needs of the working population and employers must be addressed. We must be aware of patterns of mental disorder among occupational groups and industry sectors. Effective policies and programs must be based on solid evidence. For example, one study reported that workers with MDD missed an average of 32 days of work in a 12-month assessment period [5], while another found that about 30% of work disability claims in Canada were attributed to mental illness, predominant depression, and other mood disorders [#CITATION_TAG].","Major trends in the literature are also commented on, and significant gaps in knowledge are identified.",['This discussion paper explores the state of knowledge about the prevalence of mental illness and its effect on the working population.']
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. BACKGROUND: Global and regional projections of mortality and burden of disease by cause for the years 2000, 2010, and 2030 were published by Murray and Lopez in 1996 as part of the Global Burden of Disease project. In all three scenarios there is a dramatic shift in the distribution of deaths from younger to older ages and from communicable, maternal, perinatal, and nutritional causes to noncommunicable disease causes. The risk of death for children younger than 5 y is projected to fall by nearly 50% in the baseline scenario between 2002 and 2030. The proportion of deaths due to noncommunicable disease is projected to rise from 59% in 2002 to 69% in 2030. Global HIV/AIDS deaths are projected to rise from 2.8 million in 2002 to 6.5 million in 2030 under the baseline scenario, which assumes coverage with antiretroviral drugs reaches 80% by 2012. Total tobacco-attributable deaths are projected to rise from 5.4 million in 2005 to 6.4 million in 2015 and 8.3 million in 2030 under our baseline scenario. Tobacco is projected to kill 50% more people in 2015 than HIV/AIDS, and to be responsible for 10% of all deaths globally. The three leading causes of burden of disease in 2030 are projected to include HIV/AIDS, unipolar depressive disorders, and ischaemic heart disease in the baseline and pessimistic scenarios. Depression is currently the fourth leading medical condition contributing to global burden of disease and is estimated to rise to second by the year 2030 [#CITATION_TAG].","This paper describes the methods, assumptions, input data, and results. METHODS AND FINDINGS: Relatively simple models were used to project future health trends under three scenarios--baseline, optimistic, and pessimistic--based largely on projections of economic and social development, and using the historically observed relationships of these with cause-specific mortality rates.","['To address the widespread demand for information on likely future trends in global health, and thereby to support international health policy and priority setting, we have prepared new projections of mortality and burden of disease to 2030 starting from World Health Organization estimates of mortality and burden of disease for 2002.']"
"Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. CONTEXT Uncertainties exist about prevalence and correlates of major depressive disorder (MDD). DESIGN Face-to-face household survey conducted from February 2001 to December 2002. CONCLUSIONS Major depressive disorder is a common disorder, widely distributed in the population, and usually associated with substantial symptom severity and role impairment. While the recent increase in treatment is encouraging, inadequate treatment is a serious concern. Similar statistics are found for Europe [2] and the United States [#CITATION_TAG].",PARTICIPANTS Household residents ages 18 years or older (N = 9090) who responded to the NCS-R survey. Clinical reinterviews used the Structured Clinical Interview for DSM-IV. Emphasis on screening and expansion of treatment needs to be accompanied by a parallel emphasis on treatment quality improvement.,"['OBJECTIVE To present nationally representative data on prevalence and correlates of MDD by Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV) criteria, and on study patterns and correlates of treatment and treatment adequacy from the recently completed National Comorbidity Survey Replication (NCS-R).']"
"Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. CTCs are extremely rare in healthy subjects and patients with nonmalignant diseases but present in various metastatic carcinomas with a wide range of frequencies. It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancers and not in healthy subjects or those with benign disease [#CITATION_TAG].","Experimental Design: The CellSearch system was used to enumerate CTCs in 7.5 mL of blood. Blood samples spiked with cells from tumor cell lines were used to establish analytical accuracy, reproducibility, and linearity. Prevalence of CTCs was determined in blood from 199 patients with nonmalignant diseases, 964 patients with metastatic carcinomas, and 145 healthy donors. Detection of >=2 CTCs occurred at the following rates: 57% (107 of 188) of prostate cancers, 37% (489 of 1,316) of breast cancers, 37% (20 of 53) of ovarian cancers, 30% (99 of 333) of colorectal cancers, 20% (34 of 168) of lung cancers, and 26% (32 of 125) of other cancers.","['Purpose: The purpose of this study was to determine the accuracy, precision, and linearity of the CellSearch system and evaluate the number of circulating tumor cells (CTCs) per 7.5 mL of blood in healthy subjects, patients with nonmalignant diseases, and patients with a variety of metastatic carcinomas.']"
"Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. CONTEXT Molecular profiling of prostate cancer has led to the identification of candidate biomarkers and regulatory genes. A CPC was defined according to the criteria of (international society of hematotherapy and genetic engineering) ISHAGE [#CITATION_TAG] and the expression of P504S according to the consensus of the american association of pathologists [12].","DESIGN Four gene expression data sets from independent DNA microarray analyses were examined to identify genes expressed in prostate cancer (n = 128 specimens). AMACR levels were examined using prostate cancer tissue microarrays in 342 samples representing different stages of prostate cancer progression. Protein expression was characterized as negative (score = 1), weak (2), moderate (3), or strong (4). Tissue microarrays to assess AMACR expression in specimens consisting of benign prostate (n = 108 samples), atrophic prostate (n = 26), prostatic intraepithelial neoplasia (n = 75), localized prostate cancer (n = 116), and metastatic prostate cancer (n = 17) demonstrated mean AMACR protein staining intensity of 1.31 (95% confidence interval, 1.23-1.40), 2.33 (95% CI, 2.13-2.52), 2.67 (95% CI, 2.52-2.81), 3.20 (95% CI, 3.10-3.28), and 2.50 (95% CI, 2.20-2.80), respectively (P<.001).","['OBJECTIVES To determine the expression and clinical utility of alpha-methylacyl coenzyme A racemase (AMACR), a gene identified as being overexpressed in prostate cancer by global profiling strategies.']"
"Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. Automated imaging of prostate-specific cancer cells from the blood provides a measure of circulating tumor cell half-life after tumor resection. Circling Cancers Out Oftentimes a patient and his or her clinician learn together at the flip of a radiological imaging scan that a solid tumor, previously removed, has returned either at the original site or at new locations to which it has spread. A failure of cancer treatment--but could it have been predicted? Whether it is freely floating tumor DNA or tumor cells, the circulation of cancer-derived material in the blood holds great promise for the early detection and prevention of cancer metastases. The accurate identification, enumeration, and molecular classification of blood-borne cells--although postulated more than 140 years ago--remain the greatest challenge. Now, in a small cohort of individuals with and without prostate cancer, Stott and colleagues have used a silicon microfluidic cell-capture technology that, when coupled to an automated imaging system, enables the detection and enumeration of prostate cancer cells fished out from the blood. These cells express a surface protein that uniquely identifies epithelial cells in the circulation. Rare circulating tumor cells (CTCs) are present in the blood of patients with metastatic epithelial cancers but have been difficult to measure routinely. We report a quantitative automated imaging system for analysis of prostate CTCs, taking advantage of prostate-specific antigen (PSA), a unique prostate tumor-associated marker. In previously untreated patients followed longitudinally, the numbers of CTCs declined after the initiation of effective therapy. This has implications on systems based on EpCAM, Cytokeratin, or PSA alone and may explain why no significant differences were found on the frequency of CPCs detected in early prostate cancer and controls [13] [14] [#CITATION_TAG].","Once efficiently captured by antibody to this protein, the cells are counterstained with antibodies that are prostate-specific and that indicate cell proliferation, suggesting that these circulating cells are ready to repopulate distant metastatic sites. The specificity of PSA staining enabled optimization of criteria for baseline image intensity, morphometric measurements, and integration of multiple signals in a three-dimensional microfluidic device. In a pilot analysis, we detected CTCs in prostate cancer patients with localized disease, before surgical tumor removal in 8 of 19 (42%) patients (range, 38 to 222 CTCs per milliliter).","['In the hope of ultimately applying personalized treatments to cancer patients, based on ""real-time"" monitoring of the tumor cell genetic makeup, this approach to identifying these circulating cells early on moves us beyond the finality of the films currently offered to patients in the clinic.']"
"Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. Colorectal cancer (CRC) is the third most common cancer type, and third highest in mortality rates among cancer-related deaths in the United States. Originating from intestinal epithelial cells in the colon and rectum, that are impacted by numerous factors including genetics, environment and chronic, lingering inflammation, CRC can be a problematic malignancy to treat when detected at advanced stages. Chemotherapeutic agents serve as the historical first line of defense in the treatment of metastatic CRC. In recent years, however, combinational treatment with targeted therapies, such as vascular endothelial growth factor, or epidermal growth factor receptor inhibitors, has proven to be quite effective in patients with specific CRC subtypes. Current research into the efficacy of immunotherapy, particularly immune checkpoint inhibitor therapy (ICI) in mismatch repair deficient and microsatellite instability high (dMMR-MSI-H) CRC tumors have shown promising results, but its use in other CRC subtypes has been either unsuccessful, or not extensively explored. A CPC was defined according to the criteria of (international society of hematotherapy and genetic engineering) ISHAGE [11] and the expression of P504S according to the consensus of the american association of pathologists [#CITATION_TAG].",,"['This Review will focus on the current status of immunotherapies, including ICI, vaccination and adoptive T cell therapy (ATC) in the treatment of CRC and its potential use, not only in dMMR-MSI-H CRC, but also in mismatch repair proficient and microsatellite instability low (pMMR-MSI-L)']"
"Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. Prostate cancer is the most commonly diagnosed cancer in men and the second leading cause of cancer deaths. The serum prostate specific antigen (PSA) is the only biomarker routinely used in screening. The thirty women acting as controls were all CPC negative; in the 409 men, the frequency of malignant CPC detection increased significantly with age and PSA level and is associated with a biopsy positive for cancer [#CITATION_TAG].",For this purpose mononuclear cells were separated from blood using differential centrifugation and then prostate cells were identified by using standard immunocytochemical method.,"['The aim of this study was to develop a system to test the presence of circulating prostate cells in men without a diagnosis of prostate cancer in relation with age, serum PSA levels and prostate biopsy by determining the co-expression of several markers such as CD82, HER-2 and matrix metalloproteinase 2 (MMP-2).']"
"Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. alpha-Methylacyl-CoA racemase (AMACR) is a mitochondrial and peroxisomal enzyme involved in the metabolism of branched-chain fatty acid and bile acid intermediates. Recently, AMACR has been demonstrated to be over-expressed in localized and metastatic prostate cancer, suggesting that it may be an important tumor marker. AMACR protein over-expression was found in a number of cancers, including colorectal, prostate, ovarian, breast, bladder, lung, and renal cell carcinomas, lymphoma, and melanoma. Reverse transcriptase-polymerase chain reaction for AMACR using laser capture microdissected prostate tissue confirmed gene over-expression at the mRNA level. The use of the biomarker P504S, although not prostate specific [#CITATION_TAG], has facilitated the differentiation between normal, dysplastic, and malignant tissues in prostate biopsy samples.","A survey of online Expressed Sequence Tags (ESTs) and Serial Analysis of Gene Expression (SAGE) databases revealed that AMACR was over-expressed in multiple cancers. Based on prior work, AMACR protein expression was divided into two categories: negative (negative to weak staining intensity) and positive (moderate to strong staining intensity).",['This study examines AMACR expression in a variety of human cancers and their precursor lesions.']
"5G technology is using millimeter-wave band to improve the wireless communication system. However, narrow transmitter and receiver beams have caused the beam coverage area to be limited. Due to propagation limitations of mm wave band, beam forming technology with multi-beam based communication system, has been focused to overcome the problem. This paper demonstrates several shapes of microstrip array antennas, such as rectangular and triangular patch antennas array. Modern wireless communication systems require a low profile, lightweight, high gain and simple structure antennas to ensure reliability, mobility, and high efficiency [#CITATION_TAG].","Specifically, 4x1, 2x1, and single element of both shapes are designed and simulated by a full wave simulator (IE3d). Moreover, this paper presents a comparison between both rectangular and triangular antenna arrays.","['In this paper, several designs of micorstip arrays antennas, suitable for wireless communication applications, are presented.']"
"5G technology is using millimeter-wave band to improve the wireless communication system. However, narrow transmitter and receiver beams have caused the beam coverage area to be limited. Due to propagation limitations of mm wave band, beam forming technology with multi-beam based communication system, has been focused to overcome the problem. In the recent years the development in communication systems requires the development of low cost, minimal weight and low profile antennas that are capable of maintaining high performance over a wide spectrum of frequencies. This technological trend has focused much effort into the design of a microstrip patch antenna. The length of the antenna is nearly half wavelength in the dielectric; it's a very critical parameter, which governs the resonant frequency of the antenna. Circular polarizations, dual characteristics, dual frequency operation, frequency agility, broad bandwidth, feed line flexibility and beam scanning can be easily obtained from these patch antennas [#CITATION_TAG].","Therefore, a novel particle swarm optimization method based on IE3D was used to design an inset feed linearly polarized rectangular microstrip patch antenna with four element array. In view of design, selection of the patch width and length are the major parameters along with the feed line depth. Desired patch antenna design was simulated by IE3D simulator program. Initially we set our antenna as a single patch and after evaluating the outcomes of antenna features, operation frequency, radiation patterns, reflected loss, efficiency and antenna gain, and then we transformed it to a 2x1 linear array. Then we analyzed the 4x1 linear antenna array to increase directivity, gain, efficiency, and have better radiation patterns.","['The objective of this paper is to design, and fabricate an inset fed rectangular microstrip patch antenna.']"
"5G technology is using millimeter-wave band to improve the wireless communication system. However, narrow transmitter and receiver beams have caused the beam coverage area to be limited. Due to propagation limitations of mm wave band, beam forming technology with multi-beam based communication system, has been focused to overcome the problem. The difference in the measured and simulated results is mainly caused by the shift in the resonant frequencies [#CITATION_TAG].","The design is based on merging two slots into a diversity Y-shaped slot antenna using an inverted configuration. In order to achieve good isolation between the two slots, a slot stub is added at the merging corner part forming a Y-shaped slot antenna. The effects of different angles of the Y-junction in the diversity antenna on the mutual coupling are analyzed through HFSS simulations.",['An integrated diversity antenna based on dual-feed cavity-backed slot is proposed and studied.']
"5G technology is using millimeter-wave band to improve the wireless communication system. However, narrow transmitter and receiver beams have caused the beam coverage area to be limited. Due to propagation limitations of mm wave band, beam forming technology with multi-beam based communication system, has been focused to overcome the problem. But a problem inherent to phased arrays antennas is strong mutual coupling between element, producing degradation of the overall performance of the antenna array, so, in order to reduce this disadvantage, the designed antenna is altered by a defected ground structure. However, Antenna 4 has very low bandwidth with a simulated bandwidth of 0.51 dB and measured bandwidth of 0.60 dB. The low bandwidth is due to the strong mutual coupling between the elements that degrade the overall performance of the antenna array [#CITATION_TAG].","When greater directivity is required that can't be obtained by a single antenna, antenna arrays are used, this way, a Microstrip Patch Antenna (MPA) provides features such as low cost, light weight, thin profile, with appropriate size dimensions for be assembled with other components into a small case, moreover, it should satisfy gain, resonance frequency, impedance, as well as bandwidth requirements.","['This way, the focus of this work is a microstrip patch antenna array design for wireless lan application (2.45 GHz), using the HFSS simulation software and carrying experimental tests out in the electromagnetic compatibility laboratory.']"
,,
"Currently, cryptography is in wide use as it is being exploited in various domains from data confidentiality to data integrity and message authentication. Basically, cryptography shuffles data so that they become unreadable by unauthorized parties. However, clearly visible encrypted messages, no matter how unbreakable, will arouse suspicions. Fundamentally, steganography conceals secret data into innocent-looking mediums called carriers which can then travel from the sender to the receiver safe and unnoticed. This paper proposes a novel steganography scheme for hiding digital data into uncompressed image files using a randomized algorithm and a context-free grammar. Notations are presented for numbers, numerical variables, Boolean variables , relations, n-dimensional arrays, functions, operators and algebraic expressions. L'autcur caracterise brievement la syntaxe et les regles d'inter-pretation du langage algebrique international propose a la Conference de Zurich (ACM-GAMM) puis en donne un expose formel et complet. II indique les notations utilisees pour designer les nombres, les variables numeriques ou booIeennes, les relations, les agencements pluri-dimensionnels, les fonctions, les operateurs et les expressions algebriques. Ce langage permet d'exprimer difierentes operations: affectation de valeurs aux variables, execution conditionnelle des expressions, procedes iteratifs, formation d'expressions complexes a partir d'une suite d'expressions elementaires, definition de nouvelles expressions pour des operations arbitraires, reemploi et modification de certaines parties du programme. Le lang age envisage est conyu pour permettre d'exprimer la quasi totalite des procedes de calcul numerique de maniere com-mode et concise a l'aide d'un nombre relativement restreint de regles de syntaxe et d'expressions-types. Its formalism was originally set by Chomsky [14] and Backus [#CITATION_TAG], independently of each other.","Means are provided in the language for the assignment of values to variables, conditional execution of statements , iterative procedures, formation of compound statements from sequences of statements, definition of new statements for arbitrary procedures, and the re-use and alteration of program segments.","['This paper gives a summary of the syntax and interpretation rules of the proposed international algebraic language put forward by the Zurich ACM-GAMM Conference, followed by a formal , complete presentation of the same information.', 'The proposed language is intended to provide convenient and concise means for expressing virtually all procedures of numerical computation while employing relatively few syntactical rules and types of statement.']"
"Currently, cryptography is in wide use as it is being exploited in various domains from data confidentiality to data integrity and message authentication. Basically, cryptography shuffles data so that they become unreadable by unauthorized parties. However, clearly visible encrypted messages, no matter how unbreakable, will arouse suspicions. Fundamentally, steganography conceals secret data into innocent-looking mediums called carriers which can then travel from the sender to the receiver safe and unnoticed. This paper proposes a novel steganography scheme for hiding digital data into uncompressed image files using a randomized algorithm and a context-free grammar. The grammar of a language is a device that describes the structure of that language. The grammar is comprised of a set of rules whose goal is twofold: first these rules can be used to create sentences of the associated language and only these, and second they can be used to classify whether a given sentence is an element of the language or not. In [1] Chomsky describes three possible options of increasing complexity for English grammars: Finite-state, Phrase Structure and Transformational. Its formalism was originally set by Chomsky [#CITATION_TAG] and Backus [15], independently of each other.",,"['The goal of a linguist is to discover grammars that are simple and yet are able to fully span the language.', ""This paper briefly present these three grammars and summarizes Chomsky's analysis and results which state that finite-state grammars are inadequate because they fail to span all possible sentences of the English language, and phrase structure grammar is overly complex.""]"
"Currently, cryptography is in wide use as it is being exploited in various domains from data confidentiality to data integrity and message authentication. Basically, cryptography shuffles data so that they become unreadable by unauthorized parties. However, clearly visible encrypted messages, no matter how unbreakable, will arouse suspicions. Fundamentally, steganography conceals secret data into innocent-looking mediums called carriers which can then travel from the sender to the receiver safe and unnoticed. This paper proposes a novel steganography scheme for hiding digital data into uncompressed image files using a randomized algorithm and a context-free grammar. Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of `model adaptation'. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the `transfer' can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). A context-free grammar or CFG is a mathematical system for modeling the structure of languages such as natural languages like English, French and Arabic, or computer programming languages like C++, Java, and C# [#CITATION_TAG].",,"['This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing.']"
,,
"A GROWING BODY OF RESEARCH SUGGESTS THAT jazz musicians concatenate stored auditory and motor patterns during improvisation. And what does it mean to be creative? Is a creative individual a master-of-all trades or a master of one? In other words, is creativity a generalized attribute or is it a domain-specific attribute? In Creativity: The Psychology of Creative Potential and Realization, authors ponder these questions and discuss the attributes that lead people to be creative in various fields such as the arts and letters, the sciences, and business. Researchers and students alike will find these discussions delightfully intriguing. The study of creativity is burgeoning and multidisciplinary, in that it involves approaches of social, personality, cognitive, clinical, biological, differential, developmental, and educational psychology. REATIVITY RESEARCH IS TYPICALLY FOCUSED on creative processes and products conceived without time constraints (#CITATION_TAG).",,['The emphasis of this volume is on the theoretical issue of whether the attributes that lead to creativity in one domain are the same as those that lead to creativity in another domain.']
"It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. Short stories are considered as good resources that can be used in language classrooms. The present study was conducted in a small class of junior secondary school students in order to investigate if they became more interested and more confident in English with the use of short stories. #CITATION_TAG and Megawati and Anugerahwati's (2012) studies with secondary school students in Hong Kong and Indonesia, respectively, investigated the use of stories to enhance students' motivation and their writing ability.","And, the narrative (or storytelling) approach is believed to help students understand the story easily.","['Laine (1997) suggests that in foreign language classes where there are children who are not motivated and who are low achievers, a story, if it is well-chosen, can help change their attitudes to the language.']"
"It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. As far as we can recall, we first drew on Labov and Waletzky (1967/this issue; hence forth L&W) in the late 1970s when we were beginning to develop genre theory within the general framework of systemic functional linguistics (StrL). Our main sources for thinking about generic structure at the time were Mitchell (195711975), the classic Firthian study of buying and selling in a Moroccan marketplace, and Hasan (1977), a seminal SFL paper that focussed on appointment making. Over the years, their work had a continuing influence on the development of this model, especially with respect to two strands of the research. Plurn (1988, in prcss) in particular was inrigued by the possibility of developing a sociolinguistic interview that was specifically designed to ""elicit"" genres, including nanatives. In both strands, the role of evaluation, flagged by L&W as construing the point of narrative, became more and more crucial. A good deal of useful work for stories has been done with genre pedagogy in particular #CITATION_TAG description of narrative genre.",,"[""Our aim was to develop a social model of genre that generalized across these and other text types (Christie & Martin, 1997; Eggins & Martin, 1997; Martin, 1985/ 1989, 1992; Ventola, 1987), and we appreciated having L&W's work to draw on."", 'One strand was community based and oriented to mapping the repertoire of genres through which people enact their lives.', 'The other strand was school based (Rothery, 1990) and concerned with mapping the repertoire of genres used by students to succeed in school and to redrstribute control of these to students who were not accessing them (Hasan & Williams, 1985/1996; Martin, 1993).', 'Here we were concerned with deconstructing the kinds of narrative students were expected to write and critique (Rothery & Macken, l99l).']"
"Dissolved organic carbon (DOC) may be removed, transformed, or added during water transit through lakes, resulting in changes in DOC composition and pigmentation (color). However, the process-based understanding of these changes is incomplete, especially for headwater lakes. However, in highly pigmented brown-water lakes (absorbance at 420 nm > 7 m -1 ) biological processes dominated, and there was no systematic relationship between color loss and WTT. Increased color in surface waters, or browning, can alter lake ecological function, lake thermal stratification and pose difficulties for drinking water treatment. While browning of surface waters is widespread and well documented, little is known about why some lakes resist it. Despite 40 years of increased terrestrial inputs of colored substances to western lake basins, the eastern basin has resisted browning over this time period. Some studies have reported relative losses of colored DOC across lake basins with increasing theoretical residence times (#CITATION_TAG; Curtis and Schindler, 1997), while other studies have found preferential loss of noncolored DOC in laboratory biodegradation experiments (Hansen et al., 2016) and in time series analyses of brown headwater lakes (Berggren et al., 2009).","Mechanisms suggested to cause browning include increased dissolved organic carbon (DOC) and iron concentrations, as well as a shift to more colored DOC.","['Here, we present a comprehensive study of Malaren, the third largest lake in Sweden.']"
"As other authors have noted, as a concept, resilience involves some potentially serious conflicts or contradictions, for example between stability and dynamism, or between dynamic equilibrium (homeostasis) and evolution. This may be the case for disaster risk reduction, which involves transformation rather than preservation of the ""state of the system"". ""An arrow never lodges in a stone: often it recoils upon its sender."" 347-407), Archbishop of Constantinople The historical etymology of the term resilience The word resilience, together with its various derivatives, has a long and diverse history. This paper examines the development over historical time of the meaning and uses of the term resilience. The objective is to deepen our understanding of how the term came to be adopted in disaster risk reduction and resolve some of the conflicts and controversies that have arisen when it has been used. The paper traces the development of resilience through the sciences, humanities, and legal and political spheres. In order to gain a deeper and more Published by Copernicus Publications on behalf of the European Geosciences Union. The debate on disaster resilience has continued to grow, albeit at a slow pace, since the 2005 World Conference on Disaster Reduction held in Kobe, Hyogo, Japan. One of the most important and striking aspects is that despite the conceptual differences, the resilience and vulnerability paradigms are still locked together and are increasingly being treated as if they are one and the same. The reason for this is not a difficult one. However, the notion of ""bounce back"" differentiates resilience from vulnerability. The ""bounce back"" notion is important to the extent that it liberates resilience from the vulnerability conundrum. Yet, the ""bounce back"" notion does not seem to acknowledge that disasters are accompanied by change. Three arguments are presented in this paper. It stems from resilire, resilio, Latin for ""bounce"" -hence the idea of ""bouncing back"" (#CITATION_TAG).","Resilience and vulnerability are viewed as opposite sides of the same coin (Twigg 2007). First, the ""bounce forward"" ability conceptualisation of resilience has implications on disaster research and scholarship.","['This paper posits that resilience should be viewed as the ability to ""bounce forward"" and ""move on"" following a disaster (Manyena 2009).', 'It helps us to re-think about the underlying philosophical arguments, particularly those around structure and agency.']"
"The growing movement to establish professional counseling as a distinct profession, based on an increasingly narrow definition of professional identity, is particularly relevant to counseling psychologists and professional counselors and has implications for the broader field of psychology. These restrictions reduce services to the public and threaten the viability of counseling psychology and professional counseling in the U.S. These challenges also have significant implications for counseling psychologists in Europe and internationally given similar efforts. It is with great respect and appreciation for Hansen's work as well as our shared commitments to humanistic counseling and counseling as a profession that I offer the following response. HUMANISM: THE FOUNDATION OF PROFESSIONAL COUNSELING AND RELATED FIELDS As Hansen points out, numerous people and movements have been credited with the founding of counseling as a profession. Although Frank Parsons and early advocates for educational and career development are often credited with influencing the foundation of the diverse field of counseling (Brown & Trusty, 2005), historically, theoretically, and practically, Sigmund Freud and colleagues' psychoanalytic/psychodynamic theory is more often considered the first force in counseling and psychology. B. F. Skinner and colleagues' behaviorism, having developed later, which is similarly referred to as the second force (Ivey, D'Andrea, & Ivey, 2011), also influenced counseling and related fields. Yet, as Hansen (2012) notes, professional counseling is founded on humanism, the third force in counseling and psychology. Like its antecedents, humanistic counseling developed in a historical and sociopolitical context that is continuing to evolve. Humanistically oriented counselors consider people's strengths and possibilities, recognizing that all people have the capacity to develop, to grow, to heal, and to maximize their own potential, that is, to ""actualize"" (Raskin & Rogers, 1995, p. 128). Overall, humanists are interested in normal and optimal functioning as well as people's phenomenological subjective experience (Kirschenbaum, 2007). Humanistic counseling, then, is a holistic approach that emphasizes healthy human development, human strengths and wellness, and understanding people in their environmental contexts (Lundin, 1996). Using a variety of creative approaches centered on the counselor's attitude toward the client (Rogers, 1951), humanistically oriented counselors partner with clients, often attending to Rogers's (1957) ""necessary and sufficient conditions"" (p. ... We, therefore, suggest training programs intentionally incorporate professional history and social justice advocacy that extends beyond the curriculum (Brady-Amoon, #CITATION_TAG).","Second, I counter Hansen's position that the counseling profession has and ideally ought to be grounded solely in the humanities. Third and last, I provide a further extension of Hansen's vision for the future of counseling, a vision that encompasses renewed respect for human complexity, multiple perspectives, and the optimal development of human potential.","[""This article offers additional support for Hansen's (2012) position that humanism and a renewed respect for human complexity are essential to counseling."", ""In this article, I offer support for Hansen's overall position that humanism has provided a foundational framework for the present and future of professional counseling.""]"
"Since 2010, there has been repository growth in East Asia, South America, and Eastern Europe, especially in Taiwan, Brazil, and Poland. During the period, some countries, including France, Italy, and Spain, have maintained steady growth, whereas other countries, notably China and Russia, have experienced limited growth. Globally, repositories are predominantly institutional, multidisciplinary and English-language based. They typically use open-source OAI-compliant software but have immature licensing arrangements. Although the size of repositories is difficult to assess accurately, available data indicate that a small number of large repositories and a large number of small repositories make up the repository landscape. Major factors affecting both the initial development of repositories and their take-up include IT infrastructure, cultural factors, policy initiatives, awareness-raising activity, and usage mandates. Mandates are likely to be crucial in determining future repository development. This paper reviews the worldwide growth of openaccess (OA) repositories, 2005 to 2012, using data collected by the OpenDOAR project. Cultural dissimilarities across countries have played a significant role in open access development. IDT has also been used at a different level by #CITATION_TAG to explain different adoption patterns of both OA journals and repositories worldwide.",Both maps and tables are used to support the analysis. The diffusionist theory is reviewed and applied to the understanding of open access.Findings - The paper discovers that technology is not the only factor determining the diffusion pattern of information systems as discussed in the literature.,['Purpose - This article aims to explore the geographic distribution of open access practices in the world from a diffusionist perspective.Design/methodology/approach - The article applies a tempo-spatial analysis to examine the diffusion movement of open access practices from the West to the entire world during the past several decades.']
"In the service computing paradigm, a service broker can build new applications by composing network-accessible services offered by loosely coupled independent providers. In this paper, we address the admission control problem for a a service broker which offers to prospective users a composite service with a range of different Quality of Service (QoS) classes. However, a self-adaptive SOA system has to be carefully designed in order not to compromise the system scalability and availability. In this paper, to overcome the limitations of the aforementioned results we study the admission problem for the MOdel-based SElf-adaptation of SOA systems (MOSES) service broker we proposed in [#CITATION_TAG, 7], which manages a composite service offering differentiated QoS service classes.","Theintroductionofself-adaptationandself-management techniques in a service-oriented system can allow to meet in a changing environment the levels of service formally defined with the system users in a Service Level Agreement (SLA). To evaluate theperformance of the brokering service, we have carried out an extensive set of experiments on different implementations of the system architecture using workload generators that are based on open and closed system models.",['In this paper we present the design and performance evaluation of a brokering service that supports at runtime the self-adaptation of composite services offered to several concurrent users with different service levels.']
"The ecological interactions parasitic insects have with their hosts may contribute to their prodigious diversity, which is unrivaled among animals. Many insects assumed to be polyphagous generalists have been shown to consist of several differentiated races, each occupying a different host-niche. The sunflower maggot fly, Strauzia longipennis, has long been thought to consist of two or more races due to its substantial intraspecific morphological variation. Evidence that mitochondrial genomes and morphological traits have moved between lineages implies a model of speciation-with-gene-flow for S. longipennis races. The likelihood of speciation in the face of homogenizing gene flow (i.e. without complete geographical isolation) is one of the most debated topics in evolutionary biology. For example, weak genetic differentiation between taxa could be due to recent divergence, gene flow, or a combination of these factors. Nonetheless, a number of convincing examples of speciation with gene flow have recently emerged, owing in part to the development of new analytical methods designed to estimate gene flow specifically. A recent example of speciation with gene flow in salamanders ( Niemiller et al. Further, recent empirical and theoretical work demonstrating that populations can diverge even in the face of effective migration (#CITATION_TAG; Nosil and Feder 2012), as well as new information about the permeability of genomes to between-lineage gene flow (Turner et al. 1999; Michel et al. 2010; Gompert et al. 2010) both suggest that persistent hybridization between closely related lineages may occur without leading to lineage fusion or breakdown of reproductive barriers.","2008 ) further advances our understanding of this phenonemon, by showing that gene flow between cave and spring salamanders was ongoing during speciation, rather than having occurred after a long period of allopatric divergence.",['Demonstrating this phenonemon is hampered by the difficulty of isolating the effects of time since population divergence vs. gene flow on levels of molecular genetic differentiation.']
"Lipids comprise the bulk of the dry mass of the brain. In addition to providing structural integrity to membranes, insulation to cells and acting as a source of energy, lipids can be rapidly converted to mediators of inflammation or to signaling molecules that control molecular and cellular events in the brain. The advent of soft ionization procedures such as electrospray ionization (ESI) and atmospheric pressure chemical ionization (APCI) have made it possible for compositional studies of the diverse lipid structures that are present in brain. These include phospholipids, ceramides, sphingomyelin, cerebrosides, cholesterol and their oxidized derivatives. These proteins may be potential therapeutic targets since they transport lipids required for neuronal growth or convert lipids into molecules that control brain physiology. In this review, we examine the structure of the major lipid classes in the brain, describe methods used for their characterization, and evaluate their role in neurological diseases. The potential utility of characterizing lipid markers in the brain, with specific emphasis on disease mechanisms, will be discussed. Combining lipidomics and proteomics will enhance existing knowledge of disease pathology and increase the likelihood of discovering specific markers and biochemical mechanisms of brain diseases. Abstract: Phospholipase A2 (PLA2) is the name for the class of lipolytic enzymes that hydrolyze the acyl group from the sn-2 position of glycerophospholipids, generating free fatty acids and lysophospholipids. The products of the PLA2-catalyzed reaction can potentially act as second messengers themselves, or be further metabolized to eicosanoids, platelet-activating factor, and lysophosphatidic acid. The presence of PLA2 in the central nervous system, accompanied by the relatively large quantity of potential substrate, poses an interesting dilemma as to the role PLA2 has during both physiologic and pathologic states. Several different PLA2 enzymes exist in brain, some of which have been partially characterized. However, under pathological situations, increased PLA2 activity may result in the loss of essential membrane glycerophospholipids, resulting in altered membrane permeability, ion homeostasis, increased free fatty acid release, and the accumulation of lipid peroxides. These processes, along with loss of ATP, may be responsible for the loss of membrane phospholipid and subsequent neuronal injury found in ischemia, spinal cord injury, and other neurodegenerative diseases. The incorporation of PUFAs into phospholipid subclasses is highly choreographed such that most PUFAs are initially incorporated into 1,2-diacyl phospholipids subclasses before they are remodeled into the ether-linked phospholipids classes by coenzyme A (CoA)-dependent or CoA-independent transacylase activities [#CITATION_TAG, 84, 126, 165, 167].","They are classified into two subtypes, CA2+-dependent and Ca2+-independent, based on their catalytic dependence on Ca2+.",['This review outlines the current knowledge of the PLA2 found in the central nervous system and attempts to define the role of PLA2 during both physiologic and pathologic conditions.']
"After at least a decade of parallel tool development, parallelization of scientific applications remains a significant undertaking. Typically parallelization is a specialized activity supported only partially by the programming tool set, with the programmer involved with parallel issues in addition to sequential ones. The aim of parallel programming tools is to automate the latter without sacrificing performance and portability, allowing the programmer to focus on algorithm specification and development. We present EULERGROMOS, a parallelization of the GROMOS molecular dynamics program which is based on a spatial decomposition. Numerous MD parallelizations have been described in the literature, ranging from the easy to implement replicated algorithm [6, 20] to the more difficult to implement spatial decomposition [#CITATION_TAG, 30], which is generally more scalable.","Several algorithms have been used for parallel molecular dynamics, including the replicated algorithm and those based on spatial decompositions. The replicated algorithm stores the entire system's coordinates and forces at each processor, and therefore has a low overhead in maintaining the data distribution. Spatial decompositions distribute the data, providing better locality and scalability with respect to memory and computation. EULERGROMOS parallelizes all molecular dynamics phases, with most data structures using O(N/P) memory. We also compare EULERGROMOS with an earlier parallelization of GROMOS, UHGROMOS, which uses the replicated algorithm.>",['The paper focuses on the structure of EULERGROMOS and analyses its performance using molecular systems of current interest in the molecular dynamics community.']
"After at least a decade of parallel tool development, parallelization of scientific applications remains a significant undertaking. Typically parallelization is a specialized activity supported only partially by the programming tool set, with the programmer involved with parallel issues in addition to sequential ones. The aim of parallel programming tools is to automate the latter without sacrificing performance and portability, allowing the programmer to focus on algorithm specification and development. Computational molecular dynamics is an important application requiring large amounts of computing time. Parallel processing offers very high performance potential, but irregular problems like molecular dynamics have proven difficult to map onto parallel machines. In this paper, we describe the practicalities of porting a basic molecular dynamics computation to a distributedmemory machine. This set of algorithms represents an important class of unstructured problems in scientific .. The ease of implementation of an MD algorithm is important given the need for multiple algorithms to address the variability encountered in mapping molecular dynamics algorithms onto parallel architectures [#CITATION_TAG, 9].","We also argue that algorithm replacement may be necessary in parallelization, a task which cannot be performed automatically.",['1 Introduction  The purpose of this paper is to examine the practicalities of parallelizing the basic algorithms of molecular dynamics for distributed-memory multiprocessors using annotations to sequential Fortran programs.']
"In PD, comparisons among diabetic and nondiabetic and anuric patients and patients with residual renal function are frequent, but comparisons between patients undergoing PD as first option versus PD as a second option after haemodialysis are scarce [7] [8] [9] [#CITATION_TAG].","Methods: In a 1:2 matched case-control study, we compared patient and technique survival between patients initially treated with HD for at least 3 months and then transferred to PD (transfer group) and patients started on and continuing with PD (no-transfer group). The main reasons for transfer to PD were vascular access problems and cardiovascular disease.",['Objective: Our study aimed to evaluate clinical outcomes of patients transferred to peritoneal dialysis (PD) because of complications related to hemodialysis (HD).']
"In PD, comparisons among diabetic and nondiabetic and anuric patients and patients with residual renal function are frequent, but comparisons between patients undergoing PD as first option versus PD as a second option after haemodialysis are scarce [#CITATION_TAG] [8] [9] [10].",,"['This study analyzes data on patients transferred from their initial modality at the University Hospital Gent between 1978 and 1996 for the reasons for, and outcomes from, the transfer.', 'This is due to the differences in the reasons for transfer, which are PD-related complications in the case of PD patients, and cardiovascular problems in the case of HD patients.']"
"The finding that neurotoxicity influences the patients' daily life is a similar result to earlier studies [14, 15, [25] [#CITATION_TAG] [27] [28] [29], but those studies had cross-sectional cohorts, other disease stages, and different study designs.","Participants were 91 colorectal cancer patients treated with OXA-based chemotherapy. Neurological assessment, clinical Total Neuropathy Score(c) (TNSc(c)) and nerve conduction studies were performed at baseline (T0), the end of chemotherapy (T1) and 2 years (T2) after discontinuation of chemotherapy.",['This prospective study sought to identify the potential reversibility of oxaliplatin-induced peripheral neuropathy (OXAIPN) by following-up its long-term course 2 years after discontinuation of oxaliplatin (OXA)-based chemotherapy.']
"Dose-limiting neurotoxicity is a major side effect of oxaliplatin treatment, producing initial acute neurotoxicity and chronic neuropathy with increasing exposure. The finding that neurotoxicity influences the patients' daily life is a similar result to earlier studies [14, 15, [#CITATION_TAG] [26] [27] [28] [29], but those studies had cross-sectional cohorts, other disease stages, and different study designs.",,['The improvement in survival for patients with early-stage colorectal cancer treated with oxaliplatin has highlighted the need for valid and reliable assessment of peripheral neuropathy.The objective of this paper was to explore neuropathic symptoms in oxaliplatin-treated patients as assessed using different methods.Consecutive symptomatic patients reporting peripheral neuropathy after oxaliplatin chemotherapy for colorectal cancer were interviewed using a semi-structured clinical interview.']
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Every user has a distinct background and a specific goal when searching for information on the Web. Effective personalization of information access involves two important challenges: accurately identifying the user context and organizing the information in such a way that matches the particular context. They usually generate user models that describe user interests according to a set of features [#CITATION_TAG].",A spreading activation algorithm is used to maintain the interest scores based on the user's ongoing behavior.,"[""The goal of Web search personalization is to tailor search results to a particular user based on that user's interests and preferences."", 'We present an approach to personalized search that involves building models of user context as ontological profiles by assigning implicitly derived interest scores to existing concepts in a domain ontology.']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. The amount of scholarly data available on the web is steadily increasing, enabling different types of analytics which can provide important insights into the research activity. Unfortunately, human crafted classifications do not satisfy these criteria, as they evolve too slowly and tend to be too coarse-grained. Current automated methods for generating ontologies of research areas also present a number of limitations, such as: i) they do not consider the rich amount of indirect statistical and semantic relationships, which can help to understand the relation between two topics - e.g., the fact that two research areas are associated with a similar set of venues or technologies; ii) they do not distinguish between different kinds of hierarchical relationships; and iii) they are not able to handle effectively ambiguous topics characterized by a noisy set of relationships. In order to do so, we characterized all SN publications according to their associated research topics by exploiting the Computer Science Ontology (CSO), a large-scale automatically generated taxonomy of research areas [#CITATION_TAG].","In particular, Klink-2 analyses networks of research entities (including papers, authors, venues, and technologies) to infer three kinds of semantic relationships between topics. It also identifies ambiguous keywords (e.g., ""ontology"") and separates them into the appropriate distinct topics - e.g., ""ontology/philosophy"" vs. ""ontology/semantic web"".","['In order to make sense of and explore this large-scale body of knowledge we need an accurate, comprehensive and up-to-date ontology of research topics.', 'In this paper we present Klink-2, a novel approach which improves on our earlier work on automatic generation of semantic topic networks and addresses the aforementioned limitations by taking advantage of a variety of knowledge sources available on the web.']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Recommender systems are used to provide filtered information from a large amount of elements. They provide personalized recommendations on products or services to users. Colombo-Mendoza et al [#CITATION_TAG] propose RecomMetz, a context-aware mobile recommender system based on Semantic Web technologies.","Recommender systems can be developed using different techniques and algorithms where the selection of these techniques depends on the area in which they will be applied. The system proposed is called RecomMetz, and it is a context-aware mobile recommender system based on Semantic Web technologies. In addition, location, crowd and time were considered as three different kinds of contextual information in RecomMetz. In a nutshell, RecomMetz has unique features: (1) the items to be recommended have a composite structure (movie theater + movie + showtime), (2) the integration of the time and crowd factors into a context-aware model, (3) the implementation of an ontology-based context modeling approach and (4) the development of a multi-platform native mobile user interface intended to leverage the hardware capabilities (sensors) of mobile devices.","['The recommendations are intended to provide interesting elements to users.', 'This paper proposes a recommender system in the leisure domain, specifically in the movie showtimes domain.', 'In detail, a domain ontology primarily serving a semantic similarity metric adjusted to the concept of ""packages of single items"" was developed in this research.']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Due to their powerful knowledge representation formalism and associated inference mechanisms, ontology-based systems are emerging as a natural choice for the next generation of KMSs operating in organizational, interorganizational as well as community contexts. User models, often addressed as user profiles, have been included in KMSs mainly as simple ways of capturing the user preferences and/or competencies. Razmerita et al. [#CITATION_TAG] describe OntobUM, an ontology-based recommender that integrates three ontologies: i) the user ontology, which structures the characteristics of users and their relationships, ii) the domain ontology, which defines the domain concepts and their relationships, and iii) the log ontology, which defines the semantics of the user interactions with the system.","We extend this view by including other characteristics of the users relevant in the KM context and we explain the reason for doing this. The proposed user modeling system relies on a user ontology, using Semantic Web technologies, based on the IMS LIP specifications, and it is integrated in an ontology-based KMS called Ontologging. We are presenting a generic framework for implicit and explicit ontology-based user modeling.","['This paper is presenting a generic ontology-based user modeling architecture, (OntobUM), applied in the context of a Knowledge Management System (KMS).']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. (c) Finding Experts By Semantic Matching of User Profiles Rajesh Thiagarajan, Geetha Manjunath, Markus Stumptner HP Laboratories HPL-2008-172 semantics, similarity matching, ontologies, user profile Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Thiagarajan et al. [#CITATION_TAG] use a different strategy by representing user profiles as bags-ofwords and weighing each term according to the user interests derived from a domain ontology.","However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results. However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results. However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results.","['We propose new metrics for computing semantic similarity of user profiles using spreading activation networks derived from ontologies.', 'We propose new metrics for computing semantic similarity of user profiles using spreading activation networks derived from ontologies.', 'We propose new metrics for computing semantic similarity of user profiles using spreading activation networks derived from ontologies.']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. The main advantages of these solutions are i) the ability to exploit the domain knowledge for improving the user modelling process, ii) the ability to share and reuse system knowledge, and iii) the alleviation of the cold-start and data sparsity problems [16, #CITATION_TAG].","Our two experimental systems, Quickstep and Foxtrot, create user profiles from unobtrusively monitored behaviour and relevance feedback, representing the profiles in terms of a research paper topic ontology. A novel profile visualization approach is taken to acquire profile feedback. Research papers are classified using ontological classes and collaborative recommendation algorithms used to recommend papers seen by similar people on their current topics of interest. Two small-scale experiments, with 24 subjects over 3 months, and a large-scale experiment, with 260 subjects over an academic year, are conducted to evaluate different aspects of our approach. Ontological inference is shown to improve user profiling, external ontological knowledge used to successfully bootstrap a recommender system and profile visualization employed to improve profiling accuracy.","['We explore a novel ontological approach to user profiling within recommender systems, working on the problem of recommending on-line academic research papers.']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Recommender systems have the effect of guiding users in a personalized  way to interesting objects in a large space of possible options. Content-based  recommendation systems try to recommend items similar to those a given user has  liked in the past. Indeed, the basic process performed by a content-based recommender  consists in matching up the attributes of a user profile in which preferences  and interests are stored, with the attributes of a content object (item), in order to  recommend to the user new interesting items. Content-based recommender systems [#CITATION_TAG] rely on a pre-existing domain knowledge to suggest items more similar to the ones that the user seems to like.","The first part of the chapter presents the basic concepts and terminology of contentbased  recommender systems, a high level architecture, and their main advantages  and drawbacks. The second part of the chapter provides a review of the state of  the art of systems adopted in several application domains, by thoroughly describing  both classical and advanced techniques for representing items and user profiles. The most widely adopted techniques for learning user profiles are also presented. The last part of the chapter discusses trends and future research which might lead  towards the next generation of systems, by describing the role of User Generated  Content as a way for taking into account evolving vocabularies, and the challenge  of feeding users with serendipitous recommendations, that is to say surprisingly  interesting items that they might not have otherwise discovered","['This chapter provides an overview of  content-based recommender systems, with the aim of imposing a degree of order on  the diversity of the different aspects involved in their design and implementation.']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. The last 10 years have seen a massive increase in the amount of Open Access publications in journals and institutional repositories. The open availability of large volumes of state-of-the-art knowledge online has the potential to provide huge savings and benefits in many fields. However, in order to fully leverage this knowledge, it is necessary to develop systems that (a) make it easy for users to discover and access this knowledge at the level of individual resources, (b) explore and analyse this knowledge at the level of collections of resources and (c) provide infrastructure and access to raw data in order to lower the barriers to the research and development of systems and services on top of this knowledge. We are thus exploring the option of using datasets such as CrossRef 12, Dimensions 13, OpenCitations [11], and Core [#CITATION_TAG].","Consequently, we present the CORE (COnnecting REpositories) system, a large-scale Open Access aggregation, outlining its existing functionality and discussing the future technical development.","['In this paper, we argue why these requirements should be satisfied and show that current systems do not meet them.']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Originality/value - Scholars, publishers and institutions may freely build upon, enhance and reuse the open citation data for any purpose, without restriction under copyright or database law. We are thus exploring the option of using datasets such as CrossRef 12, Dimensions 13, OpenCitations [#CITATION_TAG], and Core [12].","Design/methodology/approach - The Open Citation Corpus is a new open repository of scholarly citation data, made available under a Creative Commons CC0 1.0 public domain dedication and encoded as Open Linked Data using the SPAR Ontologies.",['The paper aims to discuss this issue.']
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. We assess the similarity of two semantic vectors using the cosine similarity [#CITATION_TAG], since this measure relies on the orientation but not the magnitude of the topic weights in the vector space, allowing us to compare editorial products associated with a different number of chapters.",,"['This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared']"
"Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Increasingly, organizations are adopting ontologies to describe their large catalogues of items. These ontologies need to evolve regularly in response to changes in the domain and the emergence of new requirements. This operation needs to take into account a variety of factors and in particular reconcile user requirements and application performance. This API supports a number of applications, including Smart Book Recommender, Smart Topic Miner [5], the Technology-Topic Framework [6], a system that forecasts the propagation of technologies across research communities, and the Pragmatic Ontology Evolution Framework [#CITATION_TAG], an approach to ontology evolution that is able to select new concepts on the basis of their contribution to specific computational tasks.","An important step of this process is the selection of candidate concepts to include in the new version of the ontology. Current ontology evolution methods focus either on ranking concepts according to their relevance or on preserving compatibility with existing applications. In this paper, we propose the Pragmatic Ontology Evolution (POE) framework, a novel approach for selecting from a group of candidates a set of concepts able to produce a new version of a given ontology that (i) is consistent with the a set of user requirements (e.g., max number of concepts in the ontology), (ii) is parametrised with respect to a number of dimensions (e.g., topological considerations), and (iii) effectively supports relevant computational tasks. Our approach also supports users in navigating the space of possible solutions by showing how certain choices, such as limiting the number of concepts or privileging trendy concepts rather than historical ones, would reflect on the application performance.","['However, they do not take in consideration the impact of the ontology evolution process on the performance of computational tasks - e.g., in this work we focus on instance tagging, similarity computation, generation of recommendations, and data clustering.']"
This paper presents a participatory methodology to design cards on social issues with the purpose to democratise knowledge among co-designers on the learning content of educational games. The Everyday Sexism Project documents everyday examples of sexism reported by volunteer contributors from all around the world. The content of reports in various languages submitted to Everyday Sexism is a valuable source of crowdsourced information with great potential for feminist and gender studies. The lived experiences were extracted from a website called everydaysexism.com where people shared their experiences on everyday sexism (#CITATION_TAG).,"We used topic-modelling techniques to extract emerging topics and concepts from the reports, and to map the semantic relations between those topics.","['In this paper, we take a computational approach to analyze the content of reports.']"
"This paper presents a participatory methodology to design cards on social issues with the purpose to democratise knowledge among co-designers on the learning content of educational games. Although there is a consensus on the instructional potential of Serious Games (SGs), there is still a lack of methodologies and tools not only for design but also to support analysis and assessment. Most of the participatory models to design educational games are founded on educational theories and game design (see for example: Amory, 2007; #CITATION_TAG).","The LM-GM model includes a set of pre-defined game mechanics and pedagogical elements that we have abstracted from literature on game studies and learning theories. Designers and analysts can exploit these mechanics to draw the LM-GM map for a game, so as to identify and highlight its main pedagogical and entertainment features, and their interrelations.","['Filling this gap is one of the main aims of the Games and Learning Alliance (http://www.galanoe.eu) European Network of Excellence on Serious Games, which has a focus upon pedagogy-driven SGs.', 'This paper relies on the assumption that the fundamental aspect of SG design consists in the translation of learning goals/practices into mechanical element of gameplay, serving to an instructional purpose beside that of play and fun.', 'This paper proposes the Learning Mechanics-Game Mechanics (LM-GM) model, which supports SG analysis and design by allowing reflection on the various pedagogical and game elements in an SG.']"
"This paper presents a participatory methodology to design cards on social issues with the purpose to democratise knowledge among co-designers on the learning content of educational games. Since the term 'glass ceiling' was first coined in 1984, women have made great progress in terms of leadership equality with men in the workplace. Despite this, women are still under-represented in the upper echelons of organizations.In this volume, leading psychologists from the United States, Canada, and the European Union go beyond social commentary, anecdotal evidence and raw statistics to explain and offer remedies for this continued inequality, based on empirical evidence. Subtle barriers to women's advancement to and success in leadership positions are a major focus, such as women being recruited for upper-level positions that are associated with a high risk of failure or women managers being stereotyped as either competent or warm (but not both). Solutions that can be practically implemented are offered at different levels of analysis, including organizational (e.g., affirmative action), work group (e.g., diversity management), and individual (e.g., cross-cultural networking).Other obstacles associated with breaking through the glass ceiling include more nuanced forms of gender stereotyping, tokenism, and sexual harassment. Everyday sexism rests on the concept that sexism is either faced or reproduced on a daily basis but is rarely noticed (#CITATION_TAG; Becker and Swim, 2011).",,"[""As this volume explores women's current experiences in the workplace, a critical emphasis is making visible what women encounter as their career trajectory ascends and suggesting how they can enhance their career choices and thrive in the hard-won positions they attain.""]"
"Most Performance-based Research Funding Systems (PRFS) draw on peer review and bibliometric indicators, two different methodologies which are sometimes combined. A common argument against the use of indicators in such research evaluation exercises is their low correlation at the article level with peer review judgments. In recent years, the extent of formal research evaluation, at all levels from the individual to the multiversity has increased dramatically. At the institutional level, there are world university rankings based on an ad hoc combination of different indicators. These are extremely costly and time consuming. Mingers [#CITATION_TAG] recently completed an investigation that collected total citation counts from Google Scholar (GS) for the top 50 academics 1 from each UK institute and he found strong correlations with overall REF rankings.","There are also national exercises, such as those in the UK and Australia that evaluate research outputs and environment through peer review panels. Several citation-based metrics are collected from GS for all 130 UK universities. These are used to evaluate performance and produce university rankings which are then compared with various rankings based on the 2014 UK Research Excellence Framework (REF).",['This paper evaluates the possibility of using Google Scholar (GS) institutional level data to evaluate university research in a relatively automatic way.']
"Most Performance-based Research Funding Systems (PRFS) draw on peer review and bibliometric indicators, two different methodologies which are sometimes combined. A common argument against the use of indicators in such research evaluation exercises is their low correlation at the article level with peer review judgments. The MAG is a heterogeneous graph comprised of over 120 million publication entities and related authors, institutions, venues and fields of study. It is also the largest publicly available dataset of citation data. As such, it is an important resource for scholarly communications research. On the other hand, there are certain limitations to completeness. Only 30 million papers out of 127 million have some citation data. While these papers have a good mean total citation count that is consistent with expectations, there is some level of noise when extreme values are considered. Other current limitations of MAG are the availability of affiliation information, as only 22 million papers have these data, and the normalisation of institution names. Additionally, studies by [#CITATION_TAG] and [12] have recently confirmed how comprehensive the MAG citation data are.","As the dataset is assembled using automatic methods, it is important to understand its strengths and limitations, especially whether there is any noise or bias in the data, before applying it to a particular task.","['In this paper we analyse a new dataset of scholarly publications, the Microsoft Academic Graph (MAG).']"
"Most Performance-based Research Funding Systems (PRFS) draw on peer review and bibliometric indicators, two different methodologies which are sometimes combined. A common argument against the use of indicators in such research evaluation exercises is their low correlation at the article level with peer review judgments. Two versions of a well-designed randomized controlled trial that differed only in the direction of the finding of the principal study end point were submitted for peer review to 2 journals in 2008-2009. In an examination of one of the most critical forms of bias, that of publication bias, Emerson [#CITATION_TAG] noted that reviewers were much more likely to recommend papers demonstrating positive results over those that demonstrated null or negative results.",,"['If positive-outcome bias exists, it threatens the integrity of evidence-based medicine.We sought to determine whether positive-outcome bias is present during peer review by testing whether peer reviewers would (1) recommend publication of a ""positive"" version of a fabricated manuscript over an otherwise identical ""no-difference"" version, (2) identify more purposefully placed errors in the no-difference version, and (3) rate the ""Methods"" section in the positive version more highly than the identical ""Methods"" section in the no-difference version.']"
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. This corresponds to a lack of social pathways through which ideas can flow. But given enough connections, isolated groups may converge into a single mass, which encompasses the majority of nodes, and in which everyone is potentially connected to everyone else by some path (#CITATION_TAG).","We consider two scientists to be connected if they have authored a paper together, and construct explicit networks of such connections using data drawn from a number of databases, including MEDLINE (biomedical research), the Los Alamos e-Print Archive (physics), and NCSTRL (computer science). We show that these collaboration networks form ""small worlds"" in which randomly chosen pairs of scientists are typically separated by only a short path of intermediate acquaintances.",['We investigate the structure of scientific collaboration networks.']
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. This article examines the use of seasonal climate forecasting in public and private efforts to mitigate the impacts of drought in Ceara, Northeast Brazil. Here, forecasts have been directed towards small scale, rainfed agriculturalists as well as state and local level policymakers in the areas of agriculture, water management, and emergency drought relief. On the other hand, climate forecasting has the potential to offer a dramatic opportunity for state and local level bureaucracies to embark on a path of proactive drought planning. It is not clear from our analysis that such work is being done yet in the NACP (#CITATION_TAG; Lemos et al., 2002).","First, the current level of skill of the forecasts is inadequate for the needs of policy development and farmer decisionmaking. Second, forecast information application has been subject to distortion, misinterpretation and political manipulation. Third, focus on the forecast as a product until recently neglected to take into account end users' needs and decisionmaking behavior.","['In assessing possibilities and constraints of forecast application in Ceara, the present analysis takes into account three types of variables: (a)characteristics of the forecasts; (b) policymaking systems; and (c)institutional environments.']"
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Tropospheric ozone can be dangerous to human health, can be harmful to vegetation, and is a major contributor to climate warming. Black carbon also has significant negative effects on health and air quality and causes warming of the atmosphere. (p. 183) present results of an analysis of emissions, atmospheric processes, and impacts for each of these pollutants. Tropospheric ozone and black carbon (BC) contribute to both degraded air quality and global warming. These investigations may also include the specific role humans play in the carbon cycle, such as the impact of human-generated emissions or the consequences of climate change to agriculture and food systems (Berthelot et al., 2002; Bradbear and Friel, 2013; Dempewolf et al., 2014; #CITATION_TAG).","Seven measures were identified that, if rapidly implemented, would significantly reduce global warming over the next 50 years, with the potential to prevent millions of deaths worldwide from outdoor air pollution. We considered ~400 emission control measures to reduce these pollutants by using current technology and experience. We identified 14 measures targeting methane and BC emissions that reduce projected global mean warming ~0.5degC by 2050. The selected controls target different sources and influence climate on shorter time scales than those of carbon dioxide-reduction measures. Implementing both substantially reduces the risks of crossing the 2degC threshold.",['This strategy avoids 0.7 to 4.7 million annual premature deaths from outdoor air pollution and increases annual crop yields by 30 to 135 million metric tons due to ozone reductions in 2030 and beyond.']
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. The field of network analysis has attracted interest from scholars coming from a wide range of disciplines as it provides valuable theoretical and methodological toolkits to investigate complex systems of social relations. Network analysis is a research approach that prioritizes relationships between social units as opposed to focusing on attributes of the individual units themselves (#CITATION_TAG).","Furthermore, network theories and methods can examine dynamics present at multiple levels of analysis, from individual- to global-levels. For the context of social interactions, I expand structural balance evaluation to signed and directed networks, and apply this approach to examine 12 social networks. For the context of organizational communication, I demonstrate the application of multilevel modeling for egocentric networks to examine factors associated with the formation of interdisciplinary ties in a scientific organization. In addition, I leverage an extended version of structural balance evaluation for signed and directed networks to examine the sources of tension present in three organizational networks. Third, I provide a case study of response dynamics during the 2010 Haiti earthquake by examining collaboration networks prescribed by national guidelines for response, and interaction networks of the actual collaborations that took place during the earthquake response.","['This dissertation proposes several solutions to the advancement of network analysis theories and methods with specific applications in the domains of social, organizational, and crisis scenarios.', 'In this thesis, I present substantive insights into the application of several network analysis theories and applications to the (1) social, (2) organizational, and (3) crisis response settings.', 'Altogether, this work contributes to the growing literature on the theories and applications of network analysis to real-world social networks.']"
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Scientific collaboration has become a major issue in science policy. The tremendous growth of collaboration among nations and research institutions witnessed during the last twenty years is a function of the internal dynamics of science as well as science policy initiatives. The need to survey and follow up the collaboration issue calls for statistical indicators sensitive enough to reveal the structure and change of collaborative networks. In this context, bibliometric analysis of co-authored scientific articles is one promising approach. It is institutionally co-authored if it has more than one author address, suggesting that the authors come from various institutions, departments, or other kinds of units"" (#CITATION_TAG).",,"['This paper discusses the relationship between collaboration and co-authorship, the nature of bibliometric data, and exemplifies how they can be refined and used to analyse various aspects of collaboration.']"
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Such adaptation is a key component of securing the world's future food production. These investigations may also include the specific role humans play in the carbon cycle, such as the impact of human-generated emissions or the consequences of climate change to agriculture and food systems (Berthelot et al., 2002; Bradbear and Friel, 2013; #CITATION_TAG; Shindell et al., 2012).",,"['The main objective of the""Adapting Agriculture to Climate Change"" project is to collect and protect the genetic diversity of a portfolio of plants with the characteristics required for adapting the world\'s most important food crops to climate change.', 'The initiative also aims to make available this diversity in a form that plant breeders can readily use to produce varieties adapted to the new climatic conditions that farmers, particularly in the developing world, are already encountering.', 'This paper serves to inform interested researchers of this important initiative and encourage collaboration under its umbrella.']"
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Motivated by the rapid increase in atmospheric CO2 due to human activities since the Industrial Revolution, several international scientific research programs have analyzed the role of individual components of the Earth system in the global carbon cycle. Our knowledge of the carbon cycle within the oceans, terrestrial ecosystems, and the atmosphere is sufficiently extensive to permit us to conclude that although natural processes can potentially slow the rate of increase in atmospheric CO2, there is no natural ""savior"" waiting to assimilate all the anthropogenically produced CO2 in the coming century. Overcoming this limitation requires a systems approach. Scientists frequently study these biogeochemical cycles in the context of subsystems such as the terrestrial biosphere (land-based living systems), oceanic systems (both organic and inorganic forms of carbon), and the atmosphere (#CITATION_TAG).",,['Our knowledge is insufficient to describe the interactions between the components of the Earth system and the relationship between the carbon cycle and other biogeochemical and climatological processes.']
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Author's accepted version (post-print).Qualitative content analysis consists of conventional, directed and summative approaches for data analysis. Content analysis may be approached either inductively, in a purely exploratory context, or deductively when seeking to test known ideas or compare changes in content over time (#CITATION_TAG).","They are used for provision of descriptive knowledge and understandings of the phenomenon under study. However, the method underpinning directed qualitative content analysis is insufficiently delineated in international literature. Various international databases were used to retrieve articles related to directed qualitative content analysis. A review of literature led to the integration and elaboration of a stepwise method of data analysis for directed qualitative content analysis. The proposed 16-step method of data analysis in this paper is a detailed description of analytical steps to be taken in directed qualitative content analysis that covers the current gap of knowledge in international literature regarding the practical process of qualitative data analysis. An example of ""the resuscitation team members' motivation for cardiopulmonary resuscitation"" based on Victor Vroom's expectancy theory is also presented. The directed qualitative content analysis method proposed in this paper is a reliable, transparent, and comprehensive method for qualitative researchers.",['This paper aims to describe and integrate the process of data analysis in directed qualitative content analysis.']
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Scientometric mapping and analysis using bibliographic data is a well-established methodology, for which a variety of approaches, techniques, and automated tools have been developed (#CITATION_TAG).",Different techniques and software tools have been proposed to carry out science mapping analysis.,"['Science mapping aims to build bibliometric maps that describe how specific disciplines, scientific domains, or research fields are conceptually, intellectually, and socially structured.', 'The aim of this article is to review, analyze, and compare some of these software tools, taking into account aspects such as the bibliometric techniques available and the different kinds of analysis.']"
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. The complexity of these interactions and the fact that both scholarship and management have only recently begun to address this complexity have provided the impetus for us to present one synthesis of scale and cross-scale dynamics. Previous research has shown that just because research is policy relevant, does not mean that policy makers will use it (#CITATION_TAG).","In doing so, we draw from multiple cases, multiple disciplines, and multiple perspectives. In this synthesis paper, and in the accompanying cases, we hypothesize that the dynamics of cross-scale and cross-level interactions are affected by the interplay between institutions at multiple levels and scales.","['We suggest that the advent of co-management structures and conscious boundary management that includes knowledge co-production, mediation, translation, and negotiation across scale-related boundaries may facilitate solutions to complex problems that decision makers have historically been unable to solve']"
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Food price is one way through which climate change may affect health. The complex and dynamic nature of pricing mechanisms makes it difficult to predict precisely how prices will be impacted. Should prices rise disproportionately among healthy foodstuffs compared to less healthy foods there may be adverse health outcomes if less expensive and less healthy foods are substituted. These investigations may also include the specific role humans play in the carbon cycle, such as the impact of human-generated emissions or the consequences of climate change to agriculture and food systems (Berthelot et al., 2002; #CITATION_TAG; Dempewolf et al., 2014; Shindell et al., 2012).","We outline areas where there are particular vulnerabilities for food systems and food prices arising from climate change, particularly global commodity prices; agricultural productivity; short term supply shocks; and less direct factors such as input costs and government policies. We use Australia as a high-income country case study to consider these issues in more detail.","['The aim of this study of the global and Australian food systems is to provide  a whole-of-system analysis of food price vulnerabilities, highlighting the key pressure points across the food system through which climate change could potentially have the greatest impact on consumer food prices and the implications for population health.', 'The ultimate objective of this research is to identify the pathways through the food system via which climate change may affect food prices and ultimately population health, thereby providing evidence for food policy which takes into account environmental and health considerations']"
"The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. The U.S. North American Carbon Program (NACP) sponsored an ""all-scientist"" meeting to review progress in understanding the dynamics of the carbon cycle of North America and adjacent oceans, and to chart a course for improved integration across scientific disciplines, scales, and Earth system boundaries. In 2007, The U.S. North American Carbon Program (NACP) sponsored its first ""all-scientist"" meeting to review progress in understanding the dynamics of the carbon cycle of North America and adjacent oceans, and to chart a course for improved integration across scientific disciplines, scales, and Earth system boundaries (#CITATION_TAG).",,"['Herein we report on themes to integrate the diversity of NACP science and fill significant gaps for understanding and managing the North American carbon cycle: integration among disciplines involving land, atmosphere, and ocean research; strengthening data management infrastructure to support modeling and analysis; identification of study regions that are critical for reducing uncertainties in the North American carbon balance; and integrating biophysical science with the human dimensions of carbon management and decision support.']"
As neoadjuvant chemoradiation is established Electronic supplementary material The online version of this Article ( PATIENTS AND METHODS: A classification of complications published by one of the authors in 1992 was critically re-evaluated and modified to increase its accuracy and its acceptability in the surgical community. The new grading system still mostly relies on the therapy used to treat the complication. Perioperative complications were graded by Dindo's classification [#CITATION_TAG].,"Modifications mainly focused on the manner of reporting life-threatening and permanently disabling complications. The classification was tested in a cohort of 6336 patients who underwent elective general surgery at our institution. The reproducibility and personal judgment of the classification were evaluated through an international survey with 2 questionnaires sent to 10 surgical centers worldwide. The classification was considered to be simple (92% of the respondents), reproducible (91%), logical (92%), useful (90%), and comprehensive (89%). The answers of both questionnaires were not dependent on the origin of the reply and the level of training of the surgeons.","['OBJECTIVE: Although quality assessment is gaining increasing attention, there is still no consensus on how to define and grade postoperative complications.']"
"As neoadjuvant chemoradiation is established Electronic supplementary material The online version of this Article ( Target volume definitions for radiotherapy in pancreatic ductal adenocarcinoma (PDAC) vary substantially. Some groups aim to treat the primary tumor only, whereas others include elective lymph nodes (eLNs). eLNs close to the primary tumor are often included unintentionally within the treatment volume, depending on the respective treatment philosophies. Three-dimensional treatment planning was mandatory for radiotherapy at 1.8 Gy to 55.8 Gy (tumor) or 50.4 Gy [regional lymph nodes, planning target volume (PTV ≤ 800 ml)] [#CITATION_TAG].","Clinical target volumes (CTVs) included the peripancreatic, para-aortic, paracaval, celiac trunk, superior mesenteric, and portal vein lymph node areas. A large variability of eLN coverage was found for the various subregions according to the respective contouring strategies.This is the first study to directly compare the percentage of anatomical coverage of eLNs according to four PTVs in the same patient cohort. Potential practical consequences are discussed in detail.","['We aimed to measure the percentages of anatomical coverage of eLNs by comparing four different contouring guidelines.Planning target volumes (PTVs) were contoured using planning computed tomography (CT) scans of 11 patients with PDAC based on the Oxford, RTOG (Radiation Therapy Oncology Group), Michigan, and SCALOP (Selective Chemoradiation in Advanced Localised Pancreatic Cancer trial) guidelines.']"
"As neoadjuvant chemoradiation is established Electronic supplementary material The online version of this Article ( SETTING A district hospital and referral center in Basingstoke, England. This progress was due to standardizing surgical therapy [#CITATION_TAG] worldwide and by the implementation of multimodal therapy [4] [5] [6].","DESIGN A prospective consecutive case series. INTERVENTIONS Anterior resections (n = 465) with low stapled anastomoses (407 total mesorectal excisions), abdominoperineal resections (n = 37), Hartmann resections (n = 10), local excisions (n = 4), and laparotomy only (n = 3). Preoperative radiotherapy was used in 49 patients (7 with abdominoperineal resections, 38 with anterior resections, 3 with Hartmann resections, and 1 with laparotomy). In future clinical trials of adjuvant chemotherapy and radiotherapy, strategies should incorporate total mesorectal excision as the surgical procedure of choice.",['OBJECTIVE To examine the role of total mesorectal excision in the management of rectal cancer.']
"Cognitive neuroscience boils down to describing the ways in which cognitive function results from brain activity. Exactly how cognitive function inherits the physical dimensions of neural activity, though, is highly non-trivial, and so are generally the corresponding dimensions of cognitive phenomena. In spite of their general use, these assumptions hold true to a high degree of approximation for many cognitive (viz. fast perceptual) processes, but have their limitations for other ones (e.g., thinking or reasoning). It covers a broad range of non-stationary aging and stationary driven systems such as structural glasses, spin glasses, coarsening systems, ferromagnetic models at criticality, trap models, models with entropy barriers, kinetically constrained models, sheared systems and granular media. Suitably modified versions of the FDT also hold for out-of-equilibrium systems (Cugliandolo et al., 1997; #CITATION_TAG; Pottier and Mauger, 2004; Allegrini et al., 2007; Aquino et al., 2007).","The review is divided into four main parts: (1) an introductory section explaining basic notions related to the existence of the FDT in equilibrium and its possible extension to the glassy regime (QFDT), (2) a description of the basic analytical tools and results derived in the framework of some exactly solvable models, (3) a detailed report of the current evidence in favour of the QFDT and (4) a brief digression on the experimental evidence in its favour.","['This review reports on the research done during past years on violations of the fluctuation-dissipation theorem (FDT) in glassy systems.', 'It is focused on the existence of a quasi-fluctuation-dissipation theorem (QFDT) in glassy systems and the current supporting knowledge gained from numerical simulation studies.', 'This review is intended for inexpert readers who want to learn about the basic notions and concepts related to the existence of the QFDT as well as for the more expert readers who may be interested in more specific results.']"
"Cognitive neuroscience boils down to describing the ways in which cognitive function results from brain activity. Exactly how cognitive function inherits the physical dimensions of neural activity, though, is highly non-trivial, and so are generally the corresponding dimensions of cognitive phenomena. In spite of their general use, these assumptions hold true to a high degree of approximation for many cognitive (viz. fast perceptual) processes, but have their limitations for other ones (e.g., thinking or reasoning). Classical within-subject analysis in functional magnetic resonance imaging (fMRI) relies on a detection step to localize which parts of the brain are activated by a given stimulus type. The originality of this contribution is twofold. By nature, scaling analysis requires the use of long enough signals with high frequency sampling rate. The break-down of scale invariance (Ciuciu et al., 2011 (#CITATION_TAG Zilber et al., 2012) is tantamount to velocity changes.","This is usually achieved using model-based approaches. First, we propose a synthetic, consistent, and comparative overview of the various stochastic processes and estimation procedures used to model and analyze scale invariance. Notably, it is explained how multifractal models are more versatile to adjust the scaling properties of fMRI data but require more elaborated analysis procedures. To this end, we make use of a localized 3-D echo volume imaging (EVI) technique, which has recently emerged in fMRI because it allows very fast acquisitions of successive brain volumes. A voxel-based systematic multifractal analysis has been performed over both kinds of data.","['Here, we propose an alternative exploratory analysis.']"
"Cognitive neuroscience boils down to describing the ways in which cognitive function results from brain activity. Exactly how cognitive function inherits the physical dimensions of neural activity, though, is highly non-trivial, and so are generally the corresponding dimensions of cognitive phenomena. In spite of their general use, these assumptions hold true to a high degree of approximation for many cognitive (viz. fast perceptual) processes, but have their limitations for other ones (e.g., thinking or reasoning). These variants differ in the way the exchange interactions are treated. This is a consequence of the fluctuation-dissipation theorem (FDT), which establishes a general relationship between the (equilibrium) internal autocorrelation C(t) of fluctuations of some observable of the system in the absence of the disturbance and the (non-equilibrium) response R(t) of a system to small external perturbations (#CITATION_TAG).","One of these variants, named dRPA-II, is original to this work and closely resembles the second-order screened exchange (SOSEX) method. We discuss and clarify the connections among different RPA formulations. We derive the spin-adapted forms of all the variants for closed-shell systems and test them on a few atomic and molecular systems with and without range separation of the electron electron interaction.",['ABSTRACT:We explore several random phase approximation (RPA) correlation energy variants within the adiabatic-connection fluctuationdissipation theorem approach.']
"The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. &lt;p&gt;&lt;i&gt;Evaluating complex interventions is complicated. The Medical Research Council's evaluation framework (2000) brought welcome clarity to the task. In 2000, the Medical Research Council (MRC) published a framework&lt;sup&gt;1&lt;/sup&gt; to help researchers and research funders to recognise and adopt appropriate methods. The research design was based on Patton's [25] description of qualitative process evaluation and guidelines for the evaluation of complex interventions [#CITATION_TAG].","They present various problems for evaluators, in addition to the practical and methodological difficulties that any successful evaluation must overcome. The framework has been highly influential, and the accompanying BMJ paper is widely cited.&lt;sup&gt;2&lt;/sup&gt; However, much valuable experience has since accumulated of both conventional and more innovative methods.",['In this article we summarise the issues that prompted the revision and the key messages of the new guidance.']
"The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. These resources have been described as patients' embodied knowledge of health and illness. The notion of embodiment as an essential part for health is a relatively new idea in modern Western society and is poorly theorized in models for health promotion. The theory asserts that individuals with chronic illness have bodily knowledge that constitutes an important resource for coping and health, and that patient's bodily knowledge is developed through a dynamic, nonlinear process of learning and health-related change in interaction with the environment [#CITATION_TAG].","Methods: Grounded theory methodology informed data collection and analysis. In-depth interviews were conducted with 56 men and women who were diagnosed with chronic obstructive pulmonary disease, inflammatory bowel disease or stroke.","['This study explored patterns of patients` experiences/bodily knowledge and actions that contribute to well-being and health in chronic illness.', 'This study is the first to examine Bodyknowledging, which is a process that describes the utilization of bodily knowledge to engage in health-related change.']"
The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [#CITATION_TAG] [13].,"It also explored the differential effectiveness of the intervention for subjects with specific diseases and comorbidities. METHODS The study was a six-month randomized, controlled trial at community-based sites comparing treatment subjects with wait-list control subjects. Participants were 952 patients 40 years of age or older with a physician-confirmed diagnosis of heart disease, lung disease, stroke, or arthritis. Health behaviors, health status, and health service utilization, as determined by mailed, self-administered questionnaires, were measured.","['OBJECTIVES This study evaluated the effectiveness (changes in health behaviors, health status, and health service utilization) of a self-management program for chronic disease designed for use with a heterogeneous group of chronic disease patients.']"
"The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. Summaries of research concerning people with various long-term conditions show that they have much in common as they face the challenges of trying to live as well as possible within the context of physical, mental, or social discomfort and limitation [5] - [#CITATION_TAG].","DESIGN The model was derived from a metasynthesis of qualitative research about the reported experiences of adults with a chronic illness. The 292 primary research studies included a variety of interpretive research methods and were conducted by researchers from numerous countries and disciplines. METHODS Metastudy, a metasynthesis method developed by the author in collaboration with six other researchers consisted of three analytic components (meta-data-analysis, metamethod, and metatheory), followed by a synthesis component in which new knowledge about the phenomenon was generated from the findings. The Shifting Perspectives Model indicated that living with chronic illness was an ongoing and continually shifting process in which an illness-in-the-foreground or wellness-in-the-foreground perspective has specific functions in the person's world.","['PURPOSE To present the Shifting Perspectives Model of Chronic Illness, which was derived from a metasynthesis of 292 qualitative research studies.']"
The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. This publication addresses the limited use of qualitative methods in neglected tropical disease (NTD) programmes. The research design was based on Patton's [#CITATION_TAG] description of qualitative process evaluation and guidelines for the evaluation of complex interventions [26].,The review assessed how qualitative methods are currently used by NTD programmes and identified qualitative approaches from other health and development programmes with the potential to strengthen the design of MDA interventions. Systematic review articles were reviewed and searched using key terms conducted on Google Scholar and PubMed. The topic of human resources for qualitative investigations is explored and a guide to improve MDAs using qualitative methods is introduced.,['It describes a scoping literature review conducted to inform the development of a guide to inform the use of rapid qualitative assessments to strengthen NTD mass drug administration (MDA) programmes.']
The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [12] [#CITATION_TAG].,"The major hypothesis is that during the 2-year period CDSMP participants will experience improvements or less deterioration than expected in health status and reductions in health care utilization. Design.Longitudinal design as follow-up to a randomized trial. Participants.Eight hundred thirty-one participants 40 years and older with heart disease, lung disease, stroke, or arthritis participated in the CDSMP. Health status (self-rated health, disability, social/role activities limitations, energy/fatigue, and health distress), health care utilization (ER/outpatient visits, times hospitalized, and days in hospital), and perceived self-efficacy were measured.","['Objectives.To assess the 1- and 2-year health status, health care utilization and self-efficacy outcomes for the Chronic Disease Self-Management Program (CDSMP).']"
"The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. The intervention group (n = 61) consisted of nine subgroups geographically spread through the eastern part of Norway and met for four hours every 2 weeks from February 1997 to October 1997. Studies of the outcomes showed that patients with musculoskeletal pain reported reduced pain, increased pain-coping abilities and reduced health care consumption after completion VTP [#CITATION_TAG] [18].","The learning program was based on personal construct theory. The theory included the following: (1) participation in an educational program is related to a favorable outcome across the outcome measures (pain, pain coping, management of daily life, absenteeism, and use of health care), (2) patients with high agency orientation (i.e., inner-directed) cope with their pain and manage daily life in a better manner than do patients with low agency orientation (i.e., outer-directed), and (3) patients with high personal control, measured in terms of agency orientation, in terms of health locus of control, or in both terms, will benefit more from the educational program than will patients with low personal control.The study was a randomized controlled study.One hundred and sixteen patients with chronic musculoskeletal pain and high absenteeism answered a questionnaire before and after the intervention program.",['This study evaluated the effects of a group learning program on patients with chronic musculoskeletal pain and high absenteeism and investigates what characterizes those patients who may benefit from such a program.']
"The Arctic climate is changing at an unprecedented rate. What consequences this may have on the Arctic marine ecosystem depends to a large degree on how its species will respond both directly to elevated temperatures and more indirectly through ecological interactions. But despite an alarming recent warming of the Arctic with accompanying sea ice loss, reports evaluating ecological impacts of climate change in the Arctic remain sparse. Here, based upon a large-scale field study, we present basic new knowledge regarding the life history traits for one of the most important species in the entire Arctic, the polar cod (Boreogadus saida). Primates are characterized by relatively late ages at first reproduction, long lives and low fertility. Understanding the optimal allocation of reproductive effort, and specifically reduced reproductive effort, has been one of the key problems motivating the development of life-history theory. Because of their unusual constellation of life-history traits, primates play an important role in the continued development of life-history theory. The tendency for primates to specialize in high-quality, high-variability food items may make them particularly susceptible to environmental variability and explains their low reproductive-effort tactics. Understanding life history strategies, the schedules of growth, fecundity and mortality [#CITATION_TAG], is imperative for our ability to predict species' responses to environmental change.","Such tactics are particularly consistent with the predictions of stochastic demographic models, suggesting a key role for environmental variability in the evolution of primate life histories.","['In this review, I present the evidence for the reduced reproductive effort life histories of primates and discuss the ways that such life-history tactics are understood in contemporary theory.', 'I discuss recent applications of life-history theory to human evolution and emphasize the continuity between models used to explain peculiarities of human reproduction and senescence with the long, slow life histories of primates more generally']"
"The Arctic climate is changing at an unprecedented rate. What consequences this may have on the Arctic marine ecosystem depends to a large degree on how its species will respond both directly to elevated temperatures and more indirectly through ecological interactions. But despite an alarming recent warming of the Arctic with accompanying sea ice loss, reports evaluating ecological impacts of climate change in the Arctic remain sparse. Here, based upon a large-scale field study, we present basic new knowledge regarding the life history traits for one of the most important species in the entire Arctic, the polar cod (Boreogadus saida). The distribution of polar cod is more static than that of capelin, whose distribution extends further north in warm years and fluctuates greatly based on predator-prey relationships. The species occur sympatrically in the Barents Sea, with large standing biomasses (0.5-1.5x106 t polar cod versus 3-4x106 t capelin). They overlap in distribution in the southern and eastern Barents Sea, whereas polar cod are most abundant in the icy waters of the Arctic. Both species aggregate in large schools and utilize zooplankton food sources, such as calanoid copepods. Global warming, with reduction in sea ice and increase in temperature, is expected to affect these two species differently. Polar cod will likely lose the sympagic (ice-associated) part of its life cycle and become more restricted in pelagic distribution during summer, whereas the capelin stock may expand to the north and east, although with considerable interannual fluctuations. In high Arctic marine ecosystems, the polar cod (Boreogadus saida) is regarded as a key link in the food web between lower and higher trophic levels [19, #CITATION_TAG].","Their distributions are dependent on water masses, with polar cod being associated with cold, sub-zero Arctic water, whereas capelin is distributed further south into Atlantic water masses.","['Abstract Polar cod and capelin are key species in Arctic and sub-Arctic marine food webs, respectively, and the objective of this study is to compare and contrast the two species.']"
"The Arctic climate is changing at an unprecedented rate. What consequences this may have on the Arctic marine ecosystem depends to a large degree on how its species will respond both directly to elevated temperatures and more indirectly through ecological interactions. But despite an alarming recent warming of the Arctic with accompanying sea ice loss, reports evaluating ecological impacts of climate change in the Arctic remain sparse. Here, based upon a large-scale field study, we present basic new knowledge regarding the life history traits for one of the most important species in the entire Arctic, the polar cod (Boreogadus saida). The interest in fishing-induced life-history evolution has been growing in the last decade, in part because of the increasing number of studies suggesting evolutionary changes in life-history traits, and the potential ecological and economic consequences these changes may have. Among the traits that could evolve in response to fishing, growth has lately received attention. However, critical reading of the literature on growth evolution in fish reveals conceptual confusion about the nature of 'growth' itself as an evolving trait, and about the different ways fishing can affect growth and size-at-age of fish, both on ecological and on evolutionary time-scales. It is important to separate the advantages of being big and the costs of growing to a large size, particularly when studying life-history evolution. Importantly, there are tight couplings among altered energy acquisition, survival, and consequently responses in energy allocation [#CITATION_TAG].","We define important concepts and outline the processes that must be accounted for before observed phenotypic changes can be ascribed to growth evolution. When listing traits that could be traded-off with growth rate, we group the mechanisms into those affecting resource acquisition and those governing resource allocation. We summarize potential effects of fishing on traits related to growth and discuss methods for detecting evolution of growth. We also challenge the prevailing expectation that fishing-induced evolution should always lead to slower growth.","['In this review, we explore the selection pressures on growth and the resultant evolution of growth from a mechanistic viewpoint.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. This technique examines the language used by informants to describe the experience of quitting. It was also found that many individuals experienced vivid dreams about smoking while undergoing the cessation process. [28] This was followed in the late 1980s and 1990s by three in-depth sociological studies (from the US and Sweden) investigating unassisted cessation as a phenomenon in its own right, [29, 31, #CITATION_TAG] and one US sociological study in which unassisted cessation data were reported but this was not the primary focus of the study.","The Development Research Model, as introduced by James Spradley (1979) was employed with a sample of people who quit smoking without the aid of formal treatment.",['This study investigates the phenomenon of spontaneous recovery through qualitative methodology.']
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Context A significant proportion of smokers who quit do so on their own without formal help (i.e. without professionally or pharmacologically mediated assistance), yet research into how smokers quit focuses primarily on assisted methods of cessation. Conclusions The majority of Australian smokers quit or attempt to quit unassisted, yet little research has been dedicated to understanding this process. Almost all research that reported unassisted cessation referenced it as a comparator to the focal point of assisted cessation. [#CITATION_TAG, 10] Many smokers also appear to quit unplanned as a consequence of serendipitous events, [11] throwing into question the predictive validity of some of these cognitive models.","Methods In January 2013, four e-databases and the grey literature were searched for articles published 2005-2012 on smoking cessation in Australia. Articles focusing solely on interventions designed to stimulate cessation were excluded, as were articles focusing solely on assisted cessation, leaving articles reporting on smokers who quit unassisted. Data from articles reporting on unassisted cessation were extracted and grouped into related categories.","['Objective To systematically review recent smoking cessation research in Australia, a nation advanced in tobacco control, to determine what is known about smokers who quit unassisted in order to (1) inform a research agenda to develop greater understanding of the many smokers who quit unassisted and (2) elucidate possible lessons for policy and mass communication about cessation.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The study was a qualitative study carried out in 2002 by focus groups of 32 male secondary school students in Hong Kong who were either current smokers or had recently given up smoking. [30] Subsequent to this, no qualitative studies were identified that focused on unassisted cessation per se: the six post-2000 studies (from Hong Kong, US, UK, Canada and Norway) had as their primary focus either cessation in general [#CITATION_TAG, 34, [36] [37] [38] or health behaviour change.","Subjects were students (grades 8-10) attending two full-day secondary schools in Hong Kong. Several barriers to quitting were reported, including boredom, peer influence, the urge to smoke, school work pressure, the wish to do something with their hands, difficulty in concentrating, and the ready availability of free cigarettes from peers.","['The aim of this study was to investigate the attitudes of Chinese adolescents toward smoking, giving up smoking, and smoking cessation programs presently available.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. [28] This was followed in the late 1980s and 1990s by three in-depth sociological studies (from the US and Sweden) investigating unassisted cessation as a phenomenon in its own right, [29, #CITATION_TAG, 32] and one US sociological study in which unassisted cessation data were reported but this was not the primary focus of the study.","A sample of 58 subjects who quit their addiction without any treatment was interviewed. The subjects were interviewed about their life background, addiction history, pre-resolution events (life events and internal factors), strategies used which helped subjects to reach recovery and other information related to the addiction. The Rosenbaum test was administered to test current self-regulation.","['Abstract This study is an attempt towards better understanding of a natural process of recovery.', 'It presents an investigation of recovery without treatment (or natural recovery) from tobacco, snuff (snus), drug and alcohol abuse.', 'The purpose of this exploratory study was to investigate how people manage to recover from addictions without formal treatment and if there are common factors and coping strategies for resolution.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Smoking is considered to be the single most preventable factor influencing health. Smoking cessation advice has traditionally been given on an individual basis. Six stages in the smoking story emerged, from the start of smoking, where friends had a huge influence, until maintenance of the possible cessation. Spouses had vital influence in stopping, relapses and continued smoking. Often smoking cessation seemed to happen unplanned, though sometimes it was planned. For smoking cessation, it is essential to be aware of the influence of friends and family members, especially a spouse. People may stop smoking unplanned, even when motivation is not obvious. [30] Subsequent to this, no qualitative studies were identified that focused on unassisted cessation per se: the six post-2000 studies (from Hong Kong, US, UK, Canada and Norway) had as their primary focus either cessation in general [33, 34, [36] [37] [#CITATION_TAG] or health behaviour change.","General practitioners (GP) are encouraged to advise on smoking cessation at all suitable consultations. Interviews with 18 elderly smokers and ex-smokers about their smoking and decisions to smoke or quit  were analysed with qualitative content analysis across narratives. A narrative perspective was applied. The informants were influenced by ""all the others"" at all stages. Eliciting life-long smoking narratives may open up for a fruitful dialogue, as well as prompting reflection about smoking and adding to the motivation to stop","['Our aim was to gain  insights that may help general practitioners understand why people smoke, and why smokers stop and then remain quitting and, from this, to find fruitful approaches to the dialogue about stopping smoking.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The idea that most smokers quit without formal assistance is widely accepted, however, few studies have been referenced as evidence. Unassisted quit attempts ranged from a high of 95.3% in a study in Christchurch, New Zealand, between 1998 and 1999, to a low of 40.6% in a national Australian study conducted between 2008 and 2009. However, across and within countries over time, it appears that there is a trend toward lower prevalence of making quit attempts without reported assistance or intervention.Copyright (c) 2013 Elsevier Ltd. All rights reserved. [9, #CITATION_TAG] Many smokers also appear to quit unplanned as a consequence of serendipitous events, [11] throwing into question the predictive validity of some of these cognitive models.",,"['The purpose of this study is to systematically review the literature to determine what proportion of adult smokers report attempting to quit unassisted in population-based studies.A four stage strategy was used to conduct a search of the literature including searching 9 electronic databases (PUBMED, MEDLINE (OVID) (1948-), EMBASE (1947-), CINAHL, ISI Web of Science with conference proceedings, PsycINFO (1806-), Scopus, Conference Papers Index, and Digital Dissertations), the gray literature, online forums and hand searches.A total of 26 population-based prevalence studies of unassisted quitting were identified, which presented data collected from 1986 through 2010, in 9 countries.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. SETTING Twenty-four general practices in Nottinghamshire, UK. It is important, therefore, that smoking cessation services offer flexible and adaptable support which can be used readily by potential quitters. Subsequent to this, no qualitative studies were identified that focused on unassisted cessation per se: the six post-2000 studies (from Hong Kong, US, UK, Canada and Norway) had as their primary focus either cessation in general [33,34,[35]#CITATION_TAG[38] or health behaviour change",DESIGN Qualitative study using semi-structured interviews with 20 smokers and ex-smokers. Participants Smokers and ex-smokers who reported that their most recent attempt to quit smoking was unplanned.,['AIMS To gain a greater understanding of the process of unplanned attempts to quit smoking and the use of support in such attempts.']
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Increasing rates of smoking cessation is one of the most effective measures available to improve population health. Younger quitters were more likely to use unassisted methods such as cold turkey; older or less educated quitters were more likely to use assisted methods such as prescribed medication or advice from a general practitioner.The majority of recent quitters quit cold turkey or cut down before quitting, and reported that these methods were helpful. [#CITATION_TAG,46] It is widely accepted that searching the qualitative literature is difficult","In this survey of recent quitters, we simultaneously examined rates of use and perceived helpfulness of various cessation methods.Recent quitters (within 12 months; n = 1097) completed a telephone survey including questions relating to 13 cessation methods. Indices of use and perceived helpfulness for each method were plotted in a quadrant analysis. Socio-demographic differences were explored using bivariate and multivariate analyses.From the quadrant analysis, cold turkey, NRT and gradual reduction before quitting had high use and helpfulness; GP advice had high use and lower helpfulness. Remaining methods had low use and helpfulness.","[""To advance the goal of increasing successful cessation at the population level, it is imperative that we understand more about smokers' use of cessation methods, as well as the helpfulness of those methods in real-world experiences of quitting.""]"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Parfit and Dancy argue that desires couldn't be reason giving because if they were they would have to give us reasons for their own retention. Rationally endorsed desires determine on the humean view which action of ours is autonomous and what we have reason to do. The lesson from Parfit's and Dancy's argument is that rationally endorsed desires are also normatively self determining; they provide the normative foundation for their own retention. Commentators have often wondered how, according to humeans, could our practical identity, whose causal history is normatively arbitrary, ground our reasons for action. It could be fruitful for future research to further examine the meaning of willpower, and particularly its relationship to other more tightly defined concepts such as self-efficacy, [61] self-regulation[62] and self-determination, [#CITATION_TAG] from the perspective of both researchers and smokers.","It shows that on the humean view of practical reason the same relations or processes that ground autonomous action and determine what we have reason to do also (partly) determine who we have reason to be. The humean answer is that parts of our practical identity - namely, our rationally endorsed desires - justify their own retention and are thus normatively non-arbitrary.","[""In this paper I analyze Dancy's and Parfit's argument, provide a humean reply, describe the implications of that reply for the humean view and give some examples of normatively self determining desires""]"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Ninety-five percent of ex-smokers have quit smoking without professional assistance. Research investigating self-initiated smoking cessation has increased to the point where a sizable data base on this phenomenon is now available. First, we were aware of a small but not unsubstantial body of quantitative evidence on smokers who quit unassisted; [#CITATION_TAG] [49] [50] [51] [52] and second, in the course of our literature search we had identified a considerable number of qualitative studies on smoking cessation.",,"['In this paper we review the research literature that has investigated unaided smoking cessation in order to better understand the quitting process, to identify useful strategies for prospective quitters, and to provide suggestions for professionally based interventions.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Qualitative research is increasingly valued as part of the evidence for policy and practice, but how it should be appraised is contested. [#CITATION_TAG] Concern with procedural correctness can unduly focus attention on the reporting of the research process and divert attention away from the analytical content of the research.","Various appraisal methods, including checklists and other structured approaches, have been proposed but rarely evaluated. Papers were assigned, following appraisals, to 1 of 5 categories, which were dichotomized to indicate whether or not papers should be included in a systematic review. Patterns of agreement in categorization of papers were assessed quantitatively using kappa statistics, and qualitatively using cross-case analysis.Agreement in categorizing papers across the three methods was slight (kappa =0.13; 95% CI 0.06-0.24).","['We aimed to compare three methods for appraising qualitative research papers that were candidates for inclusion in a systematic review of evidence on support for breast-feeding.A sample of 12 research papers on support for breast-feeding was appraised by six qualitative reviewers using three appraisal methods: unprompted judgement, based on expert opinion; a UK Cabinet Office quality framework; and CASP, a Critical Appraisal Skills Programme tool.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The reality is that one needs to be motivated to prompt action to stop smoking, but this is not sufficient in and of itself to ensure that cessation is maintained. [#CITATION_TAG, 54] Motivation has been identified as critical to explaining cessation success.","METHODS Data are from three wave-to-wave transitions of the International Tobacco Control Four (ITC-4) country project. Smokers' responses at one wave were used to predict the likelihood of making an attempt and among those trying the likelihood of maintaining an attempt for at least a month at the next wave. For both outcomes, hierarchical logistic regressions were used to explore the predictive capacity of seven measures of motivation to quit smoking, controlling for a range of other known or possible predictors.",['AIM To explore whether measures of motivation to quit smoking have different predictive relationships with making quit attempts and the maintenance of those attempts.']
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Nicotine replacement therapies (NRTs) have been demonstrated to be effective in clinical trials but may have lower efficacy when purchased over-the-counter (OTC). [#CITATION_TAG] Further complicating the relationship, some regard commitment as a component of motivation, [57] operationalizing motivation as, for example, 'determination to quit' [58] or 'commitment to quit'. [59] The greater research interest in reasons for quitting or pros and cons of quitting (i.e., motivation) as opposed to commitment may be because motivation is simpler to measure, for example by asking people to rate or rank reasons, costs or benefits.",,"['The aims are to (a) investigate the prevalence of and reasons for premature discontinuation of stop-smoking medications (including prescription only) and (b) how these differ by type, duration of use, and source (prescription or OTC).The sample includes 1,219 smokers or recent quitters who had used medication in the last year (80.5% NRT, 19.5% prescription only).']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The Health Development Agency (www.hda.nhs.uk) is the national authority and information resource on what works to improve people's health and reduce health inequalities in England. iii Contents Foreword iv Summary 1 What is the role of qualitative approaches in traditional trials and experimental studies? 4 At what point in the development of a field of knowledge is it appropriate to pull qualitative and quantitative learning together? 7 Are there hierarchies of evidence within the different types of qualitative investigation? 11 Advice from the NHS Centre for Reviews and Dissemination 11 The interpretive/integrative distinction 11 Narrative summary 12 Thematic analysis 15 Grounded theory 15 Meta-ethnography 17 Estabrooks, Field and Morse's aggregation of findings approach 19 Qualitative meta-analysis 19 Qualitative meta-synthesis 19 Meta-study 21 Miles and Huberman's cross-case data analysis techniques 22 Content analysis 23 Case survey 24 Qualitative comparative analysis 24 Bayesian meta-analysis 25 Meta-needs assessment 27 Discussion 28 Quantitising and qualitising 28 Issues in qualitative synthesis 28 Conclusions 31 References 32 Quant-and-qual.indd 3/26/2004, 10:10 AM 3 iv In 2000 the Health Development Agency (HDA) was established to, among other things, build the evidence base in public health, with particular reference to reducing inequalities in health (Department of Health, 2001). Since then the HDA has been engaged in a programme of developing methodologies and protocols to do precisely that (Swann et al., 2002). There are two important ideas behind the HDA's remit, political and scientific. The political imperative is a clear commitment to tackling the long-term problem at the heart of public health - that, as the health of the population as a whole continues to improve, at the same time the gradient in inequalities in health across the population, from the most to the least advantaged becomes worse (Acheson, 1998) There is, in other words, a long-standing problem of inequalities in health. The scientific principle is that the best available evidence should be used in order to ... [#CITATION_TAG, 18] By integrating individual qualitative research studies into a qualitative synthesis, new insights and understandings can be generated and a cumulative body of empirical work produced. [19] Such syntheses have proven useful to health policy and practice.",6 Generating hypotheses and questions 6 Informing the selection of outcomes for review 6 Extending or guiding sampling 6 Providing explanations and informing conclusions 6 What constitutes good evidence from qualitative studies?,"['It gathers evidence and produces advice for policy makers, professionals and practitioners, working alongside them to get evidence into practice.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. BACKGROUND Healthcare professionals frequently advise patients to improve their health by stopping smoking. Such advice may be brief, or part of more intensive interventions. In some trials, subjects were at risk of specified diseases (chest disease, diabetes, ischaemic heart disease), but most were from unselected populations. The most common setting for delivery of advice was primary care. There was insufficient evidence, from indirect comparisons, to establish a significant difference in the effectiveness of physician advice according to the intensity of the intervention, the amount of follow up provided, and whether or not various aids were used at the time of the consultation in addition to providing advice. Only one study determined the effect of smoking advice on mortality. Yet, although these interventions are efficacious, [6] [7] [#CITATION_TAG] the majority of smokers who quit successfully do so without using them, choosing instead to quit unassisted, that is without pharmacological or professional support.","SEARCH STRATEGY We searched the Cochrane Tobacco Addiction Group trials register and the Cochrane Central Register of Controlled Trials (CENTRAL). SELECTION CRITERIA Randomized trials of smoking cessation advice from a medical practitioner in which abstinence was assessed at least six months after advice was first provided. DATA COLLECTION AND ANALYSIS We extracted data in duplicate on the setting in which advice was given, type of advice given (minimal or intensive), and whether aids to advice were used, the outcome measures, method of randomization and completeness of follow up. We used the most rigorous definition of abstinence in each trial, and biochemically validated rates where available. Subjects lost to follow up were counted as smokers. Where possible, meta-analysis was performed using a Mantel-Haenszel fixed effect model. Other settings included hospital wards and outpatient clinics, and industrial clinics.",['OBJECTIVES The aims of this review were to assess the effectiveness of advice from physicians in promoting smoking cessation; to compare minimal interventions by physicians with more intensive interventions; to assess the effectiveness of various aids to advice in promoting smoking cessation and to determine the effect of anti-smoking advice on disease-specific and all-cause mortality.']
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. BACKGROUND Telephone services can provide information and support for smokers. Counselling may be provided proactively or offered reactively to callers to smoking cessation helplines. Participants were mostly adult smokers from the general population, but some studies included teenagers, pregnant women, and people with long-term or mental health conditions. Most studies (100/104) assessed proactive telephone counselling, as opposed to reactive forms.Among trials including smokers who contacted helplines (32,484 participants), quit rates were higher for smokers receiving multiple sessions of proactive counselling (risk ratio (RR) 1.38, 95% confidence interval (CI) 1.19 to 1.61; 14 trials, 32,484 participants; I2 = 72%) compared with a control condition providing self-help materials or brief counselling in a single call. There is currently insufficient evidence to assess potential variations in effect from differences in the number of contacts, type or timing of telephone counselling, or when telephone counselling is provided as an adjunct to other smoking cessation therapies. Yet, although these interventions are efficacious, [6] [#CITATION_TAG] [8] the majority of smokers who quit successfully do so without using them, choosing instead to quit unassisted, that is without pharmacological or professional support.","SEARCH METHODS We searched the Cochrane Tobacco Addiction Group Specialised Register, clinicaltrials.gov, and the ICTRP for studies of telephone counselling, using search terms including 'hotlines' or 'quitline' or 'helpline'. SELECTION CRITERIA Randomised or quasi-randomised controlled trials which offered proactive or reactive telephone counselling to smokers to assist smoking cessation. DATA COLLECTION AND ANALYSIS We used standard methodological procedures expected by Cochrane. We pooled studies using a random-effects model and assessed statistical heterogeneity amongst subgroups of clinically comparable studies using the I2 statistic. In trials including smokers who did not call a quitline, we used meta-regression to investigate moderation of the effect of telephone counselling by the planned number of calls in the intervention, trial selection of participants that were motivated to quit, and the baseline support provided together with telephone counselling (either self-help only, brief face-to-face intervention, pharmacotherapy, or financial incentives).","['OBJECTIVES To evaluate the effect of telephone support to help smokers quit, including proactive or reactive counselling, or the provision of other information to smokers calling a helpline.']"
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Nurses, allied health professionals, clinicians, and researchers increasingly use online access to evidence in the course of patient care or when conducting reviews on a particular topic. Qualitative research has an important role in evidence-based health care. Online searching for qualitative studies can be difficult, however, resulting in the need to develop search filters. [22] We used empirically derived qualitative research filters where available (MED-LINE, [23] CINAHL [#CITATION_TAG] and PsycINFO [25]) (Table 1).",The authors conducted an analytic survey comparing hand searches of journals with retrievals from CINAHL for candidate search terms and combinations. Empirically derived search strategies combining indexing terms and textwords can achieve high sensitivity and high specificity for retrieving qualitative studies from CINAHL.,['The objective of this study was to develop optimal search strategies to retrieve qualitative studies in CINAHL for the 2000 publishing year.']
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Researchers and practitioners have problems retrieving qualitative studies. Empirically derived search strategies combining textwords can effectively, but not perfectly, retrieve qualitative studies from PsycINFO. [22] We used empirically derived qualitative research filters where available (MED-LINE, [23] CINAHL [24] and PsycINFO [#CITATION_TAG]) (Table 1).","To determine if search strategies can identify qualitative studies, 64 journals published in 2000 were hand searched using explicit methodological criteria to identify qualitative studies. The authors tested multiple search strategies using 4,985 potential search terms in PsycINFO (Ovid Technologies) and compared the results with the hand search data to calculate retrieval effectiveness. A total of 125 qualitative studies were identified. Single-term and multiple-term strategies had sensitivities (maximizing retrieval of qualitative studies) up to 94.4% and specificities (minimizing retrieval of nonqualitative studies and reports) up to 98.6% with ranges of precision and accuracy. Search strategies included terms that were variations of interview, qualitative, themes, and experience.",['Search strategies that can easily and effectively retrieve these studies from large databases such as PsycINFO are therefore important.']
"Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Our findings are reported in two parts: (1) how much and what kind of qualitative research has explored unassisted cessation (Tables 3 and 4); (#CITATION_TAG) what are the views and experiences of smokers who quit unassisted? (Table 5).","Design/methodology/approach: The quantitative approach in this study was carried out by collecting survey data using a questionnaire instrument directly applied to 400 respondents in some cities in Indonesia. The analysis is conducted with SPSS, Wrap-PLS and Structural Equation Models (SEM).","['Purpose: This research examines the attitude and behavior of corruption/fraud using the social construct, the theory of fraud triangle, Theory of Planned Behavior, and social psychology.']"
"Contrary to the work of Valenzuela et al. (2015) [1], we find abstract similarity one of the most predictive features. This work looks in depth at several studies that have attempted to automate the process of citation importance classification based on the publications' full text. The importance of a research article is routinely measured by counting how many times it has been cited. However, treating all citations with equal weight ignores the wide variety of functions that citations perform. Zhu et al. [#CITATION_TAG] combine these earlier approaches and suggest a range of 40 classification features including both semantic and metric features to determine influence.","By asking authors to identify the key references in their own work, we created a data set in which citations were labeled according to their academic influence. Using automatic feature selection with supervised machine learning, we found a model for predicting academic influence that achieves good performance on this data set using only four features. The performance of these features inspired us to design an influence-primed h-index (the hip-index). Unlike the conventional h-index, it weights citations by how many times a reference is mentioned.","['We want to automatically identify the subset of references in a bibliography that have a central academic influence on the citing paper.', 'For this purpose, we examine the effectiveness of a variety of features for determining the academic influence of a citation.']"
"Contrary to the work of Valenzuela et al. (2015) [1], we find abstract similarity one of the most predictive features. This work looks in depth at several studies that have attempted to automate the process of citation importance classification based on the publications' full text. Being cited is a popular measure of the scientific contribution of a scientific paper and consequently a well-used measure of the academic reputation of the authors, their institutions, and the journal that published it [1-6]. In common citation index systems, like ISIWeb of Science, Scopus, and Google Scholar, all citations are treated equally. However, all authors would agree that references listed in the bibliography of a paper often differ greatly in their contribution to that paper. Some references are indispensable; they directly stimulate hypotheses or provide essential methods. By contrast, some other references are cited just for background information or are incidentally mentioned. An early analysis of 575 references in 30 articles published in Physical Review has shown that about 40% of the references are perfunctory, which raises doubts about the use of citations as a measure of scientific contribution [7]. To solve this problem, researchers have advocated a different strategy to that of simply counting the references in a bibliography, i.e. In recent years, the number of approaches for automatic classification of citations by key words or phrases has grown [9-11]. The underlying hypothesis is very simple. Those important references that make a major contribution to a given study appear in the text more frequently, while references providing only background information are mentioned just once in the text. Hou et al. (2011) [#CITATION_TAG] first suggest the idea of using an internal citation count based on the full text of a research paper rather than just the bibliography to determine influence.","With this approach, the functions of citations are classified by analyzing the contexts in which the references appear. By counting the appearance of each reference in the text, we can obtain a new citation frequency that reflects the scientific contribution of each reference more accurately. We tested our hypothesis by examining first whether closely related references appear more frequently in texts.","['Here, we present a simple alternative approach to improve the accuracy of citations as a measure of scientific contribution: counting citations in texts.']"
"Contrary to the work of Valenzuela et al. (2015) [1], we find abstract similarity one of the most predictive features. This work looks in depth at several studies that have attempted to automate the process of citation importance classification based on the publications' full text. The last 10 years have seen a massive increase in the amount of Open Access publications in journals and institutional repositories. The open availability of large volumes of state-of-the-art knowledge online has the potential to provide huge savings and benefits in many fields. However, in order to fully leverage this knowledge, it is necessary to develop systems that (a) make it easy for users to discover and access this knowledge at the level of individual resources, (b) explore and analyse this knowledge at the level of collections of resources and (c) provide infrastructure and access to raw data in order to lower the barriers to the research and development of systems and services on top of this knowledge. Open Access repositories such as that provided by CORE 1 [#CITATION_TAG] are allowing researchers to utilise the full text of research papers and articles in ways not possible with the meta-data o↵ered by bibliographic databases alone.","Consequently, we present the CORE (COnnecting REpositories) system, a large-scale Open Access aggregation, outlining its existing functionality and discussing the future technical development.","['In this paper, we argue why these requirements should be satisfied and show that current systems do not meet them.']"
"Observations on munition workers, most of them women, are organised to examine the relationship between their output and their working hours. The well-known enduring controversy on the interpretation of Ricardo's wage theory, and by implication on classical wage theory, has undoubtedly been fuelled by the existence of some inconsistencies in Ricardo's writings. However, as far as the factors affecting normal wages are concerned, these inconsistencies may carry less weight than is usually believed.. Due to these similarities, the controversy has tended to neglect a decisive point for the interpretation of the theory of wages in Ricardo and other classical economists, namely, the meaning of 'demand for labour' in classical thought. 32 This was the opinion of Hicks (1932 #CITATION_TAG who believed that 'probably it has never entered the heads of most employers that it was at all conceivable that hours could be shortened and output maintained. But it is clear that there were a few who had realised it (Robert Owen, for instance, p. 107)'.","I also maintain that there is a third point of view concerning the interpretation of wage theory in the classical economists, which has not been accurately understood and discussed in earlier surveys of the controversy. Unlike the others, this Alternative interpretation, as I shall label it for brevity, centers on the absence of a systematic decreasing relation between real wages and employment in Ricardo and other classical economists. The Alternative interpretation will be presented in some detail, and some questions posed by the New view will be assessed from the point of view of this alternative interpretation.Classical economists; Classical theory of wages; David Ricardo; Adam Smith","['The present paper aims to provide a critical overview of the controversy concerning the interpretation of the theory of wages in classical economists, offering a somewhat unusual perspective.', 'I contend that there are major similarities between the two interpretations that have been regarded as the main contenders, the so-called New view and Fix wage interpretations.']"
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Although the province of Nova Scotia, Canada is located in a region considered to have a high prevalence of multiple sclerosis (MS), epidemiologic data are limited. In 2010, the age-standardized prevalence of MS per 100,000 population was 266.9 (95% CI: 257.1- 277.1) and incidence was 5.17 (95% CI: 3.78-6.56) per 100,000 persons/year. From 1990-2010 the prevalence of MS rose steadily but incidence remained stable. MS prevalence in Nova Scotia is among the highest in the world, similar to recent prevalence estimates elsewhere in Canada. We aimed to estimate the incidence and prevalence of MS in BC, Canada using previously validated case definitions of MS [10, #CITATION_TAG] based on health administrative data.",Methods: We used provincial administrative claims data to identify persons with MS. We validated administrative case definitions using the clinical database of the province's only MS Clinic; agreement between data sources was expressed using a kappa statistic. We then applied these definitions to estimate the incidence and prevalence of MS from 1990 to 2010.,['Objective: We aimed to validate an administrative case definition for MS and to use this to estimate the incidence and prevalence of MS in Nova Scotia.']
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Several studies suggest an increasing prevalence of multiple sclerosis (MS) in Canada. From 1998 to 2006, the average age- and sex-adjusted annual incidence of MS per 100,000 population was 11.4 (95% confidence interval [CI] 10.7-12.0). The age-adjusted prevalence of MS per 100,000 population increased from 32.6 (95% CI 29.4-35.8) in 1984 to 226.7 (95% CI 218.1-235.3) in 2006, with the peak prevalence shifting to older age groups.The prevalence of multiple sclerosis (MS) in Manitoba is among the highest in the world. We aimed to estimate the incidence and prevalence of MS in BC, Canada using previously validated case definitions of MS [#CITATION_TAG, 11] based on health administrative data.","To validate the case definition, questionnaires were mailed to 2,000 randomly selected persons with an encounter for demyelinating disease, requesting permission for medical records review. We used diagnoses abstracted from medical records as the gold standard to evaluate candidate case definitions using administrative data.From 1984 to 1997, cases of MS using claims data were defined as persons with > or = 7 medical contacts for MS. From 1998 onward, cases were defined as persons with > or = 3 medical contacts.","['We aimed to validate a case definition for MS using administrative health insurance data, and to describe the incidence and prevalence of MS in Manitoba, Canada.We used provincial administrative claims data to identify persons with demyelinating disease using International Classification of Diseases 9/10 codes and prescription claims.']"
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Background:                  Multiple sclerosis (MS) is the most common cause of neurological disability in young adults worldwide and approximately half of those affected are in Europe. Quality was generally higher in the more recent studies, which also tended to use current diagnostic criteria. Prevalence and incidence estimates tended to be higher in the more recent studies and were higher in the Nordic countries and in northern regions of the British Isles. Few studies examined ethnicity. Epidemiological data at the national level was uncommon and there were marked geographical disparities in available data, with large areas of Europe unrepresented and other regions well-represented in the literature. It is estimated that more than two million people live with this disease worldwide [1], although the incidence and prevalence vary geographically [2] [#CITATION_TAG] [4].","The assessment of differential incidence and prevalence across populations can reveal spatial, temporal and demographic patterns which are important for identifying genetic and environmental factors contributing to MS. However, study methodologies vary and the quality of the methods can influence the estimates. Methods:                  A comprehensive literature search was performed to obtain all original population-based studies of MS incidence and prevalence in European populations conducted and published between January 1985 and January 2011. Only peer-reviewed full-text articles published in English or French were included. All abstracts were screened for eligibility and two trained reviewers abstracted the data and graded the quality of each study using a tool specifically designed for this study.",['This study aimed to systematically review European studies of incidence and prevalence of MS and to provide a quantitative assessment of their methodological quality.']
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Background: The incidence and prevalence of multiple sclerosis (MS) varies considerably around the world. No previous study has performed a comprehensive review examining the incidence and prevalence of MS across the Americas. The majority of studies examined North American regions (n = 25). Heterogeneity was high among all studies, even when stratified by country. Only half of the studies reported standardized rates, making comparisons difficult. There is a need for future studies of MS prevalence and incidence to include uniform case definitions, employ comparable methods of ascertainment, report standardized results, and be performed on a national level. It is estimated that more than two million people live with this disease worldwide [1], although the incidence and prevalence vary geographically [#CITATION_TAG] [3] [4].","Methods: A comprehensive literature search was performed using MEDLINE and EMBASE from January 1985 to January 2011. Search terms included 'multiple sclerosis', 'incidence', 'prevalence' and 'epidemiology'. Only full-text articles published in English or French were included. Study quality was assessed using an assessment tool based on recognized guidelines and designed specifically for this study. Other factors such as sex distribution, ethnic make-up and population lifestyle habits should also be considered.","['The purpose of this study was to systematically review and assess the quality of studies estimating the incidence and/or prevalence of MS in North, Central and South American regions.']"
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Background:                  Multiple sclerosis (MS) is the most common cause of neurological disability in young adults worldwide and approximately half of those affected are in Europe. Quality was generally higher in the more recent studies, which also tended to use current diagnostic criteria. Prevalence and incidence estimates tended to be higher in the more recent studies and were higher in the Nordic countries and in northern regions of the British Isles. Few studies examined ethnicity. Epidemiological data at the national level was uncommon and there were marked geographical disparities in available data, with large areas of Europe unrepresented and other regions well-represented in the literature. Both incidence and prevalence were calculated per 100,000 people using the BC mid-year population and were age and sex standardized to the 2001 Canadian population, for consistency to prior Canadian work [10, 11, #CITATION_TAG].","The assessment of differential incidence and prevalence across populations can reveal spatial, temporal and demographic patterns which are important for identifying genetic and environmental factors contributing to MS. However, study methodologies vary and the quality of the methods can influence the estimates. Methods:                  A comprehensive literature search was performed to obtain all original population-based studies of MS incidence and prevalence in European populations conducted and published between January 1985 and January 2011. Only peer-reviewed full-text articles published in English or French were included. All abstracts were screened for eligibility and two trained reviewers abstracted the data and graded the quality of each study using a tool specifically designed for this study.",['This study aimed to systematically review European studies of incidence and prevalence of MS and to provide a quantitative assessment of their methodological quality.']
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Survival has improved for both the BC general population and for people with MS in BC over the past 30 years [#CITATION_TAG].","Methods Clinical and demographic data of MS patients registered with the British Columbia MS clinics (1980-2004) were linked to provincial death data, and patients were followed until death, emigration or study end (31 December 2007). Mortality relative to the general population was examined using standardised mortality ratios. Excess mortality associated with patient characteristics and time period of cohort entry was assessed by relative survival modelling.",['Objective To examine mortality and factors associated with survival in a population based multiple sclerosis (MS) cohort.']
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Searching for local-level causes of the disease may therefore not be as productive as investigating etiological factors operating at the population level. Similar observations have been made in the past [30] [31] [32] [33] [#CITATION_TAG], although others have reported either no relationship or a negative association with SES [35].","By using administrative health data, we identified all incident cases of MS in Manitoba from 1990 to 2006 (n = 2,290) and geocoded them to 230 neighborhoods in the City of Winnipeg and 268 municipalities in rural Manitoba. Age-standardized incidence rates for 1990-2006 (combined) were calculated for each region. By using the spatial scan statistic, we identified high-rate clusters in southwestern (incidence rate ratio (IRR) = 1.48) and central Winnipeg (IRR = 1.54) and low-rate clusters in north-central Winnipeg (IRR = 0.52) and northern Manitoba (IRR = 0.48).","['In this study, we describe the geospatial variation in the incidence of multiple sclerosis (MS) in Manitoba, Canada, and the sociodemographic characteristics associated with MS incidence.', 'This may require an examination of macro-level differences in environmental exposures between high- and low-incidence regions of the world.']"
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Similar observations have been made in the past [30] [31] [#CITATION_TAG] [33] [34], although others have reported either no relationship or a negative association with SES [35].","Ninety-three Israeli-born MS patients identified in country-wide studies and 94 age- and sex-matched controls were interviewed. The questionnaire covered a large span of factors at ages 0,10 and onset of the disease, with particular emphasis on socioeconomic status (SES) and sanitary conditions (SAN).",['This case-control study was aimed at identifying environmental risk factors for multiple sclerosis (MS).']
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Similar observations have been made in the past [30] [#CITATION_TAG] [32] [33] [34], although others have reported either no relationship or a negative association with SES [35].","In previous papers of this series, we explored the epidemiology of MS, examining the effects of race, sex, geography, latitude and climate, migration, age at onset, population ancestry, and individual ethnicity on the risk of MS, using an unusually large cohort of MS cases and pre-illness matched controls comprising US veterans of World War II (WWII) and the Korean Conflict (KC).","['In this paper, we examine primarily the effect of other factors on the risk of MS in this cohort and their relation to those previously studied.']"
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. The International Panel on MS Diagnosis presents revised diagnostic criteria for multiple sclerosis (MS). The rising prevalence in BC cannot be explained by increasing numbers of new MS cases; our incidence rates remained relatively stable over the 13-year period despite changes in MS diagnostic criteria [#CITATION_TAG] and increasing availability of disease-modifying drugs.","Magnetic resonance imaging is integrated with clinical and other paraclinical diagnostic methods. The revised criteria facilitate the diagnosis of MS in patients with a variety of presentations, including ""monosymptomatic"" disease suggestive of MS, disease with a typical relapsing-remitting course, and disease with insidious progression, without clear attacks and remissions.",['The focus remains on the objective demonstration of dissemination of lesions in both time and space.']
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. ABSTRACT: A province wide prevalence study on multiple sclerosis (MS) was conducted in British Columbia (B.C.). These rates are among the highest reported in Canada or elsewhere. neurologists made this study unique in its scope and accuracy of diagnosis. Other than a study based on self-reported MS [21], the last province-wide estimate of MS prevalence, which used clinically confirmed definitions, was 93.3/100,000 in 1982 [#CITATION_TAG].","239,412 neurologists' files were hand searched by one researcher using modified Schumacher criteria for classification. Other sources used during the study for identifying MS patients were the MS Clinic, general practitioners, ophthalmologists, urologists, specialized facilities such as long term care facilities and rehabilitation centres, and patient self-referrals. A total of 4,620 non-duplicated cases were identified and classified.",['The major portion of this study was a review of all the files of neurologists practicing in B.C.']
"Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Prevalence ranged from 180 (95%CI: 90/260) in Quebec to 350 (95%CI: 230/470) in Atlantic Canada. Other than a study based on self-reported MS [#CITATION_TAG], the last province-wide estimate of MS prevalence, which used clinically confirmed definitions, was 93.3/100,000 in 1982 [22].","Methods: This study used data from the Canadian Community Health Survey, a large general health survey (n/131,535) conducted in 2000/2001. Subjects aged 18 and over were included in the current analysis (n/116,109). Prevalence was computed in five regions (Atlantic, Quebec, Ontario, Prairies and British Columbia). Logistic regression was used to compare regions and examine for confounding/interaction by age and sex. Logistic regression revealed no statistical difference between the odds of MS in Quebec, Ontario and British Columbia adjusted for age and sex.","['Objective: To describe the regional distribution of multiple sclerosis (MS) prevalence in Canada, controlling for age and sex.']"
"Implementation of adequate conversational structures is a key issue in developing successful interactive user interfaces. A way of testing the adequacy of the structures is to prove the correct orientation of each communicative action towards a preceding action. The terms cognitive affordance (Norman's perceived affordance) and physical affordance (Norman's real affordance) refer to parallel and equally important usability concepts for interaction design, to which sensory affordance plays a supporting role. We argue that the concept of physical affordance carries a mandatory component of utility or purposeful action (functional affordance). Another attempt to extend and refine Norman's concept of real and perceived affordance came from Hartson [#CITATION_TAG].","We define and use four complementary types of affordance in the context of interaction design and evaluation: cognitive affordance, physical affordance, sensory affordance, and functional affordance.","[""In reaction to Norman's (1999) essay on misuse of the term affordance in human-computer interaction literature, this article is a concept paper affirming the importance of this powerful concept, reinforcing Norman's distinctions of terminology, and expanding on the usefulness of the concepts in terms of their application to interaction design and evaluation.""]"
"The crucial question in the public debate of extreme events is increasingly whether and to what extent the event has been caused by anthropogenic warming. This study thus highlights the challenges of probabilistic event attribution of complex weather events and identifies Part of the EQUIP special issue of Climatic Change This article is part of a Special Issue on ""Managing Uncertainty in Predictions of Climate and Its Impacts"" edited by Andrew Challinor and Chris Ferro. Not every event is linked to climate change. The rainfall associated with the devastating Thailand floods can be explained by climate variability. But long-term warming played a part in the others. While La Nina contributed to the failure of the rains in the Horn of Africa, an increased frequency of such droughts there was linked to warming in the Western Pacific- Indian Ocean warm pool. Europe's record warm temperatures would probably not have been as unusual if the high temperatures had been caused only by the atmospheric flow regime without any long-term warming. One of the most recent series of studies in this field (#CITATION_TAG; Peterson et al. 2013) attempts to answer this question for several extreme events that occurred in 2011 and 2012, with one of the studies being an attribution study of exceptionally warm Novembers in central England (Massey et al. 2012), which explicitly applies PEA.",Calculating how the odds of a particular extreme event have change...,"['To help foster the growth of this science, this article illustrates some approaches to answering questions about the role of human factors, and the relative role of different natural factors, for six specific extreme weather or climate events of 2011.']"
"The crucial question in the public debate of extreme events is increasingly whether and to what extent the event has been caused by anthropogenic warming. This study thus highlights the challenges of probabilistic event attribution of complex weather events and identifies Part of the EQUIP special issue of Climatic Change This article is part of a Special Issue on ""Managing Uncertainty in Predictions of Climate and Its Impacts"" edited by Andrew Challinor and Chris Ferro. The 2010 summer heat wave in western Russia was extraordinary, with the region experiencing the warmest July since at least 1880 and numerous locations setting all-time maximum temperature records. Studies into the European heat wave of 2003 (Stott et al. 2004, the England and Wales floods of 2000 (Pall et al. 2011), and the Russian heat wave of 2010 (#CITATION_TAG; Rahmstorf and Coumou 2011; Otto et al. 2012) have sought to determine to what extent the risks of these events occurring have changed because of anthropogenic greenhouse gas emissions, many of them using the emerging method of probabilistic event attribution (PEA).","Model simulations and observational data are used to determine the impact of observed sea surface temperatures (SSTs), sea ice conditions and greenhouse gas concentrations.",['This study explores whether early warning could have been provided through knowledge of natural and human-caused climate forcings.']
"The new mobile wireless computer technologies and social media applications using Web 2.0 platforms have recently received attention from those working in health promotion as a promising new way of achieving their goals of preventing ill-health and promoting healthy behaviours at the population level. There is very little critical examination in this literature of how the use of these digital technologies may affect the targeted groups, in terms of the implications for how individuals experience embodiment, selfhood and social relationships. It is argued that m-health technologies produce a digital cyborg body. This article addresses these issues, employing a range of social and cultural theories to do so. Increasingly intense, multifaceted and integrated forms of surveillance are a central  feature of Western national security attempts to counter the violence of 'Islamic  terrorism'. However, there has been a lack of research examining contemporary regimes  of surveillance as profoundly racialized. Such a regime of mass  surveillance effectively puts all Muslims under suspicion. This conditions a paranoid surveillant racism, through which  Muslim populations become modulated as an unknowable threat of death and  destruction As this suggests, central to a critical analysis of the use of the new social media and mobile devices to promote health is a recognition of these technologies as part of 'surveillance society', a term used by some writers to denote the increasing ubiquity of surveillance technologies in everyday life which are used to record, survey, monitor and discipline people (for example, #CITATION_TAG; Lyon, 2007; Lyon, 2010; Bennett, 2011).","We  advance an analysis of a racialized surveillant assemblage, that is generative of mutable,  algorithmically determined profiles of the Muslim-as-terrorist. We highlight that,  paradoxically, mass data-mining operations stifle, rather than aid, the identification of  actual terrorist threats.","['This study examines how counter-terrorism  efforts are underpinned by ill-conceived accounts of radicalisation that pre-emptively  construct Muslim-migrants as a threat to national security, thereby justifying practices  of mass surveillance that further propagate racist discourses of uncertainty and risk.']"
"Traditional theories of forgetting are wedded to the notion that cue-overload interference procedures (often involving the A-B, A-C list-learning paradigm) capture the most important elements of forgetting in everyday life. However, findings from a century of work in psychology, psychopharmacology, and neuroscience converge on the notion that such procedures may pertain mainly to forgetting in the laboratory and that everyday forgetting is attributable to an altogether different form of interference. In contrast, theories grounded in the notion of retroactive interference (#CITATION_TAG; see also Mednick, Cai, Shuman, Anagnostaras, & Wixted, 2011) insist that during slow-wave sleep synaptic plasticity in the hippocampus is null.",,"['According to this idea, recently formed memories that have not yet had a chance to consolidate are vulnerable to the interfering force of mental activity and memory formation (even if the interfering activity is not similar to the previously learned material).']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. The diffusion of innovation has been defined traditionally as the process by which an innovation is communicated through a social system to its members. The diffusion process is one of the most widely studied of all social processes with research in all of the social sciences education geography and business. Such widespread interest results in frequent new findings but also has the disadvantage that researchers interested in the subject must keep up with and know the language of several disciplines. The process of diffusion has been linked to characteristics of the innovation itself, the social system (community of potential adopters), channels of communication, and time (Rogers, 1995; #CITATION_TAG).",The 4 parts of the diffusion process are: 1) the innovation 2) channels of communication 3) time and 4) the social system. Chapter 1 introduces basic concepts such as the S-shaped curve. Chapter 2 presents a fundamental deterministic diffusion model that allows for the review and integration of several widely cited models: the external-influence model the internal-influence model and the mixed-influence model. Chapter 3 discusses flexible diffusion models which are pattern-sensitive and can accommodate a variety of diffusion patterns. Chapter 4 includes selected advanced diffusion models such as dynamic models multi-innovation models space and time models and models that incorporate change agents. Chapter 5 discusses the applications of diffusion models in different contexts and disciplines.,"['This manual provides an introduction that brings together findings from several disciplines and sets them out in a clear and consistent manner.', 'This chapter also emphasizes both the explicit and the implicit assumptions underlying the fundamental model.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. The urban-rural divide is increasing in modern societies calling for geographical extensions of social influence modelling. Diffusion of a new idea, product or service is defined as the spread of its use in a population of potential adopters (#CITATION_TAG; King et al., 1994).","The spreading process is modelled within the Bass diffusion framework that enables us to compare the differential equation version with an agent-based version of the model run on the empirical network. Although both models can capture the macro trend of adoption, they have limited capacity to describe the observed trends of urban scaling and distance decay. Controlling for the threshold distribution enables us to eliminate the bias induced by local network structure on predicting local adoption peaks.","['In this work, we analyze the spatial adoption dynamics of iWiW, an Online Social Network (OSN) in Hungary and uncover empirical features about the spatial adoption in social networks.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. European Journal of Information Systems (2007) 16, 103-105. doi:10.1057/palgrave.ejis.3000678 According to a report by Kable (a leading provider of public sector research), U.K. central government information and communication technology (ICT) spend, which represents one of the largest in the world, will grow by 21% over the next 3 years reaching d3.2 billion by 2007/2008 and is expected to reach d4.2 in 2010/2011 (Kable, 2006). In contrast, U.K. local authorities are expected to deliver a total of d1.2 billion in accumulated efficiency savings by 2007/2008 directly as a result of such e-Government investments (IDABC, 2005). However, the parliamentary office of science and technology (POST) recently reported that the cost of cancelled or over-budget government ICT projects over the last 6 years is greater than d1.5 billion (POST, 2003). Clearly, questioning normative approaches to e-Government provision building and the appropriateness of existing toolsets. Therefore, there is much need as reflected by Irani et al. Innovations in ICT offer rich opportunities for governments to significantly improve the delivery of their services and to interact more openly with their constituents. High-quality experiences with responsive, integrated private sector information systems have led citizens to expect the same kinds of experience from public bodies and agencies (Hazlett and Hill, 2003). Consequently, citizens and businesses are demanding more effective and efficient delivery of services as well as an improvement in the quality of information received (Ongaro, 2004). While ICT can be used to transform the way in which governments offer their services, it should be acknowledged that a detailed business case for implementing the technology must be undertaken with emphasis on cultural change, greater degree of commitment, organisational structure and business processes (Liu and Hwang, 2003). However, e-Government is not a simple matter. Although governments have eagerly looked forward to a digital future since the mid-1990s, their efforts to turn vision into reality have come up against a variety of challenges (Ke and Wei, 2004). According to Strejcek and Theil (2002), many ICT projects initiated by governments fail because they are poorly coordinated and because agencies act too independently. Others believe that e-Government is an evolutionary phenomenon and that therefore e-Government initiatives should be derived and implemented accordingly (Layne and Lee, 2001; Lee et al., 2005). Nonetheless, learning from the past and from the experiences of others is essential for improving the way in which governments can deliver better services. Yet, as innovations in technology emerge, governments will be faced with new demands and challenges. This special issue of European Journal of Information Systems presents a series of papers that examine the past, present and future aspects of e-Government. Yao and Murphy argue that the intention to use a voting technology can affect a voter's decision to participate or not in the election process. In many western countries, it is especially local governments that are developing one-stop shops that serve as a point-of-entry to the whole range of government (Ling, 2002; Ho, 2002; Beynon-Davies and Martin, 2004; #CITATION_TAG) in such a way that the relationship between public sector organisations and citizens is re-engineered (Beynon- Davies and Martin, 2004).","We begin with a paper examining how user perceptions of the characteristics of REVS (Remote Electronic Voting Systems) such as availability, mobility, accuracy, privacy protection and ease of use, affect their intention to use it.",['(2005) to explore e-Government capacity building such that value for the taxpayer is realised.']
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. Abstract Following on from five years of 'electronic government', the Labour Government has recently announced a new five year plan for 'transformational government'. Like its predecessor, t-government emphasises the important role of information technology in enabling the delivery of modernised public services. Modernisation is defined as an increasing emphasis on citizen choice, personalisation of services and understanding and responding to service user needs. Arguably the most significant citizen-focused technology of the e-government era was customer relationship management. Recently, the idea of integration was further pushed by it being given strong support (OECD, 2009) and the related academic discussion of personalised integrated services (#CITATION_TAG; Peterson et al., 2007; Homburg and Dijkshoorn, 2011).",,['This paper explores the appropriateness of the t-government agenda by drawing upon lessons learned from the preceding e-government era.']
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. One respondent explained how one's own organisation could serve as a source of relevant knowledge (see also #CITATION_TAG):",,['The purpose of this research was to develop a model of how managers construe organizational events as a change unfolds.']
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. When the school opened in 1971, the existing 86 medical schools all offered similar programs: two years of basic science training in lecture halls and laboratories, followed by two years of direct contact with patients in clinical settings. In the new school, students were taught didactically only during the first year. Hence, institutionalism emphasises the persuasive control over the practices, beliefs and belief systems of individuals or organisations through an institution's sway (#CITATION_TAG, in King et al., 1994.","During the second year, each student was assigned to a community physician who acted as an advisor and who discussed with students those patients afflicted with the diseases the student was currently studying. The author found the case of this medical school to be of particular interest from an organizational viewpoint in that: (1) the early development of the school was shaped by the first dean's entrepreneurial activity, ambitions, visions, strengths, and weaknesses; (2) the uncertainty resulting from the school's novelty forced individuals to assume new roles and face unclear performance criteria; and (3) the transition of an innovative school to an institutionalized one was problematic because it modified the decision-making process. The author suggests that those things which lead to an organization's success during its early years are not the same as those that lead to longer-run success. A comparative analysis of the birth, life, and death of organization is advocated.","['Kimberly presents a case study of the birth and early development of an innovative medical school.', 'He says that a new organization creates new norms, values, and procedures whereas the elements of an existing organization interact within an established culture.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. Thus, a key problem for theory and research is to specify the conditions under which behavior is more likely to resemble one end of this continuum or the other. In short, what is needed are theories of when rationality is likely to be more or less bounded. Advancements in the discipline of organisational sociology in recent decades, such as the emergence of 'new institutionalism ' (DiMaggio and Powell, 1983; #CITATION_TAG), have highlighted the significance of the professional and/or legal rules, cognitive structures, norms and the prevailing values in which innovation takes place.","The former is premised on the assumption that individuals are constantly engaged in calculations of the costs and benefits of different action choices, and that behavior reflects such utility-maximizing calculations. In the latter model, by contrast, \u27oversocialized\u27 individuals are assumed to accept and follow social norms unquestioningly, without any real reflection or behavioral resistance based on their own particular, personal interests. We suggest that these two general models should be treated not as oppositional but rather as representing two ends of a continuum of decision-making processes and behaviors.","['[Excerpt] Our primary aims in this effort are twofold: to clarify the independent theoretical contributions of institutional theory to analyses of organizations, and to develop this theoretical perspective further in order to enhance its use in empirical research.', 'There is also a more general, more ambitious objective here, and that is to build a bridge between two distinct models of social actor that underlie most organizational analyses, which we refer to as a rational actor model and an institutional model.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. Since its publication in 2000, Public Management Reform has established itself as the standard text in the field, presenting a comparative analysis of recent changes in Public Management and Public Administration in a range of countries in Europe, North America, and Australasia. In order to be able to explain the diffusion, we focus on a population of 441 potential adopters in a single national jurisdiction, the Netherlands, which can be seen as a decentralised unitary state (Esping- Andersen, 1990; #CITATION_TAG).","This completely rewritten second edition radically expands, develops, and updates the original. Empirical data has been brought up to date, so as to cover many key developments of the last few years. The theoretical framework of the book has been further developed, including a challenging new interpretation of the trends in continental Europe, which are seen here as markedly different from the Anglo-American style 'New Public Management'. It is organized in an integrated format, within an overall theoretical framework that identifies the main pressures for, and trajectories of, change. It includes a multi-dimensional analysis of the results of reform, and a chapter reflecting on the dynamic relationship between management reform and politics.","['This second edition provides an unparalleled synthesis of developments in Australia, Belgium, Canada, Finland, France, Germany, Italy, the Netherlands, New Zealand, Sweden, the UK, the USA, and the European Commission.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. The role of local governments in attracting roots tourists is one of most important factors analyzed in the studies of diaspora tourism. Governments of several countries have actively sought to promote varied forms of roots tourism in order to attract members of their respective diasporas. In contrast, African American roots tourism in Brazil is marked by the almost complete inaction of the government, at both the state and federal levels. This type of tourism was initiated and continues to develop largely as the result of tourist demand, and with very little participation on the part of the state. Our hope is that, by focusing on 'agency' alongside 'structure' (Orlikowksi and Barley, 2001), more light will be shed on the process of technological and organisational change (#CITATION_TAG) such that, eventually, the diffusion of egovernment will be better understood.","The chapter also analyzes whether the left-leaning Workers' Party, then in charge of the state government, challenged the longstanding discourse of baianidade (Bahianness) that has predominantly represented blackness (in tourism and other realms) through domesticated and stereotypical images.","['This chapter analyzes the belated response of the state government of Bahia to African American tourism, examining how the inertia that dominated since the late 1970s was later replaced by a more proactive, although still inadequate, position, when the state tourism board, Bahiatursa, founded the Coordination of African Heritage Tourism to cater specifically to the African American roots tourism niche.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. As knowledge brokers, IT professionals see themselves as facilitating the flow of knowledge about both IT and business practices across the boundaries that separate work units within organizations. Brokering practices include gaining permission to cross organizational boundaries, surfacing and challenging assumptions made by IT users, translation and interpretation, and relinquishing ownership of knowledge. Consequences of brokering are the transfer of both business and IT knowledge across units in the organization From fieldwork observations, and informed by our theoretical discussion of Scandinavian Institutionalism, we could see that translation, transfusion and repackaging of knowledge and ideas does not take place in a vacuum, but is a social integration process in which specific actors play a role (Czarniawska and Sevon, 2005; see also #CITATION_TAG).","A qualitative analysis of interviews conducted with 23 IT professionals and business users in a large manufacturing and distribution company is summarized in a conceptual framework showing the conditions, practices, and consequences of knowledge brokering by IT professionals. The framework suggests that brokering practices are conditioned by structural conditions, including decentralization and a federated IT management organization, and by technical conditions, specifically shared IT systems that serve as boundary objects.","['This interpretive case study examines knowledge brokering as an aspect of the work of information technology professionals.', 'The purpose of this exploratory study is to understand knowledge brokering from the perspective of IT professionals as they reflect upon their work practice.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. However, a striking similarity is the resistance of municipalities to the coordination approaches. Dutch municipal governments are relatively autonomous vis-à-vis central government with respect to management issues, including the design, implementation and maintenance of electronic services ( #CITATION_TAG).",Data were collected through content analyses and interviews.,"[""The objective of this article is to present an insight into how Denmark and the Netherlands seek to coordinate the integration of electronic service deliveryand examines the similarities and differences between the two countries' approaches."", 'The focus is on ICT projects, where central government and municipalities both participate in integrated digital solutions.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. The governance of an organizational information technology (IT) infrastructure is steadily shifting away from pure hierarchical and market mechanisms toward hybrid and partnership modes that involve external vendors. In particular, IT outsourcing has recently emerged as a significant administrative innovation in an organization's IT strategy. Innovation through mimicry is likely to occur when innovations are socially visible (Mahajan and Peterson, 1985; Dos Santos and Peffers, 1998), when causes, conditions and consequences are known (absence of causal ambiguity, Barney, 1991; #CITATION_TAG) and when the success of the innovation is unlikely to be determined by path dependencies (Barney, 1991; Loh and Venkatraman, 1992).","For this purpose, we generated a comprehensive sample of outsourcing contracts in the US using an electronic bibliometric search process. Subsequently, we considered the widely-publicized Eastman Kodak's outsourcing decision as a critical event to assess whether this internal influence is more pronounced in the post- Kodak regime than in the pre- Kodak regime.",['This paper seeks to explore the sources of influence in the adoption of this innovation.']
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. Literature reports the experiences with e-government initiatives as chaotic and unmanageable, despite recent numerous initiatives at different levels of government and academic and practitioners' conferences on e-government. E-government presents a number of challenges for public administra-tors. Various authors have argued that various 'stages' or 'levels of maturity' can be discerned in electronic service delivery (#CITATION_TAG; Anderson and Henriksen, 2005), ranging from public organisations offering one-way transmission of static information, through organisations offering online transactions, to organisations offering integrated services (which incorporate information from sources external to the organisation and through which the citizen may interact in the first place).",These stages outline the multi-perspective transformation within government structures and functions as they make transitions to e-government through each stage. Technological and organizational challenges for each stage accompany these descriptions.,"[""To help public administrators think about e-government and their organizations, this article describes different stages of e-government development and proposes a 'stages of growth ' model for fully functional e-government.""]"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. Change in the organization is usually analysed in the context of the organization in which the change is taking place. Change is always relative, but not simply to what the organization has done before. What appears to be major progress from an organizational perspective may look like stagnation in a wider context. In particular, they seem to have paid little attention to the essential contribution of external information to internal change. Information is so fundamental to the learning required for deliberate change that it is not unreasonable to see change as an information process. Change in the organization is seen as a process in which the finding and acquisition of external information are critical. These activities may be---and sometimes must be---beyond the control of the organization. Thus, those who would manage change face the challenge of managing without control. Framing: translating pressure into local priorities and opportunities According to Sahlin and Wedlin (2008; see also Silva and Hirscheim, 2007), and in line with the Scandinavian Institutionalism mentioned in the discussion on theoretical antecedents of diffusion, knowledge and ideas cannot simply be transfused from one organisation to the other: rather, ideas, concepts and knowledge has to be repackaged and re-embedded (Isabella, 1990; #CITATION_TAG; Sahlin and Wedlin, 2008).","Organizational change should also be relative to what is going on outside the organization. So, too, is mixing these external bits of information with those already in use within the organization. The information perspective indicates that organizational change is largely dependent on the information activities of individual employees acting on their own account as much as that of the organization. The information perspective suggests that the concentration on structure and control inherent in notions of the learning organization may be mimical to the real learning required for organizational change.","['This paper suggests that notions of the ""learning organization"" have often emphasied internal aspects of the change process and neglected the external.', 'This view affords an information perspective of change in the organization, and a revealing contrast to the usual organizational perspective.', 'It has profound implications for those who seek to understand and manage the process of change.']"
"In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. In the nineteen-eighties, the Tessec expert system was developed and several studies showed that this system could improve administrative decision making under the Netherlands' General Assistance Act. Now, at the beginning of the 21st century, there is a series of new expert systems on the Dutch market, the MR-systems. There is however one important difference: the MR-systems are widely used. This shift has facilitated the adoption of legal expert systems. Implementing technology in anticipation of future audits has also been reported by #CITATION_TAG in a study on the adoption of legal expert systems in municipal social security systems.",,"['In this article, the author tries to find a sound explanation for both the failure of Tessec, and the success of the MR systems.', 'As he argues, this explanation can be found in a shift in attitude towards the role of legislation during the past decade.']"
"It argues that the analysis of the US Interagency Working Group on Social Cost of Carbon did not go far enough into the tail of low-probability, high-impact scenarios, and, via its approach to discounting, it mis-estimated climate risk, possibly hugely. This note considers the treatment of risk and uncertainty in the recently established 'social cost of carbon' (SCC) for analysis of federal regulations in the United States. In response to the increasing impact of regulation, several governments have introduced economic analysis as a way of trying to improve regulatory policy. We find that there is growing interest in the use of economic tools, such as benefit-cost analysis; however, the quality of analysis in the U.S. and European Union frequently fails to meet widely accepted guidelines. Perhaps unsurprisingly, the reality of carrying out benefit-cost analysis of federal regulations falls short of best practice, and it does not appear to be having a significant impact on many regulatory decisions, except higher-profile cases (#CITATION_TAG).",,"['This paper provides a comprehensive assessment of government-supported economic analysis of regulation.', 'To address this situation, we recommend pursuing an agenda in which economics plays a more central role in regulatory decision making.']"
"It argues that the analysis of the US Interagency Working Group on Social Cost of Carbon did not go far enough into the tail of low-probability, high-impact scenarios, and, via its approach to discounting, it mis-estimated climate risk, possibly hugely. This note considers the treatment of risk and uncertainty in the recently established 'social cost of carbon' (SCC) for analysis of federal regulations in the United States. Economic evaluation of climate policy traditionally treats uncertainty by appealing to expected utility theory. Yet our knowledge of the impacts of climate policy may not be of sufficient quality to be described by unique probabilistic beliefs. In such circumstances, it has been argued that the axioms of expected utility theory may not be the correct standard of rationality. By contrast, several axiomatic frameworks have recently been proposed that account for ambiguous knowledge. In the present context, this means that the decision-maker focuses more on models yielding higher estimates of the SCC (see also #CITATION_TAG).",,"['In this paper, we apply static and dynamic versions of a smooth ambiguity model to climate mitigation policy.']"
"It argues that the analysis of the US Interagency Working Group on Social Cost of Carbon did not go far enough into the tail of low-probability, high-impact scenarios, and, via its approach to discounting, it mis-estimated climate risk, possibly hugely. This note considers the treatment of risk and uncertainty in the recently established 'social cost of carbon' (SCC) for analysis of federal regulations in the United States. Climate change is a serious and urgent issue. The Earth has already warmed by 0.7degC since around 1900 and is committed to further warming over coming decades simply due to past emissions. On current trends, average global temperatures could rise by 2-3degC within the next fifty years or so, with several degrees more in the pipeline by the end of the century if emissions continue to grow. affect the essential components of lives and livelihoods of people around the world -- water supply, food production, human health, availability of land, and ecosystems. It looks in particular at how these impacts intensify with increasing amounts of warming. The latest science suggests that the Earth's average temperature will rise by even more than 5 or 6degC if feedbacks amplify the warming effect of greenhouse gases through the release of carbon dioxide from soils or methane from permafrost. In general, impact studies have focused predominantly on changes in average conditions and rarely examine the consequences of increased variability and more extreme weather. In addition, almost all impact studies have only considered global temperature rises up to 4 or 5degC and therefore do not take account of threshold effects that could be triggered by temperatures higher than 5 or 6degC Doing so permits greater confidence that the target will be met, while the existence of a long-run quantity target in the first place can be supported by reasoning about the efficiency of price and quantity instruments under uncertainty (#CITATION_TAG).","Throughout the chapter, changes in global mean temperature are expressed relative to pre-industrial levels (1750-1850). The chapter builds up a comprehensive picture of impacts by incorporating two effects that are not usually included in existing studies (extreme events and threshold effects at higher temperatures).","['This chapter examines the increasingly serious impacts on people as the world warms.', 'This chapter examines how the physical changes in climate .']"
"It argues that the analysis of the US Interagency Working Group on Social Cost of Carbon did not go far enough into the tail of low-probability, high-impact scenarios, and, via its approach to discounting, it mis-estimated climate risk, possibly hugely. This note considers the treatment of risk and uncertainty in the recently established 'social cost of carbon' (SCC) for analysis of federal regulations in the United States. A critical issue in climate change economics is the specification of the so-called ""damages function"" and its interaction with the unknown uncertainty of catastrophic outcomes. In these examples, the primary reason for keeping GHG levels down is to insure against high-temperature catastrophic climate risks.Economic A recent paper by Ackerman and Stanton (2011) attempts to answer it using the DICE model, applying a functional form proposed by #CITATION_TAG.",The paper gives some numerical examples of the indirect value of various greenhouse gas (GHG) concentration targets as insurance against catastrophic climate change temperatures and damages. These numerical exercises suggest that we might be underestimating considerably the welfare losses from uncertainty by using a quadratic damages function and/or a thin-tailed temperature distribution.,['This paper asks how much we might be misled by our economic assessment of climate change when we employ a conventional quadratic damages function and/or a thin-tailed probability distribution for extreme temperatures.']
"It argues that the analysis of the US Interagency Working Group on Social Cost of Carbon did not go far enough into the tail of low-probability, high-impact scenarios, and, via its approach to discounting, it mis-estimated climate risk, possibly hugely. This note considers the treatment of risk and uncertainty in the recently established 'social cost of carbon' (SCC) for analysis of federal regulations in the United States. Much of the economic analysis of climate change revolves around two big questions: What is the economic cost associated with the impacts of climate change under alternative GHG emissions scenarios? What is the economic cost of reducing GHG emissions? The economic aspect of the policy debate intensified with the publication in the UK of the Stern Review of the Economics of Climate Change (Stern, 2006). This conclusion has been criticized by many economists, particularly in the United States, where Professor William Nordhaus of Yale, the leading American expert on climate economics, concludes that the economically optimal policy involves only a modest rate of emission reduction in the near term, followed by larger reductions later (Nordhaus 2008). The disagreement between Stern and Nordhaus has aroused considerable interest. Much of the existing discussion focuses on the difference in the discount rate - Stern uses a consumption rate of discount average 1.4% per annum, while Nordhaus uses one averaging 4%. 1 However, I believe that another important factor is the difference in the raw assessment of undiscounted damages from climate change. Furthermore, they also question the damage estimates of the models at low temperatures, drawing on work by Michael #CITATION_TAG that argues damages could also be significantly higher in this realm.",,"['Because of limited space, that difference is the focus of this chapter.']"
"It argues that the analysis of the US Interagency Working Group on Social Cost of Carbon did not go far enough into the tail of low-probability, high-impact scenarios, and, via its approach to discounting, it mis-estimated climate risk, possibly hugely. This note considers the treatment of risk and uncertainty in the recently established 'social cost of carbon' (SCC) for analysis of federal regulations in the United States. There is an increasing demand for putting a shadow price on the environment to guide public policy and incentivize private behaviour. In practice, setting that price can be extremely difficult as uncertainties abound. There is often uncertainty not just about individual parameters but about the structure of the problem and how to model it. Using recent reviews of the literature, we concluded that the range of estimates of the present SCC was a factor of ten larger than the corresponding range of estimates of the present MAC (#CITATION_TAG).","We consider how to determine the overall target for environmental protection, how to set shadow prices to deliver that target, and how we can learn from the performance of policies to revise targets and prices.","['In this paper, we propose some practical steps for setting prices in the face of these difficulties, drawing on the example of climate change.']"
"Synthetic biology aims at reconstructing life to put to the test the limits of our understanding. The use of this still elusive category permits us to interact with reality via construction of self-consistent models producing predictions which can be instantiated into experiments. While the present theory of information has much to say about the program, with the creative properties of recursivity at its heart, we almost entirely lack a theory of the information supporting the machine. The underlying heuristics explored here is that an authentic category of reality, information, must be coupled with the standard categories, matter, energy, space and time to account for what life is. It is argued that the account of Savage-Rumbaugh's ape language research in Savage-Rumbaugh, Shanker and Taylor (1998. Apes, Language and the Human Mind. Oxford University Press, Oxford) is profitably read in the terms of the theoretical perspective developed in Clark (1997. The authors, though, make heavy going of a critique of what they take to be standard approaches to understanding language and cognition in animals, and fail to offer a worthwhile theoretical position from which to make sense of their own data. A great many works dealing with the study of the brain (Edelman 1987), or of cognition (#CITATION_TAG; Ryle 1949) has taken into account this type of information.","This model of 'distributed' cognition helps makes sense of the lexigram activity of Savage-Rumbaugh's subjects, and points to a re-evaluation of the language behaviour of humans","[""The contribution made by Clark's work is to show the range of ways in which cognition exploits bodily and environmental resources.""]"
"Synthetic biology aims at reconstructing life to put to the test the limits of our understanding. The use of this still elusive category permits us to interact with reality via construction of self-consistent models producing predictions which can be instantiated into experiments. While the present theory of information has much to say about the program, with the creative properties of recursivity at its heart, we almost entirely lack a theory of the information supporting the machine. The underlying heuristics explored here is that an authentic category of reality, information, must be coupled with the standard categories, matter, energy, space and time to account for what life is. Locke's account is inadequate because of its failure  to allow any role to an agent who creates or understands the representational relation. Representation occurs mysteriously by means of mediating entities. This inadequacy becomes even more evident in contrast with Aquinas' account of the activity of mind: representation for Aquinas is an act (not an object) of mind. Descartes' conception of 'ideas' is ambivalent between act  and object; and, in examining some of the problems of his account, we shall see the importance of an intentional interpretation of representation: that it is an act of thought about some object. It is not because humans have a sign system that they can achieve 'disengagement' from any particular 'here-and-now' context: other animals also use representative items, but not in the same way as humans do. Something more is required than just the availability of some physical item which can stand in for something else. There are various kinds of signifiers - broadly speaking, those which are used wittingly and those which are  unwittingly used - and what gives a signifier its particular structure is the manner of its use. What kind of act is it, and how is it possible? Trying to match models with reality allowed scientists to progress by producing better and better adequation with reality (Danchin 1992; #CITATION_TAG).","A reading of Aquinas  also illuminates the reasons for the extreme passivity of Locke's account, since its origins are clearly to be seen in the Thomist account, but Locke's version has lost the most important feature of the Thomist account. Part II develops the issues raised earlier within the  particular context of signification and semiotic theory. The semiotic theories of Saussure, Peirce .and Piaget are examined, with occasional reference to Frege. Part II, then, deals with questions about the representative item and how it comes to represent other items.","['Part I raises the problems that will shape our investigation of representation.', 'The theme of Chapters 4 to 6 is that a signifier  is only as good as the intelligence which uses it.', 'Part III focuses on the activity of representation.', 'In Chapter 7 a distinction is made between active and passive senses of representation, and arising  out of this distinction we study the particular case of images, considered as a form of representation.']"
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. The last two decades have seen a surge in the interest in research based on Functional Magnetic Resonance Imaging (fMRI) data. Decoding of cognitive states based on fMRI activation profiles has become a very active topic in this area. fMRI data is very high dimensional and noisy. However, there is a dearth of datasets to work on. Decoding of cognitive states using classifiers trained across multiple subjects is a challenging task because of differences in anatomy and cognition. Beginnings are starting to be made in reproducing data across as well as within subjects (#CITATION_TAG; Raizada and Connolly, 2012).","Selecting features to analyze from the dataset is a key step in the analysis of fMRI data. In this paper we apply PCA, ICA and five non-linear dimensionality reduction techniques to the fMRI data. The reduced datasets are then used to train classifiers to solve a multiple-subject decoding problem.",['The aim of this work is to analyze which technique can provide the best feature selection to capture the commonality across multiple subjects.']
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Multiple previous studies suggest that activity in the subgenual anterior cingulate cortex (sgACC; Brodmann area 25) predicts outcome in CT for depression, but these results have not been prospectively replicated. Subgenual anterior cingulate activity, in particular, may reflect processes that interfere with treatment (eg, emotion generation) in addition to its putative regulatory role; alternately, its absence may facilitate treatment response. Pretreatment levels of cingulate activity can even predict response to CBT with some reliability (Konarski et al., 2009; Ritchey et al., 2011; #CITATION_TAG).","DESIGN Two inception cohorts underwent assessment with functional magnetic resonance imaging using different scanners on a task sensitive to sustained emotional information processing before and after 16 to 20 sessions of CT, along with a sample of control participants who underwent testing at comparable intervals.",['OBJECTIVE To examine whether sgACC activity is a reliable and robust prognostic outcome marker of CT for depression and whether sgACC activity changes in treatment.']
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Recent research in multivoxel pattern-based fMRI analysis has led to considerable success at decoding within individual subjects. However, the goal of being able to decode across subjects is still challenging: It has remained unclear what population-level regularities of neural representation there might be. On the contrary, to decode across subjects, it is beneficial to abstract away from subject-specific patterns of neural activity and, instead, to operate on the similarity relations between those patterns: Our new approach performs decoding purely within similarity space. Beginnings are starting to be made in reproducing data across as well as within subjects (Accamma and Suma, 2012; #CITATION_TAG).",,"['A central goal in neuroscience is to interpret neural activation and, moreover, to do so in a way that captures universal principles by generalizing across individuals.', 'Here, we present a novel and highly accurate solution to this problem, which decodes across subjects between eight different stimulus conditions.', 'The key to finding this solution was questioning the seemingly obvious idea that neural decoding should work directly on neural activation patterns.']"
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. For example, very similar changes in those brain areas are seen after CBT and other psychological treatments for anxiety (Furmark et al., 2002; Paquette et al., 2003; Straube et al., 2006; #CITATION_TAG; Freyer et al., 2011), schizophrenia (Wykes et al., 2002), eating disorders (Vocks et al., 2011) and",Cognitive-behavioral therapy modified the neural circuits involved in the regulation of negative emotions and fear extinction in judged treatment responders.,['This systematic review aims to investigate neurobiological changes related to cognitive-behavioral therapy (CBT) in anxiety disorders detected through neuroimaging techniques and to identify predictors of response to treatment.']
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. The use of reasoning from activation to mental functions, known as ""reverse inference,"" has been previously criticized on the basis that it does not take into account how selectively the area is activated by the mental process in question. Fairly similar stimuli, for example chairs and shoes (Norman et al., 2006; deCharms, 2008; #CITATION_TAG), or over-riding categories of images such as living or non-living (Naselaris et al., 2012) can now be recognized from pattern information fMRI (Formisano and Kriegeskorte, 2012) data.",,"['A common goal of neuroimaging research is to use imaging data to identify the mental processes that are engaged when a subject performs a mental task.', 'In this Perspective, I outline the critique of informal reverse inference and describe a number of new developments that provide the ability to more formally test the predictive power of neuroimaging data']"
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Questions pertaining to the neurobiological effects of psychotherapy are now considered among the most topical in psychiatry. For example, very similar changes in those brain areas are seen after CBT and other psychological treatments for anxiety (Furmark et al., 2002; #CITATION_TAG; Straube et al., 2006; Porto et al., 2009; Freyer et al., 2011), schizophrenia (Wykes et al., 2002), eating disorders (Vocks et al., 2011) and","In order to do so, fMRI was used in subjects suffering from spider phobia (n = 12) to measure, before and after effective CBT, regional brain activity during the viewing of film excerpts depicting spiders. Normal control subjects were also scanned (once) while they were exposed to the same film excerpts. For normal control subjects (n = 13), only the left middle occipital gyrus and the right inferior temporal gyrus were significantly activated. In phobic subjects before CBT, the activation of the dorsolateral prefrontal cortex (BA 10) may reflect the use of metacognitive strategies aimed at self-regulating the fear triggered by the spider film excerpts, whereas the parahippocampal activation might be related to an automatic reactivation of the contextual fear memory that led to the development of avoidance behavior and the maintenance of spider phobia.","['The goal of the present functional magnetic resonance imaging (fMRI) study, which constitutes the first neuroimaging investigation of the effects of cognitive-behavioral therapy (CBT) using an emotional activation paradigm, was to probe the effects of CBT on the neural correlates of spider phobia.']"
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Little is known, however, about neural bases of the cognitive control of emotion. The present study employed functional magnetic resonance imaging to examine the neural systems used to reappraise highly negative scenes in unemotional terms. There is decreased activity in the limbic system, especially (Benson et al., 1999) Cornelissen et al., 2013 the amygdala, with dorsolateral prefrontal cortex becoming relatively more active and orbitomedial and cingulate cortex less so; a move toward normality from patterns observed before treatment (#CITATION_TAG; Goldapple et al., 2004; Malhi et al., 2004; Ritchey et al., 2011; Höflich et al., 2012) and consistent with what is know of the processing of emotional stimuli (Simpson et al., 2000; Northoff et al., 2004; Leppänen, 2006; Beck, 2008).",Reappraisal of highly negative scenes reduced subjective experience of negative affect.,['&amp; The ability to cognitively regulate emotional responses to aversive events is important for mental and physical health.']
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. For example, very similar changes in those brain areas are seen after CBT and other psychological treatments for anxiety (Furmark et al., 2002; Paquette et al., 2003; Straube et al., 2006; Porto et al., 2009; Freyer et al., 2011), schizophrenia (#CITATION_TAG), eating disorders (Vocks et al., 2011) and",METHOD Three groups (patients receiving control therapy or CRT and a healthy control group) were investigated in a repeated measures design using the two-back test. Brain activation changes were identified after accounting for possible task-correlated motion artefact.,['AIMS To determine whether there are concomitant brain activation changes as a result of engaging in cognitive remediation therapy (CRT).']
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Individuals with bipolar disorder manifest the full spectrum of emotions ranging from depression to mania. The stimuli produced activation in both patients and comparison subjects in brain regions previously implicated in the generation and modulation of affect, in particular the prefrontal and anterior cingulate cortices. The differential patterns of activation inform us about bipolar depression and have potential diagnostic and therapeutic significance. There is decreased activity in the limbic system, especially (Benson et al., 1999) Cornelissen et al., 2013 the amygdala, with dorsolateral prefrontal cortex becoming relatively more active and orbitomedial and cingulate cortex less so; a move toward normality from patterns observed before treatment (Ochsner et al., 2002; Goldapple et al., 2004; #CITATION_TAG; Ritchey et al., 2011; Höflich et al., 2012) and consistent with what is know of the processing of emotional stimuli (Simpson et al., 2000; Northoff et al., 2004; Leppänen, 2006; Beck, 2008).","We therefore examined ten depressed female subjects with bipolar affective disorder, and ten age-matched and sex-matched healthy comparison subjects using functional magnetic resonance imaging (fMRI) while viewing alternating blocks of captioned pictures designed to evoke negative, positive or no affective change. The activation paradigm involved the presentation of the same visual materials over three experiments alternating (experiment 1) negative and reference; (experiment 2) positive and reference and (experiment 3) positive and negative captioned pictures. The activation in patients, when compared with healthy subjects, involved additional subcortical regions, in particular the amygdala, thalamus, hypothalamus and medial globus pallidus.",['In attempting to understand the functional substrates of mood we attempted to identify brain regions associated with the cognitive generation of affect in bipolar depressed patients.']
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. A thorough investigation of the neural effects of psychotherapy is needed in order to provide a neurobiological foundation for widely used treatment protocols. Cognitive behavioural therapy in phobia resulted in decreased activity in limbic and paralimbic areas. However, a one to one correspondence between change in depression and change in specific brain areas may be over stated (#CITATION_TAG; Frewen et al., 2008; Dichter et al., 2012).",,"['This paper reviews functional neuroimaging studies on psychotherapy effects and their methodological background, including the development of symptom provocation techniques.']"
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Changes in neural activity of cortical-limbic regions that subserve hypervigilance and emotion regulation may represent biologically oriented change mechanisms that mediate symptom improvement of CT for IBS. For example, very similar changes in those brain areas are seen after CBT and other psychological treatments for anxiety (Furmark et al., 2002;Paquette et al., 2003;Straube et al., 2006;Porto et al., 2009;Freyer et al., 2011), schizophrenia ( Wykes et al., 2002), eating disorders (Vocks et al., 2011) and Irritable Bowel Syndrome ( #CITATION_TAG et al., 2006)","Five healthy controls and 6 Rome II diagnosed IBS patients underwent psychological testing followed by rectal balloon distention while brain neural activity was measured with O-15 water positron emission tomography (PET) before and after a brief regimen of CT. Pre-treatment resting state scans, without distention, were compared to post-treatment scans using statistical parametric mapping (SPM). Neural activity in the parahippocampal gyrus and inferior portion of the right cortex cingulate were reduced in the post-treatment scan, compared to pre-treatment (x, y, z coordinates in MNI standard space were -30, -12, -30, P=0.017; 6, 34, -8, P=0.023, respectively). Limbic activity changes were accompanied by significant improvements in GI symptoms (e.g., pain, bowel dysfunction) and psychological functioning (e.g., anxiety, worry).",['This study sought to identify brain regions that underlie symptom changes in severely affected IBS patients undergoing cognitive therapy (CT).']
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Little is known about how psychological treatments work. However, certain conceptual and practical difficulties arise when studying psychological treatments, most especially deciding how best to conceptualise the treatment concerned and how to accommodate the fact that most psychological treatments are implemented flexibly. However, evidence is lacking as to what specifically changes as a consequence of psychotherapy (see, for example, #CITATION_TAG).",,"['In this paper, these difficulties are discussed, and strategies and procedures for overcoming them are described.']"
"A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. It is noted that one logical consequence of this is that we would not expect RCTs to be capable of isolating effects that are specific to 'type of therapy' and 'diagnosis'. The outcomes of psychodynamic, personcentred, and behavioral psychotherapy are broadly equivalent despite their varieties of approaches and targets for therapeutic change (Stiles et al., 2008; #CITATION_TAG) perhaps because they work via common final paths (Mansell, 2011).","We note that, despite thirty years of meta-analytic reviews tending to support the finding of therapy equivalence, this view is still controversial and has not been accepted by many within the psychological therapy community; we explore this from a theory of science perspective. It is further argued that the equivalence of ostensibly different therapies is an inevitable consequence of the methodology that has dominated this field of investigation; namely, randomised controlled trials [RCTs]. The implicit assumptions of RCTs are analysed and it is argued that what we know about psychological therapy indicates that it is not appropriate to treat 'type of therapy' and 'diagnosis' as if they were independent variables in an experimental design. Rather, RCTs would only be expected to be capable of identifying the non-specific effects of covariates, such as those of therapist allegiance. It is further suggested that those non-specific effects that have been identified via meta-analysis are not trivial findings, but rather characterise important features of psychological therapy.","[""In this article, the assertion that different psychological therapies are of broadly similar efficacy-often called the 'Dodo Bird Verdict'-is contrasted with the alternative view that there are specific therapies that are more effective than others for particular diagnoses.""]"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Fluctuating fuel prices, rising congestion, longer commutes, and related environmental and human health effects have combined to once more draw the interest of governments, commuters, and firms toward the concept of travel demand management (TDM). While TDM is not new, the proliferation of mobile telephony, fixed Internet, and associated applications has created fresh prospects for the implementation of commuter focused TDM strategies. One recent example is Carpool Zone, an on-line carpool-matching tool deployed and managed by the TDM group at Metrolinx, the regional transportation planning agency within Canada's largest metropolitan region, the Greater Toronto and Hamilton Area. In the literature most authors distinguish between householdbased and non-household-based carpools which are also called internal and external carpools respectively (#CITATION_TAG; Correia and Viegas, 2011; Ferguson, 1997a; Morency, 2007; Teal, 1987).",,"['The main hypothesis guiding this work is that the carpool formation and use process is sensitive to personal and household characteristics, space, time, travel cost, and workplace TDM policies.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Considering spatial structure, the more congested downtown areas, associated with a high transit access, less parking availability and higher parking costs, are stronger correlated with a higher use of SOV alternatives (#CITATION_TAG).","We review the literature on the following topics: 1) employee ridesharing behavior and attitudes, 2) relationships between workplace characteristics and ridesharing behavior, 3) impacts of public programs on ridesharing behavior and, 4) effectiveness of employer-based ridesharing programs. We begin with a brief introduction on the origins of the current policy interest in ridesharing and the development of Regulation XV.",['This paper summarizes the literature on the effectiveness of employee ridesharing programs.']
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Copsey, 'Travelling to work: will people move out of their cairs? Copyright (c) 2001 Elsevier Science Ltd.Recent years have seen a huge rise in the levels of car ownership. The numbers of journeys made and kilometers traveled by car are increasing. Increased road transport affects health in a number of ways, including road traffic accidents, air and noise pollution, psychological well being and health related accessibility issues. A further consequence of the growth in the number of cars is traffic congestion. Ninety-seven and 88% of staff at the respective companies travel to work by car. Additionally, it appears that many people live too far from the workplace to cycle or use public transport.Peer reviewe However, the majority of workers do not carpool, the advantages of carpooling are most of the time not strong enough to entice commuters to give up the comfort of driving alone (Comsis Corporation, 1993; Hwang and Giuliano, 1990; #CITATION_TAG; Tsao and Lin, 1999).",,"[""This paper examines employee's perceptions of their modal choice during the journey to work, and addresses what factors influence modal choice, and whether people can be moved out of their cars to other more sustainable forms of transport.""]"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Employer transport plans (ETPs) are increasingly seen by transport planners as one of potential means to manage the demand for private transport. Drawing on US, UK and Dutch experience, it argues that only a minority of employers will voluntarily implement ETPs because they will be seen by the majority as an unnecessary and potentially costly diversion from their normal business activities. Finally, involving the private sector also reduces the burden of transport policies on the public budget (Cairns et al., 2008; Roby, 2010; #CITATION_TAG).",Instead it urges the adoption of an approach which uses fiscal measures to encourage organizations to adopt ETPs,"[""Such plans seek to reduce trips to work by car by providing, through individual employers, a targeted, integrated package of incentives and disincentives to influence commuters' choice of mode for travel to and from the workplace."", 'This paper makes a critical assessment of the potential of ETPs to reduce trips by car to and from workplaces.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Traffic congestion has been a pervasive problem in many urban areas of this country. Basic questions about this potential include the following. Can the current population density, origin-destination distribution, tolerable pick-up and drop-off delays, departure time distribution, and the tolerance for deviation from preferred departure time support a sizable carpooling population that can make a significant contribution to traffic demand reduction? Could the proportion of long trips that are likely candidates for carpooling (e.g., those long trips with same O-D) be so small that no significant traffic demand reduction could be expected from carpooling? The potential depends on many factors, some of which are more amenable to quantification than others. Under the assumptions made in the paper, carpooling among unrelated partners has little potential for demand reduction. Finding a carpool partner with the same origin and destination zone may be difficult, especially in low-density areas (#CITATION_TAG) and at larger distances from the destination.","Our approach to assessing the potential is to separate such quantifiable factors from the rest, and then, based on these quantifiable factors, identify likely upper bounds for the potential. For our numerical study, we use the job and worker data of the city of Los Angeles to approximate the worker/job density. An entropy optimization model that is equivalent to the gravity model is used for trip distribution.","['This paper studies the potential of carpooling among unrelated partners (i.e., inter-household carpooling) for demand reduction during peak commute hours.', 'This paper focuses on a simplified urban sprawl in which the densities of workers and jobs are uniform over an infinitely large flat geographical area.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Reported travel times are usually rounded in multiples of five minutes. Ignoring the phenomenon of rounding leads to biased estimation results for shorter distances. Some authors classify carpool trips on the basis of the types of matching between origins and destinations (Morency, 2007; #CITATION_TAG).",This calls for special statistical techniques. A concise analysis is carried out for carpoolers.,"['This paper gives a detailed empirical analysis of the relationships between different indicators of costs of commuting trips by car: difference as the crow flies, shortest travel time according to route planner, corresponding travel distance, and reported travel time.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Rapid motorization and fuel cost hike over the past few years have made carpool a new mode of travel in Chinese cities. But transportation policy makers have been rather ambivalent, if not indifferent, about carpool. Unlike cities in highly motorized societies, little is known about carpooling behavior in emerging economies such as China. What are the current practice and issues of carpool in Chinese cities? How do carpools in China compare with those in the motorized Western cities? Can carpools help Chinese cities mitigate the negative impacts of rapid motorization? Are foreign policies such as High-Occupancy-Vehicle (HOV) lanes transferable to China? More recently, carpooling was also advocated during the 2008 Olympics in Beijing as a response to driving restrictions (#CITATION_TAG).",Policy suggestions are proposed to Chinese decision makers.Chinese city Carpool HOV Transport policy,"['This paper provides an initial discussion of carpooling in China by exploring a series of questions.', 'Acknowledging the social benefits of voluntary carpooling, this paper argues: (1) bus lanes may be a better choice than HOV lanes when converting general motor vehicle lanes; (2) policies subsidizing carpoolers cannot be justified on either efficiency or equity grounds because a marginal carpooler is more likely transitioning from a transit user or non-motorized traveler than from a driver.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. In transport scheme appraisal, savings in travel time typically represent a substantial proportion of the benefits of a scheme--benefits used to justify its often enormous financial costs. Travel demand analysis treats travel time and activity time as separate, albeit acknowledging an interdependency. The paper challenges these approaches by exploring how travel time can be, and is, being used 'productively' as activity time, and what enhancements to time use might be emerging in the 'information age'. These sustainable mobility policies are called mobility management or travel/transportation demand management (TDM) to stress that the focus is not on infrastructure supply but on managing the de-mand-side, i.e. using the transport system in the most optimal way to fulfil our lifestyle needs (Frändberg and Vilhelmson, 2010; #CITATION_TAG).","Such benefits are founded on the assumption that travel time is unproductive, wasted time in-between 'real' activities and which should be minimised.","['This paper, focused primarily on UK data and debates, considers the potential significance of travel time use within past, present and future patterns of mobility.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. However, Government must be clear about its objectives for travel plans, if this potential is to be achieved. Finally, involving the private sector also reduces the burden of transport policies on the public budget (Cairns et al., 2008; Roby, 2010; #CITATION_TAG).","It first outlines the policy background for travel plans in the UK, and reasons for their implementation. It then presents a conceptual model of travel plan development, which is used to analyse the development of travel plans in a number of case studies.","['This paper reviews the evidence that travel plans have their intended effect, which is to reduce the number of employees commuting alone by car to their place of work.', 'It then seeks to explain how take up and hence the effect of travel plans could be made more widespread.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Abstract Many studies model the effects of the built environment on travel behaviour. Usually, results are controlled for socio-economic differences and sometimes socio-psychological differences among respondents. However, these studies do not mention why after all a relationship should exist between travel behaviour and spatial, socio-economic and personality characteristics. time geography, activity-based approach) and social psychology (e.g. Theory of Planned Behaviour, Theory of Repeated Behaviour). Standard mode choice research takes the individual or the household as the unit of observation since individual and household characteristics determine the choice process (#CITATION_TAG).","Answering this query involves combining and linking theories stemming from transport geography (e.g. Comparable to customary theories in transport geography, this conceptual model considers travel behaviour as derived from locational behaviour and activity behaviour. But the conceptual model adds concepts such as 'lifestyle', 'perceptions', 'attitudes' and 'preferences' which indirectly influence travel behaviour.","['Using key-variables from these theories, this paper aims to develop a conceptual model for travel behaviour.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Most studies on the link between the built environment and modal choice characterize and model this relationship by objectively measureable characteristics such as density and diversity. Recently, within the debate on residential self-selection, attention has also been paid to the importance of subjective influences such as the individual's perception of the built environment and his/her residential attitudes and preferences, resulting in models that take account of both the objective and subjective characteristics of the built environment. However, self-selection might occur on other points than residential location as well. Accordingly, what people at your workplace think and do (the subjective norm, corporate culture) influences your travel behaviour (Bonham and Koth, 2010; Heinen et al., 2011; McDonald, 2007; #CITATION_TAG).","To this end, a modal choice model for leisure trips is developed using data on personal lifestyles and attitudes, collected via an Internet survey, and estimated using a path model consisting of a set of simultaneously estimated equations between observed variables. Moreover, we compared the results of a model with and without these subjective influences.","['Expanding the analysis to also include both objective and subjective characteristics at other model levels (i.e., not only stage of life characteristics but also personal lifestyles; not only car availability but also travel attitudes, not only modal choice but also mode specific attitudes) is the purpose of this paper.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. In their continuous battle against congestion and pollution, governments nowadays promote rail as an environmentally friendly Single Occupant Vehicle (SOV) alternative. For comparison, the energy consumption per person kilometre for rail is 0.14 kWh/pkm while that for a car is 0.48 kWh/pkm (Boussauw and Witlox 2009), and electric trains have no direct emissions of air pollutants like PM 10 and NO x. Modal shift policies often target the daily commute since home to work travel is concentrated in the congested peak hours and commuters' travel behaviour is more regular. Furthermore, both cities and public transport are predominant in the sustainable mobility paradigm (Banister 2008). Indeed, rail can keep jobs in city centres accessible. Rail accessibility is seen as the inverse of the sum of waiting and walking time between workplace and railway station, the calculation method and an example are given in #CITATION_TAG.",,"['Besides environmental objectives, employment concerns motivate governments to invest in railways.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. The paper closes with an example of a multi-level land use, transport and environment model ranging from the European to the grid cell level. Although our main focus is on workplaces, a multilevel perspective is used (#CITATION_TAG).","It starts with a history of urban transport and land-use models and observes a trend towards increasing conceptual, spatial and temporal resolution stimulated by improved data availability, higher computer speed and better theories about mobility and location of individual behaviour.","['Abstract This paper discusses the usefulness of the trend towards microsimulation in urban transport and land-use modelling for the planning practice.', 'As a possible solution to the macro-micro debate, it calls for a theory of multi-level models according to which for each planning task there is an appropriate level of conceptual, spatial and temporal resolution.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. The number of bus-based Park and Ride schemes in the UK has grown substantially over the past 40 years as a result of its encouragement by the Government as a tool to deal with increasing traffic congestion and traffic-related pollution. Analogously, carpool parkings and park and ride facilities should be carefully planned since they often generate additional traffic and encourage car-oriented land use development outside urban areas (#CITATION_TAG; Parkhurst, 2000).",The authors identify phases of development of Park and Ride since its emergence as a local solution to transport capacity constraints in historic towns. Policy goals are identified against which a review of literature is used to highlight its effectiveness.,['The aim of this paper is to analyse the degree to which Park and Ride is effective in the contemporary policy context.']
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Density is a key component in the recent surge of mixed-use neighborhood developments. Empirical research has shown an inconsistent picture on the impact of density. In particular, it is unclear whether it is the density or the variables that go long with density that affect people's travel behavior. Many existing studies on density neglect confounding factors, for example, residential self-selection, generalized travel cost, accessibility, and access to transit stations. In addition, most still use a single trip as their observation unit, even though trip chaining is well recognized. In our model, parking seems thus a measure for agglomerations and density (#CITATION_TAG).",,"['The goal of this paper is to assess the role of density in affecting mode choice decisions in home-based work tours, while controlling for confounding factors.', 'The study advances the field by analyzing the role of the built environment on home-based work tours.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. The authors caution that the shortfall between expected demand for and supply of world oil production could result in extremely high oil prices; catastrophic climate change offers another issue of contention. These transformations would allow considerable movement of people and freight by land in an era of severe energy constraints and concerns about carbon emissions. It was promoted during World War II to deal with oil and rubber shortages and during the oil crisis of the 1970s (Ferguson, 1997b (Ferguson,, 2000 #CITATION_TAG).",Then the authors describe how the high oil prices in particular could give rise to two or more revolutions in land transport during the first half of the 21st century: the replacement of internal combustion engines by electric motors and the widespread powering of these motors directly from the electric grid rather than from on-board fuel.,"['This book reviews the challenges that face industrialized societies as they cope with growing dependence on transport fueled by low-priced oil.', 'The book begins with an exploration of past transport revolutions, to gain insight into the nature and dynamics of profound change.', 'Topics include current transport, with a focus on energy use and adverse impacts; the politics and business of transport; and organizational and technical innovations that could ensure effective, secure movement of people and goods in ways that minimize environmental impacts and make the best use of renewable sources of energy.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. It is found that carpooling is sensitive to tra%0c congestion reduction only when a congestion externality-based tolling scheme is implemented. Flexitime and the promotion of carpool are then seen as conflicting mobility management measures (Buliung et al., 2010; #CITATION_TAG).",The logit-based stochastic model involves the consideration on preference option of mode choice.,"['For studying carpooling problems, this paper presents two models, namely deterministic and stochastic, and gives the economic explanations to the model solutions.', 'We investigate the jockeying behavior of work commuters between carpooling and driving alone modes through solving each model for both no-toll equilibrium and social optimum.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Scholars have recently noted the role that employers can play as ""mediating institutions"" for public policy. Mediating institutions connect the private lives of individuals with public policy concerns by communicating societal norms to members and providing social contexts that encourage a commitment to these norms. Despite the potential importance of employers as mediating institutions for public policy, little scholarly attention has been devoted to employer mediation behavior. What factors influence an employer's willingness to mediate policy problems? And how effective are employers as mediating institutions? Employers have a privileged relationship with their employees and are therefore regularly used as intermediaries between government and individual travellers (#CITATION_TAG; Ferguson, 1997b Ferguson,, 2007.","The mediation behaviors of interest relate to employer efforts to mitigate traffic congestion and air quality problems by enabling employee ""commute options,"" which are alternatives to single-occupancy vehicle commuting to work. Drawing on theories of organization behavior, the study hypothesizes that self-interest, organizational control, and association membership will affect willingness to provide commute options. Both sets of hypotheses are supported by statistical analyses of data from a cross-sectional mail survey of metropolitan Atlanta organizations.","['Accordingly, this study examines two research questions.']"
"Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Abstract In recent years, there has been a growing interest in a range of transport policy initiatives which are designed to influence people's travel behaviour away from single-occupancy car use and towards more benign and efficient options, through a combination of marketing, information, incentives and tailored new services. In transport policy discussions, these are now widely described as 'soft' factor interventions or 'smarter choice' measures or 'mobility management' tools. In 2004, the UK Department for Transport commissioned a major study to examine whether large-scale programmes of these measures could potentially deliver substantial cuts in car use. Finally, involving the private sector also reduces the burden of transport policies on the public budget (#CITATION_TAG; Roby, 2010; Rye, 2002).",,"['The purpose of this article is to clarify the approach taken in the study, the types of evidence reviewed and the overall conclusions reached.']"
"To understand price changes one must determine the relative impact of supply and demand shifts on price. When this possibility is exhausted, the total factor productivity change for the industry may come to a halt. Productivity growth, or technical change, can be analyzed using techniques such as stochastic frontiers or indices (Kumbhakar and Lovell 2000; Coelli et al. 2005). and #CITATION_TAG find productivity growth has slowed down since the early 2000s, suggesting the salmon farming industry has developed into a mature industry with lower growth rates.",The Malmquist Productivity Index (MPI) is used.,"['Abstract In this study we measure change in total factor productivity for production of Atlantic salmon in Norway from 2001 to 2008.', 'This is due to a regress in the technical change component of the MPI.']"
"To understand price changes one must determine the relative impact of supply and demand shifts on price. However, in the short run supply is likely to be constrained by the biological production process, regulations, and capacity constraints. In the long run, this changes substantially as supply becomes elastic. Productivity growth caused prices to decline rapidly in the 1980s and 1990s (#CITATION_TAG).",,"[""In this article, we estimate a restricted profit function for Norwegian salmon producers, which allows us to examine the industry's short-run and long-run supply responsiveness separately.""]"
"To understand price changes one must determine the relative impact of supply and demand shifts on price. Commodity price booms are best explained by macroeconomic rather than market-specific factors. The demand for grains and oilseeds as biofuel feedstocks was the main cause of the price rise but macroeconomic and financial factors explain its extent. The futures market may be an important monetary transmission mechanism, but it is commodity investors, not speculators, who, by investing in commodities as an asset class, may have generalized prices rises across markets.Food prices, commodity prices, money, futures markets #CITATION_TAG emphasizes the impact of common factors on the general level of agricultural food prices.",,"['I argue that the rise in food prices over 2007 and the first half of 2008 should be seen as part of the wider commodity boom which is largely the result of rapid economic growth in China and throughout Asia in a context of loose money and in which, because of previous low investment, supply was inelastic.']"
"To understand price changes one must determine the relative impact of supply and demand shifts on price. These issues have received a great deal of attention in recent years, reflecting the environmental, economic and cultural importance of salmon to Americans  and the fact that salmon issues span many important policy debates ranging from environmental protection to trade policy. Existing literature has found that farmed Atlantic salmon determines the prices for both farmed and wild salmon (Asche et al., 1999), and farmed salmon represents the major share of world supply and trade (#CITATION_TAG).",,['This report examines economic and policy issues related to wild and farmed salmon in North America.']
"To understand price changes one must determine the relative impact of supply and demand shifts on price. The competition between farmed salmon and wild caught Pacific salmon has received some attention previously. However, this was before frozen Atlantic salmon emerged as an important product form in the market. L'article traite de l'importance de diverses sortes d'un meme type de produit et de leur provenance dans les etudes de structure de marche au sein du marche europeen du saumon. La concurrence entre le saumon d'elevage et le saumon sauvage du Pacifique a deja fait l'objet d'etudes a ce sujet, mais c'etait avant l'emergence du saumon de l'Atlantique surgele sur le marche. C'est une question importante parce que ce dernier produit a plus de chance de concurrence directement le saumon du Pacifique-le plus souvent vendu surgele - que le saumon de l'Atlantique frais. Nous avons utilise un systeme de demande quasi ideale pour estimer l'etat respectif de la demande de saumon de l'Atlantique frais et surgele et du saumon du Pacifique surgele au sein de l'Union europeenne. Previous measures of supply and demand shifts have specified these shifts relative to the expected quantityܳ ாȁாௌୀாௌ or ܳ ாȁூୀூ (#CITATION_TAG; Marsh 2003; Kinnucan and Myrland 2006).",,"['This paper addresses the importance of different product forms and their origin when considering the market structure in the European salmon market.', 'In this paper, an almost ideal demand system is used to estimate the demand for fresh Atlantic salmon, frozen Atlantic salmon and frozen Pacific salmon in the European Union.']"
"To understand price changes one must determine the relative impact of supply and demand shifts on price. World market prices for major food commodities such as grains and vegetable oils have risen sharply to historic highs of more than 60 percent above levels just 2 years ago. Many factors have contributed to the runup in food commodity prices. Recent factors that have further tightened world markets include increased global demand for biofuels feedstocks and adverse weather conditions in 2006 and 2007 in some major grain- and oilseed-producing areas. Other factors that have added to global food commodity price infl ation include the declining value of the U.S. dollar, rising energy prices, increasing agricultural costs of production, growing foreign exchange holdings by major food-importing countries, and policies adopted recently by some exporting and importing countries to mitigate their own food price infl ation. #CITATION_TAG identifies several demand and supply factors that simultaneously contributed to a recent increase in food prices.",,['This report discusses these factors and illustrates how they have contributed to food commodity price increases.']
"To understand price changes one must determine the relative impact of supply and demand shifts on price. In recent years, there have been a number of excellent advance-level books published on performance measurement. The book contains computer instructions and output listings for the SHAZAM, LIMDEP, TFPIP, DEAP and FRONTIER computer programs. Productivity growth, or technical change, can be analyzed using techniques such as stochastic frontiers or indices (Kumbhakar and Lovell 2000; #CITATION_TAG). and Vassdal and Holst (2011) find productivity growth has slowed down since the early 2000s, suggesting the salmon farming industry has developed into a mature industry with lower growth rates.","The second edition of An Introduction to Efficiency and Productivity Analysis is designed to be a general introduction for those who wish to study efficiency and productivity analysis. For each method, a detailed introduction to the basic concepts is presented, numerical examples are provided, and some of the more important extensions to the basic methods are discussed. Indeed, the 2nd Edition maintains its uniqueness: (1) It is a well-written introduction to the field. (2) It outlines, discusses and compares the four principal methods for efficiency and productivity analysis in a well-motivated presentation. (3) It provides detailed advice on computer programs that can be used to implement these performance measurement methods.","['The book provides an accessible, well-written introduction to the four principal methods involved: econometric estimation of average response models; index numbers, data envelopment analysis (DEA); and stochastic frontier analysis (SFA).', 'Of special interest is the systematic use of detailed empirical applications using real-world data throughout the book.', 'This book, however, is the first systematic survey of performance measurement with the express purpose of introducing the field to a wide audience of students, researchers, and practitioners.']"
"Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. An important topic in the evolution of language is the kinds of grammars that can be computed by humans and other animals. Fitch and Hauser (F&H; 2004) approached this question by assessing the ability of different species to learn 2 grammars, (AB)(n) and A(n) B(n) . This article replicates F&H's data and reports new controls using either categories similar to those in F&H or less salient ones. Indeed, when familiarized with A(n) B(n) exemplars, participants failed to discriminate A(3) B(2) and A(2) B(3) from A(n) B(n) items, missing the crucial feature that the number of As must equal the number of Bs. There is strong evidence, including from self-reports after the experiment, that participants who learn non-configurational properties do so by counting and comparing the number of stimuli of each class, and participants without syntactic disorder more consistently attend to configurational (order) than nonconfigurational (counting) structure (de Vries, Monaghan, Knecht, & Zwitserlood, 2008; #CITATION_TAG; Perruchet & Rey, 2005;. Some non-impaired individuals learn the configurational structure only, but none learn only non-configurational structures.","A(n) B(n) was taken to indicate a phrase structure grammar, eliciting a center-embedded pattern. (AB)(n) indicates a grammar whose strings entail only local relations between the categories of constituents. In their experiments, the A constituents were syllables pronounced by a female voice, whereas the B constituents were syllables pronounced by a male voice.","['This study proposes that what characterizes the A(n) B(n) exemplars is the distributional regularities of the syllables pronounced by either a male or a female rather than the underlying, more abstract patterns.']"
"Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. Naming ability on the PALPA54 subtest (#CITATION_TAG) indicated residual lexical capacity with scores of 59/60 for spoken (with no penalty for phonemic paraphasias as long as the target was recognizable) and 59/60 for written naming.","Intended both as a clinical instrument and research tool, PALPA is a set of resource materials enabling the user to select language tasks that can be tailored to the investigation of an individual patient's impaired and intact abilities. The materials consist of sixty rigorously controlled tests of components of language structure such as orthography and phonology, word and picture semantics and morphology and syntax. The tests make use of simple procedures such as lexical decision, repetition and picture naming and have been designed to assess spoken and written input and output modalities. Each test is also accompanied by detailed instructions of how and why it was constructed, how to use it, and by presenter's forms and marking sheets.",['PALPA has been designed as a comprehensive psycholinguistic assessment of language processing in adult acquired aphasia.']
"Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. Four years previously, he was diagnosed with logopenic PPA (#CITATION_TAG).","Criteria for the 3 variants of PPA--nonfluent/agrammatic, semantic, and logopenic--were developed by an international group of PPA investigators who convened on 3 occasions to operationalize earlier published clinical descriptions for PPA subtypes. Patients are first diagnosed with PPA and are then divided into clinical variants based on specific speech and language features characteristic of each subtype. Classification can then be further specified as ""imaging-supported"" if the expected pattern of atrophy is found and ""with definite pathology"" if pathologic or genetic data are available. The working recommendations are presented in lists of features, and suggested assessment tasks are also provided.",['This article provides a classification of primary progressive aphasia (PPA) and its 3 main variants to improve the uniformity of case reporting and the reliability of research results.']
"Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. The same region is engaged to a greater extent when a syntactic violation is present and structural unification becomes difficult or impossible. In addition, we argue that the Chomsky hierarchy is not directly relevant for neurobiological systems.Copyright (c) 2010 Elsevier Inc. All rights reserved. AGL tasks engage neural language areas, in particular left inferior frontal regions (Bahlmann, Schubotz, & Friederici, 2008; Bahlmann, Schubotz, Mueller, Koester, & Friederici, 2009; Petersson, Folia, & Hagoort, 2012; #CITATION_TAG).","We used a simple right-linear unification grammar in an implicit artificial grammar learning paradigm in 32 healthy Dutch university students (natural language FMRI data were already acquired for these participants). Instead, we argue that the left inferior frontal region is a generic on-line sequence processor that unifies information from various sources in an incremental and recursive manner, independent of whether there are any processing requirements related to syntactic movement or hierarchically nested structures.","['In this paper we examine the neurobiological correlates of syntax, the processing of structured sequences, by comparing FMRI results on artificial and natural language syntax.']"
"Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. At test, they were told of the rule set, but not told what it was. AGL (#CITATION_TAG) is a commonly employed paradigm that tests processing of sequence structure.","A single experiment is reported that investigated implicit learning using a conjunctive rule set applied to natural words. Participants memorized a training list consisting of words that were either rare-concrete and common-abstract or common-concrete and rare-abstract. Instead, they were shown all four word types and asked to classify words as rule-consistent words or not. Participants classified the items above chance, but were unable to verbalize the rules, even when shown a list that included the categories that made up the conjunctive rule and asked to select them. An analysis of the materials demonstrated that conscious micro rules (i.e., chunk knowledge) could not have driven performance.",['We propose that such materials offer an alternative to artificial grammar for studies of implicit learning']
"Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. The verb phrase morphology (be+TNS V+PastP, e.g., is pushed), which in child development first emerges as the state passive (e.g., it's broken), appears to be grounded in stative use and biased towards assigning its subject an inactive role (Brooks & Tomasello, 1999; Israel, Johnson, & Brooks, 2001; #CITATION_TAG).","It employed usage-based principles including 'constructional grounding'; using short structures as the basis for acquiring long structures, and 'construction conspiracy'; encouraging analogies between partially overlapping constructions.",['This study taught the passive to two children with specific language impairment (aged 8;1 and 8;2).']
"Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. In particular, most of syntax (long thought to be there) is not located in Broca's area and its vicinity (operculum, insula, and subjacent white matter). This cerebral region, implicated in Broca's aphasia, does have a role in syntactic processing, but a highly specific one: It is the neural home to receptive mechanisms involved in the computation of the relation between transformationally moved phrasal constituents and their extraction sites (in line with the Trace-Deletion Hypothesis). By contrast, basic combinatorial capacities necessary for language processing - for example, structure-building operations, lexical insertion - are not supported by the neural tissue of this cerebral region, nor is lexical or combinatorial semantics. The dense body of empirical evidence supporting this restrictive view comes mainly from several angles on lesion studies of syntax in agrammatic Broca's aphasia. Syntactic abilities are nonetheless distinct from other cognitive skills and are represented entirely and exclusively in the left cerebral hemisphere. Although more widespread in the left hemisphere than previously thought, they are clearly distinct from other human combinatorial and intellectual abilities. Combinatorial aspects of the language faculty reside in the human left cerebral hemisphere, but only the transformational component (or algorithms that implement it in use) is located in and around Broca's area. English passives are ""harder"" with regard to a number of variables (Caplan & Waters, 1999; Drai & Grodzinsky, 2006; Druks, 2002; #CITATION_TAG; Mauner, Fromkin, & Cornell, 1993) as they contain more words, more functional morphemes, have a non-canonical word order and, in some theories, involve a transformation from canonical order (or ""movement"" of constituents).","Five empirical arguments are presented: experiments in sentence comprehension, cross-linguistic considerations (where aphasia findings from several language types are pooled and scrutinized comparatively), grammaticality and plausibility judgments, real-time processing of complex sentences, and rehabilitation.","['A new view of the functional role of the left anterior cortex in language use is proposed.', 'It is also involved in the construction of higher parts of the syntactic tree in speech production.']"
"Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. We review the concept of working memory as a short-duration system in which small amounts of information are simultaneously stored and manipulated in the service of accomplishing a task. We summarize the argument that syntactic processing in sentence comprehension requires such a storage and computational system. We present a theory of the divisions of the verbal working memory system and suggestions regarding its neural basis. English passives are ""harder"" with regard to a number of variables (#CITATION_TAG; Drai & Grodzinsky, 2006; Druks, 2002; Grodzinsky, 2000; Mauner, Fromkin, & Cornell, 1993) as they contain more words, more functional morphemes, have a non-canonical word order and, in some theories, involve a transformation from canonical order (or ""movement"" of constituents).","We then ask whether the working memory system used in syntactic processing is the same as that used in verbally mediated tasks that involve conscious controlled processing. Evidence is brought to bear from various sources: the relationship between individual differences in working memory and individual differences in the efficiency of syntactic processing; the effect of concurrent verbal memory load on syntactic processing; and syntactic processing in patients with poor short-term memory, patients with poor working memory, and patients with aphasia.",['This target article discusses the verbal working memory system used in sentence comprehension.']
"Integration of audio-visual information p. 3 In adults, decisions based on multisensory information can be faster and/or more accurate than those relying on a single sense. Here we studied speeded responding to audio-visual targets, a key multisensory function whose development remains unclear. Multisensory integration describes a process by which information from different sensory systems is combined to influence perception, decisions, and overt behavior. Despite a widespread appreciation of its utility in the adult, its developmental antecedents have received relatively little attention. Multisensory integration in superior colliculus develops with postnatal sensory experience, which seems to be learnt via the cortico-SC projection (#CITATION_TAG).","Of particular interest here are two sets of experimental observations: (1) cortical influences appear essential for multisensory integration in the SC, and (2) postnatal experience guides its maturation.","['Here we review what is known about the development of multisensory integration, with a focus on the circuitry and experiential antecedents of its development in the model system of the multisensory (i.e., deep) layers of the superior colliculus.']"
"Integration of audio-visual information p. 3 In adults, decisions based on multisensory information can be faster and/or more accurate than those relying on a single sense. Here we studied speeded responding to audio-visual targets, a key multisensory function whose development remains unclear. When visual information is available, human adults, but not children, have been shown to reduce sensory uncertainty by taking a weighted average of sensory cues. In the absence of reliable visual information (e.g. extremely dark environment, visual disorders), the use of other information is vital. These tasks have found absent or suboptimal pooling of signals until much later in childhood (Gori et al., 2008; Nardini et al., 2008; Nardini et al., 2010; Gori et al., 2012; Nardini et al., 2013; #CITATION_TAG).","In the first experiment, adults and children aged 5 to 11 years judged the relative sizes of two objects in auditory, haptic, and non-conflicting bimodal conditions. In Experiment 2, different groups of adults and children were tested in non-conflicting and conflicting bimodal conditions. In Experiment 1, adults reduced sensory uncertainty by integrating the cues optimally, while children did not. In Experiment 2, adults and children used similar weighting strategies to solve audio-haptic conflict.",['Here we ask how humans combine haptic and auditory information from childhood.']
"The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. In the last few decades, the world has witnessed an enormous growth in the volume of foreign direct investment (FDI). The global stock of FDI reached US$ 7.5 trillion in 2003 and accounted for 11% of world Gross Domestic Product, up from 7% in 1990. Substantial FDI inflows went into transition countries. Inflows into one of the region's largest recipient, the Russian Federation, almost doubled, enabling Russia to become one of the five top FDI destinations in 2005-2006. FDI inflows in Russia have increased almost threefold from 13.6% in 2003 to 35% in 2007. In 2003, these flows were twice greater than those into China; whilst in 2007 they were six times larger. Efficient government institutions are argued by many economists to foster FDI and growth as a result. However, the magnitude of this effect has yet to be measured. This casts some doubt on the productivity of the investment in public capital in these regions as it might be that bureaucrats may prefer to use these infrastructural projects for rent extraction. Countries with high levels of public sector corruption are found to receive less foreign aid, by Alesina and Weder (2002), and less foreign direct investment, by #CITATION_TAG.","Using a regional data-set it concentrates on three areas relating to FDI. Secondly, it quantifies the impact of perceived corruption on the volume of FDI stocks simultaneously estimating the impact of the investment in public capital such as telecommunications and transportation networks on FDI in the presence of corruption.","['This thesis takes a Political Economy approach to explore, empirically, the potential impact of malfunctioning governmental institutions, proxied by three indices of perceived corruption, on FDI stocks accumulation/distribution within Russia over the period of 2002-2004.', 'Firstly, it considers the significance, the size and the sign of the impact of perceived corruption on accumulation of FDI stocks within Russia.', ""In particular, it addresses the question whether more corrupt regions in Russia are also those that could have accumulated more of FDI stocks, and investigates whether those 'more corrupt' regions would have had lower level of public capital investment.""]"
"The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. Méon and Sekkat (2005), Pellegrini and Gerlagh (2004), #CITATION_TAG, and Mauro (1995) find that it leads to lower investment.","It starts from an endogenous growth model and extends it to account for the detrimental effects of corruption on the potentially productive components of government spending, namely military and investment spending. Second, allowing for the cyclical economic fluctuations in specific countries leaves the estimated elasticities close to those of the full sample. Third, there are significant conditioning variables that need to be taken into account, namely the form of  government, political instability and natural resource endowment.",['This paper considers the effects of corruption and government spending on economic growth.']
"The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. The type of corruption charge is an important determinant of vote loss. Welch and Hibbing (1997), Dimock and Jacobson (1995), and #CITATION_TAG provide similar evidence in the case of U.S. Chang et al. (2010), Fernández-Vázquez and Rivero (2010), Manzetti and Wilson (2007) and Golden (2006) offer some explanations as to why this is so.","This assessment includes a consideration of the victory or defeat of alleged or convicted corrupt candidates, and an examination of the impact of corruption charges on electoral turnout and percentage of votes polled by the accused candidates.",['This study assesses the electoral impact of charges of corruption on candidates in contests for the U.S. House of Representatives in five elections from 1968 to 1978.']
"The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. #CITATION_TAG provide yet another case of voter retaliation depending on the severity of corruption unearthed, its reliability and how well it is disseminated.",,['This paper examines the extent to which access to information enhances political accountability.']
"The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. Tanzi and Davoodi (1997), and #CITATION_TAG argue that corruption shifts public expenditures from growth-promoting to lowproductivity projects.",It provides a synthetic review of recent studies that analyze this phenomenon empirically.,['This paper discusses the possible causes and consequences of corruption.']
"The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. Scholars have long suspected that political processes such as democracy and corruption are important factors in determining economic growth. Studies show, however, that democracy has only indirect effects on growth, while corruption is generally accepted by scholars as having a direct and negative impact on economic performance. Although corruption certainly occurs in democracies, the electoral mechanism inhibits politicians from engaging in corrupt acts that damage overall economic performance and thereby jeopardize their political survival. This would explain why Pellegrini and Gerlagh (2008) and Lederman et al. (2005) find the level of corruption to be lower, and #CITATION_TAG, its harm on economic growth to be less, in democratic countries.","Using time-series cross-section data for more than 100 countries from 1982-97, we show that corruption has no significant effect on economic growth in democracies, while non-democracies suffer significant economic harm from corruption.","[""We argue that one of democracy's indirect benefits is its ability to mitigate the detrimental effect of corruption on economic growth.""]"
"The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. The type of corruption charge is an important determinant of vote loss. Welch and Hibbing (1997), Dimock and Jacobson (1995), and #CITATION_TAG provide similar evidence in the case of U.S. Chang et al. (2010), Fernández-Vázquez and Rivero (2010), Manzetti and Wilson (2007) and Golden (2006) offer some explanations as to why this is so.","This assessment includes a consideration of the victory or defeat of alleged or convicted corrupt candidates, and an examination of the impact of corruption charges on electoral turnout and percentage of votes polled by the accused candidates.",['This study assesses the electoral impact of charges of corruption on candidates in contests for the U.S. House of Representatives in five elections from 1968 to 1978.']
"The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. Traffic accidents result in 1 million deaths annually worldwide, though the burden is disproportionately felt in poorer countries. Typically, fatality rates from disease and accidents fall as countries develop. Traffic deaths, however, regularly increase with income, at least up to a threshold level, before declining. For example, Escaleras et al. (2007) show that it raises fatalities from earthquakes, and #CITATION_TAG from traffic accidents.",,"['While we confirm this by analyzing 1,356 country-year observations between 1982 and 2000, our purpose is to consider the role played by public sector corruption in determining traffic fatalities.']"
"This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service ""free at the point of access"", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland ""outcomes frameworks"" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. There is increasing interest in whether observational data can usefully supplement, enhance, or even replace clinical trials evidence for the efficacy of interventions. This article describes the development of methods of analysis of routine clinical outcomes data (using ICD10, HoNOS65+ and a developing intervention coding system) in an old age psychiatry service in South London. Some of these were routinely implemented in some English services from 1997 (#CITATION_TAG), whilst the majority hung back (Gilbody, House, & Sheldon, 2002).","The minimum dataset necessary, the construction of a database and some core analyses are described.",['To an understanding of the practical and cultural changes necessary for this in psychiatry must be added appreciation of the importance of feedback of appropriately analysed aggregated outcomes data to clinicians.']
"This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service ""free at the point of access"", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland ""outcomes frameworks"" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. Background High smoking prevalence is a major public health concern for people with mental disorders. The 'CRIS-IE-Smoking' application used General Architecture for Text Engineering (GATE) natural language processing software to extract smoking status information from open-text fields. However, limited information on smoking status remained a challenge. New techniques of natural language processing of electronic clinical records (#CITATION_TAG) are now being applied to the extraction of interventions thus avoiding burdening the clinicians or service users with extra data-gathering.",Methods All cases had been referred to a secondary mental health service between 2008-2011 and received a diagnosis of schizophreniform or bipolar disorder. A combination of CRIS-IE-Smoking with data from structured fields was evaluated for coverage and the prevalence and demographic correlates of current smoking were analysed.,"['The study focused on those aged over 15 years who had received active care from the mental health service for at least a year (N=1,555).']"
"This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service ""free at the point of access"", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland ""outcomes frameworks"" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. Priebe et al (#CITATION_TAG) has reported the results of subjective quality of life items in DIALOG (see Table 1), a structured communication tool in mental health services.","Abstract Background DIALOG is an intervention to structure the communication between patient and key worker, which has been shown to improve patient outcomes in community mental health care. As part of the intervention, patients provide ratings of their subjective quality of life (SQOL) on eight Likert type items and their treatment satisfaction on three such items. Method Data were taken from 271 patients who received the DIALOG intervention. All patients were diagnosed with schizophrenia or a related disorder and treated in community mental health care. For SQOL and treatment satisfaction as assessed in the DIALOG intervention, we established the internal consistency (Cronbach's alpha), the convergent validity of SQOL items (correlation with Manchester Short Assessment of Quality of Life [MANSA]) and treatment satisfaction items (correlation with Client Satisfaction Questionnaire [CSQ]), the concurrent validity (correlations with Positive and Negative Syndrome Scale [PANSS]) and the sensitivity to change (t-test comparing ratings of the first and last intervention). We also explored the factorial structure of the eight SQOL items. SQOL items loaded on two meaningful factors, one capturing health and personal safety and one reflecting other life domains.",['This study explored the psychometric qualities of the outcome data generated in the DIALOG intervention to explore whether they may be used for evaluating treatment outcomes.']
"This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service ""free at the point of access"", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland ""outcomes frameworks"" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. Purpose Commissioning has been a central plank of health and social are policy in England for many years now, yet there are still debates about how effective it is in delivering improvements in care and outcomes. Social inclusion of people with experience of mental health is one of the goals that commissioners would like to help services to improve but such a complex outcome for people can often be undermined by contractual arrangements that fragment service responses rather than deliver holistic support. In the UK, however, the word ""outcomes"" has recently been heard at every level of government (#CITATION_TAG) and, as ever imperfectly translated into action or actual resources, it is helping drive the RCOM process forwards, as is excitement about ""value"" (Porter et al., 2006).",Design/methodology/approach The paper is a conceptual discussion and case description of the use of Alliance Contracts to improve recovery services and social inclusion in mental health care in one locality.,"['In this paper we discuss a form of commissioning, Alliance Contracting, and how it has been allied with a Social Inclusion Outcomes Framework (SIOF)in Stockport to begin to improve services and outcomes.']"
"This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service ""free at the point of access"", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland ""outcomes frameworks"" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. Background: National curriculum assessment (NCA) in England has been in place for nearly 20 years. It has its origins in a political desire to regulate education, holding schools accountable. However, its form and nature also reflect educational and curriculum concerns and technical assessment issues. Sources of evidence: The sources quoted are in the public domain, but in addition to academic articles, include political biographies and published official papers. This change reflects the political purposes of the system for accountability, and the pressure associated with this has led to growing criticism of the effects on children and their education. In the 1990s the UK Department of Education became rabid about routine educational testing, eventually pushing through dramatic reforms in the teeth of professional opposition (#CITATION_TAG).",,['Purpose: The aim of the article is to provide a narrative account of the development and changes in NCA in England from its initiation to 2008 and to explain the reasons for these.']
"This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service ""free at the point of access"", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland ""outcomes frameworks"" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. BACKGROUND The Mental Health Recovery Star (MHRS) is a popular outcome measure rated collaboratively by staff and service users, but its psychometric properties are unknown. Despite Killaspy et al's critique (#CITATION_TAG), experience by UKRCOM members suggests that it is a useful tool for a collaborative approach to care planning and perhaps outcomes measurement.",METHOD A total of 172 services users and 120 staff from in-patient and community services participated. Interrater reliability of staff-only ratings and test-retest reliability of staff-only and collaborative ratings were assessed using intraclass correlation coefficients (ICCs). The influence of collaboration on ratings was assessed using descriptive statistics and ICCs.,"[""AIMS To assess the MHRS's acceptability, reliability and convergent validity.""]"
"This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service ""free at the point of access"", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland ""outcomes frameworks"" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. It is argued that the failures to meet the evidence-of-effectiveness challenge, the authority challenge, the conflicting hierarchy challenge, and the definition-of-evidence challenge diminish arguments for the superiority of EBM. When the notion of a hierarchy of evidence became more amenable to challenge (#CITATION_TAG) and, at the same time, information technology began to achieve reasonable levels of reliability, the conditions became once again ripe for RCOM.","In the second part of the essay, recent developments in the theory of EBM are discussed with specific reference to what is termed the Oslerian turn, and a relationship between EBM and rationality is entertained.",['This paper examines four challenges that proponents of evidence-based medicine (EBM) must address to establish its claims to universality and legitimacy.']
"Lagrangian Floer homology in a general case has been constructed by Fukaya, Oh, Ohta and Ono, where they construct an A [?] They developed obstruction and deformation theories of the Lagrangian Floer homology theory. -objects, which are Hochschild and cyclic homology for an A [?] -objects and Chevalley-Eilenberg or cyclic Chevalley-Eilenberg homology for their underlying L [?] -algebra with a non-trivial primary obstruction, Chevalley-Eilenberg Floer homology vanishes, whose proof is inspired by the comparison with cluster homology theory of Lagrangian submanifolds by Cornea and Lalonde. Also for A ∞, or L ∞ -algebras with m 0 = 0 such homology theories have been known (we refer readers to [#CITATION_TAG] for the definitions using non-commutative geometry).",This framework is based on noncommutative geometry as expounded by Connes and Kontsevich. The developed machinery is then used to establish a general form of Hodge decomposition of Hochschild and cyclic cohomology of $C_\infty$-algebras. This generalizes and puts in a conceptual framework previous work by Loday and Gerstenhaber-Schack.Comment: This 54 pages paper is a substantial revision of the part of   math.QA/0410621 dealing with algebraic Hodge decompositions of Hochschild and   cyclic cohomology theories. The main addition is the treatment of cohomology   theories corresponding to unital infinity-structure,"['This paper builds a general framework in which to study cohomology theories of strongly homotopy algebras, namely $A_\\infty, C_\\infty$ and $L_\\infty$-algebras.']"
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. Face to face conversation necessarily involves a great deal of bodily movement beyond that required for speaking. Eye movements (Richardson et al., 2007), postural sway (Shockley et al., 2009), and even blinking (#CITATION_TAG) have all been found to become subtly intertwined in conversation, leading to a dynamic entanglement of the participants.","Gaze and blinking in dyadic conversation are examined, along with their relation to speech turn. Eight pairs provide 15 minutes of conversation each, including five participants who partake in two dyads each. Many aspects of systematic variation are found to be relatively invariant within the individual, but individuals display large qualitative differences, one from the other.","['We seek to understand the systematic variation of such para-linguistic activity as a function of the ebb and flow of conversation.', 'This facilitates a thorough examination of the rich covariation of gaze and blinking both within an individual and as a function of the dyad.']"
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. For some time I have been thinking of the predicament of present-day linguistics in terms of what I venture to call ""the written language bias"". It seems to me that a great number of our explicit or implicit theories, our methods and preferences are heavily influenced by the very long traditions of analyzing mainly, or only, certain kinds of written language. This bock was largely written in t he academic year 1980/81 when I was employed by the Swedish Research Council for the Humanities and Social Sciences. When I worked at it, I profited from many ideas and suggestions of other people. Among these schalars I want to single out two, Jens Allwood and Ragnar Ronunetveit, with whom I had many inspiring discussions I will argue that the way in which we conventionally treat of the phenomenon called ""language"" is overly restrictive, and seems more appropriate to the characterization of writing than speaking/listening (#CITATION_TAG).",,['This essay represents a first attempt on my part to systematize some thoughts about this.']
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. This approach is rooted in dynamical approaches to coordination that are levelagnostic, seeking to understand emergent phenomena at one level (e.g., the dyad) as arising through processes of self-organization from the constrained interaction of autonomous components at a lower level (the speaker/listeners) (#CITATION_TAG; Latash, 2008).",,"[""In this article, we comprehensively investigate the dynamic patterns of 50 thousands of researchers' activities in Sciencenet, the largest multi-disciplinary academic community in China.""]"
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. Joint speech is familiar from collective prayer, protest chants, and many other contexts in which group purpose finds expression. An experimental form of joint speech, called synchronous speech, has been studied and some findings are recounted here. However the larger question of how and why joint speaking arises remains to be studied. I present a dynamical systems perspective on the coordination that joint speakers employ and show how it can account for some, but not yet all, aspects of the observed phenomenon. A discussao enfatiza os aspectos coordenativos de fa- lar e fazer musica, e minimiza o papel da comunicacao de mensagens (message passing) com o qual a linguistica tem se preocupado tradicionalmente. A fala conjunta, quando um grupo de pessoas diz o mesmo texto, ao mesmo tempo, e identificada como uma questao importante nes- se continuum, ligando fala e musica. A fala conjunta e comum na prece coletiva, em gritos de protesto e em muitos outros contextos em que o proposito do grupo encontra expressao. No entanto, a questao mais ampla de como e por que surge a fala conjunta precisa ainda ser estudada. Apresento uma perspectiva de sistemas dinamicos acerca da coordena- cao empregada pelos individuos que falam em conjunto e mostro como ela pode explicar alguns, mas nao todos, aspectos do fenomeno observado. ""Joint speaking"" is an umbrella term I have coined to cover all occasions in which the same words are uttered by multiple people in unison (#CITATION_TAG).","The argument is made by emphasising the coordinative aspects to speaking and music making, and downplaying the role of message passing with which linguistics has traditionally been concerned. A continuum is identified from silent speech on one hand to full blown music and song on the other. Joint speech, where a group of people say the same text at the same time, is identified as an important point on this continuum, linking speech and music.",['This paper argues that music and speech are not accidentally related as some have claimed.']
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. Recommender systems learn about user preferences over time, automatically finding things of similar interest. Recommender systems do, however, suffer from cold-start problems where no initial information is available early on upon which to base recommendations.Semantic knowledge structures, such as ontologies, can provide valuable domain knowledge and user information. However, acquiring such knowledge and keeping it up to date is not a trivial task and user interests are particularly difficult to acquire and maintain. This approach is rooted in dynamical approaches to coordination that are levelagnostic, seeking to understand emergent phenomena at one level (e.g., the dyad) as arising through processes of self-organization from the constrained interaction of autonomous components at a lower level (the speaker/listeners) (Kelso, 1995; #CITATION_TAG).",This reduces the burden of creating explicit queries. The ontology is used to address the recommender systems cold-start problem. The recommender system addresses the ontology's interest-acquisition problem.,['This paper investigates the synergy between a web-based research paper recommender system and an ontology containing information automatically extracted from departmental databases available on the web.']
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. Synchronously read speech has shown to reduce a high degree of speaker variability of reading exhibited by speakers in laboratory recording; e.g., pause placement and duration, and speech rate. However, quantitative analysis of speech rate has rarely been found in studies on synchronous speech. Speech produced in these constrained laboratory settings is remarkably unremarkable, and the technique of having subjects speak in synchrony has been used as a device for obtaining unmarked speech in several phonetic studies (Krivokapić, 2007; #CITATION_TAG; O'Dell et al., 2010; Dellwo and Friedrichs, 2012).","Consistency and variability of speech rate are compared in both reading types across repetitions within a subject, across subjects, and across dialects. This global pattern is consistent across dialects, and stylized local variation of speech rate over prosodic uni...","['This study examines Mandarin Chinese (2 dialects from Taiwan and Shanghai), which is a syllable-time language and thus expected to exhibit a relatively stable speech rate, in both read-alone and read-together speech.']"
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. tivist and embodied theories of mind are, although pretty radical, not radical enough, because such theories buy into the representationalist doctrine that perceptual experi-ence (along with other forms of 'basic ' mentality) possesses representational content. It implies that perceptual experience lacks reference, truth conditions, accuracy conditions, or conditions of satisfaction. It is, I think, unpersuasive. However, it is not necessary to appeal to such unobservable constructs from a hidden Cartesian world (#CITATION_TAG).","Chapters 1-3 situate REC in relation to rival theories, Chapters 4-6 defend the book's core, anti-representationalist thesis, Chapter 7 argues that REC is superior to more orthodox enactivist and embodied theories, and Chapter 8 argues that REC sheds new light on phenomenal consciousness. (2) Therefore it is probable that the correct naturalistic account of representational content will be provided by a theory that appeals to facts about human natura",['I here focus on the anti-representationalist argument of Chapters 4-6.']
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. Participation in such activities requires not only especially powerful forms of intention reading and cultural learning, but also a unique motivation to share psychological states with others and unique forms of cognitive representation for doing so. The result of participating in these activities is species-unique forms of cultural cognition and evolution, enabling everything from the creation and use of linguistic symbols to the construction of social norms and individual beliefs to the establishment of social institutions. Human children's skills of shared intentionality develop gradually during the first 14 months of life as two ontogenetic pathways intertwine: (1) the general ape line of understanding others as animate, goal-directed, and intentional agents; and (2) a species-unique motivation to share emotions, experience, and activities with other persons. The other great apes do not have such a contrast, and their ability to align their gaze is severely limited, and based on head direction rather than the eyes-although chimpanzees and bonobos in particular do display some evidence of understanding the visual perspective of another (#CITATION_TAG).",,"['We propose that the crucial difference between human cognition and that of other species is the ability to participate with others in collaborative activities with shared goals and intentions: shared intentionality.', 'In support of this proposal we argue and present evidence that great apes (and some children with autism) understand the basics of intentional action, but they still do not participate in activities involving joint intentions and attention (shared intentionality).']"
"to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' ""cogito""), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. In an experimental study, an adult attempted to teach novel words to 10 17-month-old children. The ability to follow each other's gaze thus facilitates the sharing of attention, and has been demonstrated to structure mother-child interactions, while inducing the abilty to take part in languaging (#CITATION_TAG).","In the first study, 24 children were videotaped at 15 and 21 months of age in naturalistic interaction with their mothers. Episodes of joint attentional focus between mother and child--for example, joint play with an object--were identified. Inside, as opposed to outside, these episodes both mothers and children produced more utterances, mothers used shorter sentences and more comments, and dyads engaged in longer conversations.","[""This paper reports 2 studies that explore the role of joint attentional processes in the child's acquisition of language.""]"
"Throughout Earth's history, life has increased greatly in abundance, complexity, and diversity. At the same time, it has substantially altered the Earth's environment, evolving some of its variables to states further and further away from thermodynamic equilibrium. For instance, concentrations in atmospheric oxygen have increased throughout Earth's history, resulting in an increased chemical disequilibrium in the atmosphere as well as an increased redox gradient between the atmosphere and the Earth's reducing crust. This is applied to the processes of planet Earth to characterize the generation and transfer of free energy and its dissipation, from radiative gradients to temperature and chemical potential gradients that result in chemical, kinetic, and potential free energy and associated dynamics of the climate system and geochemical cycles. Here, I present this hierarchical thermodynamic theory of the Earth system. This perspective allows us to view life as being the means to transform many aspects of planet Earth to states even further away from thermodynamic equilibrium than is possible by purely abiotic means. In this perspective pockets of low-entropy life emerge from the overall trend of the Earth system to increase the entropy of the universe at the fastest possible rate. The Earth system is maintained in a unique state far from thermodynamic equilibrium, as, for in- stance, reflected in the high concentration of reactive oxygen in the atmosphere. The myriad of processes that transform energy, that result in the motion of mass in the atmosphere, in oceans, and on land, processes that drive the global water, carbon, and other biogeo- chemical cycles, all have in common that they are irre- versible in their nature. Entropy production is a general consequence of these processes and measures their de- gree of irreversibility. The proposed principle of max- imum entropy production (MEP) states that systems are driven to steady states in which they produce en- tropy at the maximum possible rate given the prevailing constraints. Nonequilibrium thermodynamics and the MEP principle have potentially wide-ranging implications for our understanding of Earth system functioning, how it has evolved in the past, and why it is habitable. Entropy production allows us to quantify an objective direction of Earth system change (closer to vs further away from thermodynamic equilibrium, or, equivalently, towards a state of MEP). These processes generally operate away from a state of thermodynamic equilibrium and there is some evidence that these operate in steady states at which they maximize their dissipative activity, or, almost equivalently, maximize power generation or entropy production (the proposed Maximum Entropy Production (MEP) principle, [23] [24] [25] [#CITATION_TAG] [27]).","Applications of the MEP principle are discussed, ranging from the strength of the atmospheric circulation, the hydrological cycle, and biogeochemical cycles to the role that life plays in these processes. When a max- imum in entropy production is reached, MEP implies that the Earth system reacts to perturbations primarily with negative feedbacks.","['In this review, the basics of nonequilibrium thermodynamics are described, as well as how these apply to Earth system processes.', 'This perspective is likely to allow us to better understand and predict its function as one entity, how it has evolved in the past, and how it is modified by human activities in the future.']"
"Throughout Earth's history, life has increased greatly in abundance, complexity, and diversity. At the same time, it has substantially altered the Earth's environment, evolving some of its variables to states further and further away from thermodynamic equilibrium. For instance, concentrations in atmospheric oxygen have increased throughout Earth's history, resulting in an increased chemical disequilibrium in the atmosphere as well as an increased redox gradient between the atmosphere and the Earth's reducing crust. This is applied to the processes of planet Earth to characterize the generation and transfer of free energy and its dissipation, from radiative gradients to temperature and chemical potential gradients that result in chemical, kinetic, and potential free energy and associated dynamics of the climate system and geochemical cycles. Here, I present this hierarchical thermodynamic theory of the Earth system. This perspective allows us to view life as being the means to transform many aspects of planet Earth to states even further away from thermodynamic equilibrium than is possible by purely abiotic means. In this perspective pockets of low-entropy life emerge from the overall trend of the Earth system to increase the entropy of the universe at the fastest possible rate. During the time, 3.2 x 109 years, that life has been present on Earth, the physical and chemical conditions of most of the planetary surface have never varied from those most favourable for life. The geological record reads that liquid water was always present and that the pH was never far from neutral. During this same period, however, the Earth's radiation environment underwent !arge changes. At the same time hydrogen was escaping to space from the Earth and so causing progressive changes in the chemical environment. It may have been that these physical and chemical changes always by blind chance fo!lowed the path whose bounds are the conditions favouring the continued existence of life. lt is widely believed that the abundance of the principal gases N 2 and 0 2 is determined by equilibrium chemistry. One of the larger problems in the atmospheric sciences is that of reconciling this belief with the uncomfortable fact that these same gases are cycled by the Biosphere with a geometric mean residence time measured in thousands of years. The more thoroughly the inventory of an individual gas is audited the more certain it seems that inorganic equilibrium or steady state processes determine its atmospheric concentration but the same audit frequently further reveals the extent of its biological involvement. A lucid account of contemporary information on the problem of the cycle of gases is in the paper of Junge (1972). Thus on Earth the simultaneous presence of 0 2 and CH4 at the present concentrations is a violation of the rules of equilibrium chemistry of no less than 30 orders of magnitude. Indeed so great is the disequilibrium among the gases of the Earth's atmosphere that it tends towards a combustible mixture, whereas the gases of Mars and Venus are close to chemical equilibrium and are more like combustion products. The anomalous nature of the atmosphere has been known since Lewis & Randall (1923) first commented that at the pE and pH of the Earth the stable compound of nitrogen is the NO:! ion in the oceans; gaseous nitrogen should not be present. In spite of reminders by Hutchinson Tellus XXVI (1974), 1-2 ATMOSPHERIC HOMEOSTASIS BY AND FOR THE BIOSPHERE 3 (1954) and Sillen (1966) this anomaly has remained unnoticed in the debate on atmospheric cycles. Given the chemical composition of a planetary atmosphere it is possible to infer the presence or absence of life, Hitchcock & Lovelock (1967). The notion of the biosphere as an active adaptive control system able to maintain the Earth in homeostasis we are calling the 'Gaia' hypothesis, Lovelock (1972). This commonality of living organisms and purely physical dissipative processes led Lovelock and Margulis [#CITATION_TAG] to use the metaphor of describing Earth as a superorganism.",A starting point is a consideration of the profoundly anomalous composition of the Earth's atmosphere when it is compared with that of the expected atmosphere of a planet interpolated between Mars and Venus. To do this the entire ensemble of reactive gases constituting the atmosphere needs be considered and when this is dorre information is made available which is otherwise inaccessible when each gas is considered separately in isolation. Hence forward the word Gaia will be used to describe the biosphere and all of those parts of the Earth with which it actively interacts to form the hypothetical new entity with properties that could not be predicted from the sum of its parts.,"['This paper offers an alternative explanation that, early after life began it acquired control of the planetary environment and that this homeostasis by and for the biosphere has persisted ever since.', 'This paper presents a new view of the atmosphere, one in which it is seen as a component part of the biosphere rather than as a mere environment for life.', ""This approach applied to the present problem of the anomaly of the chemical distribution of the gases of the atmosphere, offers a strong suggestion that the Earth's atmosphere is more than merely anomalous; it appears to be a contrivance specifically constituted for a set of purposes."", 'This paper examines the hypothesis that the total ensemble of living organisms which constitute the biosphere can act as a single entity to regulate chemical composition, surface pH and possibly also climate.']"
"Throughout Earth's history, life has increased greatly in abundance, complexity, and diversity. At the same time, it has substantially altered the Earth's environment, evolving some of its variables to states further and further away from thermodynamic equilibrium. For instance, concentrations in atmospheric oxygen have increased throughout Earth's history, resulting in an increased chemical disequilibrium in the atmosphere as well as an increased redox gradient between the atmosphere and the Earth's reducing crust. This is applied to the processes of planet Earth to characterize the generation and transfer of free energy and its dissipation, from radiative gradients to temperature and chemical potential gradients that result in chemical, kinetic, and potential free energy and associated dynamics of the climate system and geochemical cycles. Here, I present this hierarchical thermodynamic theory of the Earth system. This perspective allows us to view life as being the means to transform many aspects of planet Earth to states even further away from thermodynamic equilibrium than is possible by purely abiotic means. In this perspective pockets of low-entropy life emerge from the overall trend of the Earth system to increase the entropy of the universe at the fastest possible rate. International audienceAfter a career as a chemist and engineer, James Lovelock proposed the Gaia hypothesis in the 1970's with Lynn Margulis, a biologist. From the beginning Lovelock saw Gaia as a grand idea, challenging the way biology and geology should be carried out, and up to our very conception of nature. This chapter recalls the rich context in which the hypothesis was elaborated in the 1960's and 1970's. Wheras evolutionary biologists ridiculed it as a pseudo-metaphor comparing the Earth with an organism, Gais has generated new research programs in the Earth sciences and has been embraced by the environmental counterculture as a new conception of nature and of our relationships with the Earth The resulting view of the Earth system is then compared to the Gaia hypothesis of James Lovelock [40, #CITATION_TAG, 30] and the role of human activity in the global work budget is being discussed.",It then traces Gaia's contrasted reception.,['The hypothesis highlights the important influence that living beings have on their geological environment to speculate about the possibility of a regulation of the planetary environment.']
"Throughout Earth's history, life has increased greatly in abundance, complexity, and diversity. At the same time, it has substantially altered the Earth's environment, evolving some of its variables to states further and further away from thermodynamic equilibrium. For instance, concentrations in atmospheric oxygen have increased throughout Earth's history, resulting in an increased chemical disequilibrium in the atmosphere as well as an increased redox gradient between the atmosphere and the Earth's reducing crust. This is applied to the processes of planet Earth to characterize the generation and transfer of free energy and its dissipation, from radiative gradients to temperature and chemical potential gradients that result in chemical, kinetic, and potential free energy and associated dynamics of the climate system and geochemical cycles. Here, I present this hierarchical thermodynamic theory of the Earth system. This perspective allows us to view life as being the means to transform many aspects of planet Earth to states even further away from thermodynamic equilibrium than is possible by purely abiotic means. In this perspective pockets of low-entropy life emerge from the overall trend of the Earth system to increase the entropy of the universe at the fastest possible rate. Abstract: The East Sea is undergoing physical changes caused by global warming: deepening oxygen minimum layer and increasing temperature. However, any correspondence to biogeochemical change was not noticed yet. However, note that the East Sea is still high concentration in oxygen compared with the condition denitrified. The present-day average value for this influx is about 342 W m −2 [#CITATION_TAG], which by using the surface area of the Earth, 511 • 10 12 m 2, yields a global total sum of about 175 000 TW (1 TW = 10 12 W).","Here, we present indirect evidences of denitrification in response to a global warming and estimate the amount of denitrification by the linear inverse model. Intense N/P ratio and oxygen minimum zones are founded between 900m and 2200m in the Ulleung Basin. Two stations are strongly expected to occur denitrification, where is located within the Ulleung Basin, show a series of phenomena at specific depths that nitrate profile is reversed, N/P ratio is less than 12.4, and nitrite shows a peak.","['Therefore, we need to note the future change of the East Sea']"
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. Recent research on early word learning suggests that children's behavior when-generalizing novel nouns integrates their prior vocabulary knowledge with the specifics of the task. Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (#CITATION_TAG; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007).","In ***1 condition, we used a combination of training and stimulus factors predicted to produce a bias to generalize nouns by shape similarity. We then reduced this shape bias and amplified a bias to generalize nouns by material similarity via manipulations of training and stimuli across 3 other conditions.",['This study examines how these factors interact on the moment-to-moment time scale of the training children receive and the sequence of stimuli they are shown.']
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. We argue that what infants learn about naming nonsolid substances is contextually bound - most nonsolids that toddlers are familiar with are foods and thus, typically experienced when sitting in a highchair. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (#CITATION_TAG).","We examine developmental interactions between context, exploration, and word learning. We asked whether 16-month-old children's naming of nonsolids would improve if they were tested in that typical context. Furthermore, context-based differences in exploration drove differences in the properties attended to in real-time.",['We discuss what implications this context-dependency has for understanding the development of an ontological distinction between solids and nonsolids.']
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. Although vocabulary acquisition requires children learn names for multiple things, many investigations of word learning mechanisms teach children the name for only one of the objects presented. This is problematic because it is unclear whether children's performance reflects recall of the correct name-object association or simply selection of the only object that was singled out by being the only object named. Children introduced to one novel name may perform at ceiling as they are not required to discriminate on the basis of the name per se, and appear to rapidly learn words following minimal exposure to a single word. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (#CITATION_TAG) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007;. For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011).","We introduced children to four novel objects. For half the children, only one of the objects was named and for the other children, all four objects were named. Only children introduced to one word reliably selected the target object at test.",['This demonstration highlights the over-simplicity of one-word learning paradigms and the need for a shift in word learning paradigms where more than one word is taught to ensure children disambiguate objects on the basis of their names rather than their degree of salience']
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. Children acquire attentional biases that help them generalize novel words to novel objects. However, evidence supporting this proposal to date comes from studies of group means. Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schöner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; #CITATION_TAG).",Researchers have proposed that these biases arise from regularities in the early noun vocabulary children learn and suggest that the specifics of the biases should be tied to the specifics of individual children's vocabularies. We show that individual differences in vocabulary structure predict individual differences in novel noun generalization.,['The current study examines the relations between the statistics of the nouns young children learn and the similarities and differences in the biases they demonstrate.']
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. Minority-race children in North America and Europe often show less own-race favoritism than children of the majority (White) race, but the reasons for this asymmetry are unresolved. Black children (3-13 years) tested in a Black township preferred people of their own gender but not race. Relative familiarity and numerical majority/minority status therefore do not fully account for children's racial attitudes, which vary with the relative social status of different racial groups.2011 Blackwell Publishing Ltd. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (#CITATION_TAG).","We assessed South African children's preferences for members of their country's majority race (Blacks) compared to members of other groups, including Whites, who ruled South Africa until 1994 and who remain high in status.","[""The present research tested South African children in order to probe the influences of group size, familiarity, and social status on children's race-based social preferences.""]"
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. A well-known characteristic of working memory (WM) is its limited capacity. The source of such limitations, however, is a continued point of debate. Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (#CITATION_TAG).","Here we provide a cross-domain survey of studies and theories of WM capacity development, which reveals a complex picture: dozens of studies from 50 papers show nearly universal increases in capacity estimates with age, but marked variation across studies, tasks, and domains. We argue that the full pattern of performance cannot be captured through traditional approaches emphasizing single causes, or even multiple separable causes, underlying capacity development. Rather, we consider WM capacity as a dynamic process that emerges from a unified cognitive system flexibly adapting to the context and demands of each task.",['Developmental research is positioned to address this debate by jointly identifying the source(s) of limitations and the mechanism(s) underlying capacity increases.']
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. People are remarkably smart: they use language, possess complex motor skills, make non-trivial inferences, develop and use scientific theories, make laws, and adapt to complex dynamic environments. It is argued that conceptual development progresses from simple perceptual grouping to highly abstract scientific concepts. This paper reviews a large body of empirical evidence supporting this proposal. All categories are not created equal: categories vary in complexity and withincategory similarity (#CITATION_TAG).","This proposal of conceptual development has four parts. First, it is argued that categories in the world have different structure. Third, these systems exhibit differential maturational course, which affects how categories of different structures are learned in the course of development.",['Much of this knowledge requires concepts and this paper focuses on how people acquire concepts.']
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. A large body of evidence supports the importance of focused attention for encoding and task performance. Yet young children with immature regulation of focused attention are often placed in elementary-school classrooms containing many displays that are not relevant to ongoing instruction. We know that environment matters because there are significant effects of household chaos (Petrill et al., 2004), excessive classroom decorations (#CITATION_TAG) and environmental noise (for a review see, Klatte et al., 2013) on children's cognition.","We placed kindergarten children in a laboratory classroom for six introductory science lessons, and we experimentally manipulated the visual environment in the classroom.","[""We investigated whether such displays can affect children's ability to maintain focused attention during instruction and to learn the lesson content.""]"
"Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category ""toys to pick up before bed"" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the ""just previous"" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, ""boring"" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term ""categorization."" Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as ""how do prototype representations compare between 6 and 8 months of age?"" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. Children's early noun vocabularies are dominated by names for shape-based categories. However, along with shape, material and colour are also important features of many early categories. shape) or that shared two features (e.g. Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (#CITATION_TAG; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007).","Preschool children and adults were presented with test objects that shared only one feature (e.g. After each trial, participants were asked, 'how did you know that was your [novel name]?' Overall, participants generalized novel names on the basis of shape more when objects shared shape and a second feature with the exemplar. All participants provided shape-based explanations of their choices, but explanations were increasingly more abstract across development.","[""In the current study, we investigate how the number of shared features among objects influences children's novel noun generalizations, explanations for these generalizations and spontaneous speech.""]"
"Interestingly, this framework also coincides with recent findings obtained by #CITATION_TAG.",,['We propose that biases in attitude and stereotype formation might arise as a result of learned differences in the extent to which social groups have previously been predictive of behavioral or physical properties.']
"Deep brain stimulation (DBS) is a standard therapy for several movement disorders, and the list of further indications that are investigated is growing rapidly. BACKGROUND Deep brain stimulation (DBS) is a recent treatment modality. Few studies have examined referral practices for DBS. PATIENTS Reviewed were 197 medical records of patients referred for DBS between December 1, 2005, and November 30, 2009. Referrals by movement disorder specialists vs other sources differed significantly in their percentages of good candidates (66.7% vs 40.4%, P = .002) and possible future candidates (14.7% vs 32.7%, P = .02) but not poor candidates (18.7% vs 25.0%, P = .60). Adequate expertise is necessary, as movement disorder specialists are more likely to identify good candidates for DBS (#CITATION_TAG).",Yearly percentages were computed. Referral sources were categorized as movement disorder specialists vs non-movement disorder physicians and self-referred.,['OBJECTIVE To review referral patterns to a large movement disorders center to investigate the current level of knowledge surrounding DBS candidacy.']
"Deep brain stimulation (DBS) is a standard therapy for several movement disorders, and the list of further indications that are investigated is growing rapidly. Academic Centers in Selection of DBS Candidates [COMPRESS], NeuroTrax Corp., Bellaire, TX, USA) with traditional triage by a movement disorders specialized neurologist as the gold standard. Several factors contribute to this underrating: Referring clinicians may underestimate the number of suitable patients (#CITATION_TAG), women are under-represented in those referred (Setiawan et al. 2006), and the amount of suitable candidates could increase, if patients would be referred earlier to DBS (Charles et al. 2012; Schüpbach et al. 2013).",,['Objective:  The objective of this study is to compare a computerized deep brain stimulation (DBS) screening module (Comparing Private Practice vs.']
"Deep brain stimulation (DBS) is a standard therapy for several movement disorders, and the list of further indications that are investigated is growing rapidly. Case registries are indispensable for preventing a publication bias and its negative consequences, namely faulty evaluations of therapies, flawed therapy recommendations, unpromising treatment attempts and unneeded clinical studies (Müller and Christen 2011; #CITATION_TAG; Schläpfer and Fins 2010; Woopen et al. 2012).",,"['A 2-day consensus conference was held to examine scientific and ethical issues in the application of deep brain stimulation for treating mood and behavioral disorders, such as major depression, obsessive-compulsive disorder, and Tourette syndrome.The primary objectives of the conference were to (1) establish consensus among participants about the design of future clinical trials of deep brain stimulation for disorders of mood, behavior, and thought and (2) develop standards for the protection of human subjects participating in such studies.Conference participants identified 16 key points for guiding research in this growing field.The adoption of the described guidelines would help to protect the safety and rights of research subjects who participate in clinical trials of deep brain stimulation for disorders of mood, behavior, and thought and have further potential to benefit other stakeholders in the research process, including clinical researchers and device manufactures.']"
"Deep brain stimulation (DBS) is a standard therapy for several movement disorders, and the list of further indications that are investigated is growing rapidly. Guidelines for safe and ethical conduct of such procedures have previously and independently been proposed by various local and regional expert groups. In particular, an international expert panel has recently stated in a consensus paper that ""until scientifically proven otherwise, DBS is not superior to ablative surgery for psychiatric disorders"" (#CITATION_TAG).","Methods To expand on these earlier documents, representative members of continental and international psychiatric and neurosurgical societies, joined efforts to further elaborate and adopt a pragmatic worldwide set of guidelines. Researchers are encouraged to design randomised controlled trials, based on scientific and data-driven rationales for disease and brain target selection. Experienced multidisciplinary teams are a mandatory requirement for the safe and ethical conduct of any psychiatric neurosurgery, ensuring documented refractoriness of patients, proper consent procedures that respect patient's capacity and autonomy, multifaceted preoperative as well as postoperative long-term follow-up evaluation, and reporting of effects and side effects for all patients.","['These are intended to address a broad range of neuropsychiatric disorders, brain targets and neurosurgical techniques, taking into account cultural and social heterogeneities of healthcare environments.', 'Interpretation This consensus document on ethical and scientific conduct of psychiatric surgery worldwide is designed to enhance patient safety.']"
"Previous research has shown that people form impressions of potential leaders from their faces and that certain facial features predict success in reaching prestigious leadership positions. However, much less is known about the accuracy or meta-accuracy of face-based leadership inferences. Here we examine a simple, but important, question: Can leadership domain be inferred from faces? However, people are surprisingly bad at evaluating their own performance on this judgment task: We find no relationship between how well judges think they performed and their actual accuracy levels. Recent research has shown that rapid judgments about the personality traits of political candidates, based solely on their appearance, can predict their electoral success. This suggests that voters rely heavily on appearances when choosing which candidate to elect. Yet, the human mind often relies on superficial cues to form judgments or make decisions, and the choice of which leader to select is no exception: A large and growing literature shows that facial appearances predict success in reaching prestigious leadership positions (Antonakis & Jacquart, 2013; #CITATION_TAG).","We also reanalyze previous data to show that facial competence is a highly robust and specific predictor of political preferences. Finally, we introduce a computer model of face-based competence judgments, which we use to derive some of the facial features associated with these judgments.",['Here we review this literature and examine the determinants of the relationship between appearance-based trait inferences and voting.']
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. : social phobia has been under-recognised and under-treated in many countries. Little is known about its economic impact. The defining questions for social phobia have not been studied much before. The number of identified subjects is small and thus raises the possibility of type II errors. They also incur considerable healthcare costs, especially relating to the use of primary care services, experience high levels of productivity losses and receive higher social benefits compared with people in the general population [#CITATION_TAG] [7] [8].",Methods: secondary analysis of 1993-1994 Psychiatric Morbidity Survey data compared 63 people with social phobia and 8501 people without psychiatric morbidity. Limitations: analyses were performed post hoc on data collected for other purposes.,"['This study aimed to identify the economic consequences of social phobia for individuals, health services and wider society.']"
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is common, debilitating and associated with high societal costs. Intervention costs of both treatments are offset by net societal cost reductions in a short time. Published economic analyses have explored the cost-effectiveness of a very limited range of interventions for social anxiety disorder and concluded that escitalopram [53], group CBT [54] [55] [#CITATION_TAG] and computer-based self-help [55] [56] [57] are cost-effective options.",We conducted a 4-year follow-up study of participants who had received ICBT or CBGT for SAD within the context of a randomized controlled non-inferiority trial. The cost-effectiveness analyses were conducted taking a societal perspective.,['The aim of the study was to investigate the clinical effectiveness and cost-effectiveness of ICBT compared to cognitive behavioral group therapy (CBGT) four years post-treatment.']
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is highly prevalent and associated with a substantial societal economic burden, primarily due to high costs of productivity loss. Cognitive behavior group therapy (CBGT) is an effective treatment for SAD and the most established in clinical practice. Internet-based cognitive behavior therapy (ICBT) has demonstrated efficacy in several trials in recent years. No study has however investigated the cost-effectiveness of ICBT compared to CBGT from a societal perspective, i.e. Published economic analyses have explored the cost-effectiveness of a very limited range of interventions for social anxiety disorder and concluded that escitalopram [53], group CBT [54] [#CITATION_TAG] [56] and computer-based self-help [55] [56] [57] are cost-effective options.",an analysis where both direct and indirect costs are included. We conducted a randomized controlled trial where participants with SAD were randomized to ICBT (n=64) or CBGT (n=62).,['The aim of the present study was to investigate the cost-effectiveness of ICBT compared to CBGT from a societal perspective using a prospective design.']
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. The first-year probability of relapse after recovery with a drug was estimated using pooled data from 5 placebo-controlled pharmacological RCTs on relapse prevention in adults with social anxiety disorder [26] [27] [28] [29] [#CITATION_TAG], identified by a systematic literature search; this probability was estimated to be higher that the pooled risk of relapse during maintenance treatment in RCT active drug arms, but lower than the pooled risk of relapse of responders to initial drug treatment who were subsequently randomised to placebo (thus not receiving maintenance treatment); for simplicity and due to lack of more suitable data the first year probability of relapse for drugs was assumed to equal the midpoint of the two pooled risks.","Patients with Diagnostic and Statistical Manual of Mental Disorders (Fourth Edition) generalized SAD, who met responder criteria after 10 weeks of open-label treatment with fixed-dose pregabalin (450 mg/day; n=153), were randomly assigned to 26 weeks of double-blind treatment with pregabalin (450 mg/day) or placebo. The primary a-priori outcome of time to relapse was analyzed using the Kaplan-Meier method and the log-rank test.",['The objective of this study was to evaluate the efficacy and safety of pregabalin in preventing relapse in generalized social anxiety disorder (SAD).']
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is associated with low direct costs compared to other anxiety disorders while indirect costs tend to be high. Mental comorbidities have been identified to increase costs, but the role of symptom severity is still vague. They also incur considerable healthcare costs, especially relating to the use of primary care services, experience high levels of productivity losses and receive higher social benefits compared with people in the general population [6] [7] [#CITATION_TAG].",Costs were calculated based on health care utilization and lost productivity.,"['The objective of this study was to determine the costs of SAD, and to explore the impact of symptoms and comorbidities on direct and indirect costs.Baseline data, collected within the SOPHO-NET multi-centre treatment study (N=495), were used.']"
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Magister Psychologiae - MPsychThe propensity to blush is typical of many individuals with social anxiety disorder (SAD). The relationship between SAD, blushing and functional impairment is still not completely understood however. People with social anxiety disorder have difficulty forming and retaining personal and social relationships [2], have higher risk of leaving school early and obtaining poorer qualifications [3], experience impairment in their daily functioning including work/school performance and social life [#CITATION_TAG], and report an important reduction in their quality of life compared with people without the disorder [5].","Data from thirty-eight (n=38) individuals with SAD, were collected via a larger study conducted at the MRC Anxiety and Stress Disorders Unit. Assessment tools include the Structured Clinical Interview for Axis I disorders - Patient Version (SCID -I/P), Social Phobia Inventory (SPIN) and the Blushing Propensity Scale. Demographic and clinical data were gathered and reported on. Spearman rank order correlations were used to determine relationships between variables, including blushing propensity, disability and symptom severity.",['This study has focused on the association between the propensity to blush and reported level of functional impairment due to SAD.']
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Neuroimaging has played an important part in advancing our understanding of the neurobiology of obsessive-compulsive disorder (OCD). At the same time, neuroimaging studies of OCD have had notable limitations, including reliance on relatively small samples. International collaborative efforts to increase statistical power by combining samples from across sites have been bolstered by the ENIGMA consortium; this provides specific technical expertise for conducting multi-site analyses, as well as access to a collaborative community of neuroimaging scientists. Moreover, the mean probabilities of relapse for drugs and psychological interventions estimated for the economic model (42% versus 14%, respectively) are similar to relapse rates reported in trials of these types of therapies for social anxiety disorder, very close to respective relapse rates reported for people with obsessive compulsive disorder (45% versus 12%, respectively) [#CITATION_TAG] and broadly consistent with respective figures reported for panic disorder (40% versus 5%, respectively) [52].","In this article, we outline the background to, development of, and initial findings from ENIGMA's OCD working group, which currently consists of 47 samples from 34 institutes in 15 countries on 5 continents, with a total sample of 2,323 OCD patients and 2,325 healthy controls. Additional work is ongoing, employing machine learning techniques.","['Initial work has focused on studies of cortical thickness and subcortical volumes, structural connectivity, and brain lateralization in children, adolescents and adults with OCD, also including the study on the commonalities and distinctions across different neurodevelopment disorders.']"
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Nevertheless, evidence suggests that, in constrast to pharmacological interventions, which are characterised by a relatively high relapse risk at 6 months of maintenance treatment [26] [27] [28] [29] [30], the effect of psychological interventions is well-maintained in the long-term, after end of treatment [#CITATION_TAG, 50].","Method In this study 375 patients with social phobia were randomised to treatment with sertraline or placebo for 24 weeks, with or without the addition of exposure therapy Fifty-two weeks after inclusion, 328 patients were evaluated by the same psychometric tests as at baseline and the end of treatment (24 weeks).",['Aims To examine the effect of exposure therapy and sertraline 28 weeks after cessation of medical treatment.']
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. In financially constrained health systems across the world, increasing emphasis is being placed on the ability to demonstrate that health care interventions are not only effective, but also cost-effective. To account for the uncertainty around the input parameter point estimates, a probabilistic analysis was undertaken, in which input parameters were assigned probabilistic distributions [#CITATION_TAG].","Particular emphasis is placed on the importance of the appropriate representation of uncertainty in the evaluative process and the implication this uncertainty has for decision making and the need for future research. This highly practical guide takes the reader through the key principles and approaches of modelling techniques. It begins with the basics of constructing different forms of the model, the population of the model with input parameter estimates, analysis of the results, and progression to the holistic view of models as a valuable tool for informing future research exercises. ABOUT THE SERIES: Economic evaluation of health interventions is a growing specialist field, and this series of practical handbooks will tackle, in-depth, topics superficially addressed in more general health economics books. Each volume will include illustrative material, case histories and worked examples to encourage the reader to apply the methods discussed, with supporting material provided online.","['This book deals with decision modelling techniques that can be used to estimate the value for money of various interventions including medical devices, surgical procedures, diagnostic technologies, and pharmaceuticals.', 'This book will help analysts understand the contribution of decision-analytic modelling to the evaluation of health care programmes.', 'This series is aimed at health economists in academia, the pharmaceutical industry and the health sector, those on advanced health economics courses, and health researchers in associated fields.']"
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Other packages or programmes incorporating CCBT were also considered.Electronic databases from 1966 to March 2004. Independent research is needed, particularly RCTs, that examine areas such as patient preference and therapist involvement within primary care. • use of an alternative set of utility scores, based on EQ-5D data [#CITATION_TAG] derived from a community-based mental health European survey [46].","To evaluate computerised cognitive behaviour therapy (CCBT) for the treatment of anxiety, depression, phobias, panic and obsessive-compulsive behaviour (OCD). The software packages to be considered include Beating the Blues (BtB), Overcoming Depression: a five areas approach, FearFighter (FF), Cope and BT Steps. Evidence submitted by sponsors for CCBT products.A systematic review was a review of the literature and the evidence submitted by sponsors for each of the products. A series of cost-effectiveness models was developed and run by the project team for the five CCBT products across the three mental health conditions.Twenty studies were identified in the clinical effectiveness review.","['This is in addition to concerns with the quality of evidence on response to therapy, longer term outcomes and quality of life.']"
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Research deficits lie in a lack of data for most EU countries and in a lack of studies in children and the elderly. No data are available addressing met and unmet needs for intervention and costs, and data for vulnerability and risk factors of malignant course are scarce Social anxiety disorder is one of the most persistent and common anxiety disorders, with a lifetime prevalence estimated to range between 3.9% and 13.7% in Europe [#CITATION_TAG].","Social phobia was shown to be a persistent condition with a remarkably high degree of comorbid conditions, associated impairment and disability.","['This paper provides a critical review of the prevalence of social phobia in European countries, a description of associated disability and burden and of clinical correlates and risk factors associated with social phobia.']"
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. There are other medication classes with demonstrated efficacy in social phobia (benzodiazepines, antipsychotics, alpha-2-delta ligands), but due to limited published clinical trial data and the potential for dependence and withdrawal issues with benzodiazepines, it is unclear how best to incorporate these drugs into treatment regimens. There are very few clinical trials on the use of combined medications. There is some evidence, albeit limited to certain drug classes, that the combination of medication and cognitive behavior therapy may be more effective than either strategy used alone. Generalized social phobia is a chronic disorder, and many patients will require long-term support and treatment. Several studies have assessed the clinical effectiveness of psychological and pharmacological treatments for social anxiety disorder [10] [11] [#CITATION_TAG] [13].","An optimal treatment regimen would include a combination of medication and psychotherapy, along with an assertive clinical management program. For medications, selective serotonin reuptake inhibitors and dual serotonin-norepinephrine reuptake inhibitors are first-line choices based on their efficacy and tolerability profiles.","['This article proposes a number of recommendations for the treatment of generalized social phobia, based on a systematic literature review and meta-analysis.']"
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code. Efficacy data were derived from a systematic literature review and NMA of randomised controlled trials (RCTs) of interventions for adults with social anxiety disorder [14]; the NMA, based on a random effects model [22], was conducted within a Bayesian framework using Markov Chain Monte Carlo simulation techniques implemented in WinBUGS 1.4 [#CITATION_TAG, 24].","Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented.","[""In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design.""]"
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is a prevalent, disabling disorder. There was substantial variation across medication classes in the number of dropouts due to adverse events, with an average number needed to harm of 14.4. However, evidence for the efficacy of b-blockers in treating performance anxiety was lacking. This review is an updated version of a Cochrane Review in The Cochrane Library, Issue 4, 2004. Several studies have assessed the clinical effectiveness of psychological and pharmacological treatments for social anxiety disorder [10] [#CITATION_TAG] [12] [13].",A systematic review and meta-analysis was conducted of all published and unpublished placebo-controlled randomized controlled trials (RCTs) undertaken between 1966 and 2007.,['We aimed to assess the effects of pharmacotherapy for SAD and to determine whether particular classes of medication are more effective and/or better tolerated than others.']
"Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. This is necessary for rational public health policy. Following a systematic literature search of utility data for social anxiety disorder, the economic model was populated with utility scores obtained from a Finish national health survey [#CITATION_TAG] that reported EQ-5D utility scores (estimated using the UK Time Trade-Off Tarrif) [34] for people with social anxiety disorder and people with no mental disorder over the last 12 months.",Background Measurement of health-related quality of life (HRQoL) with generic preference-based instruments enables comparisons of severity across different conditions and treatments. Method A general population survey was conducted of Finns aged 30 years and over. Psychiatric disorders were diagnose with the Composite International Diagnostic Interview and HRQoL was measured with the 15D and EQ-5D questionnaires.,['Aims To measure HRQoL decrement and loss of quality-adjusted life-years (QALYs) associated with pure and comorbid forms of depressive and anxiety disorders and alcohol dependence.']
"Autism spectrum disorders (ASDs) are a group of clinically and genetically heterogeneous neurodevelopmental disorders characterized by impaired social interactions, repetitive behaviors and restricted interests (Baird et al., 2006; Zoghbi and Bear, 2012). The genetic defects in ASDs may interfere with synaptic protein synthesis. Synaptic dysfunction caused by aberrant protein synthesis is a key pathogenic mechanism for ASDs (Kelleher and Bear, 2008; Richter and Klann, 2009; Ebert and Greenberg, 2013). The mammalian target of the rapamycin (mTOR) pathway plays central roles in synaptic protein synthesis (Hay and Sonenberg, 2004; Hoeffer and Klann, 2010; Hershey et al., 2012). Recently, Gkogkas and colleagues published exciting data on the role of downstream mTOR pathway in autism (Gkogkas et al., 2013) (Figure 1). Previous studies have indicated that upstream mTOR signaling is linked to ASDs. Mutations in tuberous sclerosis complex (TSC) 1/TSC2, neurofibromatosis 1 (NF1), and Phosphatase and tensin homolog (PTEN) lead to syndromic ASD with tuberous sclerosis, neurofibromatosis, or macrocephaly, respectively (Kelleher and Bear, 2008; Bourgeron, 2009; Hoeffer and Klann, 2010; Sawicka and Zukin, 2012). Activation of cap-dependent translation is a principal downstream mechanism of mTORC1. The eIF4E-binding proteins (4E-BPs) bind to eIF4E and inhibit translation initiation. Phosphorylation of 4E-BPs by mTORC1 promotes eIF4E release and initiates cap-dependent translation (Richter and Klann, 2009; Hoeffer and Klann, 2010) (Figure 1). Notably, one pioneering study has identified a mutation in the EIF4E promoter in autism families (Neves-Pereira et al., 2009), implying that deregulation of downstream mTOR signaling (eIF4E) could be a novel mechanism for ASDs. As an eIF4E repressor downstream of mTOR, 4E-BP2 has important roles in synaptic plasticity, learning and memory (Banko et al., 2005; Richter and Klann, 2009). Writing in their Nature article, Gkogkas and colleagues reported that deletion of the gene encoding 4E-BP2 (Eif4ebp2) leads to autistic-like behaviors in mice. Pharmacological inhibition of eIF4E rectifies social behavior deficits in Eif4ebp2 knockout mice (Gkogkas et al., 2013). Are these ASD-like phenotypes of the Eif4ebp2 knockout mice caused by altered translation of a subset mRNAs due to the release of eIF4E? Interestingly, treatment of Eif4ebp2 knockout mice with selective eIF4E inhibitor reduces NLGN protein levels to wild-type levels (Gkogkas et al., 2013). However, it cannot be ruled out that other proteins (synaptic or non-synaptic) may be affected and contribute to animal autistic phenotypes. Aberrant information processing due to altered ratio of synaptic excitation to inhibition (E/I) may contribute to Frontiers in Cellular Neuroscience www.frontiersin.org March 2013 | Volume 7 | Article 28 | 1 CELLULAR NEUROSCIENCE FIGURE 1 | The mTOR signal pathway in autism spectrum disorders. Mutations in TSC1/2, NF1, and PTEN, or loss of FMRP due to mutations of the FMR1gene, cause hyperactivity of mTORC1-eIF4E pathway and lead to syndromic ASDs. Gkogkas et al. The increased or decreased E/I ratio has been observed in ASD animal models (Chao et al., 2010; Bateup et al., 2011; Luikart et al., 2011; Schmeisser et al., 2012). Understanding the details about aberrant synaptic protein synthesis is important to formulate potential treatment for ASDs. Synaptic transmission in neurons is a measure of communication at synapses, the points of contact between axons and dendrites. The magnitude of synaptic transmission is a reflection of the strength of these synaptic connections, which in turn can be altered by the frequency with which the synapses are stimulated, the arrival of stimuli from other neurons in the appropriate temporal window, and by neurotrophic factors and neuromodulators. The ability of synapses to undergo lasting biochemical and morphological changes in response to these types of stimuli and neuromodulators is known as synaptic plasticity, which likely forms the cellular basis for learning and memory, although the relationship between any one form synaptic plasticity and a particular type of memory is unclear. RNA metabolism, particularly translational control at or near the synapse, is one process that controls long-lasting synaptic plasticity and, by extension, several types of memory formation and consolidation. Synaptic dysfunction caused by aberrant protein synthesis is a key pathogenic mechanism for ASDs (Kelleher and Bear, 2008; #CITATION_TAG; Ebert and Greenberg, 2013).",,"['Here, we review recent studies that reflect the importance and challenges of investigating the role of mRNA translation in synaptic plasticity and memory formation.']"
"Autism spectrum disorders (ASDs) are a group of clinically and genetically heterogeneous neurodevelopmental disorders characterized by impaired social interactions, repetitive behaviors and restricted interests (Baird et al., 2006; Zoghbi and Bear, 2012). The genetic defects in ASDs may interfere with synaptic protein synthesis. Synaptic dysfunction caused by aberrant protein synthesis is a key pathogenic mechanism for ASDs (Kelleher and Bear, 2008; Richter and Klann, 2009; Ebert and Greenberg, 2013). The mammalian target of the rapamycin (mTOR) pathway plays central roles in synaptic protein synthesis (Hay and Sonenberg, 2004; Hoeffer and Klann, 2010; Hershey et al., 2012). Recently, Gkogkas and colleagues published exciting data on the role of downstream mTOR pathway in autism (Gkogkas et al., 2013) (Figure 1). Previous studies have indicated that upstream mTOR signaling is linked to ASDs. Mutations in tuberous sclerosis complex (TSC) 1/TSC2, neurofibromatosis 1 (NF1), and Phosphatase and tensin homolog (PTEN) lead to syndromic ASD with tuberous sclerosis, neurofibromatosis, or macrocephaly, respectively (Kelleher and Bear, 2008; Bourgeron, 2009; Hoeffer and Klann, 2010; Sawicka and Zukin, 2012). Activation of cap-dependent translation is a principal downstream mechanism of mTORC1. The eIF4E-binding proteins (4E-BPs) bind to eIF4E and inhibit translation initiation. Phosphorylation of 4E-BPs by mTORC1 promotes eIF4E release and initiates cap-dependent translation (Richter and Klann, 2009; Hoeffer and Klann, 2010) (Figure 1). Notably, one pioneering study has identified a mutation in the EIF4E promoter in autism families (Neves-Pereira et al., 2009), implying that deregulation of downstream mTOR signaling (eIF4E) could be a novel mechanism for ASDs. As an eIF4E repressor downstream of mTOR, 4E-BP2 has important roles in synaptic plasticity, learning and memory (Banko et al., 2005; Richter and Klann, 2009). Writing in their Nature article, Gkogkas and colleagues reported that deletion of the gene encoding 4E-BP2 (Eif4ebp2) leads to autistic-like behaviors in mice. Pharmacological inhibition of eIF4E rectifies social behavior deficits in Eif4ebp2 knockout mice (Gkogkas et al., 2013). Are these ASD-like phenotypes of the Eif4ebp2 knockout mice caused by altered translation of a subset mRNAs due to the release of eIF4E? Interestingly, treatment of Eif4ebp2 knockout mice with selective eIF4E inhibitor reduces NLGN protein levels to wild-type levels (Gkogkas et al., 2013). However, it cannot be ruled out that other proteins (synaptic or non-synaptic) may be affected and contribute to animal autistic phenotypes. Aberrant information processing due to altered ratio of synaptic excitation to inhibition (E/I) may contribute to Frontiers in Cellular Neuroscience www.frontiersin.org March 2013 | Volume 7 | Article 28 | 1 CELLULAR NEUROSCIENCE FIGURE 1 | The mTOR signal pathway in autism spectrum disorders. Mutations in TSC1/2, NF1, and PTEN, or loss of FMRP due to mutations of the FMR1gene, cause hyperactivity of mTORC1-eIF4E pathway and lead to syndromic ASDs. Gkogkas et al. The increased or decreased E/I ratio has been observed in ASD animal models (Chao et al., 2010; Bateup et al., 2011; Luikart et al., 2011; Schmeisser et al., 2012). Understanding the details about aberrant synaptic protein synthesis is important to formulate potential treatment for ASDs. Autism is a complex genetic disorder, but single-gene disorders with a high prevalence of autism offer insight into its pathogenesis. Recent evidence suggests that some molecular defects in autism may interfere with the mechanisms of synaptic protein synthesis. Synaptic dysfunction caused by aberrant protein synthesis is a key pathogenic mechanism for ASDs (#CITATION_TAG; Richter and Klann, 2009; Ebert and Greenberg, 2013).",,"['We propose that aberrant synaptic protein synthesis may represent one possible pathway leading to autistic phenotypes, including cognitive impairment and savant abilities.']"
"Autism spectrum disorders (ASDs) are a group of clinically and genetically heterogeneous neurodevelopmental disorders characterized by impaired social interactions, repetitive behaviors and restricted interests (Baird et al., 2006; Zoghbi and Bear, 2012). The genetic defects in ASDs may interfere with synaptic protein synthesis. Synaptic dysfunction caused by aberrant protein synthesis is a key pathogenic mechanism for ASDs (Kelleher and Bear, 2008; Richter and Klann, 2009; Ebert and Greenberg, 2013). The mammalian target of the rapamycin (mTOR) pathway plays central roles in synaptic protein synthesis (Hay and Sonenberg, 2004; Hoeffer and Klann, 2010; Hershey et al., 2012). Recently, Gkogkas and colleagues published exciting data on the role of downstream mTOR pathway in autism (Gkogkas et al., 2013) (Figure 1). Previous studies have indicated that upstream mTOR signaling is linked to ASDs. Mutations in tuberous sclerosis complex (TSC) 1/TSC2, neurofibromatosis 1 (NF1), and Phosphatase and tensin homolog (PTEN) lead to syndromic ASD with tuberous sclerosis, neurofibromatosis, or macrocephaly, respectively (Kelleher and Bear, 2008; Bourgeron, 2009; Hoeffer and Klann, 2010; Sawicka and Zukin, 2012). Activation of cap-dependent translation is a principal downstream mechanism of mTORC1. The eIF4E-binding proteins (4E-BPs) bind to eIF4E and inhibit translation initiation. Phosphorylation of 4E-BPs by mTORC1 promotes eIF4E release and initiates cap-dependent translation (Richter and Klann, 2009; Hoeffer and Klann, 2010) (Figure 1). Notably, one pioneering study has identified a mutation in the EIF4E promoter in autism families (Neves-Pereira et al., 2009), implying that deregulation of downstream mTOR signaling (eIF4E) could be a novel mechanism for ASDs. As an eIF4E repressor downstream of mTOR, 4E-BP2 has important roles in synaptic plasticity, learning and memory (Banko et al., 2005; Richter and Klann, 2009). Writing in their Nature article, Gkogkas and colleagues reported that deletion of the gene encoding 4E-BP2 (Eif4ebp2) leads to autistic-like behaviors in mice. Pharmacological inhibition of eIF4E rectifies social behavior deficits in Eif4ebp2 knockout mice (Gkogkas et al., 2013). Are these ASD-like phenotypes of the Eif4ebp2 knockout mice caused by altered translation of a subset mRNAs due to the release of eIF4E? Interestingly, treatment of Eif4ebp2 knockout mice with selective eIF4E inhibitor reduces NLGN protein levels to wild-type levels (Gkogkas et al., 2013). However, it cannot be ruled out that other proteins (synaptic or non-synaptic) may be affected and contribute to animal autistic phenotypes. Aberrant information processing due to altered ratio of synaptic excitation to inhibition (E/I) may contribute to Frontiers in Cellular Neuroscience www.frontiersin.org March 2013 | Volume 7 | Article 28 | 1 CELLULAR NEUROSCIENCE FIGURE 1 | The mTOR signal pathway in autism spectrum disorders. Mutations in TSC1/2, NF1, and PTEN, or loss of FMRP due to mutations of the FMR1gene, cause hyperactivity of mTORC1-eIF4E pathway and lead to syndromic ASDs. Gkogkas et al. The increased or decreased E/I ratio has been observed in ASD animal models (Chao et al., 2010; Bateup et al., 2011; Luikart et al., 2011; Schmeisser et al., 2012). Understanding the details about aberrant synaptic protein synthesis is important to formulate potential treatment for ASDs. Fragile X syndrome (FXS) is a common form of inherited intellectual disability and is one of the leading known causes of autism. The mutation responsible for FXS is a large expansion of the trinucleotide CGG repeat in the 5' untranslated region of the X-linked gene FMR1. A hyperactivated mTORC1-eIF4E pathway is linked to impaired synaptic plasticity in fragile X syndrome, an autistic disorder caused by lack of fragile X mental retardation protein (FMRP) due to mutation of the FMR1 gene (Wang et al., 2010; Auerbach et al., 2011; #CITATION_TAG; Wang et al., 2012), suggesting that downstream mTOR signaling might be causally linked to ASDs.","This expansion leads to DNA methylation of FMR1 and to transcriptional silencing, which results in the absence of the gene product, FMRP, a selective messenger RNA (mRNA)-binding protein that regulates the translation of a subset of dendritic mRNAs.","['FMRP is critical for mGluR (metabotropic glutamate receptor)-dependent long-term depression, as well as for other forms of synaptic plasticity; its absence causes excessive and persistent protein synthesis in postsynaptic dendrites and dysregulated synaptic function.']"
"Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. This creates a price differential between services that are identical for the consumer and generates network externalities despite network interconnection. #CITATION_TAG find that if firms set linear prices but can discriminate between on-and off-net calls, then above cost termination charges induce 3 In the initial stages of wireless telecommunication the most important regulatory issue was the fixedto-mobile (FTM) termination rate, i.e. the price to be paid by the incumbent land-line operator for calls terminating on a mobile network.","Our companion article developed a clear conceptual framework of negotiated or regulated interconnection agreements between rival operators and studied competition between interconnected networks, under the assumption of nondiscriminatory pricing.","[""This article relaxes this assumption and allows networks to charge different prices for calls terminating on the subscriber's network and those terminating on a rivals network.""]"
"Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. #CITATION_TAG show that the result also holds when there are both direct network externalities (i.e., elastic subscription demand as in Dessein, 2003) and tariff-mediated network externalities (i.e. onand off-net price differentiation as in Gans and King, 2001).","The former reduces mobile penetration while the latter boosts it. Next, we consider the retail benchmarking approach (Jeon and Hurkens, 2008) that determines termination charges as a function of retail prices and show that this approach allows the regulator to increase penetration without distorting call volumes.Mobile Penetration, Termination Charge, Access Pricing, Networks, Interconnection, Regulation, Telecommunications","['In this paper, we study how access pricing affects network competition when subscription demand is elastic and each network uses non-linear prices and can apply termination-based price discrimination.']"
"Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. The size of a firm determines the quality of its product: when network effects are positive, a larger firm is of higher quality; when the effects are negative, a larger firm's product is of lower quality. Consumers have heterogeneous preferences towards quality (firm size), and firms compete in prices. Equilibria are characterised: for example, in any asymmetric equilibrium, it must be that congestion is not too severe. One consequence of this feature is that an increase in the number of firms in the industry can raise individual firms' profits. Two factors can bound the number of firms in a free-entry equilibrium without fixed costs: expectations, and the 'finiteness' property (Shaked and Sutton, Review of Economic Studies 49 (1982) 3-13, Econometrica 51(5) (1983) 1469-1483) of price competition 20 #CITATION_TAG take the issue of expectations serious and point out that the results change dramatically if rationally responsive beliefs are used in their pricing game.",,['This paper analyses market structure of industries that are subject to both positive and negative network effects.']
Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. Economists have been paying increasing attention to the study of situations in which consumers face a discrete rather than a continuous set of choices. Such models are potentially very important in evaluating the impact of government programs upon consumer welfare. But very little has been said in general regarding the tools of applied welfare economics indiscrete choice situations. Consumer surplus in the Logit model has been derived by #CITATION_TAG as (up to a constant),This paper shows how the conventional methods of applied welfare economics can be modified to handle such cases.,"['It focuses on the computation of the excess burden of taxation, and the evaluation of quality change.', 'Throughout, the emphasis is on providing rigorous guidelines for carrying out applied work.']"
"Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. 7.7. 6 Ofcom, 2007 For an excellent literature review on two-way access pricing we refer the reader to #CITATION_TAG.","One-way access pricing, competition bottlenects and two-way access pricing are discussed.Telecommunications; access pricing; network interconnection; ECPR",['This paper surveys the theory of access pricing and interconnection in telecommunicatons.']
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [#CITATION_TAG, 8].","METHODS Physical activity was assessed for up to 21 consecutive days using the Computer Science Applications (CSA) accelerometer. Random effects models were employed to estimate variance components for subject, day of the week, and residual error from which the number of days of assessment required to achieve 80% reliability were estimated. Inter-individual variation, or differences between subjects, was proportionally the largest source of variance (55-60% of total) in accelerometer counts and time spent in moderate to vigorous activity.",['PURPOSE To examine sources of variance in objective measures of physical activity in a group of healthy adults (N = 92) participating in a physical activity measurement study.']
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Reliability, the consistency of a test or measurement, is frequently quantified in the movement sciences literature. A common metric is the intraclass correlation coefficient (ICC). In addition, the SEM, which can be calculated from the ICC, is also frequently reported in reliability studies. However, there are several versions of the ICC, and confusion exists in the movement sciences regarding which ICC to use. The primary distinction between ICC equations is argued to be one concerning the inclusion (equations 2,1 and 2,k) or exclusion (equations 3,1 and 3,k) of systematic error in the denominator of the ICC equation. Additionally, ICC is the variance partitioning of subjects to the total variance, thus ICC is a relative and context-specific estimate that depends on the heterogeneity of the sample [26] [27] [#CITATION_TAG].","If so, the measurement schedule should be modified (removing trials where learning and/or fatigue effects are present) to remove systematic error, and ICC equations that only consider random error may be safely used. The use of ICC values is discussed in the context of estimating the effects of measurement error on sample size, statistical power, and correlation attenuation. It is shown how the SEM and its variants can be used to construct confidence intervals for individual scores and to determine the minimal difference needed to be exhibited for one to be confident that a true change in performance of an individual has occurred.","['In this review, the basics of classic reliability theory are addressed in the context of choosing and interpreting an ICC.', 'Inferential tests of mean differences, which are performed in the process of deriving the necessary variance components for the calculation of ICC values, are useful to determine if systematic error is present.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Abstract Background The number of days of pedometer or accelerometer data needed to reliably assess physical activity (PA) is important for research that examines the relationship with health. While this important research has been completed in young to middle-aged adults, data is lacking in older adults. When examining time spent in specific intensities of PA, fewer days of data are needed for accurate prediction of time spent in that activity for ActiGraph but more for the PA log. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [6, #CITATION_TAG].","Methods Participants (52 older men and women; age = 69.3 +- 7.4 years, range= 55-86 years) wore a Yamax Digiwalker SW-200 pedometer and an ActiGraph 7164 accelerometer while completing a PA log for 21 consecutive days. Mean differences each instrument and intensity between days of the week were examined using separate repeated measures analysis of variance for with pairwise comparisons. Spearman-Brown Prophecy Formulae based on Intraclass Correlations of .80, .85, .90 and .95 were used to predict the number of days of accelerometer or pedometer wear or PA log daily records needed to represent total PA, light PA, moderate-to-vigorous PA, and sedentary behaviour. To accurately predict average daily time spent in sedentary behaviour, five days of ActiGraph data are needed.","['The purpose of this study was to examine the number of days needed to predict habitual PA and sedentary behaviour across pedometer, accelerometer, and physical activity log (PA log) data in older adults.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Selection of accelerometer therefore remains primarily an issue of practicality, technical support, and comparability with other studies. Studies employing multiple accelerometers to estimate energy expenditure report only marginal improvements in explanatory power. Although the issue of epoch length has not been studied in adults, the use of count cut points based on 1-min time intervals maybe inappropriate in children and may result in underestimation of physical activity. Among children and adolescents, the number of monitoring days required ranges from 4 to 9 d, making it difficult to draw a definitive conclusion for this population. Face-to-face distribution and collection of accelerometers is probably the best option in field-based research, but delivery and return by express carrier or registered mail is a viable option.Accelerometer-based activity assessments requires careful planning and the use of appropriate strategies to increase compliance. In adults,!3-5 days of monitoring are normally considered appropriate, which is in accordance with recommendations given [#CITATION_TAG].","Accelerometers are best placed on hip or the lower back. Among adults, 3-5 d of monitoring is required to reliably estimate habitual physical activity.","['The purpose of this review is to address important methodological issues related to conducting accelerometer-based assessments of physical activity in free-living individuals.We review the extant scientific literature for empirical information related to the following issues: product selection, number of accelerometers needed, placement of accelerometers, epoch length, and days of monitoring required to estimate habitual physical activity.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [#CITATION_TAG] [13] [14] [15] [16] [17], with some exceptions (!1 day [18];!6-7 days [19, 20]).",We also examined the association between light physical activity (LPA) and sporadic (accumulated in bouts 0.05).,['Objective The primary aim of this study was to determine whether time spent in sedentary behaviors (SED) was associated with 2-hour glucose and insulin resistance in adults with abdominal obesity.']
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Imprecise measurement of physical activity variables might attenuate estimates of the beneficial effects of activity on health-related outcomes. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [12] [13] [#CITATION_TAG] [15] [16] [17], with some exceptions (!1 day [18];!6-7 days [19, 20]).","Fasting blood was taken to determine insulin, glucose, triglyceride and total, LDL and HDL cholesterol concentrations and homeostasis model-estimated insulin resistance (HOMA(IR)). Using this self-report method to quantify activity can therefore underestimate the strength of some relationships with risk factors.",['We aimed to compare the cardiometabolic risk factor dose-response relationships for physical activity and sedentary behaviour between accelerometer- and questionnaire-based activity measures.Physical activity and sedentary behaviour were assessed in 317 adults by 7-day accelerometry and International Physical Activity Questionnaire (IPAQ).']
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Physical activity (PA) and sedentary behavior (SED) may have independent effects on health and disease. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [12] [13] [14] [15] [#CITATION_TAG] [17], with some exceptions (!1 day [18];!6-7 days [19, 20]).","Multiple regression analysis was used to determine associations (partial correlations) with lipoproteins.Positive associations were detected between SED and small VLDL-P, large LDL-P and TG (partial r = 0.24 to 0.25, p  .355).","['The aim of this study was to determine associations between lipoprotein subclass particle concentrations (-P) and accelerometer-measured SED and moderate-to-vigorous PA (MVPA) in a sample of healthy adult subjects.Lipoprotein subclass particle concentrations were determined by proton nuclear magnetic resonance spectroscopy, whereas SED and MVPA were measured using Agtigraph GT1M and GT3X+ accelerometers.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [12] [13] [14] [15] [16] [17], with some exceptions (!1 day [#CITATION_TAG];!6-7 days [19, 20]).",Sedentary time during waking hours was measured by an accelerometer (5 min. A sedentary break was defined as an interruption in sedentary time (>=100 counts per minute). Metabolic syndrome was defined according to the Adult Treatment Panel (ATP) III criteria.,['OBJECTIVE This study examined the association between objectively measured sedentary activity and metabolic syndrome among older adults.']
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Accurate and reliable measurement of physical activity plays an important role in assessing effective lifestyle interventions for obesity. While several studies have found that 2-6 days are required [5, 7, 8, 6, 30], other studies have shown that 12 [4] and 16-23 days are needed [#CITATION_TAG].",They wore RT3 accelerometers during waking hours for 7 d at baseline and after a 6-month weight loss intervention that included diet and physical activity recommendations. Using 4 d of data with >or=6 h x d of wear time optimized the balance between ICC and participant burden in overweight and obese adults before and after a weight loss intervention.,"['This study examined reliability of accelerometer-based estimates of physical activity levels of overweight and obese adults before and after a lifestyle weight loss program.Participants were overweight and obese (body mass index = 25-45 kg x m) members (n = 1592; 67% female, 42% African American) of the multicenter weight loss maintenance trial.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Reliability correlations were very good to excellent (ICC = 0.70-0.90) for almost all algorithms and there were no significant differences between physical activity measures at Time 1 and Time 2.This paper presents the first assessment of test-retest reliability of the Actigraph over separate administrations in free-living subjects. In all analyses, consecutive periods of!60 minutes of zero counts (allowing for 2 minutes of non-zero counts) were defined as non-wear time [2, #CITATION_TAG] and excluded prior to scoring.","Five algorithms, varying nonwear criteria (20 vs. 60 min of 0 counts) and minimum wear requirements (6 vs. 10 hrs/day for >= 4 days) and a separate algorithm requiring >= 3 counts per min and >= 2 hours per day, were used to process the accelerometer data.Processing the accelerometer data with different algorithms resulted in different levels of counts per day, sedentary, and moderate-to-vigorous physical activity. The ActiGraph was highly reliable in measuring activity over a 7-day period in natural settings but data were sensitive to the algorithms used to process them.","['The purpose of this study was to determine 1) the test-retest reliability of adult accelerometer-measured physical activity, and 2) how data processing decisions affect physical activity levels and test-retest reliability.143 people wore the ActiGraph accelerometer for 2 7-day periods, 1 to 4 weeks apart.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [12] [13] [14] [15] [16] [17], with some exceptions (!1 day [18];!6-7 days [19, #CITATION_TAG]).","The participants were divided into those with or without MetS according to the Japanese criteria for MetS. A triaxial accelerometer was used to measure light-intensity lifestyle activity [1.6-2.9 metabolic equivalents (METs)] and sedentary time (<=1.5 METs). Logistic regression was used to predict MetS from the levels of light-intensity lifestyle activity and sedentary time with age, sex, smoking, calorie intake, accelerometer wear time, and MVPA as covariates.The odds ratios (OR) for MetS in the highest and middle tertiles of light-intensity lifestyle activity were 0.44 [95% confidence interval (CI): 0.24 to 0.81] and 0.51 (95% CI: 0.29 to 0.89) relative to the lowest tertile, after adjustment for age, sex, smoking, calorie intake, accelerometer wear time and MVPA (Ptrend = 0.012).","['Reducing sedentary time and increasing lifestyle activities, including light-intensity activity, may be an option to help prevent metabolic syndrome (MetS).', 'The purpose of the present study was to examine whether objectively measured light-intensity lifestyle activity and sedentary time is associated with MetS, independent of moderate-vigorous intensity physical activity (MVPA).The participants in this cross-sectional study were 483 middle-aged Japanese adults, aged 30-64 years.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [12] [13] [14] [15] [16] [17], with some exceptions (!1 day [18];!6-7 days [#CITATION_TAG, 20]).","MetS was defined according to the National Cholesterol Education Program Adult Treatment Panel III guidelines. Logistic regressions examined the associations between the subcomponents of physical activity and sedentary behavior and the odds of having MetS or individual risk factors.MetS was observed in 10.2% of men and 5.2% of women. Breaks in sedentary time were inversely associated with abdominal obesity (OR = 0.71, 95% confidence interval [CI] = 0.55-0.91) and hypertriglyceridemia (OR = 0.79, 95% CI 0.63-0.99).","['The aim of this study was to examine the associations of multiple, objectively measured parameters of physical activity and sedentary behavior with metabolic syndrome (MetS) and its individual components.Physical activity was measured in 370 Flemish adults (age = 41.7 +- 9.8 yr; mean +- SD) for 7 d using a SenseWear Armband.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. The use of physical activity monitors in population-based research has increased dramatically in the past decade. However, such study designs have received critique for possibly leaving to optimistic results and should be interpreted with caution [23] [#CITATION_TAG] [25].","We also update and extend previous recommendations for use of these instruments in large-scale studies, particularly with respect to selecting monitor systems in the context of technological advances that have occurred in recent years. A checklist and flowchart are provided so that investigators have more guidance when reporting key elements of monitor use in their studies.","['In this report, we review the major purpose for using physical activity monitors in different types of population-based studies (i.e., surveillance, intervention, association studies) and discuss the strengths and weaknesses for the various behavioral outcomes derived from monitors for each study type.']"
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Background: In recent years there has been a growing interest in the relationship between sedentary behaviour (sitting) and health outcomes. Only recently have there been studies assessing the association between time spent in sedentary behaviour and the metabolic syndrome. Reducing sedentary behaviours is potentially important for the prevention of metabolic syndrome The possible impaired reliability for SED compared to other variables may be of critical importance, given the increased interest in SED in the primary and secondary prevention of a range of chronic diseases as well as premature death [9] [#CITATION_TAG] [11].","Reference lists of relevant articles and personal databases were hand searched. Inclusion criteria were: (1) cross sectional or prospective design; (2) include adults &ge;18 years of age; (3) self-reported or objectively measured sedentary time; and (4) an outcome measure of metabolic syndrome. Odds Ratio (OR) and 95% confidence intervals for metabolic syndrome comparing the highest level of sedentary behaviour to the lowest were extracted for each study. Data were pooled using random effects models to take into account heterogeneity between studies. Ten cross-sectional studies (n = 21393 participants), one high, four moderate and five poor quality, were identified.",['The aim of this study is to quantify the association between sedentary behaviour and the metabolic syndrome in adults using meta-analysis.']
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. While several studies have found that 2-6 days are required [5, #CITATION_TAG, 8, 6, 30], other studies have shown that 12 [4] and 16-23 days are needed [3].","Caloric intake, movement recorders (accelerometers and pedometers), and heart rate were the measurements studied in 30 subjects who were monitored during their waking hours for 7 continuous days. An estimate was made of the number of days required to measure a 7-d period with less than 5% error. Weekdays as well as weekend days need to be included.",['The purpose of this study was to determine how many days subjects should be monitored to provide an estimate of habitual physical activity in employed men engaged in a wide range of occupations.']
"Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Physical activity declines dramatically across age groups between childhood and adolescence and continues to decline with age. Great care must be taken when interpreting self-reported physical activity in clinical practice, public health program design and evaluation, and epidemiological research. The current sample was a convenience sample exhibiting a higher activity level (but with approximately similar heterogeneity as evaluated by SDs for the different outcome variables) as compared to population estimates for the corresponding age group [34] [#CITATION_TAG] [36].",Data are described from 6329 participants who provided at least 1 d of accelerometer data and from 4867 participants who provided four or more days of accelerometer data.,"['PURPOSE To describe physical activity levels of children (6-11 yr), adolescents (12-19 yr), and adults (20+ yr), using objective data obtained with accelerometers from a representative sample of the U.S. population.']"
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Introduction of new products often challenges existing supply chains as they might not be suitable to meet changing requirements. This is especially true if additional criteria are to be met, such as, environmentally-sound production. Among others, this can be attributed to two issues, one being the management of time, the second the management of complexity. The recent research indicates that interface with customers provides valuable input, which helps to implement GSCM in an organization (#CITATION_TAG; Baines et al. 2012).","First, a brief description of supply chain, time and complexity management will be given. Next, the product-relationship matrix as a conceptual approach to supply chain management will be used to structure the discussion of time and complexity effects in a supply chain. Third, two case studies will be presented taking organic cotton at OTTO, a mail-order business, and eco-polyester at Steilmann, an apparel producer, as examples.",['The paper addresses the question of how time and complexity affect the introduction of green products.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. The majority of these GSCM studies, however, use either quantitative approaches and methodologies by collecting and analysing large samples and testing hypotheses and models, or qualitative case studies following grounded theory inspired approaches (Binder and Edwards, 2010; #CITATION_TAG).","Design/methodology/approach -To better signify such contribution, it takes insight from Merton's (1968) notion of middle-range theory as a means to create pathways of propositions that link substantive concepts and practices of OM in both context-specific and context-free operational environments.",['Purpose - The purpose of this paper is to highlight the potential of a qualitative middle-range research approach to contribute to the advancement of operations management (OM) field.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Purpose - Sustainability and environmental issues are among the most pressing concerns for modern humanity, governments and environmentally conscious business organizations. Green supply chain management has been acknowledged as a key factor to promote organizational sustainability. Green supply chain management is evolving into an important approach for organizations in emerging economies to manage their environmental responsibility. Yet, despite their importance for easing environmental degradation and providing economic benefits, study of the drivers that influence green supply chain initiatives in an emerging economy is still an under-researched area. Originality/value - The role of the drivers is crucial in motivating these firms to adopt green supply chain initiatives and facilitate their adoption. Firms in emerging countries need to realize that green supply chain initiatives can result in significant benefits to their firms, environment, and the society at large which gives them additional incentives to adopt these initiatives There is a rich body of literature on enablers of GSCM implementation and their interrelationships (Ali and Govindan, 2011; Large and Thomsen, 2011; Mathiyazhagan et al. 2013) highlighting the role of GSCM in achieving sustainability (#CITATION_TAG).",Design/methodology/approach - Structural equation model was used to analyze a set of survey data to validate the research hypotheses.,"['Using survey data collected from ISO 14001 certified organizations from Malaysia, the purpose of this paper is to propose that the drivers that motivate firms to adopt green supply chain management can be measured by a second-order construct related to the implementation of the firm\\u27s green supply chain initiatives.', 'This study uncovers several crucial relationships between green supply chain drivers and initiatives among Malaysian manufacturers.']"
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Green Human Resource Management (GHRM) and Green Supply Chain Management (GSCM) are popular subjects in the areas of human resource management (HRM) and operations management (OM), respectively. Although scholars in each of these areas are advancing the roles of GSCM and GHRM in building more sustainable organizations, there has been a significant delay in the integration of these two contemporary subjects, based on a greater gap in the integration of HRM and supply chain management (SCM). The importance of top management beliefs, practices, and commitment has been hence highlighted in the literature (Abdulrahman et al., 2014; Bag and Anand, 2014; #CITATION_TAG).",,"['Thus, the aims of this study are to propose a synergistic and integrative framework for the GHRM-GSCM relationship and to propose a research agenda for this integration.']"
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Top management commitment (TMC) -Top management commitment is vital to organizations and supply chain partners aiming to implement green and sustainable practices (#CITATION_TAG; Gattiker and Carter, 2010; Foerstl et al., 2015).","Specifically, this model explains how top management mediates the impact of external institutional pressures on the degree of usage of enterprise resource planning (ERP) systems. The hypotheses were tested using survey data from companies that have already implemented ERP systems.",['We develop and test a theoretical model to investigate the assimilation of enterprise systems in the post-implementation stage within organizations.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Since becoming editor of AMR, I have tried to find a simple way to communicate the neces-sary ingredients of a theoretical contribution. There are several excellent treatises on the sub-ject, but they typically involve terms and con-cepts that are difficult to incorporate into every-day communications with authors and review-ers. My experience has been that available frameworks are as likely to obfuscate, as they are to clarify, meaning. Besides exposure to the works of Kaplan, Dubin, and others varies widely across the Academy. Any attempt to build theory needs to answer fundamental questions (Sushil, 2012; #CITATION_TAG), related to ""what"", ""how"" and ""why"" (Whetten, 1989).",,"['This article is a rudimentary effort to fill this gap: The intent is not to create a new conceptu-alization of theory, but rather to propose severa']"
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. We followed Chen et al. (2010) and subsequent studies (#CITATION_TAG) in that we conducted a manual scan and analysis of all the abstracts and a selection of the highly cited and review papers.","We draw on Nelson's theorisation of the co-evolution of Physical and Social Technologies to redefine the SIS domain as a Complex Adaptive System (CAS) for the co-evolution of ICT and organisational capabilities and business models to create social and economic value. We conduct a meta-analysis of the domain based on a longitudinal review of SIS research over 33 years, and contrary to contemporaneous SIS literature which suggests that a paradigm shift may be necessary to address the increased turbulence, uncertainty and dynamism in the emerging competitive landscape, we find that the SIS research domain has the requisite adaptive capacity to evolve gracefully to address the challenges of the emerging networked competitive landscape. Drawing on complexity science and network theory we identify four priorities for the development of the domain for the future: conceptualisation of the SIS Domain as a CAS for the co-evolution of Physical and Social Technologies; the adoption of the network paradigm; access to a science of networks; and adoption of Complexity Science as an articulation device within SIS and across disciplines",['The purpose of this paper is to contribute to the current discourse in the Strategic Information Systems (SIS) domain about the future and identity of SIS.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Interest in the problem of method biases has a long history in the behavioral sciences. We collected data electronically using the split-survey method (#CITATION_TAG).","Despite this, a comprehensive summary of the potential sources of method biases and how to control for them does not exist.","['Therefore, the purpose of this article is to examine the extent to which method biases influence behavioral research results, identify potential sources of method biases, discuss the cognitive processes through which method biases influence responses to measures, evaluate the many different procedural and statistical techniques that can be used to control method biases, and provide recommendations for how to select appropriate procedural and statistical remedies for different types of research settings.']"
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. These criticisms have stemmed mainly from scholars who are not familiar with qualitative methods (Bitektine, 2008; #CITATION_TAG).",,"['This paper sets the stage for the special issue on the application of empirical science in operations management (OM).', 'It highlights the contributions that empirical science can make to operations management research and practice.', 'The paper provides a brief history of the empirical tradition in OM and the scholars who were the early pioneers.']"
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. A literature review is a summary of a subject field that supports the identification of specific research questions. A literature review needs to draw on and evaluate a range of different types of sources including academic and professional journal articles, books, and web-based resources. Search engines can be used to search web resources and bibliographic databases. Creating the literature review involves the stages of: scanning, making notes, structuring the literature review, writing the literature review, and building a bibliography. We conducted our systematic literature review (SLR) to identify the key enablers of GSCM and their interrelationships, following the principles set out by Tranfield et al. (2003), #CITATION_TAG and were inspired by other prominent scholars (Burgess et al., 2006; Cousins et al., 2006) that have been used in recent reviews by Chen et al. (2014) and Gunasekaran et al. (2015).",The literature search helps in the identification and location of relevant documents and other sources. Conceptual frameworks can be a useful tool in developing an understanding of a subject area.,['This article offers support and guidance for students undertaking a literature review as part of their dissertation during an undergraduate or Masters course.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Undertaking a review of the literature is an important part of any research project. However, traditional 'narrative' reviews frequently lack thoroughness, and in many cases are not undertaken as genuine pieces of investigatory science. Consequently they can lack a means for making sense of what the collection of studies is saying. These reviews can he hiased by the researcher and often lack rigour. Furthermore, the use of reviews of the available evidence to provide insights and guidance for intervention into operational needs of practitioners and policymakers has largely been of secondary importance. For practitioners, making sense of a mass of often-contrad ictory evidence has hecome progressively harder. The quality of evidence underpinning decision-making and action has heen questioned, for inadequate or incomplete evidence seriously impedes policy formulation and implementation. Over the last fifteen years, medical science has attempted to improve the review process hy synthesizing research in a systematic, transparent, and reproducihie manner with the twin aims of enhancing the knowledge hase and informing policymaking and practice. We conducted our systematic literature review (SLR) to identify the key enablers of GSCM and their interrelationships, following the principles set out by #CITATION_TAG, Rowley and Slack (2004) and were inspired by other prominent scholars (Burgess et al., 2006; Cousins et al., 2006) that have been used in recent reviews by Chen et al. (2014) and Gunasekaran et al. (2015).",The researcher both maps and assesses the relevant intellectual territory in order to specify a research question which will further develop the knowledge hase.,['The paper highlights the challenges in developing an appropriate methodology.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Scholars (e.g. Mandal and Deshmukh, 1994; #CITATION_TAG; Ali and Govindan, 2011; Sushil, 2012) have outlined two limitations of ISM, that is, it usually involves a small sample size which may not be enough for statistical reasons, and manager bias may influence the final ISM model.","In the paper, 11 enablers of Six Sigma are identified from literature survey and experts' opinion and then these are validated by questionnaire survey in India.",['Purpose - The purpose of this research paper is to study the enablers of Six Sigma and to establish relationship among them using interpretive structural modeling (ISM).Design/methodology/approach - The research paper presents a blend of theoretical framework and practical applications.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Vendor selection is one of the most important activities of a purchasing department. Traditionally, vendors are selected for their ability to meet the quality requirement, delivery performance and the price offered. However, as they are selected not only to meet the immediate requirement but also future needs, one needs to consider many other factors when selecting a reliable vendor. These criteria are dependent on all the others. Scholars (e.g. #CITATION_TAG; Soti et al., 2009; Ali and Govindan, 2011; Sushil, 2012) have outlined two limitations of ISM, that is, it usually involves a small sample size which may not be enough for statistical reasons, and manager bias may influence the final ISM model.","Analyses some of the most important criteria which have been classified into four categories: autonomous, dependent, linkage and driver depending on their driver power and dependence.",['Develops an interpretive structural model (ISM) to show the inter-relationship of different criteria and their levels of importance in the vendor selection process.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. The items were derived from existing literature (Zhu and Sarkis, 2004; #CITATION_TAG; Schoenherr, 2012; Zhang and Wang, 2014; Dubey et al. 2015).",Corporate environmental strategies are identified on the basis of their orientation towards shareholder value.,['This paper provides a discussion of the relationship between environmental and economic performance and the influence of corporate environmental strategy choice on this relationship.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. The share of services in the output of manufacturing industries increased in the large majority of European countries between 1995 and 2005 and between 2000 and 2005. Service output of manu-facturing, however, is still small compared to the output of physical products. The highest service shares are found in small countries with a high degree of openness and R&D intensity. There is a strong link between servitization and technological innovation at different levels. The service output of these countries consists predominantly of knowledge-intensive services. Examples are electrical and optical equipment, machinery, or the chemical and pharmaceutical industry. Producers of complex, customized products tend to have a higher share of services in output than producers of simple, mass-produced goods. SLR is a quite popular methodology in medical science, however in recent years it has seen significant growth in management fields (#CITATION_TAG), to synthesize and organize research findings from multiple studies.",We employ input-output data as well as data from a company survey to give a comprehensive picture of servitization across countries and industries.,['This paper provides new evidence for the servitization of European manufacturing - the trend that manufacturing firms increasingly offer services along with their physical products.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Knowledge management is one of the most important strategic resources of the firm which has been ascertained to many organizations to acquire and apply it before their competitor for achieving competitive advantages. Similarly, due to rising environmental awareness among customers, governments, NGOs, and researchers, firms are facing increasing pressure to implement environmental management practices in their operations. Realising the need to incorporate sustainability and the triple bottom line (Kleindorfer et al., 2005) as part of their strategic intent, companies focus on assessing the economic, environmental, and social impact of their activities and highlighting the relationship between sustainability and performance (Leppelt et al., 2013; #CITATION_TAG; Burritt and Schaltegger, 2012; Subramanian and Gunasekaran, 2015).",,['The purpose of this paper is to identify the influence of knowledge management capability (KMC) on green supply chain management (GSCM) practices adoption of the manufacturing firm and subsequently the impact on firm performance.']
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Purpose - To make their supply chains more socially responsible, many companies are implementing supplier assessment tools and collaborative practices. and ""What are the enablers of these mechanisms? Firms that engage in establishing strong relationships with suppliers enjoy superior performance (Giannakis, 2007; Reuter et al., 2010; #CITATION_TAG; Burritt and Schaltegger, 2012).",""".Design/methodology/approach - A structured literature review is carried out that analyses published studies, evaluates contributions, summarises knowledge and identifies managerial implications and lines for further research.Findings - Both assessment and collaboration have a positive impact on environmental performance and corporate social responsibility, although the most recent collaborative paradigm stresses that assessment alone is not enough.","['The aim of this paper is to provide a systematic literature review on the governance structures used to extend sustainability to suppliers.', 'More specifically, the authors aim to answer two questions: ""What is the impact of these mechanisms or governance structures on sustainable performance?""']"
"The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Sustainable supplier relationship management (SSRM) has become crucial in companies' sustainability efforts. A firm's corporate image, in terms of economical, environmental and social behavior, heavily depends on its supply chain and the sustainability performance of each and every chain link, including suppliers and sub-suppliers. Realising the need to incorporate sustainability and the triple bottom line (Kleindorfer et al., 2005) as part of their strategic intent, companies focus on assessing the economic, environmental, and social impact of their activities and highlighting the relationship between sustainability and performance (#CITATION_TAG; Green et al., 2012; Burritt and Schaltegger, 2012; Subramanian and Gunasekaran, 2015).","Additionally, we identified corporate strategy alignment, risk perception and the listing in sustainability indices as key influential factors, which foster and limit a focal firm's engagement in SSRM. The contribution of this paper is twofold: First, in-depth insights on how sustainability leaders within the chemical industry int roduce sustainability into their supplier relationship management processes are presented and compared to the practices of sustainability followers.","['In a multiple case study of seven European chemical companies, we investigate how firms manage their supplier relations in interdependent situations.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Infertility practice and reproductive technologies are generally seen as 'controversial' areas of scientific inquiry that raise many complex ethical issues. For example, Hooeyer [25] has shown how moral qualms around trade in human body parts are managed through systems of 'compensation' which ascribe value to biomaterials without the formation of 'markets', and Frith et al. [#CITATION_TAG] have underscored the routine engagement with ethical issues that constitutes clinical practice within the infertility clinic.",We use the concept of ethical boundary-work to develop a theory of 'settled' and 'controversial' morality to illuminate how infertility clinicians drew boundaries between different conceptions of the role ethics played in their practice. We argue that by creating a space of 'no-ethics' in their practice--part of a settled morality that does not require articulation--the informants re-appropriate an area of their practice from 'outside' influences and control.,"[""This paper presents a qualitative study that considered how clinicians constructed the role of the 'ethical' in their everyday practice."", ""An attention to areas of settled morality, usually rendered invisible by their very nature, enables us to see how clinicians manage the 'ethical' in their practice.""]"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. What is perhaps different in STS is its predominant emphasis on the use of case studies to produce theory, rather than testing theory using cases; however, this is perhaps a difference in perspective and approach rather than an indication of sharp boundaries between STS and (for instance) anthropology and sociology [#CITATION_TAG].","It then turns to more recent developments in STS, outlining the importance of material semiotics to important traditions within the discipline including those influenced by actor network theory, feminism, and postcolonialism.","['This paper starts by exploring the development of Science, Technology and Society (STS) in the UK in the late 1960s, emphasising its interdiciplinary roots, and comparing and contrasting it with the concerns of Sociology.', 'It notes, in consistency with the Foucauldian approach, that material semiotics implies that knowledge traditions are performative, helping to create the realities that they describe.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. The Body Multiple is an extraordinary ethnography of an ordinary disease. Drawing on fieldwork in a Dutch university hospital, Annemarie Mol looks at the day-to-day diagnosis and treatment of atherosclerosis. A patient information leaflet might describe atherosclerosis as the gradual obstruction of the arteries, but in hospital practice this one medical condition appears to be many other things. From one moment, place, apparatus, specialty, or treatment, to the next, a slightly different ""atherosclerosis"" is being discussed, measured, observed, or stripped away. The Body Multiple juxtaposes two distinct texts. In dialogue with one another, Mol's two texts meditate on the multiplicity of reality-in-practice. Presenting philosophical reflections on the body and medical practice through vivid storytelling, The Body Multiple will be important to those in medical anthropology, philosophy, and the social study of science, technology, and medicine. In turn, articulations of benefit itself come to be a function of the nature of disease as mediated and understood through biomedical technique [#CITATION_TAG].","This multiplicity does not imply fragmentation; instead, the disease is made to cohere through a range of tactics including transporting forms and files, making images, holding case conferences, and conducting doctor-patient conversations. Alongside Mol's analysis of her ethnographic material--interviews with doctors and patients and observations of medical examinations, consultations, and operations--runs a parallel text in which she reflects on the relevant literature.","['Mol draws on medical anthropology, sociology, feminist theory, philosophy, and science and technology studies to reframe such issues as the disease-illness distinction, subject-object relations, boundaries, difference, situatedness, and ontology.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. The phenomenal growth of global pharmaceutical sales and the quest for innovation are driving an unprecedented search for human test subjects, particularly in middle- and low-income countries. Our hope for medical progress increasingly depends on the willingness of the world's poor to participate in clinical drug trials. While these experiments often provide those in need with vital and previously unattainable medical resources, the outsourcing and offshoring of trials also create new problems. Moving between corporate and scientific offices in the United States and research and public health sites in Poland and Brazil, When Experiments Travel documents the complex ways that commercial medical science, with all its benefits and risks, is being integrated into local health systems and emerging drug markets. How are experiments controlled and how is drug safety ensured? When Experiments Travel challenges conventional understandings of the ethics and politics of transnational science and changes the way we think about global medicine and the new infrastructures of our lives. For instance, Petryna's [#CITATION_TAG] work on the outsourcing of clinical trials to middle and low income countries has revealed a range of problematic developments, including biased trial designs that ensure drugs look safer and more efficacious, and proceduralism in ethical review and administration that ''can hide contextual uncertainties'' [50: 187].",,"['In this groundbreaking book, anthropologist Adriana Petryna takes us deep into the clinical trials industry as it brings together players separated by vast economic and cultural differences.', 'Providing a unique perspective on globalized clinical trials, When Experiments Travel raises central questions: Are such trials exploitative or are they social goods?']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Particular foci of work in this vein are explorations of the place, role and impact of public bioethics in policy and biomedicine [32, #CITATION_TAG, 61, 62], and examinations of the mutual reinforcement-and perhaps co-production-of social and epistemic innovation in regards to controversial and/or promissory technoscience [22, 23].","Second, it will argue that an ""ethical"" model has emerged alongside and partially displaced a ""technical"" model of expertise in scientific governance. The article will introduce the notion of ""proper talk,"" a set of techniques for facilitating ethical debate, characterized by the active elicitation of public engagement and the inclusion of emotions and subjectivity.","['This article uses notions of ""public talk"" and ""regulation as facilitation"" to develop an account of public bioethics in the UK as a form of scientific governance, drawing on document analysis and expert interviews.', 'First, this article will show the ""ethical"" problematization of scientific governance in the UK through the emergence of the Human Genetics Commission (HGC), Nuffield Council on Bioethics (NCB), and Human Fertilisation and Embryology Authority (HFEA).', 'The article then questions whether the authority to categorize publics and identify ""proper"" ethical positions reintroduces problems of expertise in a new form.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Citizens today are increasingly expected to be knowledgeable about and prepared to engage with biomedical knowledge. This is not based on an assumption that non-scientists are 'ignorant' and are thus unable to 'appropriately' use or debate science; rather, it is underpinned by an empirically-grounded observation that some individuals may be unfamiliar with certain specificities of particular modes of research and ethical frameworks, and, as a consequence, have their autonomy compromised when invited to participate in biomedical investigations. Public bioethics will have to play a key role in such an endeavour, and indeed will contribute in important ways to the opening up of new spaces of symmetrical engagement between bioethicists, scientists and wider publics-and hence to the democratisation of the bioethical enterprise. In so doing, opportunities for more democratic forms of bioethical deliberation are also restricted [#CITATION_TAG].",,"[""In this article, I wish to reframe this 'public understanding of science' project, and place fresh emphasis on public understandings of research: an engagement with the everyday laboratory practices of biomedicine and its associated ethics, rather than with specific scientific facts."", 'Drawing on the perspectives of participants in my own sociological research on the social and ethical dimensions of neuroscience, I argue that public understanding of biomedical research and its ethics should be developed both at the community level and within the research moment itself in order to enhance autonomy and promote more socially robust science.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Abstract Neuroscientific research into mental health commands generous funding, suggesting neuroscience is understood by a variety of actors and institutions as having significant potential to enhance the therapeutic practices of psychiatrists. The article discusses the respondents ' ambivalent expectations regarding the therapeutic promise of brain research, and shows how these are structured by understandings of the ontology of personality disorder. In sum, the necessity of large material and symbolic investments in neuroscience should, perhaps, be reflected upon more critically, and analytic encounters with this discipline must keep in mind it's at times surprising commitment to the realms of the social and the psychological Within the clinic, neurological explanations for opaque conditions can sometimes have traction as a framework through which to deal with the uncertainties associated with them [#CITATION_TAG].",,"[""This article interrogates this 'therapeutic promise ' of neuroscience through the case study of the psychiatric condition personality disorder."", 'Specifically, the focus is on the promissory discourse of clinicians specialising in the management of two variants of personality disorder) antisocial personality disorder and psychopathy) and researchers investigating the neurobiology of these constructs.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. ABSTRACT Public dialogue about science, technology and medicine is an established part of the activities of a range of charities, private corporations, governmental departments and scientific institutions. However, the extent to which these activities challenge or bridge the lay-expert divide is questionable. Expertise is contested, by the public and the community of scholars who study and/or facilitate public engagement. The colonization of lay positions by expert speakers and the hybrid positioning of lay-experts was characteristic of the consensus and conservatism that emerged. Yet, often an expert-lay divide is perpetuated which closes down opportunities for more reflexive debate [#CITATION_TAG].","We examine participants ' claims to expertise and consider how this relates to their claims to credibility and legitimacy and the way in which these events unfolded. Using a combination of ethnographic and discursive analysis, we found that participants supplemented technical expertise with other expert and lay perspectives. We can also link participants ' claims to expertise to their generally positive appraisal of genetic research and services.","['In this paper, we explore the dynamics of expertise and their implications for the lay-expert divide at a series of public events about the new genetics.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. It is argued that now sociologists have got inside the black box of science, they need to make more impact upon the conduct of areas of science. The issue of what makes for `heroes' and `villains' within science is considered by tracing different styles of science. As the so-called 'Science Wars' of the 1990 s might remind us [70], influential individuals do not always take kindly to having the 'black box' [#CITATION_TAG] of their work unpacked and its contents inspected.","Two cases where scientific heroes became villains are examined: Andrew Crosse's claim to observe life emerging from a galvanic cell, and the claims by Pons and Fleischmann to observe cold fusion.",['This lecture traces the theme of opening black boxes in the sociology of science.']
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Feminist criticism of health care and ofbioethics has become increasingly rich andsophisticated in the last years of thetwentieth century. Nonetheless, this body ofwork remains quite marginalized. I believe thatthere are (at least) two reasons for this.First, many people are still confused aboutfeminism. bioethics that it is 'too close' to science recall some internal critiques, including those from feminist bioethicists who have sought especially deep critical engagement with biomedical institutions and practices [44, 59, 60] and bioethical scholarship itself [42, #CITATION_TAG, 58].",,"[""In this essay I argue for a thin,``core'' conception of feminism that is easy tounderstand and difficult to reject.""]"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Feminist bioethics poses a challenge to bioethics by exposing the masculine marking of its supposedly generic human subject, as well as the fact that the tradition does not view women's rights as human rights. bioethics that it is 'too close' to science recall some internal critiques, including those from feminist bioethicists who have sought especially deep critical engagement with biomedical institutions and practices [44, 59, 60] and bioethical scholarship itself [42, 57, #CITATION_TAG].",,"['This essay traces the way in which this invisible gendering of the universal renders the other gender invisible and silent.', ""It shows how this attenuation of the human in 'man ' is a source of sickness, both cultural and individual.""]"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Health inequities are one of the central problems in public health ethics; a feminist approach leads us to examine not only the connections between gender, disadvantage, and health, but also the distribution of power in the processes of public health, from policy making through to programme delivery. bioethics that it is 'too close' to science recall some internal critiques, including those from feminist bioethicists who have sought especially deep critical engagement with biomedical institutions and practices [44, #CITATION_TAG, 60] and bioethical scholarship itself [42, 57, 58].","The complexity of public health demands investigation using multiple perspectives and an attention to detail that is capable of identifying the health issues that are important to women, and investigating ways to address these issues.",['This paper sketches an account of public health ethics drawing upon established scholarship in feminist ethics.']
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. The use of Ritalin and other stimulant drug treatments for attention-deficit hyperactivity disorder (ADHD) raises distinctive moral dilemmas for parents; these moral dilemmas have not been adequately addressed in the bioethics literature. The considerations of agency and autonomy that are so central to ethical appraisals of biomedical technologies are likewise key issues in relation to psychopharmaceuticals [15, #CITATION_TAG].",,"[""This paper draws upon data from a qualitative empirical study to investigate parents' use of the moral ideal of authenticity as part of their narrative justifications for dosing decisions and actions."", ""I argue that this investigation of parents' moral justifications and dosing dilemmas raises questions about the validity of authenticity as a transcendent moral principle.""]"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. is a question that is frequently asked in discussions about the new genetics. It raises difficult questions about clients' and service providers' autonomy and responsibility and about which human illnesses, conditions, and characteristics ought to be the subject of research and testing. As STS scholars have shown, publics can be both knowledgeable about biomedicine and willing to engage in sustained debate and analysis about issues that bioethicists are grappling with [#CITATION_TAG, 56]; limiting participation is thus unfortunate not only for democractic reasons, but also because potentially 'useful' contributions from those outside the academiy remain unheard.","In particular, we show how differences in the amount and type of information and advice available to clients of genetic testing, the level of social support to people with particular conditions, and people's perception of stigma, suffering, and quality of life, make drawing the line highly problematic.","[""In this paper we explore a range of lay people's accounts of drawing the line.""]"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. The system of pre-emptive ethical regulation developed in the biomedical sciences has become a major threat to research in the humanities and the social sciences (HSS). Although there is growing criticism of its effects, most commentators have tended to accept the principle of regulation. Moreover, the attention of STS researchers to bioethics can, to an extent, be viewed as symptomatic of a wider debate within the social sciences about ethics regulation (see [#CITATION_TAG, 20]).",,['This paper argues that we should not make this concession and that ethical regulation is fundamentally wrong because the damage that it inflicts on a democratic society far exceeds any harm that HSS research is capable of causing to individuals.']
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. It is often suggested in the mass media and popular academic literature that scientists promote a secular and reductionist understanding of the implications of the life sciences for the concept of being human. Is adhering to this view considered to be one of the components of the notion of being a good scientist? This has long been a concern of STS scholars, who have shown extensively how scientists have views on the impact of their research on wider society but nevertheless seek to demarcate these from their professional work [#CITATION_TAG, 33, 49, 54].",When discussing this question the interviewees distinguished between their 'personal' and 'professional' views.,"['This paper explores responses of geneticists interviewed in the UK, the USA and Russia about the cultural meanings of their work.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. United Kingdom (UK) funding to build human embryonic stem cell (hESC) derivation labs within assisted conception units (ACU) was intended to facilitate the 'In-vitro fertilisation (IVF)-stem cell interface', including the flow of fresh 'spare' embryos to stem cell labs. However, in the three sites reported on here, which received this funding, most of the embryos used for hESC research came from long term cryopreservation storage and/or outside clinics. Members of staff took part in 44 interviews and six ethics discussion groups held at our study sites between February 2008 and October 2009. Within this field, the collection of 'spare embryos' is central to research; yet, the construal of an embryo as 'spare' must be achieved through careful ethical argumentation and deliberation which is itself experimental [#CITATION_TAG, 66].","Social and ethical factors include acknowledging and responding to uncertainty in classifying embryos; retaining 'fluidity' in the grading system to give embryos 'every chance'; tensions between standardisation and variation in enacting a 'fair' grading system; enhancement of patient choice and control, and prevention of regret; and incorporation of patients' values in construction of ethically acceptable embryo 'spareness' ('frozen' embryos, and embryos determined through preimplantation genetic diagnosis (PGD) to be genetically 'affected').","['In this paper we explore some of the clinical, technical, social and ethical factors that might help to explain this situation.', ""We focus here on their articulations of social and ethical, as well as scientific, dimensions in the contingent classification of 'spare' embryos, entailing uncertainty, fluidity and naturalisation in classifying work.""]"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Most of the literature on pharmacogenetics assumes that the main problems in implementing the technology will be institutional ones (due to funding or regulation) and that although it involves genetic testing, the ethical issues involved in pharmacogenetics are different from, even less than, 'traditional' genetic testing. Very little attention has been paid to how clinicians will accept this technology, their attitudes towards it and how it will affect clinical practice. Indeed, ethical issues may play a key role in the implementation of new technologies within the clinic [#CITATION_TAG].",,"['This paper presents results from interviews with clinicians who are beginning to use pharmacogenetics and explores how they view the ethics of pharmacogenetic testing, its use to exclude some patients from treatment, and how this kind of testing fits into broader debates around genetics.', ""In particular this paper examines the attitudes of breast cancer and Alzheimer's disease specialists.""]"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Social scientists often lament the fact that philosophically trained ethicists pay limited attention to the insights they generate. Science and technology studies (STS) is one such tradition that is articulating with bioethics, though sometimes fractiously [9, #CITATION_TAG].","Increased awareness of differences in styles of reasoning and objects of research interest might help to overcome the hostility, and an anthropological project is presented as an invitation to a dialogue informed by awareness of such differences.","['This paper presents an overview of tendencies in sociological and anthropological studies of morality, ethics and bioethics, and suggests that a lack in philosophical interest might be related to a tendency among social scientists to employ either a deficit model (social science perspectives accommodate the sense of context that philosophical ethics lacks), a replacement model (social scientists have finally found the ""right way"" of doing ethics), or a dismissal model (ethics should be abandoned all together as a misconstrued veil of power).']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Exchange of material originating in human bodies is essential to many health technologies but potentially conflicts with a prominent moral ideal according to which human bodies and their parts are beyond trade. How are exchange systems for something moving in and out of human beings organized? Who provides what and who receives what? When and where does money change hands? How are the specific amounts determined? For example, Hooeyer [#CITATION_TAG] has shown how moral qualms around trade in human body parts are managed through systems of 'compensation' which ascribe value to biomaterials without the formation of 'markets', and Frith et al. [17] have underscored the routine engagement with ethical issues that constitutes clinical practice within the infertility clinic.","The analysis revolves around two versions of the hip: one prosthetic version made of metal and one version made of bone, the femoral head, which is excised in conjunction with hip replacements and later used for transplantation. By answering these questions, I provide a description of the exchange form that avoids assuming it to be simply a 'market' or a 'gift economy'.","[""In this article, I suggest that the inclination to keep bodies apart from 'commercial exchange' has significant implications for the way their parts come to be exchanged."", 'I focus on the mechanisms that allow money to be generated despite the moral ideal viewing body parts as beyond trade--or, rather, the how the ideal facilitates mechanisms through which money can be generated without being viewed as profit.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Public bioethics bodies are used internationally as institutions with the declared aims of facilitating societal debate and providing policy advice in certain areas of scientific inquiry raising questions of values and legitimate science. In the United States, bioethical experts in these institutions use the language of consensus building to justify and define the outcome of the enterprise. However, the implications of public bioethics at science-policy boundaries are underexamined. Political interest in such bodies continues while their influence on societal consensus, public debate, and science policy remains ambiguous. Particular foci of work in this vein are explorations of the place, role and impact of public bioethics in policy and biomedicine [#CITATION_TAG, 48, 61, 62], and examinations of the mutual reinforcement-and perhaps co-production-of social and epistemic innovation in regards to controversial and/or promissory technoscience [22, 23].",,"['This article presents a theoretical discussion of public bioethics bodies as boundary organizations and examines them in terms of relationship to the moral and cognitive authority of science and other forms of expertise, mechanisms for public participation in controversial science policy, and the deployment of consensus models.']"
"Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. It is no surprise that bioethics and sociology developed an adversarial relationship: bioethicists value the clear descriptions of ethically charged situations provided by social scientists, but doubt the ability of those trained in social science to logically derive and discern 'the good'. For their part, social scientists - experts in observing and collecting data about the way the world is- find bioethicists ignorant of the ways elegantly crafted solutions to ethical problems get altered when incarnated in varied social and cultural settings. This discipline-centred self-affirmation can be a satisfying exercise, but it offers nothing to the project of promoting more moral medicine and health care. Science and technology studies (STS) is one such tradition that is articulating with bioethics, though sometimes fractiously [#CITATION_TAG, 24].",,"['Focusing on four themes found in recent sociological research in bioethics - ethics of research, the creation of moral boundaries, bioethics and social policy, and the bioethical imagination - this anthology offers practical models of co-operative work where the strengths of each discipline are brought together to advance our understanding of bioethical issues and to show the way toward just and effective social policy.']"
"Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. Cyanobacteria can form associations with species from most groups of Bryophytes. The class Bryophyta (mosses or musci), is divided in three sub-classes: Sphagnidae (Sphagnum or peat mosses) Andreaeidae (granite or rock mosses) Bryidae (true mosses) Associations between members of Bryophyta (mosses or musci) and cyanobacteria are found, but special symbiotic structures comparable to those in liverworts and hornworts have never been described. The associations probably range from relationships that are simply fortuitous, through more or less specific epiphytic associations, to intracellular colonisation of dead moss cells by the cyanobacteria. We know very little about how the associations are formed, how the cyanobacteria infect or colonise the mosses, if special genes are involved, or if nutrients are exchanged between the two partners. Except for a few studies, our knowledge about cyanobacteria-moss associations is based on work from polar or sub-polar environments. In the more extreme terrestrial polar habitats, cyanobacteria constitute not only the dominant phototrophs and nitrogen fixers, but also most of the microbial ecosystem biomass (Vincent, 2000). In the areas covered by vegetation, cyanobacteria in association with mosses are the main source of fixed nitrogen and are important for the productivity of the ecosystem. The technique, described in detail by #CITATION_TAG, is nondestructive and allows for optical cross-section observations of intact moss samples.","The Bryophytes contains three classes: Hepatophyta (liverworts) Anthocerophyta (hornworts) Bryophyta (mosses or musci) In the first two classes, Hepatophyta (liverworts) and Anthocerophyta (hornworts), true symbiotic associations between the bryophytes and cyanobacteria are found. These symbioses are described in the previous chapter.",['In this chapter we will use the terms epiphytic associations and intracellular colonisation to describe associations between cyanobacteria and mosses.']
"Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. In the twelve years since the first review article dealing with chemical constituents of the Hepaticae appeared in this series as Volume 42 (19), several short reviews concerned with chemical constituents of bryophytes have been published (22, 96, 144, 265, 271, 647, 649, 650). In 1988, a Symposium on Chemistry and Chemical Taxonomy of Bryophytes was organised on the behalf of the Phytochemical Society of Europe; the proceedings of this meeting appeared as a book entitled Bryophytes: Their Chemistry and Chemical Taxonomy (651). The physiological and biochemical aspects of bryophytes have also been described in a recent book Bryophytes Development: Physiology and Biochemistry (139). Liverworts are known for rich composition of secondary compounds (#CITATION_TAG 1995, Mues 2000), many of which exhibit antimicrobial properties",,"['The symposium concerned itself with phytochemical, biochemical, botanical, chemotaxonomical, pharmaceutical, biotechnological and environmental aspects of bryophytes as well as with the synthesis of the terpenoids and aromatic compounds bryophytes elaborate.']"
"Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. The Sphagnum angustifolium stem transports nitrogen fixed and exuded by Nostoc muscorum upwards. Theoretical stoichiometric ratio of ethylene to nitrogen (3:2) is rarely attained and must always be corrected by parallel uptake of labelled 15 N 2 for each organism and possibly location (#CITATION_TAG 1980;Millbank 1981)",,['Laboratory work on Sphagnum blue-green algal associations is described.']
"Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. There is growing recognition that classifying terrestrial plant species on the basis of their function (into 'functional types') rather than their higher taxonomic identity, is a promising way forward for tackling important ecological questions at the scale of ecosystems, landscapes or biomes. These questions include those on vegetation responses to and vegetation effects on, environmental changes (e.g. changes in climate, atmospheric chemistry, land use or other disturbances). There is also growing consensus about a shortlist of plant traits that should underlie such functional plant classifications, because they have strong predictive power of important ecosystem responses to environmental change and/or they themselves have strong impacts on ecosystem processes. Large international research efforts, promoted by the IGBP-GCTE Programme, are underway to screen predominant plant species in various ecosystems and biomes worldwide for such traits. In contrast to the effort put into developing extensive international methodological protocols and databases for quantitative analysis of vascular plant traits ( #CITATION_TAG et al","It features a practical handbook with step-by-step recipes, with relatively brief information about the ecological context, for 28 functional traits recognised as critical for tackling large-scale ecological questions.","['This paper provides an international methodological protocol aimed at standardising this research effort, based on consensus among a broad group of scientists in this field.']"
"Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. Evidence that lichenized blue-green algae are among the principal agents of N2 fixation on drier terrain in the Arctic and Subarctic has prompted attempts to quantify N-input by lichens in these habitats. The nitrogenase activity in Stereocaulon paschale mats in spruce-lichen woodland of central subarctic Canada has been examined in relation to thallus water content, temperature and incident radiation. It is suggested that simple predictive models are not yet able to accurately describe levels of nitrogenase activity in nature and that estimates of N-input on an annual or seasonal basis may be precarious. Leaching of metabolites from, and decomposition of the thallus are the two principal potential pathways for nitrogen, subsequent to fixation by cyanophilic lichens. Quantitative information on the operation of these pathways in nature is required. Lichenized blue-green algae are probably among the principal agents of nitrogen (N2) fixation in drier terrestrial habitats of the Arctic and Subarctic (Schell & Alexander, 1973; Kallio & Kallio, 1975; Crittenden, 1975; Huss-Danell, 1977). Several investigators working in boreal-arctic systems have used measurements of nitrogenase activity in lichens obtained either under field conditions or in the laboratory to derive estimates of N-input per unit area by the lichen biomass in situ on a seasonal or annual basis. The major shortcoming of such estimates is that their accuracy is unknown: the results of in situ measurements of nitrogenase activity over long periods have not been compared with those of predictive procedures applied to the same period. We have recently been examining the N2 fixing capabilities of S. paschale mats in spruce-lichen woodland in the Abitau-Dunvegan Lakes region of the Northwest Territories (60?21'N, 106'54'W). NITROGEN FIXATION BY STEREOCAULON PASCHALE IN SPRUCE-LICHEN WOODLAND--A CASE STUDY The importance of Stereocaulon paschale as a major component of spruce-lichen woodland in central subarctic Canada has been reviewed by Kershaw (1977). The maintenance of large areas of open Stereocaulon-spruce woodland in this region is dependent upon the high frequency of forest fire, the latter being a natural facet of the environment in the boreal forest (see, e.g. In the AbitauDunvegan Lakes region the periodicity of fire is such that spruce woodland older than 200 y is infrequently encountered on drier terrain and the average reburn interval is less than 100 y (Maikawa & Kershaw, 1976). Stereocaulon paschale usually dominates the lichen synusia in spruce-lichen woodland of between 60 and 130 y old where almost pure carpets of this lichen may occur (Fig. The acetylene (C2H2-) reduction technique (Stewart, Fitzgerald & Burris, 1967) with modifications after Stewart et al. Intensive monitoring programs of this kind were conducted on five occasions: 24-25 June, 6-7, 10-11 and 18-19 August 1976 and 10-11 May 1977. On 17 August 1976 the lichen mat was wetted by rainfall commencing at 04.15 h and monitoring of nitrogenase activity began at 14.30 h the following day. Photosynthetically active radiation (Fig. These data together ascertain the significance of biological N 2 fixation to the N-limited vegetation in the Subarctic ( #CITATION_TAG and Kershaw 1978).In conclusion, our multispecies comparison of N 2 fixation rates of cryptogams, both under standardised conditions and in situ, applying a direct nitrogen detection method, has provided strong evidence for differential nitrogen input by cryptogam taxa in the Subarctic, and substantiates their importance to the nitrogen economy of these cold biomes","Similarly, Horne (1972) predicted annual N2 fixation by Collema pulposum (= C. tenax) on mossy gravel surfaces of Signy Island, Antarctica. This study was conducted while one of us (P.D.C.) The field studies were conducted in typical S. paschale woodland estimated by Maikawa (1976) to have been last burned in 1898. Sequential C2H2-reduction assays were performed during periods of natural diffuse radiation to avoid the acute problem of excessive elevation of thallus temperature within the incubation bottles that occurs at high levels of irradiance and each assay was begun within 2 min of removing the pseudopodetium from the lichen mat. Thallus temperature within the lichen mat was measured with micro-thermocouples (42 gauge wire) arranged in thermopiles and with the junctions embedded in the main stem of pseudopodetia within the upper 15 mm of the lichen canopy (Fig. A similar method was employed to monitor thallus temperature within the incubation bottles (Fig. 2b) thus providing a check on the extent of deviations from operative temperature in situ. 2b) was measured with a quantum sensor (Lambda Instruments Corporation), and pseudopodetia were weighed on an electrobalance immediately prior to incubation in order that water content could be determined after dry weight estimates had been obtained (Fig.","['In this paper the results of in situ measurements of nitrogenase activity are discussed with particular reference to the problems associated with predictive modelling of N2 fixation by lichens.', '(1971) was used to examine diurnal variation in rates of nitrogenase activity in a S. paschale mat in relation to thallus water content, temperature and incident photosynthetically active radiation (PAR-photon flux density in the 400-700 nm wavelength range).']"
"Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. Biological nitrogen (N) fixation is the primary source of N within natural ecosystems, yet the origin of boreal forest N has remained elusive. The boreal forests of Eurasia and North America lack any significant, widespread symbiotic N-fixing plants. With the exception of scattered stands of alder in early primary successional forests, N-fixation in boreal forests is considered to be extremely limited. Nitrogen-fixation in northern European boreal forests has been estimated at only 0.5 kg N ha(-1) yr(-1); however, organic N is accumulated in these ecosystems at a rate of 3 kg N ha(-1) yr(-1) (ref. Our limited understanding of the origin of boreal N is unacceptable given the extent of the boreal forest region, but predictable given our imperfect knowledge of N-fixation. and the ubiquitous feather moss, Pleurozium schreberi (Bird) Mitt. Previous efforts have probably underestimated N-fixation potential in boreal forests. Fixation rates also depend on the successional age of boreal forests via differences in environmental moisture, nutrition and possible other factors (DeLuca et al. 2008; Lagerström et al. 2009; #CITATION_TAG).",,['Herein we report on a N-fixing symbiosis between a cyanobacterium (Nostoc sp.)']
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Many different approaches for statistical modelling of cumulative disease progression have been proposed in the literature, including simple path models up to complex restricted Bayesian networks. Important fields of application are diseases such as cancer and HIV. These two very different diseases have typical courses of disease progression, which can be modelled partly by consecutive and partly by independent steps. We often find that the true model class used for generating data is outperformed by either a less or a more complex model class. The more flexible conjunctive Bayesian networks can be used to fit oncogenetic trees, whereas mixtures of oncogenetic trees with three tree components can be well fitted by mixture models with only two tree components. I provide next a brief review of the main methods, including recent developments, but see more extensive reviews in [#CITATION_TAG, 14].","Tumour progression is measured by means of chromosome aberrations, whereas people infected with HIV develop drug resistances because of genetic changes of the HI-virus. Different models are compared via simulations to analyse how they work if some of their assumptions are violated. In a simulation study, we evaluate how models perform in terms of fitting induced multivariate probability distributions and topological relationships.",['This paper gives an overview of the different progression models and points out their advantages and drawbacks.']
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Cancer can be a result of accumulation of different types of genetic mutations such as copy number aberrations. MILP is a Non-deterministic Polynomial-time complete (NP-complete) problem for which very good heuristics exists. Longitudinal data would be better suited for this problem but it is much harder to obtain and crosssectional data is (and will remain) the main source of data (e.g., the growing number of genomes available through international sequencing projects) for addressing these and similar problems [5, #CITATION_TAG].","In order to model cancer progression, we propose Progression Networks, a special case of Bayesian networks, that are tailored to model disease progression. Progression networks have similarities with Conjunctive Bayesian Networks (CBNs) [1],a variation of Bayesian networks also proposed for modeling disease progression. We also describe a learning algorithm for learning Bayesian networks in general and progression networks in particular. We reduce the hard problem of learning the Bayesian and progression networks to Mixed Integer Linear Programming (MILP). We tested our algorithm on synthetic and real cytogenetic data from renal cell carcinoma. We also compared our learned progression networks with the networks proposed in earlier publications.",['Finding the order in which the genetic events have occurred and progression pathways are of vital importance in understanding the disease.']
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Cytogenet.122, 101-109] to derive graph models for chromosome breaks in melanoma. I provide next a brief review of the main methods, including recent developments, but see more extensive reviews in [13, #CITATION_TAG].","We describe several analytical techniques for use in developing genetic models of oncogenesis including: methods for the selection of important genetic events, construction of graph models (including distance-based trees, branching trees, contingency trees and directed acyclic graph models) from these events and methods for interpretation of the resulting models. The models can be used to make predictions about: which genetic events tend to occur early, which events tend to occur together and the likely order of events. Unlike simple path models of oncogenesis, our models allow dependencies to exist between specific genetic changes and allow for multiple, divergent paths in tumor progression. A variety of genetic events can be used with the graph models including chromosome breaks, losses or gains of large DNA regions, small mutations and changes in methylation. As an application of the techniques, we use a recently published cytogenetic analysis of 206 melanoma cases [Nelson et al. Among our predictions are: (1) breaks in 6q1 and 1q1 are early events, with 6q1 preferentially occurring first and increasing the probability of a break in 1q1 and (2) breaks in the two sets [1p1, 1p2, 9q1] and [1q1, 7p2, 9p2] tend to occur together.",['This study illustrates that the application of graph models to genetic data from tumor sets provide new information on the interrelationships among genetic changes during tumor progression.Copyright 2001 Academic Press.']
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Human cancer is caused by the accumulation of genetic alterations in cells. Understanding the restrictions in the temporal order of accumulation of driver mutations not only provides insights into cancer biology, but can help identify early markers of disease as well as therapeutic targets [5] [6] [7] [#CITATION_TAG] [9], and can be an instrumental tool in the search for the ""Achilles' Heel"" of oncogene addiction [3, 10, 11].","When applied to a dataset of 70 advanced colorectal cancers, our algorithm accurately predicts the sequence of APC, KRAS, and TP53 mutations previously defined by analyzing tumors at different stages of colon cancer formation. We further validate the method with glioblastoma and leukemia sample data and then apply it to complex integrated genomics databases, finding that high-level EGFR amplification appears to be a late event in primary glioblastomas.","['Here we describe a computational approach, called Retracing the Evolutionary Steps in Cancer (RESIC), to deduce the temporal sequence of genetic events during tumorigenesis from cross-sectional genomic data of tumors at their fully transformed stage.', 'RESIC represents the first evolutionary mathematical approach to identify the temporal sequence of mutations driving tumorigenesis and may be useful to guide the validation of candidate genes emerging from cancer genome surveys.']"
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Major efforts to sequence cancer genomes are now occurring throughout the world. Though the emerging data from these studies are illuminating, their reconciliation with epidemiologic and clinical observations poses a major challenge. We model tumors as a discrete time branching process that starts with a single driver mutation and proceeds as each new driver mutation leads to a slightly increased rate of clonal expansion. Two of the models used, called here ""Bozic"" (as it is based on [#CITATION_TAG]) and ""exp"" have no density dependence and lead to exponential growth.","Using the model, we observe tremendous variation in the rate of tumor development - providing an understanding of the heterogeneity in tumor sizes and development times that have been observed by epidemiologists and clinicians. Furthermore, the model provides a simple formula for the number of driver mutations as a function of the total number of mutations in the tumor.","['In the current study, we provide a novel mathematical model that begins to address this challenge.']"
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Cancer progression is driven by a small number of genetic alterations accumulating in a neoplasm. These few driver alterations reside in a cancer genome alongside tens of thousands of other mutations that are widely believed to have no role in cancer and termed passengers. Many passengers, however, fall within protein coding genes and other functional elements and can possibly have deleterious effects on cancer cells. Surprisingly, despite selection against them, passengers accumulate and largely evade selection during progression. Although individually weak, the collective burden of passengers alters the course of progression leading to several phenomena observed in oncology that cannot be explained by a traditional driver-centric view. The second set of models, called ""McF_4"" and ""McF_6"", are based on McFarland et al's work [#CITATION_TAG] and lead to logistic-like behavior, as death rate depends on total population size.","Our approach combines evolutionary simulations of cancer progression with the analysis of cancer sequencing data. In our simulations, individual cells stochastically divide, acquire advantageous driver and deleterious passenger mutations, or die. We tested predictions of the model using cancer genomic data.",['Here we investigate a potential of mildly deleterious passengers to accumulate and alter the course of neoplastic progression.']
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. In solid tumours, data on genetic alterations are  usually only available at a single point in time, allowing no direct insight into the  sequential order of genetic events. Another early model are distancebased trees [17, #CITATION_TAG], but their meaning is rather different, since the observed mutations are only placed in the leaves or terminal nodes of the tree, and the internal nodes are unobserved and unknown events, which precludes an interpretation in terms of order restrictions like ""mutation A is required for mutation B"".","In our approach, genetic tumour development  and progression is assumed to follow a probabilistic tree model. We show how  maximum likelihood estimation can be used to reconstruct a tree model for the  dependencies between genetic alterations in a given tumour type. We illustrate the  use of the proposed method by applying it to cytogenetic data from 173 cases of  clear cell renal cell carcinoma, arriving at a model for the karyotypic evolution of  this tumour",['We present a new approach for modelling the dependencies between of genetic  changes in human tumours.']
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. The role of genetic instability in driving carcinogenesis remains controversial. Genetic instability should accelerate carcinogenesis by increasing the rate of advantageous driver mutations; however, genetic instability can also potentially retard tumour growth by increasing the rate of deleterious mutation. As such, it is unclear whether genetically unstable clones would tend to be more selectively advantageous than their genetically stable counterparts within a growing tumour. Clones can acquire both advantageous and deleterious mutations, and mutator mutations that increase a cell's intrinsic mutation rate. Genetic instability occurs secondarily to selectively advantageous driver mutations. Deleterious mutations have relatively little effect on the evolution of genetic instability unless selection for additional driver mutations is very weak or if deleterious mutations are very common. Enforcing monotonicity is equivalent to considering such a mutation as a mutation in an essential housekeeping gene, which can be modeled setting s h, in the notation of [#CITATION_TAG], to ∞ (so fitness of such clones is zero).","We employ a Wright-Fisher type model that describes the evolution of tumour subclones. Within the model, cancers evolve with a mutator phenotype when driver mutations bestow only moderate increases in fitness: very strong or weak selection for driver mutations suppresses the evolution of a mutator phenotype. Our model provides a framework for studying the evolution of genetic instability in tumour progression. Our analysis highlights the central role of selection in shaping patterns of mutation in carcinogenesis.","['Here, we show the circumstances where genetic instability evolves during tumour progression towards cancer.']"
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. From new types of data to new computational methodologies, computation is engendering a revolution in social science research and with this comes the issue of facilitating data and code sharing to encourage collaboration and reproducibility in scientific publishing. A repository designed for this purpose at Harvard University, the Dataverse Network, permits authors to upload data and code with their own terms of use. 56] [57] [58] and for implementing reproducible research [#CITATION_TAG].",,"['This paper examines these terms of use for 30,090 uploads to discover barrier issues to sharing in the social sciences and compares them to those found in a survey of NIPS registrants.']"
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Contact: 2robsmith@gmail.com, jtprince@chem.byu.edu Bioinformatic research has produced a large volume of proposed algorithmic solutions to a host of problems. Whether presented as a processing step in a clinical experiment or treated in a stand-alone publication, novel bioinformatic algorithms are often not subjected to the thorough comparative evaluation endured by their counterparts in other closely related fields--such as computer science--where an algorithm unevaluated against extant methods is considered unpublishable. We argue that failure during the review/publication process to require comparative evaluation for novel algorithms is detrimental to both parties. Moreover, the lack of public implementations precludes comparison of otherwise promising approaches, which ultimately hurts practitioners [#CITATION_TAG].","To demonstrate the dilemma, we conducted a case study of novel LC-MS alignment algorithms.","['Two audiences are interested in algorithmic publications: the practitioner, who may use the algorithm, and the researcher, who will work to develop solutions superior to those extant.']"
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Most human tumors result from the accumulation of multiple genetic and epigenetic alterations in a single cell. Mutations that confer a fitness advantage to the cell are known as driver mutations and are causally related to tumorigenesis. Other mutations, however, do not change the phenotype of the cell or even decrease cellular fitness. While much experimental effort is being devoted to the identification of the functional effects of individual mutations, mathematical modeling of tumor progression generally considers constant fitness increments as mutations are accumulated. There is a rich literature about tumor progression models that focuses on the consequences of drivers, passengers, and variation in selection pressures [20, 41, 42, 44, #CITATION_TAG, 71], and a largely separate body of work [13] [14] [15] 19, 21, 31, 32, 45] that deals with understanding the restrictions and order of accumulation of mutations (but see [6] for a connection between the λ i of CBNs and selection coefficients, in the context of the Fisher-Wright model of tumor progression in [49]).",We analyze a multi-type branching process in which cells accumulate mutations whose fitness effects are chosen from a distribution. We determine the effect of the fitness distribution on the growth kinetics of the tumor.,"['In this paper we study a mathematical model of tumor progression with random fitness increments.', 'This work contributes to a quantitative understanding of the accumulation of mutations leading to cancer.Copyright 2010 Elsevier Inc. All rights reserved.']"
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Next-generation DNA sequencing technologies are enabling genome-wide measurements of somatic mutations in large numbers of cancer patients. A common approach to identify driver mutations is to find genes that are mutated at significant frequency in a large cohort of cancer genomes. However, the current understanding of the somatic mutational process of cancer [3,5,6] places two additional constraints on the expected patterns of somatic mutations in a cancer pathway. There are numerous examples of sets of mutually exclusive mutations [5,6]. Thus, it probably pays off to try to use other approaches that incorporate information about non-silent mutation rates, pathway information together with combinatorial properties of drivers in pathwayws, or functional consequences of mutations to differentiate drivers from passengers [73] [74] [#CITATION_TAG] [76] [77] [78].","Thus, each cancer patient may exhibit a different combination of mutations that are sufficient to perturb the necessary pathways. First, an important cancer pathway should be perturbed in a large number of patients. Second, since driver mutations are relatively rare and typically a single driver mutation is sufficient to perturb a pathway, a reasonable assumption is that most patients have a single driver mutation in a pathway. Thus, the genes in a driver pathway exhibit a pattern of mutually exclusive driver mutations, where driver mutations are observed in exactly one gene in the pathway in each patient.","['A major challenge in interpretation of this data is to distinguish functional driver mutations that are important for cancer development from random, passenger mutations.', 'This approach is confounded by the observation that driver mutations target multiple cellular signaling and regulatory pathways.']"
"We studied choreographer Wayne McGregor's approach to movement creation through tasking, in which he asks dancers to create movement in response to task instructions that require a great deal of mental imagery and decision making. As part of a programme of research that is developing tools to enhance choreographic practice, an interdisciplinary team of cognitive scientists, neuroscientists and dance professionals collaborated on two studies examining the mental representations used to support movement creation. The assessment of voluntary behavior in non-communicative brain injured patients is often challenging due to the existence of profound motor impairment. In the absence of a full understanding of the neural correlates of consciousness, even a normal activation in response to passive sensory stimulation cannot be considered as proof of the presence of awareness in these patients. However, no data yet exist to indicate which imagery instructions would yield reliable single subject activation. The spatial navigation and motor imagery tasks described here permit the identification of volitional brain activation at the single subject level, without a motor response. The brain activations related to these tasks have been previously described (#CITATION_TAG), therefore they are used as reference to identify brain regions related to motor and spatial imagery, and embodiment.","The two most robust mental imagery tasks were found to be spatial navigation and motor imagery. In a third experiment, where these two tasks were directly compared, differentiation of each task from one another and from rest periods was assessed blindly using a priori criteria and was correct for every volunteer. Detecting awareness in the vegetative state.",['The aim of the present study was to establish such a paradigm in healthy volunteers.']
"We studied choreographer Wayne McGregor's approach to movement creation through tasking, in which he asks dancers to create movement in response to task instructions that require a great deal of mental imagery and decision making. As part of a programme of research that is developing tools to enhance choreographic practice, an interdisciplinary team of cognitive scientists, neuroscientists and dance professionals collaborated on two studies examining the mental representations used to support movement creation. In artists, the higher synchrony in the low-frequency band is possibly due to the involvement of a more advanced long-term visual art memory and to extensive top-down processing. Other studies have focused on the underlying neural mechanisms of creativity in realms other than dance (Jung et al., 2010), especially in music (Limb and Braun, 2008) and drawing (#CITATION_TAG).","To assess the underlying synchronization, which is assumed to be the platform for general cognitive integration between different cortical regions, three measures inspired by nonlinear dynamical system theory were applied as follows: (1) index based on generalized synchronization; (2) index based on mean phase coherence; and (3) index of phase synchrony based on entropy. Strong right hemispheric dominance in terms of synchronization was found in the artists.","['Our primary question was to learn whether mentally composing drawings of their own choice produce different brain electric features in artists and laymen.', 'To this purpose, we studied multichannel electroencephalograph (EEG) signals from two broad groups (all participants were females): artists (professionally trained in visual arts) and non-artists (without any training in art).']"
"We studied choreographer Wayne McGregor's approach to movement creation through tasking, in which he asks dancers to create movement in response to task instructions that require a great deal of mental imagery and decision making. As part of a programme of research that is developing tools to enhance choreographic practice, an interdisciplinary team of cognitive scientists, neuroscientists and dance professionals collaborated on two studies examining the mental representations used to support movement creation. Creativity has long been a construct of interest to philosophers, psychologists and, more recently, neuroscientists. Recent efforts have focused on cognitive processes likely to be important to the manifestation of novelty and usefulness within a given social context. One such cognitive process - divergent thinking - is the process by which one extrapolates many possible answers to an initial stimulus or target data set. The distribution of brain regions, associated with both divergent thinking and creative achievement, suggests that cognitive control of information flow among brain areas may be critical to understanding creative cognition. Other studies have focused on the underlying neural mechanisms of creativity in realms other than dance (#CITATION_TAG), especially in music (Limb and Braun, 2008) and drawing (Bhattacharya & Petsche, 2005).","Three independent judges ranked the creative products of each subject using the consensual assessment technique (Amabile, 1982) from which a ""composite creativity index"" (CCI) was derived. Structural magnetic resonance imaging was obtained at 1.5 Tesla Siemens scanner. Cortical reconstruction and volumetric segmentation were performed with the FreeSurfer image analysis suite. A region within the lingual gyrus was negatively correlated with CCI; the right posterior cingulate correlated positively with the CCI.","['We sought to link well established measures of divergent thinking and creative achievement (Creative Achievement Questionnaire - CAQ) to cortical thickness in a cohort of young (23.7 +- 4.2 years), healthy subjects.', 'This is the first study to link cortical thickness measures to psychometric measures of creativity.']"
"We studied choreographer Wayne McGregor's approach to movement creation through tasking, in which he asks dancers to create movement in response to task instructions that require a great deal of mental imagery and decision making. As part of a programme of research that is developing tools to enhance choreographic practice, an interdisciplinary team of cognitive scientists, neuroscientists and dance professionals collaborated on two studies examining the mental representations used to support movement creation. What is the contribution of the extrastriate body area (EBA) to this network? In particular, is the EBA involved in constructing a dynamic representation of observed actions? This region is thought to hold a human body representation (#CITATION_TAG) as well as a dynamic action representation (Downing et al., 2006) that may have contributed to both imagery creation and static movement creation.","Abstract Numerous cortical regions respond to aspects of the human form and its actions. We scanned 16 participants with fMRI while they viewed two kinds of stimulus sequences. In the coherent condition, static frames from a movie of a single, intransitive whole-body action were presented in the correct order. In the incoherent condition, a series of frames from multiple actions (involving one actor) were presented. We suggest that the EBA response adapts when succeeding images depict relatively similar postures (coherent condition) compared to relatively different postures (incoherent condition).","['We propose that the EBA plays a unique role in the perception of action, by representing the static structure, rather than dynamic aspects, of the human form.']"
"We studied choreographer Wayne McGregor's approach to movement creation through tasking, in which he asks dancers to create movement in response to task instructions that require a great deal of mental imagery and decision making. As part of a programme of research that is developing tools to enhance choreographic practice, an interdisciplinary team of cognitive scientists, neuroscientists and dance professionals collaborated on two studies examining the mental representations used to support movement creation. Hallucination is a big deal in contemporary philosophy of perception. Mental Imagery has been extensively investigated both in cognitive and neuroscience laboratories (#CITATION_TAG) and in applied settings, for example to improve performance in competitive sports (Murphy, 1990).","I argue that if we take hallucinations to be a form of mental imagery, then we have a very straightforward way of arguing against disjunctivism: if hallucination is a form of mental imagery and if mental imagery and perception have some substantive common denominator, then a fortiori, perception and hallucination will also have a substantive common denominator","[""The main reason for this is that the way hallucination is treated marks an important stance in one of the most hotly contested debates in this subdiscipline: the debate between 'relationalists' and 'representationalists'.""]"
"The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. Semantic search promises to produce precise answers to user queries by taking advantage of the availability of explicit semantics of information in the context of the semantic web. Existing tools have been primarily designed to enhance the performance of traditional search technologies but with little support for naive users, i.e., ordinary end users who are not necessarily familiar with domain specific semantic data, ontologies, or SQL-like query languages. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [3] [4] 6, 9, 13, 14, #CITATION_TAG].","In contrast with existing semantic-based keyword search engines which typically compromise their capability of handling complex user queries in order to overcome the problem of knowledge overhead, SemSearch not only overcomes the problem of knowledge overhead but also supports complex queries.","['This paper presents SemSearch, a search engine, which pays special attention to this issue by hiding the complexity of semantic search from end users and making it easy to use and effective.']"
"The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. Natural language interfaces offer end-users a familiar and convenient option for querying ontology-based knowledge bases. Several studies have shown that they can achieve high retrieval performance as well as domain independence. An advantage of using the Mooney data for the user-in-the-loop evaluation is the fact that it is a well-known and frequently used data set (e.g., [#CITATION_TAG], [17] and [18]).","To that end, we introduce four interfaces each allowing a different query language and present a usability study benchmarking these interfaces.","[""This paper focuses on usability and investigates if NLIs are useful from an end-user's point of view.""]"
"The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The semantic web presents the vision of a distributed, dynamically growing knowledge base founded on formal logic. Common users, however, seem to have problems even with the simplest Boolean expressions. As queries from web search engines show, the great majority of users simply do not use Boolean expressions. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [#CITATION_TAG] [4] 6, 9, 13, 14, 16].","We address this problem by presenting a natural language interface to semantic web querying. The interface allows formulating queries in Attempto Controlled English (ACE), a subset of natural English. Each ACE query is translated into a discourse representation structure - a variant of the language of first-order logic - that is then translated into an N3-based semantic web querying language using an ontology-based rewriting framework.",['So how can we help users to query a web of logic that they do not seem to understand?']
"The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability. Usability and context Usability is not a quality that exists in any real or absolute sense. This notion is neatly summed up by Terry Pratchett in his novel ""Moving Pictures"": "" 'Well, at least he keeps himself fit,' said the Archchancellor nastily. 'That would be the senior masters, Master,' said the Bursar. 'Ah, but Master,' said the Bursar, smiling indulgently, 'the word ""fit"",as I understand it, means ""appropriate to a purpose"", and I would say that the body of the Dean is supremely appropriate to the purpose of sitting around all day and eating big heavy meals.' (Pratchett, 1990) In just the same way, the usability of any tool or system has to be viewed in terms of the context in which it is used, and its appropriateness to that context. With particular reference to information systems, this view of usability is reflected in the current draft international standard ISO 9241-11 and in the European Community ESPRIT project MUSiC (Measuring Usability of Systems in Context) (e.g., Bevan, Kirakowski and Maissel, 1991). In general, it is impossible to specify the usability of a system (i.e., its fitness for purpose) without first defining who are the intended users of the system, the tasks those users will perform with it, and the characteristics of the physical, organisational and social environment in which it will be used. Since usability is itself a moveable feast, it follows that measures of usability must themselves be dependent on the way in which usability is defined. However, the precise measures to be used within each of these classes of metric can vary widely. For example, measures of effectiveness are very obviously determined by the types of task that are carried out with the system; a measure of effectiveness of a word processing system might be the number of letters written, and whether the letters produced are free of spelling mistakes. A consequence of the context-specificity of usability and measures of usability is that it is very difficult to make comparisons of usability across different systems. Comparing usability of different systems intended for different purposes is a clear case of ""comparing apples and oranges"" and should be avoided wherever possible. It is also difficult and potentially misleading to generalise design features and experience across systems; for example, just because a particular design feature has proved to be very useful in making one system usable does not necessarily mean that it will do so for another system with a different group of users doing different tasks in other environments. If there is an area in which it is possible to make more generalised assessments of usability, which could bear cross-system comparison, it is the area of subjective assessments of usability. Subjective measures of usability are usually obtained through the use of questionnaires and attitude scales, and examples exist of general attitude scales which are not specific to any particular system (for example, CUSI (Kirakowski and Corbett, 1988)). Industrial usability evaluation The demands of evaluating usability of systems within an industrial context mean that often it is neither cost-effective nor practical to perform a full-blown context analysis and selection of suitable metrics. Often, all that is needed is a general indication of the overall level of usability of a system compared to its competitors or its predecessors. Equally, when selecting metrics, it is often desirable to have measures which do not require vast effort and expense to collect and analyse data. As can be imagined, after this period of time, users could be very frustrated, especially if they had encountered problems, since no assistance was given. The System Usability Scale (SUS) is a simple, ten-item scale giving a global view of subjective assessments of usability. It is often assumed that a Likert scale is simply one based on forcedchoice questions, where a statement is made and the respondent then indicates the degree of agreement or disagreement with the statement on a 5 (or 7) point scale. However, the construction of a Likert scale is somewhat more subtle than this. For instance, if one was interested in attitudes to crimes and misdemeanours, one might use serial murder and parking offences as examples of the extreme ends of the spectrum. For instance, respondents might be asked to respond to statements such as ""hanging's too good for them"", or ""I can imagine myself doing something like this"". Given a large pool of such statements, there will generally be some where there is a lot of agreement between respondents. In addition, some of these will be ones where the statements provoke extreme statements of agreement or disagreement among all respondents. Items where there is ambiguity are not good discriminators of attitudes. For instance, while one hopes that there would be a general, extreme disagreement that ""hanging's too good"" for those who perpetrate parking offences, there may well be less agreement about applying this statement to serial killers, since opinions differ widely about the ethics and efficacy of capital punishment. The first questionnaire is a standardised usability test called the System Usability Scale (SUS) questionnaire [#CITATION_TAG].","In addition, there is a need for ""quick and dirty"" methods to allow low cost assessments of usability in industrial systems evaluation. It is possible to talk of some general classes of usability measure; ISO 9241-11 suggests that measures of usability should cover * effectiveness ( the ability of users to complete tasks using the system, and the quality of the output of those tasks), * efficiency ( the level of resource consumed in performing tasks) * satisfaction (users' subjective reactions to using the system). If the system supports the task of controlling an industrial process producing chemicals, on the other hand, the measures of task completion and quality are obviously going to reflect that process. These sorts of considerations were very important when, while setting up a usability engineering programme for integrated office systems engineering with Digital Equipment Co. Ltd, a need was identified for a subjective usability measure. The measure had to be capable of being administered quickly and simply, but also had to be reliable enough to be used to make comparisons of user performance changes from version to version of a software product. If they were then presented with a long questionnaire, containing in excess of 25 questions it was very likely that they would not complete it and there would be insufficient data to assess subjective reactions to system usability. SUS the System Usability Scale In response to these requirements, a simple usability scale was developed. SUS is a Likert scale. Whilst Likert scales are presented in this form, the statements with which the respondent indicates agreement and disagreement have to be selected carefully. The technique used for selecting items for a Likert scale is to identify examples of things which lead to extreme expressions of the attitude being captured. When these examples have been selected, then a sample of respondents is asked to give ratings to these examples across a wide pool of potential questionnaire items. SUS was constructed using this technique. A pool of 50 potential questionnaire items was assembled. Two examples of software systems were then selected (one a linguistic tool aimed at end users, the other a tool for systems programmers) on the basis of general agreement that one was ""really easy to use"" and one was almost impossible to use, even for highly technically skilled users. 20 people from the office systems engineering group, with occupations ranging from secretary through to systems programmer then rated both systems against all 50 potential questionnaire items on a 5 point scale ranging from ""strongly agree"" to ""strongly disagree"". The items leading to the most extreme responses from the original pool were then selected. In addition, items were selected so that the common response to half of them was strong agreement, and to the other half, strong disagreement.","['This was done in order to prevent response biases caused by respondents not having to think about each statement; by alternating positive and negative items, the respondent has to read each statement and make an effort to think whether they agree or dis']"
"The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The logic-based machine-understandable framework of the Semantic Web typically challenges casual users when they try to query ontologies. An often proposed solution to help casual users is the use of natural language interfaces. Such tools, however, suffer from one of the biggest problems of natural language: ambiguities. Furthermore, the systems are hardly adaptable to new domains. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [3] [4] 6, 9, 13, #CITATION_TAG, 16].","The approach allows queries in natural language, thereby asking the user for clarification in case of ambiguities.","['This paper addresses these issues by presenting Querix, a domain-independent natural language interface for the Semantic Web.']"
"The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The casual user is typically overwhelmed by the formal logic of the Semantic Web. The gap between the end user and the logic-based scaffolding has to be bridged if the Semantic Web's capabilities are to be utilized by the general public. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [3] [4] 6, 9, #CITATION_TAG, 14, 16].","We introduce GINO, a guided input natural language ontology editor that allows users to edit and query ontologies in a language akin to English. It uses a small static grammar, which it dynamically extends with elements from the loaded ontologies. Additionally, the approach's dynamic gram-mar generation allows for easy adaptation to new ontologies.","['This paper proposes that controlled natural languages offer one way to bridge the gap.', 'We believe that the use of guided entry overcomes the habitability problem, which adversely affects most natural language systems.']"
"The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The semantic web presents the vision of a distributed, dynamically growing knowledge base founded on formal logic. Common users, however, seem to have problems even with the simplest Boolean expressions. As queries from web search engines show, the great majority of users simply do not use Boolean expressions. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [#CITATION_TAG] [3] [4] 6, 9, 13, 14, 16].","We address this problem by presenting a natural language interface to semantic web querying. The interface allows formulating queries in Attempto Controlled English (ACE), a subset of natural English. Each ACE query is translated into a discourse representation structure - a variant of the language of first-order logic - that is then translated into an N3-based semantic web querying language using an ontology-based rewriting framework.",['So how can we help users to query a web of logic that they do not seem to understand?']
"The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The Semantic Web presents the vision of a distributed, dynamically growing knowledge base founded on formal logic. Common users, however, seem to have problems even with the simplest Boolean expression. As queries from web search engines show, the great majority of users simply do not use Boolean expressions. Ginseng relies on a simple question grammar which gets dynamically extended by the structure of an ontology to guide users in formulating queries in a language seemingly akin to English. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [3] [#CITATION_TAG] 6, 9, 13, 14, 16].","We address this problem by presenting Ginseng, a quasi natural language guided query interface to the Semantic Web. Based on the grammar Ginseng then translates the queries into a Semantic Web query language (RDQL), which allows their execution.",['So how can we help users to query a web of logic that they do not seem to understand?']
"Fifty per cent of responders had needed to consider MRE under GA. Aims To survey the perceived indications for magnetic resonance imaging of the small bowel (MRE) by experts, when MR enteroclysis (MREc) or MR enterography (MREg) may be chosen, and to determine how the approach to MRE is modified when general anaesthesia (GA) is required. In this article we will give a comprehensive literature review on sedation/general anaesthesia (S/GA) and discuss the international variations in practice and options available for S/GA for imaging children.The key articles were obtained primarily from PubMed, MEDLINE, ERIC, NHS Evidence and The Cochrane Library.Recently, paediatric radiology has seen a surge of diagnostic and therapeutic procedures, some of which require children to be still and compliant for up to 1 h. It is difficult and sometimes even impossible to obtain quick and high-quality images without employing sedating techniques in certain children. As with any medical procedure, S/GA in radiological practice is not without risks and can have potentially disastrous consequences if mismanaged. Advances in knowledge Imaging children under general anaesthesia is becoming routine and preferred by operators because it ensures patient conformity and provides a more controlled environment. SB distension with large volumes of orally administered fluids, however, is generally considered a contraindication in GA, or heavy sedation, given the risk of aspiration [#CITATION_TAG] [3] [4] [5].",,"['In order to reduce any complications and practice safety in radiological units, it is imperative to carry out pre-sedation assessments of children, obtain parental/guardian consent, monitor them closely before, during and after the procedure and have adequate equipment, a safe environment and a well-trained personnel.Although the S/GA techniques, sedative drugs and personnel involved vary from country to country, the ultimate goal of S/GA in radiology remains the same; namely, to provide safety and comfort for the patients.']"
"Fifty per cent of responders had needed to consider MRE under GA. Aims To survey the perceived indications for magnetic resonance imaging of the small bowel (MRE) by experts, when MR enteroclysis (MREc) or MR enterography (MREg) may be chosen, and to determine how the approach to MRE is modified when general anaesthesia (GA) is required. In brief patients prefer MREg [46], while radiologists prefer MREc images [46] [47] [48] [49] [#CITATION_TAG].","Forty patients with suspected Crohn's disease (CD) were examined with both MRI methods. MRI per OS was performed with a 6% mannitol solution and MRE with nasojejunal intubation and a polyethylenglycol solution. MRI protocol consisted of balanced fast field echo (B-FFE), T2 and T1 sequences with and without gadolinium. Two experienced radiologists individually evaluated bowel distension and pathological findings including wall thickness (BWT), contrast enhancement (BWE), ulcer (BWU), stenosis (BWS) and edema (EDM). However, CD was diagnosed with high diagnostic accuracy (sensitivity, specificity, positive and negative predictive values: MRI per OS 88%, 89%, 89%, 89%; MRE 88%, 84%, 82%, 89%) and inter-observer agreement (MRI per OS k = 0.95; MRE k = 1). However, both methods diagnosed CD with a high diagnostic accuracy and reproducibility.",['The aim was to compare bowel distension and diagnostic properties of magnetic resonance imaging of the small bowel with oral contrast (MRI per OS) with magnetic resonance enteroclysis (MRE).']
"Fifty per cent of responders had needed to consider MRE under GA. Aims To survey the perceived indications for magnetic resonance imaging of the small bowel (MRE) by experts, when MR enteroclysis (MREc) or MR enterography (MREg) may be chosen, and to determine how the approach to MRE is modified when general anaesthesia (GA) is required. Proximal SB distension is frequently less optimal in MRFT than in MRE. We, however, focused more on the choice between MREc and MREg as this has been more controversial and more studied, albeit only in patients with Crohn's disease [45, #CITATION_TAG].","METHODS Data were collected from all patients undergoing small bowel (SB) magnetic resonance imaging (MRI) examination over a 32-mo period. Patients either underwent a magnetic resonance (MR) follow-through (MRFT) or a MR enteroclysis (MRE) in the supine position. The quality of proximal and distal SB distension as well as the presence of motion artefact and image quality were assessed by 2 radiologists. MRE is, therefore, the preferred MR examination method of the SB.",['AIM To determine if a nasojejunal tube (NJT) is required for optimal examination of enteroclysis and if patients can be examined only in the supine position.']
"Ternary algebras, constructed from ternary commutators, or as we call them, ternutators, defined as the alternating sum of products of three operators, have been shown to satisfy cubic identities as necessary conditions for their existence. The subject of ternary algebras, a special case of n-Lie algebras is generally attributed to Fillipov [#CITATION_TAG], but Filippov was following up on earlier studies that had appeared in the mathematics literature, primarily by Kurosh [2] (as remarked in [3]).",,"['In this paper, we mainly study some properties of elementary n-Lie algebras, and prove some necessary and sufficient conditions for elementary n-Lie algebras, we also give the relations between elementary n-algebras and E-algebras.Comment: 13 page']"
"Ternary algebras, constructed from ternary commutators, or as we call them, ternutators, defined as the alternating sum of products of three operators, have been shown to satisfy cubic identities as necessary conditions for their existence. These generalizations are algebraic structures in which the two entries Lie bracket has been replaced by a bracket with n entries. Three-Lie algebras have surfaced recently in multi-brane theory in the context of the Bagger-Lambert-Gustavsson model. Its appearance in Physics was due to the pioneering work of Nambu [4], and more recently, the work of Bagger and Lambert [5] renewed interest in ternary algebras in the Theoretical Physics community (see also the review article [#CITATION_TAG]).","Each type of n-ary bracket satisfies a specific characteristic identity which plays the r\^ole of the Jacobi identity for Lie algebras. Particular attention will be paid to generalized Lie algebras, which are defined by even multibrackets obtained by antisymmetrizing the associative products of its n components and that satisfy the generalized Jacobi identity (GJI), and to Filippov (or n-Lie) algebras, which are defined by fully antisymmetric n-brackets that satisfy the Filippov identity (FI). Because of this, Filippov algebras will be discussed at length, including the cohomology complexes that govern their central extensions and their deformations (Whitehead's lemma extends to all semisimple n-Lie algebras). When the skewsymmetry of the n-Lie algebra is relaxed, one is led the n-Leibniz algebras. The standard Poisson structure may also be extended to the n-ary case. We shall review here the even generalized Poisson structures, whose GJI reproduces the pattern of the generalized Lie algebras, and the Nambu-Poisson structures, which satisfy the FI and determine Filippov algebras.",['This paper reviews the properties and applications of certain n-ary generalizations of Lie algebras in a self-contained and unified way.']
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Wheeler's argument draws on analytic philosophy, continental philosophy, and empirical work to ""reconstruct"" the philosophical foundations of cognitive science in a time of a fundamental shift away from a generically Cartesian approach. Indeed, they are often seen to be philosophically flawed (#CITATION_TAG).","Wheeler begins with an interpretation of Descartes. He defines Cartesian psychology as a conceptual framework of explanatory principles and shows how each of these principles is part of the deep assumptions of orthodox cognitive science (both classical and connectionist). Wheeler then turns to Heidegger's radically non-Cartesian account of everyday cognition, which, he argues, can be used to articulate the philosophical foundations of a genuinely non-Cartesian cognitive science.","['In Reconstructing the Cognitive World, Michael Wheeler argues that we should turn away from the generically Cartesian philosophical foundations of much contemporary cognitive science research and proposes instead a Heideggerian approach.', 'He points to recent research in ""embodied-embedded"" cognitive science and proposes a Heideggerian framework to identify, amplify, and clarify the underlying philosophical foundations of this new work.', 'He focuses much of his investigation on recent work in artificial intelligence-oriented robotics, discussing, among other topics, the nature and status of representational explanation, and whether (and to what extent) cognition is computation rather than a noncomputational phenomenon best described in the language of dynamical systems theory.']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Abstract The Modularity of Mind proposes an alternative to the ""New Look"" or ""interaetionist"" view of cognitive architecture that has dominated several decades of cognitive science. Whereas interactionism stresses the continuity of perceptual and cognitive processes, modularity theory argues for their distinctness. It is argued, in particular, that the apparent plausibility of New Look theorizing derives from the failure to distinguish between the (correct) claim that perceptual processes are inferential and the (dubious) claim that they are unencapsidated, that is, that they are arbitrarily sensitive to the organism's beliefs and desires. In fact, according to modularity theory, perceptual processes are computationally isolated from much of the background knowledge to which cognitive processes have access. The postulation of autonomous, domain-specific psychological mechanisms underlying perceptual integration connects modularity theory with the tradition of faculty psychology, in particular, with the work of Franz Joseph Call. It also references #CITATION_TAG 'Modularity of Mind'.",,"['Some of these historical affinities, and some of the relations between faculty psychology and Cartesianism, are discussed in the book.']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Drawing on his breakthrough research in comparative neuroscience, Terrence Deacon offers a wealth of insights into the significance of symbolic thinking: from the co-evolutionary exchange between language and brains over two million years of hominid evolution to the ethical repercussions that followed man's newfound access to other people's thoughts and emotions. This way of conceptualizing the transition to a symbolic style of thought has a close relation with Bickerton's proposal for a transition from 'on-line' to 'off-line' thinking (Bickerton, 1996), and also to #CITATION_TAG proposal for a progression from indexical to symbolic representation.",,"['This revolutionary book provides fresh answers to long-standing questions of human origins and consciousness.', ""Informing these insights is a new understanding of how Darwinian processes underlie the brain's development and function as well as its evolution."", 'It injects a renewed sense of adventure into the experience of being human.']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Almost all the reviewers of Margaret Boden's Mind as machine have noted the obvious: at 2 volumes, 1452 pages, 134 pages of references, and seemingly infinite parenthetically cross-references, this book, longer than most editions of War and peace, is impractical, unwieldy, and inaccessible to readers. Boden did not intend Mind as machine to be a pleasant read for a weekend's leisure. Yet, according to her, this analogy, as well as its parallel ""mind as machine"", is of recent origin. It was only by the close of the nineteenth century that mechanistic theories of mind acquired respectability. These theories, however, were mere analogies; no one seriously contemplated consilience between the behaviours of machines and men. Still less did anyone outside science fiction circles propose that machines could be intelligent in the same way as humans. By the mid-1800s, Charles Babbage had invented an analytical engine, somewhat akin to a programme-controlled digital computer, but he never claimed it to have implications for psychology or biology, though perhaps his student Ada Lovelace hinted at the possibility. Thus, it was during the war years of the 1940s, at the height of collaborations between Anglo-American scientists, that computers began being developed, and with them, some investigators, such as Alan Turing, began to study questions about machine intelligence. In the 1950s, these claims led to the emergence of the multi-disciplinary field of the cognitive sciences, a discipline well provided for by philanthropic and institutional sources of support, stocked with new venues for publication, and bolstered by artificial intelligence research paradigms. It was, none the less, a field riddled with intellectual divides, which developed over the next half century. Behaviourism, then predominant, was on the wane. Seen as too universalist, it was criticized by Gestaltists, linguists, ethologists, proto-connectionists, anthropologists, and Noam Chomsky alike (the last comes bizarrely in Boden's narrative with a ""health warning"", p. 591). In this ferment, the ""mind as machine"" debate took different paths: cyberneticists, for example, assumed that the mind as a machine was identical with the body. Computational psychologists--little more than a smattering of research endeavours--treated the human mind as different from its body, and concerned themselves with questions about how the mind was different. The majority of psychologists, however, focused on what made the mind different. Always lurking in the background was the question of whether human thought was ""constituted by, or identical with"" symbolic processes (p. 702). Those questions especially plagued papers and programmes on artificial intelligence--even when their authors were uninterested in the answers. Artificial intelligence research bolstered this nascent field enormously during the last half of the century. AI research, however, was perhaps more tied to the geopolitical context of the Cold War period and the neo-Liberal period of the 1980s and 1990s than the cognitive sciences. While much AI work focused on developing programming languages and had modest goals (seek general intelligence but not human-like intelligence, appeared almost as an injunction), critics levelled numerous charges at AI-workers, despite the fact that few were seeking to understand the human mind as a machine. Seymour Papert, an early pioneer, for instance, used only simple programmes to understand thinking processes. Yet, as defence spending increased, AI's proponents and detractors became uncomfortable with the glib assertions being promulgated within policy and media exaggerations, especially the belief that enormous computer systems controlling weapon systems could be ""bug"" free in their script, and commonsensical in their behaviour. Connectionists, a new but inchoate group of psychologists, neuroscientists, and philosophers of the mind, also tore into the AI project. They argued that phenomena were represented within emerging networks (usually neurological) and not symbolic systems, which many within old-fashioned AI paradigms had claimed. In hindsight, all of AI's failed promises and faulty philosophical assumptions have led some to pronounce it a failed research programme. She observes that AI enormously advanced both itself and the cognitive sciences. In that sense, and contrary to its critics, AI continued as a fruitful area of research, but like its latest corollaries, computational neuroscience and artificial life, the field remains embryonic even today. Having read those chapters alongside M R Bennett and P M S Hacker's excellent Philosophical foundations of the neurosciences (2003), I find myself having misgivings about the conceptual foundations of much of the cognitive sciences project as outlined by Boden. In any case, Boden's volumes, despite their evident value, will aggravate many. Historians studying periods before 1945 will find fault both with her facts and pithy generalizations. Similarly, those still living cognitive scientists whose careers spanned 1945 and 2000 are bound not to recognize the caricatures of themselves, or people they knew, in her story. Such criticisms, which have already begun circulating about this work, strike me as unwarranted, especially because Boden's practitioner viewpoint brings with it the hindrances such life experience implies. Anyone failing to note Boden's polemical tone is just not awake. Putting it simply, the work is too large to be free of an agenda. However, for that same reason, criticisms of this work from other practitioners appear no less problematic. Cognitive science's reliance on computer simulation means it is not well equipped to give an answer (#CITATION_TAG).","Boden begins by noting that some might mistake ""man as machine"" for an ancient idea.","['She intended it for people whose work includes being active readers, and for them it does represent a useful work of synthesis.']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Did, for instance, Homo habilis have language, Homo erectus self-awareness or Neanderthals the capacity for analogical reasoning? While fossil endocasts may inform about brain structure, the character of past cognition must be largely inferred from the archaeological record. And to draw such inferences archaeologists need to engage with, or rather become participants in, the cognitive sciences - just as Bloch (1991) has recently argued for anthropology. This is essential since we cannot pretend to understand the ancient mind without entering debates concerning the character of the modern mind. I consider whether the Middle/Upper Palaeolithic transition may have constituted a phase in human evolution during which there was a significant development from domain specific to generalized intelligence. In this theory (#CITATION_TAG), the domain-specific entities are understood to be specialized intelligences, along the lines of (Gardner, 1993).",,"['Introduction One of the tasks facing cognitive archaeology is to contribute towards an understanding of the nature and evolution of the human mind.', 'We need to make explicit reference to past cognition when interpreting the archaeological record and to draw inferences from that data concerning ancient minds .', 'In this paper I focus on one of these debates, that concerning whether the mind is a general purpose learning mechanism or composed of a series of relatively independent mental modules - psychological mechanisms dedicated to specific tasks or behavioural domains.', 'My review of this debate suggests that a major feature of human cognitive evolution has been increased accessibility between mental modules resulting in a generalized intelligence, though one remaining within a modular architecture.']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Where it once emphasized the importance of representational multiplicity and centralized integration (e.g. #CITATION_TAG; Gregory, 1984) it now gives as much weight to exploitation of scaffolding and embodiment (e.g. Wheeler, 1994; Beer, 2000). And where it once committed to symbolic reasoning being the medium of high-level integration (e.g. Marr, 1977; Boden, 1977; Winston, 1984) it increasingly recognizes the greater potential (and neural plausibility) of probabilistic forms (e.g. Doya et al., 2007; Chater and Oaksford, 2009; Clark, 2008).","The description of the main components and mechanisms of the architecture is followed by a discussion of several domains where CHREST has already been successfully applied, such as the psychology of expert behaviour, the acquisition of language by children, and the learning of multiple representations in physics. The characteristics of CHREST that enable it to account for empirical data include: self-organisation, an emphasis on cognitive limitations, the presence of a perception-learning cycle, and the use of naturalistic data as input for learning.","['This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits non-commercial use, distribution and reproduction in any medium, provided the original work is properly cited.This paper argues that the CHREST architecture of cognition can shed important light on developing artificial general intelligence.', 'The key theme is that ""cognition is perception.""']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. In so doing, two types of theory arise. Artificial Intelligence is the study of complex information processing problems that often have their roots in some aspect of biological information processing. The solution to an information processing problem divides naturally into two parts. The choice of algorithm usually depends upon the hardware in which the process is to run, and there may be many algorithms that implement the same computation. The theory of a com-putation, on the other hand, depends only on the nature of the problem to which it is a solution. The (computational) theory of the Fourier transform is well understood, and is expresse Where it once emphasized the importance of representational multiplicity and centralized integration (e.g. Anderson, 1983; Gregory, 1984) it now gives as much weight to exploitation of scaffolding and embodiment (e.g. Wheeler, 1994; Beer, 2000). And where it once committed to symbolic reasoning being the medium of high-level integration (e.g. #CITATION_TAG; Boden, 1977; Winston, 1984) it increasingly recognizes the greater potential (and neural plausibility) of probabilistic forms (e.g. Doya et al., 2007; Chater and Oaksford, 2009; Clark, 2008).","Here, they are labelled Types 1 and 2, and their characteristics are outlined. In the first, the underlying nature of a particular computation is characterized, and its basis in the physical world is understood. One can think of this part as an abstract formulation of what is being computed and why, and I shall refer to it as the &amp;quot;theory &amp;quot; of a computation. The second part consists of particular algorithms for implementing a computation, and so it specifies how. Jardine and Sibson [6] decomposed the subject of cluster analysis in precisely this way, using the term &amp;quot;method &amp;quot; to denote what I call the theory of a computation. To make the distinction clear, let us take the case of Fourier analysis.","['The goal of Artificial Intelligence is to identify and solve tractable informatioa processing problems.', 'The goal of the subject is to identify interesting and solvable information processing problems, and solve them.']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. (This might be in the manner envisaged by #CITATION_TAG.)","To fulfil this aim, standard cognitive neuropsychological tests and experimental paradigms have been applied in an intensive series of measurements, consisting five sessions. Each of the cognitive functionings in focus (complex, verbalised naive theory of mind, executive functions and working memory) were measured repeatedly within and across these sessions, with various measure tools (2-3 times per specific tool). A test group of high functioning young adults with autism spectrum disorder (N=20) and a neurotypical control group (N=20) were matched along age, sex and IQ. A group of developmental controls (neurotypical adolescents, N=10) was also involved, in order to control if patterns of stability in the autism group mirror delayed maturation of these cognitive skills, or a qualitatively atypical pattern.","['|  The aim of this research project has been to investigate if autism spectrum conditions are characterised by an increased instability in certain cognitive functionings, specifically on short-term (temporal resolution of seconds to days).']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. It is argued that the account of Savage-Rumbaugh's ape language research in Savage-Rumbaugh, Shanker and Taylor (1998. Apes, Language and the Human Mind. Oxford University Press, Oxford) is profitably read in the terms of the theoretical perspective developed in Clark (1997. The authors, though, make heavy going of a critique of what they take to be standard approaches to understanding language and cognition in animals, and fail to offer a worthwhile theoretical position from which to make sense of their own data. Research in artificial intelligence demonstrated convincingly that symbolic reasoning machines cannot replicate the power and fluidity of human cognition (#CITATION_TAG; Beer, 2000; Wheeler, 2005).","This model of 'distributed' cognition helps makes sense of the lexigram activity of Savage-Rumbaugh's subjects, and points to a re-evaluation of the language behaviour of humans","[""The contribution made by Clark's work is to show the range of ways in which cognition exploits bodily and environmental resources.""]"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Most research in computer vision has been directed towards minimalistic approaches, in which problems are addressed on how properties of the environment can be computed from as little information as possible. Although such approaches may be scientifically well motivated they have only resulted in limited progress towards our understanding of seeing systems. Ballard, Bajcsy and others have pointed out the importance of vision being an active process which is tightly connected to behaviors. Continuous operation over time and early use of three dimensional cues are important in this context. Conceptions of cognition in which symbolic reasoning takes charge are increasingly questioned (Thelen and Smith, 1993; #CITATION_TAG).",We illustrate our proposed approach by some experiments on a real-time active system.,"['We support this thought and also propose that utilizing that the world is rich on information is essential.', 'We develop this idea to show how attention and figure-ground segmentation by an active observer using multiple cues can be separated from analyzing and recognizing what is seen in a consistent way.']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. If a martian graced our planet, it would bestruck by one remarkable similarity amongEarth's living creatures and a key difference. Concerning similarity, it would note that all living things are de Development of this style of reasoning is generally assumed to interact closely with evolution of language. But the connection is difficult to discern, partly because language seems somewhat overpowered with regard to its initial application (Dunbar, 1996), and partly because it is extremely hard to disentangle cause and effect (#CITATION_TAG).","We suggest how current developments in linguistics can be profitably wedded to work in evolutionary biology, anthropology, psychology, and neuroscience. FLB includes a sensory-motor system, a conceptual-intentional system, and the computational mechanisms for recursion, providing the capacity to generate an infinite range of expressions from a finite set of elements. We hypothesize that FLN only includes recursion and is the only uniquely human component of the faculty of language.",['We submit that a distinction should be made between the faculty of language in the broad sense (FLB) and in the narrow sense (FLN).']
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Archaeology's main contribution to the debate over the origins of modern humans has been investigating where and when modern human behavior is first recognized in the archaeological record. Most of this debate has been over the empirical record for the appearance and distribution of a set of traits that have come to be accepted as indicators of behavioral modernity. This debate has resulted in a series of competing models that we explicate here, and the traits are typically used as the test implications for these models. However, adequate tests of hypotheses and models rest on robust test implications, and we argue here that the current set of test implications suffers from three main problems: (1) Many are empirically derived from and context-specific to the richer European record, rendering them problematic for use in the primarily tropical and subtropical African continent. In addition, there are severe taphonomic problems in the application of these test implications across differing spans of time. christopher s. henshilwood is Professor at the Centr But the torrent of art, technology, ritual and symbolism that is deemed the distinctive signature of Home sapiens is seen to develop momentum somewhat later, with the change being particularly dramatic at the Middle/Upper Paleolithic transition in western Europe (#CITATION_TAG).","(2) They are ambiguous because other processes can be invoked, often with greater parsimony, to explain their character. (3) Many lack theoretical justification.","['To provide adequate tests of these models, archaeologists must first subject these test implications to rigorous discussion, which is initiated here.']"
"In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. In the Eurasian Upper Paleolithic after about 35,000 years ago, abstract or depictional images provide evidence for cognitive abilities considered integral to modern human behavior. The earliest known artistic artefact-the incised slab of shale from the Blombos Cave-is dated to more than 70,000 years ago (#CITATION_TAG).",,['Here we report on two abstract representations engraved on pieces of red ochre recovered from the Middle Stone Age layers at Blombos Cave in South Africa.']
"The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. Health professionals working with specific patient groups are often aware of these difficulties and their detrimental effect on quality of life and wellbeing. However, many health professionals lack knowledge and skills on how to support patients, either within the team or via referral to relevant service providers who have expertise in managing appearance-related distress. This is mainly owing to a lack of information about screening, assessment measures and available interventions. The resultant psychological distress and dysfunction associated with visible differences associated with disease, traumatic injury and congenital and developmental abnormality has similarly been increasingly documented over recent years (#CITATION_TAG).","Gaps in relation to screening measures, differing types of intervention, appearance-related skills training for healthcare teams, care pathways and access to services are identified. A summary of the limited range of interventions currently available has been mapped onto a stepped model of care, ranging from self-help to intensive therapies.","['The aim of this article is to consider how health professionals can support patients who experience distress as a result of living with a long-term physical health condition that has altered their appearance.', 'This article summarises the challenges for patients and health professionals surrounding body image, altered appearance and associated distress.']"
"The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. All 24 corrected item total correlations fall in the desirable range of between 0.30 and 0.70 (see #CITATION_TAG), and this is replicated in both the clinical population and the non-clinical population.","There were three stages. First, we carried out interviews with 40 patients with hand/arm disorders to develop and pilot questionnaire content. Second, in a postal survey with 165 pre- and 181 post-surgery patients, we reduced the number of items and identified scales. The Patient Outcomes of Surgery-Hand/Arm (POS-Hand/Arm) is a new surgical outcome measure that can be used before and after surgery (29 and 33 items, respectively) to evaluate and compare new techniques, surgical teams and units","['The purpose of this study was to develop and validate a new patient-based outcome measure for hand/arm disorders for use in audit, clinical trials and effectiveness studies.']"
"The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. Burn patients are rendered with physical as well as mental scars; the latter usually are more protean in their manifestations. Rehabilitation after burn can be a grueling experience and the associated stress can blemish the patient's sexuality and intimacy. There is dearth of literature regarding the quality of sexual life after burn as well as sexual rehabilitation; it is fully known that a healthy sexual life is intricately related to a person's mental well being and a sexually compromised person can never be totally happy. Issues concerning appearance and sexual difference for people with a visibly different appearance are also recognised as neglected areas such as in burns rehabilitation (#CITATION_TAG).",Quality of sexual life was investigated by making burn patients answer the Maudsley Marital Questionnaire (the sexual scale only) 6 months after burn. A control group of nonburn patients matched with the cases was made to answer the same and values were compared for significance.,"['The objective of this study was to ascertain the degree of satisfaction in postburn patients regarding their sexual lives, parameters associated with sexual dissatisfaction, if present, and ways to address the same.']"
"The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. Rigorous comparison of the reliability coefficients  of several tests or measurement procedures requires a  sampling theory for the coefficients. Analysis using Feldt's test (see #CITATION_TAG; Feldt, Woodruff & Salih, 1987) indicates that Cronbach's alpha does not significantly differ between the clinical and non-clinical sample (p = 0.598).","It also permits  researchers to test the hypothesis of equality among  several coefficients, either under the condition of independent  samples or when the same sample has been  used for all measurements. The procedures are illustrated  numerically, and the assumptions and derivations  underlying the theory are discussed","[""This paper summarizes  the important aspects of the sampling theory  for Cronbach's (1951) coefficient alpha, a widely used  internal consistency coefficient."", 'This theory enables  researchers to test a specific numerical hypothesis  about the population alpha and to obtain confidence  intervals for the population coefficient.']"
"The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. The lack of understanding of sexual functioning in relation to body image, and any accompanying lack of measurement tools have been cited as a major barrier to developing effective interventions (#CITATION_TAG; Taylor et al., 2011).","First, what does current empirical research and clinical experience teach us about each of these areas, and second, what are the most important gaps in current knowledge about body image and social functioning, respectively? The final section of the paper specifically addresses the question of what can be done, from a practical and a health policy perspective, to ensure that existing body image and social difficulties are appropriately addressed.","['This paper reviews four major topics related to the long-term psychosocial rehabilitation for burn survivors; (1) Body image adjustment process; (2) Social functioning challenges; (3) Interventions designed to address psychosocial rehabilitation challenges; and (4) Current policy developments in the USA and the UK that focus on raising the rehabilitation standards for psychosocial care for burn survivors.', 'While acknowledging the close relationship between body image distress and social functioning, these two areas are reviewed separately with the goal of addressing two specific questions.']"
"The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. At the mathematical level, a factor or principal component of a factor analysis is simply a linear combination of variables under some constraints. Firstly, data were checked for influential observations; we measured changes in the ellipsoid volume of the dataset if an observation was deleted (#CITATION_TAG).",,"['The nature of these effects as well as potential effects due to ""gross errors"" in the data set should be investigated in order to determine which observations, if any, need to be analyzed separately or excluded entirely.', 'The purpose of this paper is (1) to propose a new technique for identifying influential observations and observations containing ""gross errors"" and (2) to discuss situations under which each is likely to significantly alter the results of a factor analysis.factor analysis, influential observations']"
"The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. In order to be able to have a relevant, specific and well defined outcome variable to further assess these theoretical explorations, and also to make a meaningful assessment of interventions, a team of plastic surgeons and psychologists created the Derriford Appearance Scale 59 (#CITATION_TAG).","DESIGN Cross-sectional survey designs using clinical (out-patient and in-patient) and general population samples. METHOD Twenty-five items were selected initially from the 59 items of the original DAS59. These were refined to 24 through item analyses and the scale was standardized on 535 patients with a range of problems of appearance and on a representative general population sample (N=1, 107). It is psychometrically robust and discriminates well between patient groups, between clinical and non-clinical populations, and within the general population between those concerned, and those not concerned, about their appearance.","['OBJECTIVES To develop a psychometrically robust and widely applicable short form of the Derriford Appearance Scale, (DAS59), which (1) will reliably and validly assess the distress and difficulties experienced in living with problems of appearance, (2) is acceptable to clinical and non-clinical populations, and (3) facilitates research and clinical decision-making through good standardization and sensitivity.']"
"The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. Some people who have a visible difference (disfigurement) experience psychosocial adjustment problems that can lead to social anxiety and isolation. Applied psychologists, including health, clinical, and counseling psychologists have been at the forefront of developing interventions to support people with psychological needs arising from visible differences (#CITATION_TAG; Bessell et al., 2012b), and in developing a clearer understanding of the differentiating factors and processes between those who adjust well, and those who struggle to cope and manage with differing appearances.","Eighty-three participants were assessed at four time points using the Hospital Anxiety and Depression Scales, Derriford Appearance Scale-24, Body Image Quality of Life Inventory and Fear of Negative Evaluation (FNE). A remote-access, computer-based intervention offers the potential to provide psychosocial support more easily and in a cost-effective manner to adults with appearance-related distress.",['The aim of this study was to assess the effectiveness of a new computerised CBT-based intervention (Face IT) in reducing anxiety and appearance-related distress for individuals with visible differences.']
"OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of ""clusters"" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). The SNARE protein of Arabidopsis, SYP121, contributes to vesicle traffic and also controls the gating of K+ channels for K+ uptake by binding to the KC1 channel subunit. The SNARE (for soluble N-ethylmaleimide-sensitive factor protein attachment protein receptor) protein SYP121 (=SYR1/PEN1) of Arabidopsis thaliana facilitates vesicle traffic, delivering ion channels and other cargo to the plasma membrane, and contributing to plant cell expansion and defense. More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (#CITATION_TAG).","Here, we report isolating a minimal sequence motif of SYP121 prerequisite for its interaction with KC1. We made use of yeast mating-based split-ubiquitin and in vivo bimolecular fluorescence complementation assays for protein-protein interaction and of expression and electrophysiological analysis.","['The identity of the KC1 binding site on the SNARE protein, described in this study, points to a novel role for the channel subunit in coordinating vesicle traffic.']"
"OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of ""clusters"" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). Plant sensitive factor attachment protein receptors (SNAREs) encoded by genes of the same sub-family are generally considered as redundant in promoting vesicle-associated membrane fusion events. Nonetheless, the application of innovative experimental approaches highlighted that members of the same gene sub-family often have different functional specificities. Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (#CITATION_TAG).","When transiently overexpressed, the SYP51 and the SYP52 distributed between the TGN and the tonoplast.","['In this work, two closely related Qc-SNAREs--the AtSYP51 and the AtSYP52--are compared in their ability to influence different secretory pathways.']"
"OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of ""clusters"" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). The continuous polarized vesicle secretion in pollen tubes is essential for tip growth but the location of endo- and exocytic sub-domains remains however controversial. The specific distribution of SYP124 at the plasma membrane is affected by changes in Ca2+ levels in agreement with the importance of this ion for exocytosis. Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (#CITATION_TAG; Yang et al., 2012).",,"['In this report we aimed to show that Arabidopsis thaliana syntaxins are involved in this process and contribute to spatially define exocytosis and membrane recycling.Using GFP-fusion constructs, we imaged the distribution of pollen-specific (AtSYP124) and non-pollen syntaxins (AtSYP121 and AtSYP122) in transiently transformed Nicotiana tabacum pollen tubes.']"
"OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of ""clusters"" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). In eukaryotic endomembrane systems, Qabc-SNAREs (soluble N-ethylmaleimide-sensitive factor attachment protein receptors) on one membrane and R-SNARE on the opposing membrane assemble into a trans-QabcR-SNARE complex to drive membrane fusion. However, it remains ambiguous whether pairing of Qabc- and R-SNAREs mediates membrane fusion specificity. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (#CITATION_TAG) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011).",,"['Here, we explored the fusion specificity of reconstituted proteoliposomes bearing purified SNAREs in yeast vacuoles and other organelles.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. When we speak of consciousness we are referring to the sum total of events in awareness. The term by no means exhausts the realm of things psychological, but it does encompass all of an individual's direct experience. When we speak of the flow of consciousness we are referring to the changes that take place in consciousness over time. The events of consciousness are, of course, extremely complex and varied. They do not contain the imagery of current perceptual activity but they contain imaginai qualities that one can describe in terms of forms, colors, sounds, words, smells, tastes, temperatures, and the like. There are dream-like segments in waking states -in one study 25% of waking thought samples were rated by participants as having at least a trace of dream-like qualities Cox, 1987-1988), which agrees approximately with other results (Foulkes and Fleisher, 1975; #CITATION_TAG Klinger, -1979) -as well as there being waking-like cognitive content in dreams.","They embrace images in every sensory modality and in every degree of vividness, realism, and believability, including inner dialogue, hallucinations, reveries, and dreamlike sequences; and they also embrace qualities that are at the same time less figured and more pervasive than these--the affects.","['This chapter focuses on a broad class of these conscious contents.', 'This chapter brings together ideas and data regarding ways to observe thought, the dimensions and forms of thought, and the factors that determine the content of thought as it changes from one moment to the next.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. (Reports, 19 January 2007, p. 393) attributed activity in certain regions of the ""resting"" brain to the occurrence of mind-wandering. However, previous research has demonstrated the difficulty of distinguishing this type of stimulus-independent thought from stimulus-oriented thought (e.g., watchfulness). Indeed, it appears that the brain's default-mode network provides the substrate for mind-wandering (e.g., #CITATION_TAG; Christoff et al., 2009; Andrews-Hanna et al., 2010a; Stawarczyk et al., 2011b), a network of several ""hubs"" and ""subsystems"" (Andrews-Hanna, 2012) that probably constitutes a majority of the brain's energy consumption (Raichle, 2009).",,['Consideration of both possibilities is required to resolve this ambiguity']
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. The default mode network (DMN) is a set of brain regions that consistently shows higher activity at rest compared to tasks requiring sustained focused attention toward externally presented stimuli. The cognitive processes that the DMN possibly underlies remain a matter of debate. It has alternately been proposed that DMN activity reflects unfocused attention toward external stimuli or the occurrence of internally generated thoughts. Indeed, it appears that the brain's default-mode network provides the substrate for mind-wandering (e.g., Mason et al., 2007; Christoff et al., 2009; Andrews-Hanna et al., 2010a; #CITATION_TAG), a network of several ""hubs"" and ""subsystems"" (Andrews-Hanna, 2012) that probably constitutes a majority of the brain's energy consumption (Raichle, 2009).","Four classes of conscious experiences (i.e., being fully focused on the task, distractions by irrelevant sensations/perceptions, interfering thoughts related to the appraisal of the task, and mind-wandering) that varied along two dimensions (""task-relatedness"" and ""stimulus-dependency"") were sampled using thought-probes while the participants performed a go/no-go task. On the other hand, lateral temporal regions (also part of the DMN) were specifically related to stimulus-independent reports.",['The present study aimed at clarifying this issue by investigating the neural correlates of the various kinds of conscious experiences that can occur during task performance.']
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Research on the determinants of waking thought content (Klinger, 1978) has demonstrated the influence of current concerns on cognitive processes. The concept of current concern refers to the state of an organism between the time it becomes committed to a particular goal and the consummation or abandonment of the goal. Responsiveness to external stimulation has been shown to occur in sleeping subjects. Subjects can be induced to perform physical acts in response to cues introduced while they are asleep (Evans, Gustafson, O'Connell, Orne, & Shor, 1970; Oswald, Taylor, & Treisman, 1960; Williams, Morlock, & Morlock, 1966). Both the cues and the responses used in these investigations varied considerably: stereotyped hand movements to taperecorded names (Oswald et al., 1960), overlearned natural responses to suggestions about subjects' physical states, such as scratching one's nose in response to the suggestion that it was itchy (Evans et al., 1970), or finger movements in response to conditioned auditory stimuli (Williams et al., 1966). One investigation in a sleep laboratory using standard electroencephalography (EEG) and eye movement measures (#CITATION_TAG) administered a modified Concern Dimensions Questionnaire (CDQ; Klinger et al., 1980 Klinger et al.,, 1981 to assess seven participants' goals, followed by four consecutive nights, an adaptation night and three experimental nights, during which, five to seven times per night during Stage 1-rapid eye movement (REM) or Stage 2 sleep, the experimenters Hoelscher et al. (1981).","The hypothesis that cues related to subjects' current concerns can control attentional and cognitive processes during sleeping and dreaming was examined by presenting concern- and nonconcern-related verbal stimuli to seven male subjects during sleep Stages 2 and REM. During dichotic listening, cues related to subjects' current concerns exerted a controlling effect on attention, recall, and thought content.",['The present investigation seeks to extend these findings to effects of concernrelated cues on dream content.']
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Goal choice depends on the value and costs assigned by the chooser to each alternative (incentive) and its perceived attainability, subject to such complicating factors as forecasting biases and time frame. Commitment to a goal pursuit launches a latent, time-binding brain process (a current concern) that sensitizes the individual to respond emotionally and to notice, recall, think about, dream about, and act on cues associated with thegoal pursuit. These processes affect one another and are subject to implicit (nonconscious) as well as explicit influences. Goal pursuits vary according to whether the goal is an approach or avoidance goal, the time frame for action, the anticipation of the details and difficulties of the goal pursuit, and the degree of conflict with other goals. Emotional responses determine incentive values, serve as evaluative feedback during goal pursuits, and accompany consummation of or disengagement from the goal. The process of disengagement normally entails a sequence of emotional changes: invigoration, anger, depression, and recovery. Motivational structure (an individual's pattern of goal striving) is an important determinant of well-being, the sense that one's life is meaningful, and self-regulation. This theory, together with recent neuroscientific findings that support it, is briefly reviewed elsewhere (#CITATION_TAG).","Synopsis.--Behavior and experience are organized around the pursuit and enjoyment of goals. Accordingly, this chapter first discusses basic motivational concepts that address the processes involvedinchoosingandpursuinggoals,andplacesgoalpursuitwithintheframeworkofthetheoryof current concerns. It integrates applicable neuroscientific findings that shed light on these processes. Each of these components of goal choice and pursuit cangoawry,leadingtoavarietyofdifficultiesthatbecomereflectedinanxiety,depression,alienation, interpersonal and occupational problems, substance abuse, suicide, and other forms of psychological disturbance.","['It examines how people choose goals and traces the effects of having a goal and of the way the goal pursuit ends, in goal attainment or relinquishment.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. The ability to generate and sustain an internal train of thought unrelated to external reality frees an agent from the constraints of only acting on immediate, environmentally triggered events. This hypothesis explains at least two features of the literature on internally guided thought. This explains why internal thought is routinely associated with a state of perceptual decoupling, reflected in both measured anticorrelations between the default mode network and sensory areas and the manner in which task unrelated thoughts compromise task performance. See above the work by #CITATION_TAG,, Spreng et al. [2010], and others.)","First, access to the top-down control system is a generally accepted prerequisite of conscious experience; this explains why activation of this system and default mode activity is often observed together during periods of internally guided thought. Second, because the top-down attentional control system has a limited capacity, internally and externally driven streams can come into conflict, with the result that perceptual information must be denied attentional amplification if the internal stream is to be maintained.","['The current paper proposes that such thought is produced through cooperation between autobiographical information provided by the default mode network and a frontal-parietal control network which helps sustain and buffer internal trains of thought against disruption by the external world.', 'This paper offers a hypothesis that should help to constrain and guide interpretations, investigations, and analyses of the neural processes involved in internally driven cognition.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Previous research has found that subjects rate words that are closely related to their current concerns as affectively more arousing than other words. Additional data on the relation of emotional arousal to goal cues were obtained with skin conductance responses (#CITATION_TAG).","In the first experiment, subjects listened to audiotaped sequences of three-word clusters associated with previously measured own current concerns or with concerns of others.","['This investigation inquires whether a similar relationship occurs when arousal is measured electrodermally, and whether nonspecific (spontaneous) electrodermal activity is associated with self-generated thoughts about current concerns.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. During the many idle moments that comprise daily life, the human brain increases its activity across a set of midline and lateral cortical brain regions known as the ""default network."" Despite the robustness with which the brain defaults to this pattern of activity, surprisingly little is known about the network's precise anatomical organization and adaptive functions. Indeed, it appears that the brain's default-mode network provides the substrate for mind-wandering (e.g., Mason et al., 2007; Christoff et al., 2009; #CITATION_TAG; Stawarczyk et al., 2011b), a network of several ""hubs"" and ""subsystems"" (Andrews-Hanna, 2012) that probably constitutes a majority of the brain's energy consumption (Raichle, 2009).",,"['To provide insight into these questions, this article synthesizes recent literature from structural and functional imaging with a growing behavioral literature on mind wandering.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Originality: This is the first study to examine the effects of flow consciousness on consumer behaviour after an impulse purchase. In particular, research has not analysed the effects that flow consciousness has on negative feelings experienced after the impulse purchase of a product. Flow states lead to increased impulse buying, and if consumers are made aware that they were in a flow state, it may reduce any regret they feel after the purchase #CITATION_TAG asked participants in a laboratory to signal with a key press every time their mind shifted to a new topic, which happened on average about 5 or 6 s apart.","Methodology: The study applied a mixed methodology. First, the authors conducted two qualitative studies (focus groups) to establish the relationships between flow, flow consciousness, and regret. Second, the authors conducted a quantitative study using data collected through an online questionnaire. Participants were asked to recall a recent shopping experience. To conduct confirmatory factor analysis, the authors gathered data from 304 consumers who had searched for, and purchased, a product on Amazon (www.amazon.com). Structural equation modelling, based on covariance, was used to test the hypotheses.","[""Purpose: This research aims to identify whether subsequent consciousness of having been in a flow state - that is, flow consciousness - regarding an earlier impulse purchase affects consumers' post-purchase behaviours, specifically their feelings of consumer regret.""]"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. There is evidence that inducing negative moods increases mindwandering, perhaps because it potentiates personal concerns (#CITATION_TAG).","Positive, neutral, and negative moods were induced in participants prior to them completing a sustained attention task. Mind wandering was measured by using the frequencies of both behavioral lapses and retrospective indices of subjective experience. Relative to a positive mood, induction of a negative mood led participants to make more lapses, report a greater frequency of task irrelevant thoughts, and become less inclined to reengage attentional resources following a lapse.",['This study examined the effect of mood states on mind wandering.']
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. When the mind wanders, conscious thoughts come to mind that are only loosely related to the task being performed. This phenomenon produces tension within the cognitive sciences because the interfering nature of these thoughts is at odds with the assumption that such processes are functional in daily life. Whether mind wandering always occurs following a control failure, it is always a conscious reportable experience and so is globally available to the system. Such global availability suggests that mind wandering does indeed demand resources, in particular access to a global workspace that supports conscious experience. Although the control-failure view explains the transient occurrence of mind wandering during demanding tasks, the global availability hypothesis is consistent with all mind wandering, regardless of context; it is implied by many features of the argument proposed by McVay and Kane (2010). This debate has been most recently summarized by Smallwood (2013a), who has also suggested a resolution among four main viewpoints: the goal theory of current-concerns (e.g., Klinger, 1971 Klinger,, 1975 Klinger,, 1977 Klinger,, 2009 Klinger and Cox, 2011; and the sections above), decoupling from perception (e.g., Smallwood, 2011 Smallwood,, 2013a, executive control failure (e.g., McVay and Kane, 2009, 2010, 2012a, and metaawareness (#CITATION_TAG: becoming conscious of one's mind-wandering, a self-regulatory process).",,"['In their comment, McVay and Kane (2010) suggested that failures in executive control can create the conditions that favor mind wandering--a control-failure hypothesis that questions whether mind wandering consumes resources.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. These negative women\u27s stereotypes continue to persist in dominant popular culture, and this doublebind is overcome only by the impossible perfection of vampirism. These constructions are particularly dangerous in Young Adult literature and particularly inspirational in fanfiction There is also an extended version of this theory (#CITATION_TAG) that adds an emotional component.","Through close-textual analysis of The Twilight Saga, I demonstrate how the monstrous-feminine frames the hysterical teenage body, hypersexuality, and eternal motherhood as simultaneously unacceptable and unavoidable. The monstrous-feminine invites constructions of teenage bodies as unstable and unreliable, women\u27s sexuality as dangerous and impure, and motherhood as a requirement for a complete identity.","['ABSTRACT This dissertation argues that Bella Swan is a representation of Barbara Creed\\u27s monstrous-feminine which serves to reinforce ideologies that insist women are abject, inherently dangerous to men, and threatening to a patriarchal status quo.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. The semantic processing of language and action has been linked to the N400 component of the event-related potential (ERP). EEG evidence with eventrelated potentials indicates that infants as young as nine months react with N400 deflections (negative deflections after 400 ms poststimulus) when sequences they observe end unexpectedly (#CITATION_TAG).",The sequential nature of action ensures that an individual can anticipate the conclusion of an observed action via the use of semantic rules. The authors developed an ERP paradigm in which infants and adults observed simple sequences of actions. Adults and infants at 9 months and 7 months were assessed via the same neural mechanisms-the N400 component and analysis of the theta frequency.,['This study suggests that infants at 9 months anticipate goals and use similar cognitive mechanisms to adults in this task.']
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Recent blood oxygenation level dependent functional MRI (BOLD fMRI) studies of the human brain have shown that in the absence of external stimuli, activity persists in the form of distinct patterns of temporally correlated signal fluctuations. Raichle (2009) argues that the default-mode network is not coextensive with conscious mind-wandering, citing the continuation of the network's activity into lighter states of anesthesia and Stages 1 and 2 of sleep (#CITATION_TAG), when dreaming is most frequent, and the demonstration (Christoff et al., 2009) of executive-network elements during resting states.","For this purpose, we performed BOLD fMRI on normal subjects during varying levels of consciousness, from resting wakefulness to light (non-slow wave) sleep. Depth of sleep was determined based on concurrently acquired EEG data.","['In this work, we investigated the spontaneous BOLD signal fluctuations during states of reduced consciousness such as drowsiness and sleep.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. SummaryPrevious research has demonstrated that the emotional properties of words and their imaginability affect their recallability and that verbal material is recalled better when it is related to subjects' current concerns. Regarding the association of emotional arousal with goal-related cues, #CITATION_TAG computed 85 participants' intraindividual correlations between two kinds of their reactions to 40 words: the word's emotional ""arousal potential"" for them (""the strength of the subject's emotional reaction to the content of the word"") and ""the extent to which the word has to do with the subject's important concerns, problems, worries, or goals that currently preoccupy the subject."" The two ratings were obtained in two different phases of the experiment, separated by a distractor task.","Forty different words were presented visually under one of six orienting conditions that varied according to what the subject was asked to rate: their length, pronounceability, concreteness, defineability, the strength of emotion elicited by the word, and the relation of the word to personal concerns. Subjects were then asked to write as many words as they could recall.",['This study investigates the extent to which this effect of emotion on recall varies as a function of cognitively controllable inference processes and examines the relation of the effects of emotion to those of imaginability and concern-relatedness.']
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Interestingly, #CITATION_TAG have worked out an intervention, called the Alcohol Attention Control Treatment Program (AACTP), for reducing excessive drinkers' bias toward processing alcohol cues.","Participants were social drinkers (N=40), hazardous drinkers (N=89), and harmful drinkers (N=92). Paper-and-pencil measures were used to collect information about participants' socio-demographic characteristics, health status, motivational structure, drinking-related locus of control and situational self-confidence, readiness to change, affect, and alcohol consumption. Computerized classic, alcohol- and concerns-Stroop tests were administered. All participants were tested individually, with the order of tests counterbalanced across participants.","['The aims of the research were to (a) compare the alcohol attentional bias (AAB) of social, hazardous, and harmful drinkers and (b) assess the effects of alcohol attention-control training on the AAB and alcohol consumption of hazardous and harmful drinkers.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Affect is basic to many if not all psychological phenomena. Over the past century, 6 distinct types of relations have been suggested or implicitly presupposed in the literature. This casts doubt on the existence of a static, lawful relation between valence and arousal. The malleability and individual differences found in the structure of affect must be taken into account when studying affect and its role in other psychological phenomena. From at least Wilhelm Wundt a century ago to Lisa Feldman Barrett (e.g., #CITATION_TAG), arousal (sometimes called activation), and hedonic valence (i.e., quality of the emotional response being aroused) have been two fundamental dimensions of emotion or affect.","We critically review the available evidence for each proposal and argue that the evidence does not provide a conclusive answer. Next, we use statistical modeling to verify the different proposals in 8 data sets (with Ns ranging from 80 to 1,417) where participants reported their affective experiences in response to experimental stimuli in laboratory settings or as momentary or remembered in natural settings.",['This article examines 2 of the most fundamental properties of affective experience--valence and arousal--asking how they are related to each other on a moment to moment basis.']
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. People's minds sometimes wander from ongoing activities. Although these experiences can be pleasant and useful, they are often unintentional and precipitate mistakes. We describe research that associates normal variation in working memory capacity (WMC)--a cognitive ability that broadly predicts intellectual capabilities and accomplishments--with off-task thinking. McVay and Kane's conceptualization (e.g., Kane and McVay, 2012; #CITATION_TAG), on the other hand, features proactive executive control that keeps people focused on their goal pursuits, from which mind-wandering distracts.",,"['In this article, we adopt an individual-differences perspective in considering unwanted mind wandering as an indicator of both momentary failures of and enduring deficiencies in executive-control functions.']"
"A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. There are large individual differences in the amount of time that people desire to spend in imaginative daydreaming (Singer and Bonanno, 1990; Singer, 1966 Singer,, 1975 #CITATION_TAG), some valuing it as a resource for self-amusement and stimulation and others eager to reduce or eliminate it but unable to attain a daydream-free state.","Our sample consisted of 75 female and 15 male participants, ranging in age from 18 to 63 who responded to online announcements. Participants completed a 14-question emailed survey requesting descriptions of their fantasy habits and causes of potential distress regarding fantasy.","['The experiences of 90 individuals who self-identify as ""excessive"" or ""maladaptive"" fantasizers are summarized in this report.']"
"The evolution of black-hole binaries in vacuum spacetimes constitutes the two-body problem in general relativity. The solution of this problem in the framework of the Einstein field equations is a substantially more complex exercise than that of the dynamics of two point masses in Newtonian gravity, but it also presents us with a wealth of new exciting physics. This is an English translation of the Italian version of an encyclopedia chapter that appeared in the Italian Encyclopedia of the Physical Sciences, edited by Bruno Bertotti (1994). Under the assumption of cosmic censorship and certain energy conditions, it can furthermore be shown that if a hypersurface Σ t contains an AH, it will coincide or lie within the event horizon's cross section with Σ t [110, #CITATION_TAG].",Following requests from colleagues we have decided to make it available to a more general readership.,"['We present the motivation for constructing General Relativity, provide a short discussion of tensor algebra, and follow the set up of Einstein equations.']"
"This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. More than 100 countries have adopted a global warming limit of 2 degC or below (relative to pre-industrial levels) as a guiding principle for mitigation efforts to reduce climate change risks, impacts and damages. However, the greenhouse gas (GHG) emissions corresponding to a specified maximum warming are poorly known owing to uncertainties in the carbon cycle and the climate response. Recent G8 Communiques envisage halved global GHG emissions by 2050, for which we estimate a 12-45% probability of exceeding 2 degC--assuming 1990 as emission base year and a range of published climate sensitivity distributions. The stringency of the lower target is consistent with concurrent interpretations of the goal to keep global mean warming below 2°C (#CITATION_TAG).",,"['Here we provide a comprehensive probabilistic analysis aimed at quantifying GHG emission budgets for the 2000-50 period that would limit warming throughout the twenty-first century to below 2 degC, based on a combination of published distributions of climate system properties and observational constraints.']"
"This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. We take the approach that it is most useful to report all available model results, since any supposed technical or economic infeasibility can be assessed best in a comparison across model results (see also #CITATION_TAG).",We suggest a procedure that addresses this partiality.,['This paper warns against the risk of underestimating the costs--and the uncertainty about the costs--of achieving stringent stabilization targets.']
"This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. The implications of alternative near-term emissions targets for long-term climate goals are investigated by a concurrent study (#CITATION_TAG).",,"['This paper provides an overview of the AMPERE modeling comparison project with focus on the implications of near-term policies for the costs and attainability of long-term climate objectives.', 'Nine modeling teams participated in the project to explore the consequences of global emissions following the proposed policy stringency of the national pledges from the Copenhagen Accord and Cancun Agreements to 2030.']"
"This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. Decarbonization will lead to improved air quality, thereby reducing energy-related health impacts worldwide. At the same time, low-carbon technologies and energy-efficiency improvements can help to further the energy security goals of individual countries and regions by promoting a more dependable, resilient, and diversified energy portfolio. The cost savings of these climate policy synergies are potentially enormous Such measures (partially) describe the direct costs of mitigation, but neither include climate benefits nor as adverse side effects and co-benefits of climate policy (#CITATION_TAG; Riahi et al. 2012).","We explain how the common practice of narrowly focusing on singular issues ignores potentially enormous synergies, quite often leading to the implementation of short-sighted solutions that may have unnecessarily costly, long-term consequences. Our analysis of a large ensemble of alternate energy-climate futures, developed using MESSAGE, an integrated assessment model with considerable technological detail of the global energy system, shows that climate change policy offers a strategic entry point along the path to energy sustainability in several dimensions.","['This talk focuses on three key energy sustainability objectives: energy security improvement, climate change mitigation, and the reduction of air pollution and its human health impacts.']"
"This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. The EMF 22 international scenarios engaged ten of the world's leading integrated assessment (IA) models to focus on the combined implications of three factors integral to international climate negotiations: (1) the long-term climate-related target, expressed in this study in terms of the CO2-equivalent (CO2-e) concentration associated with the GHGs  regulated under the Kyoto Protocol, (2) whether or not this target can be temporarily exceeded prior to 2100 (""overshoot"") allowing for greater near-term flexibility, and  (3) the nature of international participation in emissions mitigation. Key model comparison studies for instance include the previous EMF climate-change-oriented studies like the EMF19 study on carbon constraints and advanced energy technologies (Weyant 2004), the EMF21 study on non-CO 2 Kyoto gas mitigation (Weyant et al. 2006), and the EMF22 study on climate control scenarios including phased participation (#CITATION_TAG).","The EMF 22 international scenarios are based on combinations of these dimensions, embodied in ten specific climate-action cases that all modeling groups in the study attempted to represent","['This paper presents an overview of the study design for, and the results of, the EMF 22 international scenarios.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Data mining is concerned with the extraction of novel and useful knowledge from large amounts of data. Simpler implementations (e.g., C5.0) limit each branch to value ranges from a single attribute, making this a linear classifier with a further restriction that each condition is an axis-parallel hyperplane (#CITATION_TAG).","Topics include data preparation and feature selection, association rules, classification, clustering,evaluation and validation, scalability, mining of spatial/text/sequence/graph/time-series etc data, privacy, data mining applications, and other topics of interest.","['This course introduces and studies the fundamental concepts, issues, tasks and techniques of data mining.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. For example, models predicting academic performance that include factors of motivation (e.g., self-efficacy, goal setting) with cognitive ability yield a lower error variance than models of cognitive ability alone, particularly at tertiary level (reviewed in Boekaerts, 2001; #CITATION_TAG).","On the basis of educational persistence and motivational theory models, the PSFs were categorized into 9 broad constructs: achievement motivation, academic goals, institutional commitment, perceived social support, social involvement, academic self-efficacy, general self-concept, academic-related skills, and contextual influences. Two college outcomes were targeted: performance (cumulative grade point average; GPA) and persistence (retention).",['This study examines the relationship between psychosocial and study skill factors (PSFs) and college outcomes by meta-analyzing 109 studies.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. It remains an active research topic (Buckingham Shum & Deakin Crick, 2012; #CITATION_TAG; Komarraju, Ramsey & Rinella, 2013), indicating the inherent difficulty in both measurement of learning (Knight, Buckinham Shum, & Littleton, 2013; Tempelaar et al., 2013), and modelling the learning process, particularly in tertiary education (Pardos et al., 2011).",,['The study investigated the association and relative influence of cognitive/motivational and demographic factors on final degree grade point average (GPA) in a single undergraduate cohort.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Such achievement goals fall into two categories: performance goals, where an individual is looking for favourable feedback, and learning goals, where an individual desires to increase competency (#CITATION_TAG; Dweck, 1986; Dweck & Leggett, 1988; Eccles & Wigfield, 2002; Eppler & Harju, 1997).",,['The purpose of this review is to document the directions and recent progress in our understanding of the motivational dynamics of school achievement.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Motivational processes influence a child's acquisition, transfer, and use of knowledge and skills, yet educationally relevant conceptions of motivation have been elusive. It has long been known that factors other than ability influence whether children seek or avoid challenges, whether they persist or withdraw in the face of difficulty, and whether they use and develop their skills effectively. However, the components and bases of adaptive motivational patterns have been poorly understood. As a resuit, commonsense analyses have been limited and have not provided a basis for effective practices. Indeed, many ""commonsense"" beliefs have been called into question or seriously qualified by recent research--for example, the belief that large amounts of praise and success will establish, maintain, or reinstate adaptive patterns, or that ""brighter"" children have more adaptive patterns and thus are more likely to choose personally challenging tasks or to persist in the face of difficulty. In the past 10 to 15 years a dramatic change has taken place in the study of motivation. During this time, the emphasis has shifted to a social-cognitive approachwaway from external contingencies, on the one hand, and global, internal states on the other. It has shifted to an emphasis on cognitive mediators, that is, to how children construe the situation, interpret events in the situation, and process information about the situation. Although external contingencies and internal affective states are by no means ignored, they are seen as part of a process whose workings are best penetrated by focusing on organizing cognitive variables. Adaptive and Maladaptive Motivational Patterns The study of motivation deals with the causes of goaloriented activity (Atkinson, 1964; Beck, 1983; Dollard & Miller, 1950; Hull, 1943; Veroff, 1969). l Adaptive motivational patterns are those that promote the establishment, maintenance, and attainment of personally challenging and personally valued achievement goals. Maladaptive patterns, then, are associated with a failure to establish reasonable, valued goals, to maintain effective striving toward those goals, or, ultimately, to attain valued goals that are potentially within one's reach. Research has clearly documented adaptive and maladaptive patterns of achievement behavior. The adaptive (""mastery-oriented"") pattern is characterized by challenge seeking and high, effective persistence in the face of obstacles. Children displaying this pattern tend to evidence negative affect (such as anxiety) and negative self-cogniCorrespondence concerning this article should be addressed to Carol S. Dweck, Department of Psychology, University of Illinois, 603 E. Daniel, Champaign, IL 61820. l The word performance will be used in several ways, not only in connection with performance goals. 10, 1040-1048 Table 1 Achievement Goals and Achievement Behavior Confidence in Theory of intelligence Goal orientation present ability Behavior pattern Entity theory (Intelligence is fixed) Incremental theory (Intelligence is malleable) > Performance goal (Goal is to gain positive judgments/avoid negative judgments of competence) > Learning goal (Goal is to increase competence) If high ---> Mastery-oriented Seek challenge but High persistence If low ~ Helpless Avoid challenge Low persistence If high > Mastery-oriented ioOr ~ ' Seek challenge (that fosters learning) High persistence tions when they confront obstacles (e.g., Ames, 1984; C. Diener & Dweck, 1978, 1980; Dweck & Reppucci, 1973; Nicholls, 1975). Although children displaying the different patterns do not differ in intellectual ability, these patterns can have profound effects on cognitive performance. In experiments conducted in both laboratory and classroom settings, it has been shown that children with the maladaptive pattern are seriously hampered in the acquisition and display of cognitive skills when they meet obstacles. Children with the adaptive pattern, by contrast, seem undaunted or even seem to have their performance facilitated by the increased challenge. If not ability, then what are the bases of these patterns? Most recently, research has suggested that children's goals in achievement situations differentially foster the two patterns. That is, achievement situations afford a choice of goals, and the one the child preferentially adopts predicts the achievement pattern that child will display. How do they shape task choice and task pursuit to facilitate or impede cognitive performance? Although relatively few studies as yet have explicitly induced and compared (or measured and compared) learning versus performance goals (see M. Bandura & Dweck, 1985; Elliott & Dweck, 1985; FarreU & Dweck, 1985; Leggett, 1985, 1986), many have manipulated the salience and value of performance goals, and hence the relative value of the two types of goals. This has been done, for example, by instituting a competitive versus individual reward structure (e.g., Ames, 1984; Ames, Ames, & Felker, 1977), by varying the alleged diagnosticity of the task vis vis important abilities (e.g., Nicholls, 1975), by introducing an audience or evaluator versus allowing the individual to perform privately or focusing his or her attention on the task (e.g., Brockner & Hulton, 1978; Carver & Scheier, 1981; E. Diener & SruU, 1979), and by presenting the task with ""test"" instructions versus ""game"" or neutral instructions (e.g., Entin & Raynor, 1973; Lekarczyk & Hill, 1969; McCoy, 1965; Sarason, 1972). Goals and Task Choice Appropriately challenging tasks are often the ones that are best for utilizing and increasing one's abilities. Recent research has shown that performance goals work against the pursuit of challenge by requiring that children's perceptions of their ability be high (and remain high) before the children will desire a challenging task (M. Bandura & D Such achievement goals fall into two categories: performance goals, where an individual is looking for favourable feedback, and learning goals, where an individual desires to increase competency (Covington, 2000; #CITATION_TAG; Dweck & Leggett, 1988; Eccles & Wigfield, 2002; Eppler & Harju, 1997).","Using recent research within the social-cognitive framework, Dweck describes adaptive and maladaptive motivational patterns and presents a research-based model of motivational processes. This model shows how the particular goals children pursue on cognitive tasks shape their reactions to success and failure and influence the quality of their cognitive performance. Specifically, the social-cognitive approach has allowed us to (a) characterize adaptive and maladaptive patterns, (b) explain them in terms of specific underlying processes, and thus (c) begin to provide a rigorous conceptual and empirical basis for intervention and practice. In contrast, the maladaptive (""helpless"") pattern is characterized by challenge avoidance and low persistence in the face of difficulty. It will also be used to refer to the child's task activity (performance of a task) and to the product of that activity (level of performance). Table 1 summarizes the conceptualization that is emerging from the research. The goals then appear to set up the different behavior patterns. 2 Learning and Performance Goals Contrasted How and why do the different goals foster the different patterns? In contrast, with learning goals the choice and pursuit processes involve a focus on progress and mastery through 2 See M. Bandura and Dweck (1985), Dweck and Elliott (1983), and Leggett (1985) for a more extensive treatment of children's theories of intelligence.","['Dweck argues that this approach has important implications for practice and the design of interventions to change maladaptive motivational processes.', 'She presents a compelling proposal for explaining motivational influences on gender differences in mathematics achievement and observes that empirically based interventions may prevent current achievement discrepancies.--The Editors Most research on effective learning and performance of cognitive tasks analyzes the particular cognitive skills required to succeed at those tasks.', 'In contrast, the focus here is on motivational processes that affect success on cognitive tasks.', 'That is, the focus is on psychological factors, other than ability, that determine how effectively the individual acquires and uses skills.', 'The present article will focus on achievement goals and their allied behavior patterns.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. For example, the number of times participants selected forgiveness solutions as problem-solving strategies on each of the WFS subscales was too low to be useful. It has been argued that this is one factor represented as a continuum from an intrinsic, behaviour-oriented state, to an extrinsic, goaloriented state (#CITATION_TAG; Atherton, 2009; Entwhistle, 2005).","A sample of 262 undergraduate students, ranging in age from 18 to 25 years and including 122 males and 140 females, completed the Motivational Style Profile (MSP), the Enright Forgiveness Inventory (EFI), the Willingness to Forgive Scale (WFS), and a Time-Factor Questionnaire (TFQ) that was designed for this study. The forgiveness ratings of the participants who achieved high-dominance in arousal-avoidance were compared to those of the participants who achieved high-dominance in arousal-seeking. Numerous conceptual and methodological factors that might account for the findings were identified. With respect to the methodological issues pertaining to the MSP, the high inter-correlations between the MSP subscales rendered it difficult to identify distinct factors. There were also methodological concerns related to the measures that were used to assess forgiveness.",['The purpose of this study was to examine the construct of forgiveness within the context of a reversal theory framework.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Self-regulation of cognition and behavior is an important aspect of student learning and academic performance in the classroom context (Corno & Mandinach, 1983; Corno & Rohrkemper, 1985). There are a variety of definitions of selfregulated learning, but three components seem especially important for classroom performance. There are two strands of expectancy motivation (Eccles & Wigfield, 2002; #CITATION_TAG):","A correlational study examined relationships between motivational orientation, self-regulated learning, and classroom academic performance for 173 seventh graders from eight science and seven English classes. A self-report measure of student self-efficacy, intrinsic value, test anxiety, self-regulation, and use of learning strategies was administered, and performance data were obtained from work on classroom assignments. First, self-regulated learning includes students' metacognitive strategies for planning, monitoring, and modifying their cognition (e.g., Brown, Bransford, Campione, & Ferrara, 1983; Corno, 1986; Zim",['The implications of individual differences in motivational orientation for cognitive engagement and self-regulation in the classroom are discussed.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. The importance of accurate estimation of student's future performance is essential in order to provide the student with adequate assistance in the learning process. If this assumption is invalid, conditional probabilities between attributes can be modelled as a Bayesian Network (#CITATION_TAG).",We presented empirical experiments on the prediction of performance with a data set of high school students containing 8 attributes.,"['To this end, this research aimed at investigating the use of Bayesian networks for predicting performance of a student, based on values of some identified attributes.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. #CITATION_TAG found the importance of learning approach varied with assessment type.",,"['This paper examines the conceptual usefulness of distinguishing between two aspects of learning goals, namely direction and effort.', ""This research builds upon and integrates three bodies of research related to self-regulation of learning, Boekaerts' work on the significance of cognitive, affective and motivational appraisals of study, Kuhl's notion of action control, and our previous work on qualitative differences in students' learning goals.""]"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Evidence is, however, mixed concerning the role of personality traits in predicting such success. Factors affecting academic performance have been the focus of research for many years (#CITATION_TAG; Lent, Brown, & Hacket, 1994; Moran & Crowley, 1979).",Less than one fifth of Final Grade variance was explained by all the individual difference variables in combination.,"[""The current study attempted to overcome various methodological limitations associated with many previous studies to examine the potency of the traits of the `five factor model of personality' in predicting academic success up to 3 years later, both directly and when controlling for intelligence and `application' (used as a proxy for motivation).""]"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. An approach to performance-based assessments that embeds assessments in digital games in order to measure how students are progressing toward targeted goals.To succeed in today's interconnected and complex world, workers need to be able to think systemically, creatively, and critically. Equipping K-16 students with these twenty-first-century competencies requires new thinking not only about what should be taught in school but also about how to develop valid assessments to measure and support these competencies. Many researchers have cited conscientiousness as compensating for lower cognitive intelligence (see Chamorro-Premuzic & Furnham, 2004, and it is a consistent predictor of academic performance across assessment type (Allick & Realo, 1997; Kappe & van der Flier, 2010; #CITATION_TAG).","Embedding assessments within games provides a way to monitor players' progress toward targeted competencies and to use that information to support learning.Shute and Ventura discuss problems with such traditional assessment methods as multiple-choice questions, review evidence relating to digital games and learning, and illustrate the stealth-assessment approach with a set of assessments they are developing and embedding in the digital game Newton's Playground.","['In Stealth Assessment, Valerie Shute and Matthew Ventura investigate an approach that embeds performance-based assessments in digital games.', ""They argue that using well-designed games as vehicles to assess and support learning will help combat students' growing disengagement from school, provide dynamic and ongoing measures of learning processes and outcomes, and offer students opportunities to apply such complex competencies as creativity, problem solving, persistence, and collaboration."", 'These stealth assessments are intended to measure levels of creativity, persistence, and conceptual understanding of Newtonian physics during game play.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Many data mining algorithms have implementations adapted for this big-data environment, for example, Decision Tree (Ben-Haim & Tom-Tov, 2010), k-NN (Liang et al., 2009), Neural Networks (Gu et al., 2013), SVM and regression (#CITATION_TAG), and supporting tools are available (Prekopcsák et al., 2011), facilitating quick analysis and feedback (Siemens & Long, 2011).",We provide rigorous analysis of the correctness and convergence of the algorithm.,"['We propose a nontrivial strategy to parallelize a series of data mining and machine learning problems, including 1-class and 2-class support vector machines, nonnegative least square problems, and $ ell_1$ regularized regression (LASSO) problems.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Bekele and Menzel (2005), Conati et al. (2002), Jonsson et al. (2005) and #CITATION_TAG argue that Bayesian networks are particularly suited to student models because of the inherent uncertainty in interpreting student behaviour, and the incompleteness of any dataset attempting to capture all factors relevant to classifying students.","A normative ITS uses a Bayesian network for long-term student modelling and decision theory to select the next tutorial action. Because normative theories are a general framework for rational behaviour, they can be used to both define and apply learning theories in a rational, and therefore optimal, way. This contrasts to the more traditional approach of using an ad-hoc scheme to implement the learning theory. A key step of the methodology is the induction and the continual adaptation of the Bayesian network student model from student performance data, a step that is distinct from other recent Bayesian net approaches in which the network structure and probabilities are either chosen beforehand by an expert, or by efficiency considerations.",['We propose and demonstrate a methodology for building tractable normative intelligent tutoring systems (ITSs).']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Evidence-centered design (ECD) is a comprehensive framework for describing the conceptual, computational and inferential elements of educational assessment. At first blush, ECD and educational data mining (EDM) might seem in conflict: structuring situations to evoke particular kinds of evidence, versus discovering meaningful patterns in available data. Learning is a latent variable, typically measured as academic performance in assessment work and examinations (#CITATION_TAG).",We first introduce ECD and relate its elements to the broad range of digital inputs relevant to modern assessment. We then discuss the relation between EDM and psychometric activities in educational assessment.,['It emphasizes the importance of articulating inferences one wants to make and the evidence needed to support those inferences.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. open access articlePupil absenteeism remains a significant problem for schools across the globe with its negative impacts on overall pupil performance being well-documented. Whilst all schools continue to emphasize good attendance, some schools still find it difficult to reach the required average attendance, which in the UK is 96\%. Similarly, #CITATION_TAG achieved high accuracy using a decision tree to predict exam performance based on a single student vector aggregated from their behaviour on an ITS.",,"['A novel approach is proposed to help schools improve attendance that leverages the market target model, which is built on association rule mining and probability theory, to target sessions that are most impactful to overall poor attendance.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. E-Learning is embedded in learning and, without an understanding of what learning encompasses, it can be difficult for academics to develop into good teachers. It is suggested that, although this may appear to be a simple aim, it is not necessarily understood or applied by university academics in their teaching. Academics may have a 'philosophy of teaching', but in many cases even this may not be consciously held or successfully implemented. Openness, conscientiousness, and intrinsic motivation are correlated with a deep learning approach, while neuroticism, agreeableness, and extrinsic motivation are associated with a shallow learning approach (Busato et al., 1999; Duff et al., 2004; #CITATION_TAG).",One inference is that university teachers need to develop a theory of learning and teaching.,"['This paper suggests that for academics to be good teachers, especially in the context of e-Learning, they need to understand learning.', 'This is especially important with the associated changes in higher education as we move towards the knowledge society.', ""A program for promoting conceptual change in academics' approaches to teaching is outlined""]"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Demographic data, such as age and gender, have been cited as significant (#CITATION_TAG), as are data gathered from learner activity on online learning systems (Bayer et al., 2012; López et al., 2012).","Participants (N= 153, 105 = male & 48= female) completed creativity test. Cumulative grade point average (CGPA) was used to select the participants. A multiple regression analysis revealed creativity, age and gender explained 0.143 of the variance in academic achievement. Multiple regression analysis showed interaction effects between creativity, age and gender as low predictors of academic achievement.","['This study examined creativity, age and gender as predictors of academic achievement.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. INTRODUCTION The manner in which students are admitted for third level education and in particular the points system have become matters of increasing public controversy in recent years. While the subject is understandably emotive, a sad feature of many of the arguments presented to date has been the very limited factual support. A student who passes this examination has certainly been fully accepted by the university in accordance with its own criteria for assessment and any subsequent failure to graduate is indicative of faults in this assessment rather than any pre-university assessments. Such analyses cannot, of course, provide definitive answers to many important questions concerning entry to third level, as many of these are essentially political in character. SOME RELATED WORK 2.1 The literature on the predictive value of school leaving, scholastic aptitude and related tests for subsequent university performance is immense. Virtually all of this work has been undertaken outside Ireland and the only detailed research published in the Irish context has been Nevin (1974). Factors affecting academic performance have been the focus of research for many years (Farsides & Woodfield, 2003; Lent, Brown, & Hacket, 1994; #CITATION_TAG).",The restriction to first year has also the important advantage of allowing the analysis of very recent university and school examinations to be presented. Our review of such work will be confined to what we consider most relevant and useful in the Irish context.,"['Our purpose in this paper is threefold: firstly to introduce some research which has and is being carried out at University College Cork on the effectiveness of performance in the Leaving Certificate examination and in such alternatives as scholastic aptitude and personality tests, as measures of subsequent university performance; secondly, to suggest some statistical methodology for the analysis of data in this area; and thirdly to offer some personal comments on aspects of the overall problem in an effort to establish a fair and hopefully constructive viewpoint and to redress some misconceptions.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. The general conception of intelligence as developing expertise is described. #CITATION_TAG asserts that intelligence tests measure a developing expertise rather than a stable attribute, and the typically high correlation between intelligence scores and academic performance is because they measure the same skill set rather than developing a causal relationship.","Then research examples are given that, in conjunction, seem odd under traditional interpretations of abilities but that make sense as a whole in the context of the developing-expertise model.",['This essay describes how intelligence can be viewed as developing expertise.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Self-control is a burgeoning research topic within sport and motivational psychology. Ryan and Deci ( 2000) define motivation simply as being ""moved to do something."" Defining how learners are motivated to behave in a certain way, and more specifically to learn, is more complex, and is characterized by a range of complementary theories that aim to explain both the level of individual motivation and the nature of the motivation (#CITATION_TAG).","Following efforts to define and contextualize self-control, characteristics of self-control are considered that have important implications for sport performance. We describe and evaluate various theoretical perspectives on self-control, including limited resources, shifting priorities, and opportunity-costs. Moreover, we integrate self-control ideas with descriptions of motivational phenomena to derive novel hypotheses concerning how self-control can be optimized during sport performance. We explain how minimizing desire-goal conflicts by fusing self-control processes and performance goals can delay aversive consequences of self-control that may impede performance.","['The research described includes sport-specific research but also studies that focus on general motivational principles that look beyond sport-specific phenomena.', 'We propose that attentional, rather than limited resource, explanations of self-control have more value for athletic performance.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. While the Big Five concept is empirical rather than a theory of personality (#CITATION_TAG), good reliability and consistency has been reported (de Raad & Schouwenburg, 1996; John et al., 2008).","The two Brazilian scales were administered to 554 participants aged 16-69 years (M = 30.6, SD = 8.6). The measurement model of each instrument was tested using confirmatory factor analysis.","['Abstract The objective of this study was to obtain evidence of the convergent and factor validity of the Reduced Scale of Big Five Personality Factors (ER5FP), with 20 items, and of the Reduced Inventory of Big Five Personality Factors (IGFP-5R), with 32 items.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Important factors include learning style (e.g., Bruinsma, 2004; Chamorro-Premuzic & Furnham, 2008; Diseth, 2011; #CITATION_TAG) and self-regulation (e.g., Nasiriyan et al., 2011; Ning & Downing, 2010).","The task involved a collaborative computer-based modeling task. In order to test the model, group measures of mastery-approach goal orientation, performance-avoidance goal orientation, self-efficacy, and achievement were employed. Students' cognitive processing was assessed using an online log-file measure.","['Purpose of the present study was to test a conceptual model of relations among achievement goal orientation, self-efficacy, cognitive processing, and achievement of students working within a particular collaborative task context.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Growing interest in data and analytics in education, teaching, and learning raises the priority for increased, high-quality research into the models, methods, technologies, and impact of analytics. Two research communities -- Educational Data Mining (EDM) and Learning Analytics and Knowledge (LAK) have developed separately to address this need. As a result, the application of data analytics to educational settings is emerging as an evolving and growing research discipline (Sachin & Vijay, 2012; Siemens & Baker, 2012), with the primary aim of exploring the value of such data in providing learning professionals, and students, with actionable information that could be used to enhance the learning environment (#CITATION_TAG; Chatti et al., 2012).",,"['This paper argues for increased and formal communication and collaboration between these communities in order to share research, methods, and tools for data mining and analysis in the service of developing both LAK and EDM fields.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Brady-Amoon and Fuertes (2011) attribute their insignificant correlation (r=0.16, n=271) to the fact that study participants included a more diverse group of students from a variety of ethnic backgrounds, thereby supporting the findings of #CITATION_TAG that the interaction between prior academic ability and GPA differs for students from different ethnic groups.",SAT/ACT scores and HSGPA were collected and used in various ways by participating institutions in the admissions process while situational judgment measures and biodata were collected for research purposes only during the first few weeks of the participating students' freshman year.,['This study was conducted to determine the validity of noncognitive and cognitive predictors of the performance of college students at the end of their 4th year in college.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Educational data mining is an emerging trend, concerned with developing techniques for exploring, and analyzing the huge data that come from the educational context. In recent years, Educational data mining has proven to be more successful at many of these educational statistics problems due to enormous computing power and data mining algorithms. As a result, the application of data analytics to educational settings is emerging as an evolving and growing research discipline (#CITATION_TAG; Siemens & Baker, 2012), with the primary aim of exploring the value of such data in providing learning professionals, and students, with actionable information that could be used to enhance the learning environment (Siemens, 2012; Chatti et al., 2012).","This paper describes how to apply the main data mining techniques such as prediction, classification, relationship mining, clustering, and social area networking to educational data.","['EDM is poised to leverage an enormous amount of research from data mining community and apply that research to educational problems in learning, cognition and assessment.', 'This paper surveys the history and applications of data mining techniques in the educational field.', 'The objective is to introduce data mining to traditional educational system, web-based educational system, intelligent tutoring system, and e-learning.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. In addition, including psychometric data in models of learning can provide useful feedback on the learning dispositions that assessment design rewards (#CITATION_TAG).","It consisted of a series of five papers: one conceptual paper on ECD, three applied case studies that use ECD and EDM tools, and one simulation study that relies on ECD for its design and EDM for its implementation.",['This special issue of JEDM was dedicated to bridging work done in the disciplines of educational and psychological assessment and educational data mining (EDM) via the assessment design and implementation framework of evidence-centered design (ECD).']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Despite all the research on student retention and success since the first conceptual mappings of student success e.g. Spady [12], there have not been equal impacts on the rates of both student success and retention. To realise the potential of learning analytics to impact on student retention and success, mega open distance learning (ODL) institutions face a number of challenges, paradoxes and opportunities. Students are not the only actors in their learning journeys and it would seem crucial that learning analytics also includes the impacts of all stakeholders on students' learning journeys in order to increase the success of students' learning. As such the notion of 'Thirdspace' as used by cultural, postmodern and identity theorists provide a useful heuristic to map the challenges and opportunities, but also the paradoxes of learning analytics and its potential impact on student success and retention. Although these two institutions share a number of characteristics, there are also some major and important differences between them. It is evident from current knowledge of the factors influencing academic performance, that such factors are interdependent (#CITATION_TAG).","We explore some of the shared challenges, paradoxes and opportunities learning analytics offer in the context of these two institutions.","[""For the purpose of this paper we critique a 'closed' view of learning analytics as focusing only on data produced by students' interactions with institutions of higher learning."", 'This paper explores some of these challenges, paradoxes and opportunities with reference to two mega ODL institutions namely the Open University in the UK (OU) and the University of South Africa (Unisa).']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Recently, there is an increasing interest in learning analytics in Technology-Enhanced Learning TEL. Generally, learning analytics deals with the development of methods that harness educational datasets to support the learning process. Learning analytics LA is a multi-disciplinary field involving machine learning, artificial intelligence, information retrieval, statistics and visualisation. We describe a reference model for LA based on four dimensions, namely data and environments what?, stakeholders who?, objectives why? As a result, the application of data analytics to educational settings is emerging as an evolving and growing research discipline (Sachin & Vijay, 2012; Siemens & Baker, 2012), with the primary aim of exploring the value of such data in providing learning professionals, and students, with actionable information that could be used to enhance the learning environment (Siemens, 2012; #CITATION_TAG).","LA is also a field in which several related areas of research in TEL converge. These include academic analytics, action analytics and educational data mining. We then review recent publications on LA and its related fields and map them to the four dimensions of the reference model. Furthermore, we identify various challenges and research opportunities in the area of LA in relation to each dimension.","['In this paper, we investigate the connections between LA and these related fields.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Five hundred and sixty-five first-year students completed a self-report questionnaire on three different occasions. Important factors include learning style (e.g., #CITATION_TAG; Chamorro-Premuzic & Furnham, 2008; Diseth, 2011; Sins et al., 2008) and self-regulation (e.g., Nasiriyan et al., 2011; Ning & Downing, 2010).",,"[""This study investigated the question of whether a student's expectancy, values and negative affect influenced their deep information processing approach and achievement at the end of the first and second academic year.""]"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Bootstrap aggregating or Bagging, introduced by Breiman (1996a), has been proved to be effective to improve on unstable forecast. Theoretical and empirical works using classification, regression trees, variable selection in linear and non-linear regression have shown that bagging can generate substantial prediction gain. However, most of the existing literature on bagging have been limited to the cross sectional circumstances with symmetric cost functions. Ensembles aggregate the predictions of a collection of classification models (#CITATION_TAG; Banfield et al., 2004).",We link quantile predictions to binary predictions in a unified framwork. Various bagging forecast combinations are used such as equal weighted and Bayesian Model Averaging (BMA) weighted combinations.,"['In this paper, we extend the application of bagging to time series settings with asymmetric cost functions, particularly for predicting signs and quantiles.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. In a longitudinal study on the causal dilemma between motivation and self-regulation, De #CITATION_TAG concluded that a learning goal orientation resulted in a deep learning approach, which in turn resulted in better self-regulation.","The participants were 110 freshmen from the engineering faculty at the Universite catholique de Louvain in Belgium, who were followed during the first three years of their university studies.","[""The aim of this study was to investigate the direction of the effect between goal orientation, self-regulation and learning strategies in order to understand the impact of these three constructs on students' achievement.""]"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. El objetivo del presente estudio fue evaluar las propiedades psicometricas del cuestionario de procesos de estudio revisado - 2 factores (CPE-R-2F) en estudiantes de ciencias de la salud en Cartagena, Colombia. Para determinar el numero de factores que explicaban el constructo se condujo analisis de factores (exploratorio). El analisis de factores confirmatorio determino la validez de constructo y el alfa de Cronbach la consistencia interna del instrumento. Learning style (deep or shallow) and selfregulated learning strategies are also relevant, and have been shown to mediate between other factors (such as factors of personality and factors of motivation) and academic performance (#CITATION_TAG; Entwhistle, 2005; Swanberg & Martinsen, 2010).",The number of factors that explained the construct was determined using exploratory factor analysis. R-SPQ-2F is a scale with acceptable internal consistency and two-factor structure with questionable construct validity.,"['Es recomendable seguir investigando sobre sus propiedades psicometricas en el futuro en otras poblaciones similares.The aim of this research was to determine the psychometric properties (construct validity and internal consistency) of The Revised Two-Factor Study Process Questionnaire (R-SPQ-2F) in health sciences students from Cartagena, Colombia.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Although numerous  studies have developed models to predict programming success, the models  struggled to achieve high accuracy in predicting the likely performance of  incoming students. Consequently, timely interventions can be put in place to prevent struggling  students from failing. This has not previously been possible. #CITATION_TAG found that adding self-efficacy and study hours improved model accuracy, but due to the small sample size (n=58) could not draw reliable conclusions from the findings.","Our approach overcomes this by providing a machine  learning technique, using a set of three significant factors, that can predict  whether students will be 'weak' or 'strong' programmers with approximately  80% accuracy after only three weeks of programming experience. The first contribution  is a longitudinal study identifying factors that influence introductory  programming success, investigating 25 factors at four different institutions. A  number of new instruments were developed by the author and a programming  self-esteem measure was shown to out-perform other previous comparable  comfort-level measures in predicting programming performance. The second contribution of the thesis is an analysis of the use of machine  learning (ML) algorithms to predict performance and is a first attempt to  investigate the effectiveness of a variety of ML algorithms to predict introductory  programming performance. Optimisations were carried out to investigate if prediction  accuracy could be further increased and an ensemble algorithm, StackingC,  was shown to improve prediction performance. The factors identified in this thesis and the associated machine learning  models provide a means to predict accurately programming performance  when students have only completed preliminary programming concepts.","['This thesis details a longitudinal study on factors that influence introductory  programming success and on the development of machine learning  models to predict incoming student performance.', 'This thesis makes three fundamental contributions.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. We describe an extraction of new features from both student data and behaviour data (or more precisely from social graph which we construct). Demographic data, such as age and gender, have been cited as significant (Naderi et al., 2009), as are data gathered from learner activity on online learning systems (#CITATION_TAG; López et al., 2012).",Then we introduce a novel method for learning classier for student failure prediction that employs cost-sensitive learning to lower the number of incorrectly classified unsuccessful students.,['This paper focuses on predicting drop-out and school failure when student data has been enriched with data derived from students social behaviour.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. In order to determine high school entrance level in the Netherlands, nowadays, much value is attached to the results of a national test of educational achievement (CITO), administered around age 12. Surprisingly, up until now, no attention has been paid to the etiology of individual differences in the results of this national test of educational achievement. It is notable that correlations between general intelligence and academic performance are stronger at secondary school level than in tertiary level education (#CITATION_TAG; Cassidy, 2011; Colom & Flores-Mendoza, 2007; Eysenck, 1994; Matarazzo & Goldstein, 1972).",,"['The aim of this study is to explore to what extent psychometric IQ and scholastic achievement, as assessed by the CITO high school entrance test, are correlated.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. In particular students in the motivated condition with a weak companion taught it many more times than in the other experimental conditions and in general worked harder. if they do not perform exactly as they are told It has been argued that this is one factor represented as a continuum from an intrinsic, behaviour-oriented state, to an extrinsic, goaloriented state (Apter, 1989; #CITATION_TAG; Entwhistle, 2005).",A LCS for Binary Boolean Algebra has been developed to explore the hypothesis that a learning companion with less expertise than the human student would be beneficial if the student taught it. The system implemented two companions with different expertise and two types of motivational conditions.,['This paper describes work carried out to explore the role of a learning companion as a teachable student of the human student.']
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. The unique focus is to facilitate, and inform, early engagement with students potentially at risk of failing (e.g., #CITATION_TAG; Lauría et al., 2013).","Course Signals was developed to allow instructors the opportunity to employ the power of learner analytics to provide real-time feedback to a student. Course Signals relies not only on grades to predict students' performance, but also demographic characteristics, past academic history, and students' effort as measured by interaction with Blackboard Vista, Purdue's learning management system. The system itself is explained in detail, along with retention and performance outcomes realized since its implementation. In addition, faculty and student perceptions will be shared.","['In this paper, an early intervention solution for collegiate faculty called Course Signals is discussed.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Artificial neural networks (ANNs) have been proved to be successfully used in a variety of pattern recognition and data mining applications. However, training ANNs on large scale datasets are both data-intensive and computation-intensive. In this paper, we present cNeural, a customized parallel computing platform to accelerate training large scale neural networks with the backpropagation algorithm. Nonetheless, NNs performance has been found to be comparable with other statistical approaches, particularly when approximating complex patterns based on numeric input values (Sargent, 2001; #CITATION_TAG).","Therefore, large scale ANNs are used with reservation for their time-consuming training to get high precision. Unlike many existing parallel neural network training systems working on thousands of training samples, cNeural is designed for fast training large scale datasets with millions of training samples. Secondly, it provides a parallel in-memory computing framework for fast iterative training. Third, we choose a compact, event-driven messaging communication model instead of the heartbeat polling model for instant messaging delivery.","['To achieve this goal, firstly, cNeural adopts HBase for large scale training dataset storage and parallel loading.']"
"Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. While there is currently much buzz about the new field of learning analytics [19] and the potential it holds for benefiting teaching and learning, the impression one currently gets is that there is also much uncertainty and hesitation, even extending to scepticism. A clear common understanding and vision for the domain has not yet formed among the educator and research community. Tertiary education providers collect an ever-increasing volume of data on their students, particularly activity data from virtual learning environments and other online resources (#CITATION_TAG).","To investigate this situation, we distributed a stakeholder survey in September 2011 to an international audience from different sectors of education. The survey was scaffolded by a conceptual framework on learning analytics that was developed based on a recent literature review. In this article, we first briefly introduce the learning analytics framework and its six domains that formed the backbone structure to our survey.",['It divides the domain of learning analytics into six critical dimensions.']
"We study various physical quantities associated with holographic s-wave superconductors as functions of the scaling dimensions of the dual condensates. In recent years, quantum phase transitions have attracted the interest of both theorists and experimentalists in condensed matter physics. These transitions, which are accessed at zero temperature by variation of a non-thermal control parameter, can influence the behavior of electronic systems over a wide range of the phase diagram. Quantum phase transitions occur as a result of competing ground state phases. The cuprate superconductors which can be tuned from a Mott insulating to a d-wave superconducting phase by carrier doping are a paradigmatic example. An interesting separate class of transitions are boundary phase transitions where only degrees of freedom of a subsystem become critical; this will be illustrated in a few examples. It is understood [#CITATION_TAG, 11] that a quantum field theoretic description of condensed matter systems is possible in the vicinity of the quantum critical point (QCP), where the relevant scale invariant theories are similar to field theories describing second-order phase transitions, for example Ginzburg-Landau theory.","Several classes of transitions will be briefly reviewed, pointing out, e.g., conceptual differences between ordering transitions in metallic and insulating systems.","['This review introduces important concepts of phase transitions and discusses the interplay of quantum and classical fluctuations near criticality.', 'The main part of the article is devoted to bulk quantum phase transitions in condensed matter systems.', 'The article is aimed on bridging the gap between high-level theoretical presentations and research papers specialized in certain classes of materials.']"
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Abstract Refactoring is the process of changing the design of a program without changing what it does. Typical refactorings, such as function extraction and generalisation, are intended to make a program more amenable to extension, more comprehensible and so on. Because of this, there is a need to give automated support to the process. While there are a number of refactoring tools available for Erlang programs, such as Wrangler [17, #CITATION_TAG] and RefactorErl [21], the number of refactorings for retrofitting concurrency is limited.","Refactorings differ from other sorts of program transformation in being applied to source code, rather than to a 'core' language within a compiler, and also in having an effect across a code base, rather than to a single function definition, say. We begin by discussing what refactoring means for functional programming languages, first in theory, and then in the context of a larger example. Next, we address system design and details of system implementation as well as contrasting the style of refactoring and tooling for Haskell and Erlang. We also discuss various extensions to the core tools, including integrating the tools with test frameworks; facilities for detecting and eliminating code clones; and facilities to make the systems extensible by users.",['This paper reflects on our experience of building tools to refactor functional programs written in Haskell (HaRe) and Erlang (Wrangler).']
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Refactoring is the process of changing the structure of a program without changing its behaviour. Refactoring has so far only really been deployed effectively for sequential programs. However, with the increased availability of multicore (and, soon, manycore) systems, refactoring can play an important role in helping both expert and non-expert parallel programmers structure and implement their parallel programs. In particular, Wrangler's API and DSL support for scripting is used in ParTE to build refactorings [#CITATION_TAG], whereas RefactorErl's program analysis support is used to find parallelisable code candidates.","To motivate our design, we refactor a number of examples in C, C++ and Erlang into good parallel implementations, using a set of formal pattern rewrite rules.",['This paper describes the design of a new refactoring tool that is aimed at increasing the programmability of parallel systems.']
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Working within parallel languages such as x10, which embodies the partitioned global address space (PGAS) model, some work has been done in loop parallelisation [#CITATION_TAG], and, while these are not included in the main release, there have been some experiments in parallelisation in the Fortran refactoring tool Photran [25].","We examine a novel refactoring, extract concurrent, that introduces additional concurrency within a loop by arranging for some user-selected code in the loop body to run in parallel with other iterations of the loop.","['In this poster, we present our vision of refactoring support for languages with a partitioned global address space memory model as embodied in the X10 programming language.']"
The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Program slicing is a technique to identify statements that may influence the computations at other statements. Precise slicing has been shown to be undecidable for concurrent programs. A more precise slicing algorithm is proposed by J. Krinke: in [#CITATION_TAG] he proposes a context-sensitive approach to slicing concurrent programs.,It extends the well known structures of the control flow graph and the (interprocedural) program dependence graph for concurrent programs with interference. This new technique does not require serialization or inlining,['This work presents the first context-sensitive approach to slice concurrent programs accurately.']
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. In the multicore era, a major programming task will be to make   programs more parallel. This is tedious because it requires changing many lines  of code, and it is error-prone and non-trivial because programmers need to ensure  non-interference of parallel operations. In common with our observation for Erlang, Dig notes in [#CITATION_TAG] that ""unlike sequential refactoring, refactoring for parallelism is likely to make the code more complex, more expensive to maintain, and less portable"".","We also present the current incarnation of this vision: a toolset that   supports several refactorings for (i) making programs thread-safe, (ii) threading   sequential programs for throughput, and (iii) improving scalability of   parallel programs.Microsoft/Intel through UPCRC Illinoispublished or submitted for publicationis peer reviewe","['We present our vision on how refactoring tools can improve   programmer productivity, program performance, and program portability.']"
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. In [#CITATION_TAG], J. Cheng extends the notion of slicing for sequential programs to concurrent programs and presents a graph-theoretical approach to slicing concurrent programs.","In addition to the usual control and data  dependences proposed and studied for sequential programs, the  paper introduces three new types of primary program dependences in  concurrent programs, named the selection dependence,  synchronization dependence, and communication dependence. The  paper also propose a new program representation for concurrent  programs, named the Process Dependence Net (PDN), which is an  arc-classified digraph to explicitly represent the five types of primary  program dependences in the programs. As a result, various notions  about slicing concurrent programs can be formally defined based on  the representation, and the problem of slicing a concurrent program  can be simply reduced to a vertex reachability problem in its PDN  representation.","['This paper extends the notion of slicing, which was  originally proposed and studied for sequential programs, to  concurrent programs and presents a graph-theoretical approach to  slicing concurrent programs.']"
"The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Erlang is gaining widespread adoption with the advent of multi-core processors and their new scalable approach to concurrency. This book helps you: Understand the strengths of Erlang and why its designers included specific features Learn the concepts behind concurrency and Erlang's way of handling it Write efficient Erlang programs while keeping code neat and readable Discover how Erlang fills the requirements for distributed systems Add simple graphical user interfaces with little effort Learn Erlang's tracing mechanisms for debugging concurrent and distributed systems Use the built-in Mnesia database and other table storage features Erlang Programming provides exercises at the end of each chapter and simple examples throughout the book. Introduction Erlang [3,#CITATION_TAG] is a functional programming language with builtin support for concurrency based on share-nothing processes and asynchronous message passing","Written by leaders of the international Erlang community -- and based on their training material -- Erlang Programming focuses on the language's syntax and semantics, and explains pattern matching, proper lists, recursion, debugging, networking, and concurrency.","['This book is an in-depth introduction to Erlang, a programming language ideal for any situation where concurrency, fault tolerance, and fast response is essential.']"
"Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. The hydrogen energy arena has seen a proliferation of futures studies: scenarios, forecasts, roadmaps and visions that describe what a hydrogen future might look like, and how it might be achieved. In 2006, the current authors published a review of 40 such studies that examined the state of the art of futures studies in the hydrogen field, and interrogated the literature for insight into the drivers, barriers, and possible transitions for hydrogen. What does the literature tell us about the evolution of the hydrogen innovation system and prospects for its growth? In order to identify key uncertainties and possibilities for hydrogen energy in the UK, we reviewed the literature [#CITATION_TAG], ran a stakeholder workshop [2], and conducted stakeholder interviews and participant observation at hydrogen stakeholder events.","; What are methodological improvements since 2006, and what knowledge gaps remain?","['Here, six years on, we present an update of that review, assessing how the future of hydrogen has changed with a review of over 30 hydrogen futures studies published between 2006-2011.']"
"Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. This is the second in a series of reports from UK ERC's Energy 2050 project. Evidence from energy systems analysis suggests that the desirability of hydrogen as a decarbonisation option depends strongly on success in bringing down costs, for both fuel cell technologies and hydrogen production, delivery and storage systems [5, 6, #CITATION_TAG].","The technologies analysed include a number of renewables (marine, bioenergy, wind and solar PV) and also other low carbon options (carbon capture and storage, nuclear power and fuel cells).Technology acceleration is analysed firstly by devising detailed technology-by technology accounts of accelerated development, and then system-level modelling of the potential impacts of this acceleration on the UK energy system from now to 2050.","['It investigates the prospects for accelerated development of a range of low carbon energy supply technologies, and the impact of this acceleration on the decarbonisation of the UK energy system.', 'The report highlights the potentially important role for low carbon supply technology acceleration in the transition to a low carbon energy system in the UK, especially over longer timescales.']"
"Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. There are several economic studies and models describing the introduction of hydrogen-powered vehicles, but most of them focus on only one segment of the car market. Most studies analyse the impacts of FCVs on the automotive industry or the demand for FCVs separately, while others look at the required hydrogen infrastructure, but none of these analyses examines the car market as a whole. Thereby about one-third of all passenger cars will be driven with hydrogen in 2040 and even two-third in 2050. Primarily subsidies and tax allowances for the vehicles are necessary to reduce the price of FCVs to the average level of modern diesel cars; otherwise consumers' acceptance of FCVs will be very low. A rapid introduction of FCVs based on these policies is necessary in order to limit the cumulative subsidies for the vehicles and the fuel-tax deficit x The scale of early infrastructure provision can be very important, with initial infrastructure availability proving very important in several studies [23, 24, #CITATION_TAG].","The analysis takes into account the actions of the whole market (consumers, automotive manufacturers, filling station owners and policymakers) and their interactions.",['This paper discusses the market penetration of fuel cell vehicles (FCVs) in Germany from the perspectives of different stakeholders.']
"Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. The political dynamics of energy transitions have been neglected in studies of transitions to low carbon energy systems [30, #CITATION_TAG], but it is clear that there will be losers.",The discussion is divided into four parts. The first examines the idea of 'governance for sustainable development'. The second considers the diffusion of power in modern societies. The third explores the extent to which this constitutes a problem for sustainable development.,['This paper explores one of the major challenges associated with governance for sustainable development: managing change in a context where power is distributed across diverse societal subsystems and among many societal actors.']
"Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. In a quest for strategic and environmental benefits, the developed countries have been trying for many years to increase the share of alternative fuels in their transportation fuel mixes. They have met very little success though. Argentina's national programme to promote CNG vehicles relied on price controls on fuel, and has been relatively successful despite almost no government involvement in infrastructure provision [#CITATION_TAG].","We conducted interviews with a wide range of stakeholders and analyzed econometrically data collected in Argentina to investigate the factors, economic, political, and others that determined the high rate of adoption of this fuel.","['In this paper, we examine the experience of Argentina with compressed natural gas.', 'A central objective of this research was to identify lessons that could be useful to developed countries in their efforts to deploy alternative fuel vehicles.']"
"Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. In Europe natural gas vehicles play a minor role. A decisive reason for this is the dependence of most European countries from gas imports. Except for Italy, there is no tradition to use natural gas as fuel. In addition, there is a lack of infrastructure (e.g. In contrast to Europe, in Latin American and Asian countries natural gas vehicles are widespread. Some countries foster natural gas vehicles because they have own gas resources. Many countries must reduce the high air pollution in big cities. Environmental reasons are the main motive for the use of natural gas vehicles in Europe. In last years, high oil prices stimulated the use of natural gas as fuel. European governments have developed incentives (e.g. However, the focus is on hybrid technology and the electric car, which, however, need further technical improvement. In contrast, the use of natural gas in conventional engines is technically mature. Additional gas imports can be avoided by further improvements of energy efficiency and the use of renewable energy. However, the cost of driving on natural gas is substantially below that of gasoline in many European countries, but this has not driven a transition in vehicle technology [#CITATION_TAG), perhaps because of the perceived inferiority of CNG vehicles or because of the lack of infrastructure.",,['tax reductions) to foster natural gas vehicles.']
"Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. Although an increasing number of AFVs have been deployed in recent years, various factors have limited this progress, such as large sunk investments in conventional technologies, limited networks of refueling stations, the typically higher cost of AFVs, and the relatively low price of oil. Given these barriers, and additional barriers specific to hydrogen, a transition to hydrogen will be a slow process, and must be supported by both near- and long-term policies that have clear and measurable goals that take hydrogen beyond fleet applications into broader vehicle markets. Because a transition to hydrogen vehicles will not occur quickly, it is necessary for the government to have consistent and integrated transportation policies combining short- and long-term goals. Multinationals may find China to be an ideal testing ground for innovative hydrogen vehicles with appropriate incentive policies and programs In the US, the major policy drivers for alternative fuels have largely come from federal governments, with limited success [#CITATION_TAG].",,"['This paper examines the experience of existing alternative fuel vehicle (AFV) programs in the US and China to provide insights into appropriate strategies for developing hydrogen vehicles and infrastructure in China.', 'These policies should draw upon resources from both governments and multinational companies to provide incentives for vehicle purchases, promote investment in infrastructure, and disseminate information to raise public awareness.']"
"People aging with HIV might have different health conditions compared with people who seroconverted at older ages. The study objective was to assess the prevalence of, and risk factors for, individual co-morbidities and multimorbidity (MM) between HIV-positive patients with a longer duration of HIV infection, and patients who seroconverted at an older age. Better understanding of the intersection of HIV, aging and health is an urgent issue due to the increasing number of people aging with HIV [1, #CITATION_TAG] as the synergistic result of two concurrent phenomenon: the increased life expectancy of people with HIV undergoing HAART, extensively demonstrated both in high [3, 4] and middle-and low-income countries [5, 6], but also the increasing number of people seroconverting HIV at an older age, as the result of a lower perception of sexual risk in older people [7, 8].",,"['Objective The aim of the study was to report on HIV and older people in the European Region, including new data stratified by subregion and year.']"
"People aging with HIV might have different health conditions compared with people who seroconverted at older ages. The study objective was to assess the prevalence of, and risk factors for, individual co-morbidities and multimorbidity (MM) between HIV-positive patients with a longer duration of HIV infection, and patients who seroconverted at an older age. Though effective anti-human immunodeficiency virus (HIV) therapies are now available, they have variable penetration into the brain. The incidence per 1,000 person-years was 6.49 pre-1997 (before highly active antiretroviral therapy was available), declining to 0.66 by 2003 to 2006. In 2003 to 2006, older age at seroconversion (relative risk = 3.24 per 10-year increase [95% confidence interval, 2.00-5.24]) and previous acquired immune deficiency syndrome diagnosis (relative risk = 4.92 [95% confidence interval, 1.43-16.92]) were associated with HIV-D risk, independently of current CD4 count. HIV-D risk appeared to increase during chronic infection, by 48% at 10 years after seroconversion compared with the lowest risk at 1.8 years.HIV-D incidence has reduced markedly since 1997. However, patients with low (<200 cells/mm3) or even intermediate (200-349 cells/mm3) CD4 counts, previous acquired immune deficiency syndrome diagnosis, longer HIV infection duration, and older age at seroconversion are at increased risk and should be closely monitored for neurocognitive disorders. Very few studies examined the impact of clinical intervention on both physical and neurocognitive impairment associated with aging [#CITATION_TAG] [22] [23] [24] [25].","The effect of duration of infection was explored using flexible parametric survival models.222 of 15,380 seroconverters developed HIV-D.","['We therefore aimed to assess changes over calendar time in the risk for HIV-associated dementia (HIV-D), and factors associated with HIV-D risk.Using Concerted Action on Seroconversion to AIDS and Death in Europe (CASCADE) data, we analyzed factors associated with time from HIV seroconversion to HIV-D using Cox models with time-updated covariates.']"
"People aging with HIV might have different health conditions compared with people who seroconverted at older ages. The study objective was to assess the prevalence of, and risk factors for, individual co-morbidities and multimorbidity (MM) between HIV-positive patients with a longer duration of HIV infection, and patients who seroconverted at an older age. The transformation of human immunodeficiency virus (HIV) from a rapidly fatal disease to a chronic manageable illness has resulted in annual increases in the numbers of people living with HIV. Associated with ageing is an accumulation of HIV-associated non-AIDS related co-morbidities, creating a complex patient group affected by multi-morbidity along with polypharmacy, functional decline and complex social issues. Moreover polifarmacy rises risks of pharmacological interactions and medical errors [18] [19] [#CITATION_TAG].",,"['With this in mind, this review seeks to explore areas where HIV (diagnosed or undetected) may impact on the work of clinical geriatricians.']"
"People aging with HIV might have different health conditions compared with people who seroconverted at older ages. The study objective was to assess the prevalence of, and risk factors for, individual co-morbidities and multimorbidity (MM) between HIV-positive patients with a longer duration of HIV infection, and patients who seroconverted at an older age. A particular concern in this population regards MM as a predictor care complexity and polypharmacy increasing the cost of clinical management [#CITATION_TAG].","We hypothesized that the increased prevalence of noninfectious comorbidities (NICMs) observed among HIV-infected patients may result in increased direct costs of medical care compared to the general population. Antiretroviral therapy (ART)-experienced HIV-infected patients (cases) were compared to age, sex, and race-matched adults from the general population, included in the CINECA ARNO database (controls). Linear regression models were constructed to evaluate predictors of total care cost for the controls and cases.There were 2854 cases and 8562 controls. We analyzed data from 29,275 drug prescription records. Lower medical costs associated with higher CD4 strata are offset by increases in the care costs needed for advancing age, particularly for NICMs.","['Our objective was to provide estimates of and describe factors contributing to direct costs for medical care among HIV-infected patients, focusing on NICM care expenditure.A case-control study analyzing direct medical care costs in 2009.']"
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Many philosophers have worried about what philosophy is. Often they have looked for answers by considering what it is that philosophers do. In this article we consider the philosophical temperament, asking an alternative question: What are philosophers like? Recognizing this aspect of the philosophical temperament, it is natural to wonder how philosophers came to be this way: Does philosophical training teach reflectivity or do more reflective people tend to gravitate to philosophy? One of the purposes in considering the various characterizations is to caution the reader from adopting one characterization without considering other possibilities (#CITATION_TAG).",We then illustrate this tendency by considering what we know about the philosophizing of a few prominent philosophers.,"['Given the diversity of topics and methods found in philosophy, however, we propose a different approach.', 'Our answer is that one important aspect of the philosophical temperament is that philosophers are especially reflective: They are less likely than their peers to embrace what seems obvious without questioning it.']"
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Models postulating 2 distinct processing modes have been proposed in several topic areas within social and cognitive psychology. Associative retrieval or pattern completion in the slow-learning system elicited by a salient cue constitutes the effortless processing mode. Theories and models that endorse or assume the kind of dichotomy just described are often referred to as dual-process or dual-system theories (Chaiken and Trope 1999, Evans 2003, 2014a, 2014b, Evans and Stanovich 2013, Frankish 2010, Hammond 1996, Samuels 2009, Sloman 1996, #CITATION_TAG, Thompson 2009, 2010, Wilson Lindsey and Schooler 2000.","The structural basis of the new model is the idea, supported by psychological and neuropsychological evidence, that humans possess 2 memory systems. One system slowly learns general regularities, whereas the other can quickly form representations of unique or novel events. The second processing mode is more conscious and effortful; it involves the intentional retrieval of explicit, symbolically represented rulesfrom either memory system and their use to guide processing. After presenting our model, we review existing dual-process models in several areas, emphasizing their similar assumptions of a quick, effortless processing mode that rests on well-learned prior associations and a second, more effortful processing mode that involves rule-based inferences and is employed only when people have both cognitive capacity and motivation.",['We advance a new conceptual model of the 2 processing modes.']
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Dual-process theories hold that there are two distinct processing modes available for many cognitive tasks: one (type 1) that is fast, automatic and non-conscious, and another (type 2) that is slow, controlled and conscious. Typically, cognitive biases are attributed to type 1 processes, which are held to be heuristic or associative, and logical responses to type 2 processes, which are characterised as rule-based or analytical. It is often claimed that System 2 is uniquely human and the source of our capacity for abstract and hypothetical thinking. Theories and models that endorse or assume the kind of dichotomy just described are often referred to as dual-process or dual-system theories (Chaiken and Trope 1999, Evans 2003, 2014a, 2014b, Evans and Stanovich 2013, #CITATION_TAG, Hammond 1996, Samuels 2009, Sloman 1996, Smith and DeCoster 2000, Thompson 2009, 2010, Wilson Lindsey and Schooler 2000.","Dual-system theories go further and assign these two types of process to two separate reasoning systems, System 1 and System 2 - a view sometimes described as 'the two minds hypothesis'. It looks at some precursors, surveys key work in the fields of learning, reasoning, social cognition and decision making, and identifies some recent trends and philosophical applications",['This study is an introduction to dual-process and dual-system theories.']
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. The lack of gender parity in philosophy has garnered serious attention recently. Previous empirical work that aims to quantify what has come to be called ""the gender gap"" in philosophy focuses mainly on the absence of women in philosophy faculty and graduate programs. There are a variety of views about why this is the case, but there is some evidence that implicit bias at various stages in the undergraduate, graduate, and career-level training and selection processes could result in fewer opportunities for advancement for women (#CITATION_TAG Tiberius 2012, Stier and Yaish 2014).","First, the biggest drop in the proportion of women in philosophy occurs between students enrolled in introductory philosophy classes and philosophy majors.","['Our study looks at gender representation in philosophy among undergraduate students, undergraduate majors, graduate students, and faculty.']"
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Experimental philosophy's much-discussed 'restrictionist' program seeks to delineate the extent to which philosophers may legitimately rely on intuitions about possible cases. The present paper shows that this program can be (i) put to the service of diagnostic problem-resolution (in the wake of J.L. Some philosophers have been so intrigued by these challenges that they have taken to vacating their metaphorical armchairs for the world of experimental psychology (Alfano and Loeb 2014, Knobe et al 2012, #CITATION_TAG. And as more philosophers have learned about the extensive literature that chal-! 1 lenges traditional ideals about rationality-and others have run their own experiments-some concerns about the rationality of philosophical practice have emerged.","Austin) and (ii) pursued by constructing and experimentally testing psycholinguistic explanations of intuitions which expose their lack of evidentiary value: The paper develops a psycholinguistic explanation of paradoxical intuitions that are prompted by verbal case-descriptions, and presents two experiments that support the explanation.","[""This debunking explanation helps resolve philosophical paradoxes about perception (known as 'arguments from hallucination')""]"
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Anyone who watches the television news has seen images of firefighters rescuing people from burning buildings and paramedics treating bombing victims. How do these individuals make the split-second decisions that save lives? Most studies of decision making, based on artificial tasks assigned in laboratory settings, view people as biased and unskilled. Gary Klein is one of the developers of the naturalistic decision-making approach, which views people as inherently skilled and experienced. Since 1985, Klein has conducted fieldwork to find out how people tackle challenges in difficult, nonroutine situations. Sources of Power is based on observations of humans acting under such real-life constraints as time pressure, high stakes, personal responsibility, and shifting conditions. The duality referred to by 'dualprocess' has many names, each with it's own story: associative vs. rulebased (Sloman 1996), heuristic vs. analytic (Evans 1984 (Evans, 1989, tacit thought vs. explicit thought (Evans and Over 1996), implicit cognition vs. explicit learning (Reber 1989), interactional vs. analytic (Levinson 1995), experiential vs. rational (Epstein 1994), quick and inflexive modules vs. intellection (Pollock 1991), intuitive cognition vs. analytical cognition (Hammond 1996), recognition primed choice vs. rational choice strategy (#CITATION_TAG), implicit inference vs. explicit inference (Johnson-Laird 1983), automatic vs. controlled processing (Shiffrin and Schneider 1977), automatic activation vs. conscious processing system (Posner and Snyder 1975, 2004), rationality vs. rationality (Evans & Over 1996, intuitive vs. reflective, model-based vs. model-free (Daw et al 2005) and system 1 vs. system 2 (see Stanovich and West 2000 for the first mention of these terms as well as a useful, albeit dated, list of dualprocess terminology; see also Frankish 2010 for a list of features commonly associated with System 1 and System 2).",,"['In addition to providing information that can be used by professionals in management, psychology, engineering, and other fields, the book presents an overview of the research approach of naturalistic decision making and expands our knowledge of the strengths people bring to difficult tasks.']"
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Do epistemic intuitions tell us anything about knowledge? Stich has argued that we respond to cases according to our contingent cultural programming, and not in a manner that tends to reveal anything significant about knowledge itself. I've argued that a cross-culturally universal capacity for mindreading produces the intuitive sense that the subject of a case has or lacks knowledge. I argue that existing work on cross-cultural variation in mindreading favors my position over Stich's Perhaps this is why philosophers will argue, explicitly or implicitly, that premises can be considered true or false in virtue of their intuitive appeal-viz., the premise just seems to be true or false (Audi 2004, Bealer 1998, Huemer 2005, #CITATION_TAG.",,"[""This paper responds to Stich's charge that mindreading is cross-culturally varied in a way that will strip epistemic intuitions of their evidential value.""]"
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. ABSTRACT--Human reasoning is often biased by stereo-typical intuitions. The nature of such bias is not clear. Some authors claim that people aremere heuristic thinkers and are not aware that cued stereotypes might be inap-propriate. Other authors claim that people always detect the conflict between their stereotypical thinking and nor-mative reasoning, but simply fail to inhibit stereotypical thinking.Hence, it is unclearwhether heuristic bias should be attributed to a lack of conflict detection or a failure of inhibition. Another possibility is that some independent process arbitrates between the two responses-cognitive scientists might hypothesize that part of the prefrontal cortex or the anterior cingulate cortex could play a role in this arbitration (Miller and Cohen 2001, Neys Vartanian and #CITATION_TAG).",Participants answered a classic deci-sion-making problem (the ''lawyer-engineer' ' problem) while the activation of brain regions believed to be involved in conflict detection (anterior cingulate) and response in-hibition (lateral prefrontal cortex) wasmonitored.,['We introduce a neuroscientific approach that bears on this issue.']
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. The differences model, which argues that males and females are vastly different psychologically, dominates the popular media. Gender differences can vary substantially in magnitude at different ages and depend on the context in which measurement occurs. Overinflated claims of gender differences carry substantial costs in areas such as the workplace and relationships. These results, as well as the results herein, corroborate the gender similarities hypothesis: ""most psychological gender differences are in the close-to-zero (d ≤ 0.10) or small (0.11 < d < 0.35) range, a few are in the moderate range (0.36 < d < 0.65), and very few are large (d < 0.66-1.00) or very large (d < 1.00)"" (#CITATION_TAG).",,"['Here, the author advances a very different view, the gender similarities hypothesis, which holds that males and females are similar on most, but not all, psychological variables.']"
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Implicit learning is an inductive process whereby knowledge of a complex environment is acquired and used largely independently of awareness of either the process of acquisition or the nature of that which has been learned. The associationist about intuition will start by outlining a fairly well accepted tenet of cognitive science: whether or not we are aware of it, we are more or less constantly learning associations between things and properties of things via experience-e.g., if we frequently experience two things or properties of things together, then we will begin to associate them (Reber 1989, #CITATION_TAG, Talbot 2009.","Characterized this way, implicit learning theory can be viewed as an attempt to come to grips with the classic epistemological issues of knowledge acquisition, representation and use. The argument is made that the process, despite its seeming cognitive sophistication, is of considerable evolutionary antiquity and that it antedates awareness and the capacity for conscious control of mentation. Various classic heuristics from evolutionary biology are used to substantiate this claim and several specific entailments of this line of argument are outlined.",['Abstract This paper is an attempt to put the work of the past several decades on the problems of implicit learning and unconscious cognition into an evolutionary context.']
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. The likelihood of electronic word-of-mouth (e-WOM) adoption is useful for academics and practitioners to understand the persuasion. To address this issue, the attitude-intention link was often assumed in information systems (IS) literature without further examinations in the persuasion contexts. Theories and models that endorse or assume the kind of dichotomy just described are often referred to as dual-process or dual-system theories (Chaiken and Trope 1999, Evans 2003, 2014a, 2014b, Evans and Stanovich 2013, Frankish 2010, Hammond 1996, Samuels 2009, Sloman 1996, Smith and DeCoster 2000, Thompson 2009, 2010, #CITATION_TAG.","This study tests the theoretical model by surveying 395 users with viewing or posting experience in an online discussion forum. Additionally, the use of central and peripheral routes to form attitudes is influenced by perceived control in online searching.","['This study develops a theoretical model, grounded in the elaboration likelihood model (ELM), to assess how recipients use central and peripheral routes to elaborate e-WOM.']"
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Perhaps this is why philosophers will argue, explicitly or implicitly, that premises can be considered true or false in virtue of their intuitive appeal-viz., the premise just seems to be true or false (Audi 2004, #CITATION_TAG, Huemer 2005, Nagel 2007.",,['along with them is this rethinking intuition the psychology of intuition and its role in philosophical inquiry that can be your partner.']
"INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Motivated by the common use of softmax selection in models of human decision-making, we study the maximum likelihood parameter estimation problem for softmax decision-making models with linear objective functions. , the ""take the best"" strategy (Gigerenzer and Goldstein 1996) inspired by statistical models using ""equal weights"" or ""tallying"" strategies (Dawes 1974, #CITATION_TAG, Einhorn and Hogarth 1975, Schmidt 1971.","We present conditions under which the likelihood function is convex. These allow us to provide sufficient conditions for convergence of the resulting maximum likelihood estimator and to construct its asymptotic distribution. In the case of models with nonlinear objective functions, we show how the estimator can be applied by linearizing about a nominal parameter value. We apply the estimator to fit the stochastic UCL (Upper Credible Limit) model of human decision-making to human subject data.","['With an eye towards human-centered automation, we contribute to the development of a systematic means to infer features of human decision-making from behavioral data.']"
"The idea that the support of local people is essential for the success of protected areas is widespread in conservation, underpinning various conservation paradigms and policies, yet it has rarely been critically examined. We present a case study from the Dominican Republic, where despite two decades of resentment with protected policies, local people are unable to signiTh cantly challenge them because of fears of violence from guards, inability to reach important political arenas, social ties with guards, and the inability to coordinate action. This paper explores the circumstances which determine whether or not local opposition to protected areas can cause them to fail. It focuses on the power relations between protected areas and local communities, and how easily they can inss uence one another. It is informed largely by Scott's concept of everyday resistance, which considers the informal subtle politics involved in social conflicts where there are constraints on the ability of some people to take open, formal action. They may chose to resist these costs through formal political opposition such as legal challenges, lobbying, and protest marches (e.g., Sullivan 2003), but more frequently through more subtle, indirect protests such as non-cooperation and sabotage (#CITATION_TAG).","These ideas are critiqued and adapted to the particular context of conservation regulation, which is distinct from many other types of rural conflict.","['This paper presents a framework to understand how conservation, in particular protected areas and national parks, are resisted, based on theories of subaltern politics and a review of thirty-four published case studies.', 'In particular, it recognises the importance of continuing banned livelihood practices such as hunting or farming in resistance, and the particular symbolism this has in conflicts.', 'As well as the theoretical contribution, by showing the variety of responses to this resistance this paper aims to make conservation practitioners more aware of the forms local resistance can take.', 'Rather than being a simple call for a more socially just conservation, it goes beyond this to provide a tool to make conservation better for both local communities and biodiversity']"
"In poor countries, labor productivity in agriculture is considerably lower than in the rest of the economy. Third, labor productivity in agriculture is severely mis-measured in the US. Its unique feature is that it gives rise to an endogenously variable number of sectors. For example, Adamopoulous and Restuccia (2014) and Donovan (2014) point to the scale or risk of farming; Restuccia et al. (2008) and #CITATION_TAG to barriers of moving workers or intermediate goods between agriculture and non-agriculture; and Lagakos and Waugh (2013) to selection of the workers in the two sectors.","Each new sector is created by a pervasive innovation, which creates a new market and into and out of which there are entry and exit of firms. In the construction of our model we found inspiration in a number of growth models, both endogenous and evolutionary as well as on empirical work on structural change. Within our model the ability to create new sectors at the right times is the crucial determinant of the growth potential of an economic system.","['In this paper we are going to analyze the dynamics of barriers to entry at the international level.', 'In fact, the main objective for which the model was initially constructed was to test some propositions implying that variety growth is a necessary requirement for long term economic development.']"
"Phonological errors were scarce in both groups. Anomia is characteristic of SD and has received substantial research attention (see #CITATION_TAG, for an analysis of a large corpus of SD naming errors).","Using a pool of 225 sets of picture naming data from 78 patients, we assessed the effects on naming accuracy of several characteristics of the target objects or their names: familiarity, frequency, age of acquisition and semantic domain (living/non-living). We also analysed the distribution of different error types according to the severity of the naming deficit.","['This study was designed to explore the nature of the anomia that is a defining feature of semantic dementia.', 'A particular focus of the study was the impact on naming of a previously unconsidered variable: the typicality of an object within its semantic category.']"
"Phonological errors were scarce in both groups. The interdependence between the two, mediated by the quality of the relationship, has direct implications for the earning of rents through collaborations. These relationship-specific expenditures can be of an internally generated nature, endogenous to the alliance form itself, and need not exceed alternative forms, while the associated benefits have the capacity to potentially exceed the alternatives. If semantic and syntactic sources of information compete during the selection of lexical items, a reduction in the strength/reliability of semantic information might increase reliance on syntactic information (#CITATION_TAG).","Based on a perspective of value, we explain how a more inclusive and integrative perspective, one which combines elements from transaction costs and resource-based theory, provides more robust insight into collaboration formation, management, and instability. In doing so, we differentiate rent-yielding firm-specific assets at the core of the resource-based view from the transaction specific assets at the core of transaction cost theory. In the search for value, we explain why the transaction costs incurred in the exchange of resources are not independent of the nature of resources to be transacted and, similarly, why the returns realized from these resources are not independent of the relationship- and transaction-specific expenditures incurred in effectively combining them and maintaining the combination. The paper contributes in three key related ways: (a) the explicit recognition of the relationship as a value-bearing asset embedded in a larger and endogenous institutional context, namely a system of resource relationships-both intraorganizational and inter-organizational-among partner firms and the collaboration, (b) the recognition of the evolving relationship between production and exchange which, at the level of the collaboration, is directly dependent on the nature, evolution, and dynamics of the relationship among the parties to the transaction, and (c) the provision of a nontrust explanation for why firms might knowingly forego opportunities to take advantage of their partners. Drawing from this, the paper occasions (a) a shift in focus from the form to the process of governance, which has direct implications for value creation and realization and (b) a shift in the primary identity of transaction-specific and relationship-specific expenditures from cost to investment in future value.","['The paper makes a crucial distinction between the potential value attainable through collaborations and its actual realization.', 'The crux of our argument is that firms enter into collaborative relationships because these are expected to yield superior value relative to alternate organizational forms in certain situations, offering potentially synergistic combinations of complementary resources and capabilities, yet such relationships are frequently prone to failure because the partner firms tend not to recognize ex ante the nature and extent of transaction-specific investment that is required in the collaborative relationship to attain these synergies.', 'In this light, transaction-specific investment in what we term relational specificity becomes imperative.']"
"Phonological errors were scarce in both groups. Although core service does not seem to have positive impact on switching costs, core ... One of the first models of speech production was based on an analysis of an extensive corpus of such errors in normal, healthy speakers (#CITATION_TAG).","economic value, relational value, and core value) and investigates their relationships with buyers' perceptions of switching costs.",['Purpose - The purpose of this study is to examine the concept of customer value and its role in building switching costs perceptions.']
"Phonological errors were scarce in both groups. The conceptual model of the impact of interfirm relational drivers on customer value recei... Nevertheless, reduction of morphological and syntactic complexity, relative to normal speakers, has been documented (#CITATION_TAG; Patterson & MacDonald, 2006).","Moderator analysis of customer characteristics suggests that increasing contact density benefits sellers that have customers with high employee turnover rates, whereas building relationships with key decisions makers generates the highest returns among customers that are more difficult to access.","['Abstract This article integrates social network and exchange theory to develop a model of customer value based on three relational drivers: relationship quality (the caliber of relational ties), contact density (the number of relational ties), and contact authority (the decision-making capability of relational contacts).']"
"Phonological errors were scarce in both groups. In order for such comparisons to be meaningful, however, the instruments used to measure the theoretical constructs of interest have to exhibit adequate cross-national equivalence. For example, closed class words are semantically 'shallow', having fewer semantic features than open class items, which could make them more vulnerable to damage within the semantic system or between semantics and word form representations (Allen & Seidenberg, 1999; Goodglass & Menn, 1985; #CITATION_TAG).","We review the various forms of measurement invariance that have been proposed in the literature, organize them into a coherent conceptual framework that ties different requirements of measure equivalence to the goals of the research, and propose a practical, sequential testing procedure for assessing measurement invariance in cross-national consumer research. The approach is based on multisample confirmatory factor analysis and clarifies under what conditions meaningful comparisons of construct conceptualizations. construct means, and relationships between constructs are possible. An empirical application dealing with the single-factor construct of consumer ethnocentrism in Belgium, Great Britain, and Greece is provided to illustrate the procedure.",['Assessing the applicability of frameworks developed in one country to other countries is an important step in establishing the generalizability of consumer behavior theories.']
"Phonological errors were scarce in both groups. Extant literature and suppliers interviewed for this study view a solution as a customized and integrated combination of goods and services for meeting a customer's business needs. In contrast, customers view a solution as a set of customer-supplier relational processes comprising (1) customer requirements definition, (2) customization and integration of goods and/or services and (3) their deployment, and (4) postdeployment customer support, all of which are aimed at meeting customers' business needs. The relational process view can help suppliers deliver more effective solutions at profitable prices. Supplier variables include c... Speech errors produced by different aphasic patient groups have formed the basis of debates about particular deficits and how they relate to the intact language system (Bates & Wulfeck, 1989; #CITATION_TAG; Rapp & Goldrick, 2000, Goldrick, 2006.",,['Abstract This study draws on depth interviews with 49 managers in customer firms and 55 managers in supplier firms and on discussions with 21 managers in two focus groups to propose a new way of thinking about customer solutions.']
"Phonological errors were scarce in both groups. Many business customers today consolidate their supply bases and implement preferred supplier programs. Consequently, vendors increasingly face the alternative of either gaining a key supplier status with their customers or being pushed into the role of a backup supplier. As product and price become less important differentiators, suppliers of routinely purchased products search for new ways to differentiate themselves in a buyer-seller relationship. At present, all models assume cascading of information (i.e. partial information from one level can be accessed by the next) from conceptual to lexico-semantic representations, as this allows multiple candidates to be activated for a particular target (Goldrick, 2006; Levelt et al., 1999); interactivity between other levels is still a topic of much debate (#CITATION_TAG; Rapp & Goldrick, 2000; Roelofs, 2004; Vigliocco & Hartsuiker, 2002).","The authors identify service support and personal interaction as core differentiators, followed by a supplier's know-how and its ability to improve a customer's time to market.",['This research investigates avenues for differentiation through value creation in business-to-business relationships.']
"Phonological errors were scarce in both groups. At present, all models assume cascading of information (i.e. partial information from one level can be accessed by the next) from conceptual to lexico-semantic representations, as this allows multiple candidates to be activated for a particular target (Goldrick, 2006; Levelt et al., 1999); interactivity between other levels is still a topic of much debate (Dell, 1986; Rapp & Goldrick, 2000; Roelofs, 2004; #CITATION_TAG).",,"['A discussion of modularity in language production processes, with special emphasis on processes for retrieving words and building syntactic structures for a to-be-uttered sentence, is presented.', ""The authors' 1st goal was to assess the extent to which information processing is encapsulated between different processing stages."", 'Their 2nd goal was to propose an altemative framework that does not assume strict encapsulation but that maintains multiple levels of integration for production.']"
"Phonological errors were scarce in both groups. ConclusionDavid Wilson (1995) has provided us with much grist for thought with his integrated framework for customer-supplier relationship development. Our models and empirical research ought to reflect this, but to date they largely have not. Understanding and actualizing value creation (and value sharing) are critical aspects of the market-sensing and customer-linking capabilities in market-driven organizations (Day 1994), yet the mechanisms underlying them and the methodologies for accurately assessing them remain largely unknown. Here, particularly for tool development research, it would seem to be an opportune time for business marketing academics and practitioners to form their own collaborative relationships for mutual gain. For example, closed class words are semantically 'shallow', having fewer semantic features than open class items, which could make them more vulnerable to damage within the semantic system or between semantics and word form representations (Allen & Seidenberg, 1999; #CITATION_TAG; Plaut & Shallice, 1993).",,"['In focusing on which constructs are ""active"" and therefore most meaningful at each stage, he has opened a new vista for research in this area.']"
"Phonological errors were scarce in both groups. This applies to frankly semantic tasks (like object naming), where typicality can be gauged by the extent to which an object or concept is characterized by shared features in its category. It also applies in tasks apparently requiring only access to a 'surface' representation (such as lexical decision) or translation from one surface representation to another (like reading words aloud), where typicality is defined in terms of the structure of the surface domain(s). what the patients still know is information that generalises to many different concepts (Patterson, 2007; #CITATION_TAG) -it seems plausible that lexico-syntactic information may be precisely the kind of abstracted, general, highly frequent information that should be preserved in SD.","The approach is then elaborated with particular regard to evidence from semantic dementia, a disorder resulting in relatively selective deterioration of conceptual knowledge, in which cognitive performance reveals ubiquitous effects of typicality.","['This paper begins with a brief description of a theoretical framework for semantic memory, in which processing is inherently sensitive to the varying typicality of its representations.']"
"Phonological errors were scarce in both groups. However, such relationships help business service firms to resist price pressures from their customers and add more value to their services over time. In contrast, closed class elements may be chosen once competition amongst grammatical features is resolved: in other words, competitive selection would apply to grammatical feature specifications but not to closed class elements (La Heij, Mak, Sander, & Willeboordse, 1998; Levelt et al., 1999; #CITATION_TAG; Schriefers, Jescheniak, & Hantsch, 2005).",,"['In this article, the author empirically examines the impact of long-term client relationships on the performance of business service firms.']"
"Phonological errors were scarce in both groups. questions in relationship marketing: why is relationship marketing so prominent now? Why do firms and consumers enter into relationships with other firms and consumers? Relationship marketing can take many forms and, as a result, relationship marketing theory has the potential to increase one's understanding of many aspects of business strategy.Research limitations/implications - The answ... In contrast, grammatical morphemes (both free and bound) are activated indirectly, via open class items, and they are intrinsic to the syntactic frame (Bock & Levelt, 1994; #CITATION_TAG; Garrett, 1984).",,"['Purpose - Drawing on resource-advantage theory and a diverse literature base, this article seeks to further the development of the explanatory foundations of relationship marketing theory by proposing, and then providing, tentative answers to three ""why?""']"
"Phonological errors were scarce in both groups. The grammatical relations of noun phrases in sentences are ordered in a hierar-chy that is reflected in a wide array of linguistic phenomena. An experiment on the formulation of sentences examined the relationship betweeen conceptual acces-sibility and grammatical relations for three levels in the hierarchy, the subject, direct object, and indirect object. There was a strong and systematic influence of conceptual accessibility on the surface syntactic structure of sentences. Converting thoughts into language requires that elements of the nonlinguistic conceptual system be mapped onto syntactic roles in sentences. The nature of the cognitive and communicative features encoded in th Lexico-semantic representations are assigned to particular roles (e.g. the agent of an action) according to a conceptual message; thus this assignment cannot be independent of conceptual-semantic content (#CITATION_TAG; Bock & Warren, 1985).",An experiment provides evidence that con-ceptual accessibility is linked to a hierarchy of grammatical relations that influences sentence formulation.,"['The hypothesis explored in this paper is that this hierarchy is related to the conceptual accessi-bility of the intended referents of noun phrases that commonly occur in particu-lar relational roles, with relations higher in the hierarchy typically occupied by noun phrases representing more accessible concepts.', 'In this paper we examine one aspect of this mapping process, arguing that an important factor in the assignment of conceptual elements to syntactic roles in the pro-duction of sentences is the ease of representing potential referents in thought, or their conceptual accessibility.']"
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Over the last several decades, there has been an increasingly robust body of ethnographic research indicating that the sharing of meat is strongly linked to the fitness pursuits of individuals, where successful male hunters achieve various forms of prestige that ultimately lead to greater reproductive success. We argue that the effects of prestige hunting and other similar displays can be traced archaeologically in subsistence, settlement, and material culture profiles, and in the gender division of labor of even the simplest foraging societies--in this case Great Basin Middle Archaic (4500-1000 B.P.) The literature suggests that companies maintain relationship bonds either because ""they have to"", due to high switching costs, or because ""they want to"", because of high relationship value (Bendapudi & Berry, 1997; de Ruyter et al., 2001; #CITATION_TAG; Gounaris, 2005; Liu, 2006).","In contrast with optimal faraging and other efficiency models that attempt to account for such behaviors, we apply costly signaling theory to explain when foraging currencies shifted from calories to prestige among Great Basin foragers.","['The application of such an approach has the ability to integrate a series of disparate, subsistence- and non-subsistence-related observations regarding Great Basin lifeways and, in so doing, revise our traditional understanding of prehistoric culture change in this region.']"
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. What are the effects of subjectivity on employee pay satisfaction and firm performance? While many authors criticize TCA for failing to recognize that value creation rather than minimizing costs is the primary goal of business exchange (e.g. Anderson, 1995; Ghosh & John, 1999; #CITATION_TAG; Zajac & Olsen, 1993), others see TCA as a starting point for analyzing value creation between exchange partners.","We examine these questions using data from a sample of 526 department managers in 250 car dealerships. Specifically, use of subjective bonuses is positively related to: (1) the extent of long-term investments in intangibles; (2) the extent of organizational interdependencies; (3) the extent to which the achievability of the formula bonus target is both difficult and leads to significant consequences if not met; and (4) the presence of an operating loss.",['This study examines two questions: When do firms make greater use of subjectivity in awarding bonuses?']
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. The demand for aggregation in evaluating managerial performance arises because reporting all the basic transactions and other nonfinancial information about performance is costly and impracticable (see Ashton [1982], Casey [1978], and Holmstrom and Milgrom [1987]). A buyer generally tries to avoid dependence on a particular supplier (#CITATION_TAG) but companies today tend to trade in some of their independence against cost savings by having fewer, heavily bound, high value suppliers (Swift & Coe, 1994).","We identify necessary and sufficient conditions on the joint density function of the signals under which linear aggregation, a simple and commonly employed way to construct a performance evaluation measure, is optimal. This characterization suggests that the linear form of aggregation is optimal for a large class of situations. We interpret these weights in terms of statistical characteristics (sensitivity and precision) of the joint distribution of the signals.","['Focusing on performance measures that are linear aggregates enables us to determine the relative weights on the individual signals in the optimal linear aggregate, since these weights are invariant for all realizations of the signals.']"
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. For companies, the ability to create superior value to customers by forging stronger relationships with them is not only considered a perquisite for survival, but also as a way to increase profitability. Because of the sensitiveness of the information, the collected customer data is not published. However, due to the limitations of the research methodology, the argument that increased customer loyalty will lead to greater customer value cannot be researched to the full extent. Suppliers, on the other hand, try to stabilize their customer base because customer retention is less costly than customer acquisition (#CITATION_TAG).",The assessment was based on the analysis of the statistical relationship between customer loyalty and customer value within the Finnish market of the case company. This study was implemented as a quantitative research by using RFM-analysis to evaluate Finnish customers based on two variables; value and loyalty.,['The objective of this case study was to assess the feasibility of adopting a customer-relationship management strategy which focuses on the maximization of customer loyalty.']
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. To obtain sufficient variance for effective analysis, we randomly asked one half of the respondents to select a well-functioning relationship as the questionnaire subject and the other half a rather problematic one (for a similar procedure see #CITATION_TAG).","A necessary and sufficient condition for imperfect information to improve on contracts based on the payoff alone is derived, and a characterization of the optimal use of such information is given.",['The role of imperfect information in a principal-agent relationship subject to moral hazard is considered.']
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Extant literature and suppliers interviewed for this study view a solution as a customized and integrated combination of goods and services for meeting a customer's business needs. In contrast, customers view a solution as a set of customer-supplier relational processes comprising (1) customer requirements definition, (2) customization and integration of goods and/or services and (3) their deployment, and (4) postdeployment customer support, all of which are aimed at meeting customers ' business needs. The relational process view can help suppliers deliver more effective solutions at profitable prices. Customer variables include adaptiveness to supplier offerings and political an This description also applies for suppliers: they generally try to leverage an existing relationship by cross-selling or offering new services (Davies et al., 2007), providing capital, information, and dedicated staff or adapting their production and logistics to customer demands (#CITATION_TAG).","Supplier variables include contingent hierarchy, documentation emphasis, incentive externality, customer interactor stability, and process articulation.",['This study draws on depth interviews with 49 managers in customer firms and 55 managers in supplier firms and on discussions with 21 managers in two focus groups to propose a new way of thinking about customer solutions.']
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Many business customers today consolidate their supply bases and implement preferred supplier programs. Consequently, vendors increasingly face the alternative of either gaining a key supplier status with their customers or being pushed into the role of a backup supplier. As product and price become less important differentiators, suppliers of routinely purchased products search for new ways to differentiate themselves in a buyer-seller relationship. After validation of the measurement models we summated the items for each construct in line with Cannon and Homburg (2001) and #CITATION_TAG.","The authors identify service support and personal interaction as core differentiators, followed by a supplier's know-how and its ability to improve a customer's time to market.",['This research investigates avenues for differentiation through value creation in business-to-business relationships.']
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Compensation can take many forms. Remuneration can come as pecuniary payments, as fringes such as health and pension benefits, or as a nonpecuniary reward such as plush office furniture that costs the firm less than it benefits the worker. A significant literature has examined the trade-offs between pecuniary and nonpecuniary compensation, the modern work having been pioneered by Rosen (1974). More recently, another body of literature has examined the selection of method of total compensation, ignoring the distinction between pecuniary and nonpecuniary payment. It has resulted in comparisons of compensation based on absolute output levels to that based on relative performance.' It has also led to explorations of the relation of compensation to experience over the work life.2 Little attention has been paid to what may be among the most important and obvious distinction in methods of compensation, namely, the choice between a fixed salary for some period of time, that is, paying on the basis of input and Some workers receive compensation that is specified in advance and not directly contingent on performance. Instead, it depends on an input measure, such as hours worked. For others, compensation is directly related to output. See Lazear and Rosen (1981), Stiglitz (1981), Holmstrom (1982), Green and Stokey (1983). See Lazear (1979, 1981) and Harris and Holmstrom (in press). Commitment occupies a central role in the study of successful relationships between firms (Gilliland & Bello, 2002; #CITATION_TAG; Hunt et al., 2006; Morgan & Hunt, 1994).","Piece rates are defined more rigorously. Among the more important factors discussed are worker heterogeneity, incentives, sorting considerations, monitoring costs, and asymmetric information.","['This work has focused on risk and incentive factors.', ""This essay is an attempt to predict a firm's choice of compensation method.""]"
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Performance measurement is an essential part of the design of any incentive system. The strength and value of incentives in organizations are strongly affected by the performance measures available. Yet, the characteristics of valuable performance measures have not been well explored in the agency literature. I would like to thank Nancy Beaulieu, Bob Gibbons, Brian Hall, Ed Lazear, Sherwin Rosen, Jasjit Singh, Lars Stole, an anonymous referee, and participants at the NAS Conference on ""Designing Incentives for the Production of Human Capital"" for comments on this paper. While many authors criticize TCA for failing to recognize that value creation rather than minimizing costs is the primary goal of business exchange (e.g. #CITATION_TAG; Ghosh & John, 1999; Madhok & Tallman, 1998; Zajac & Olsen, 1993), others see TCA as a starting point for analyzing value creation between exchange partners.","In this paper, I use a multi-task model to develop a two-parameter characterization of performance measures and show how these two parameters--distortion and risk--affect the value and use of performance measures in incentive contracts. I also use this framework to analyze the provision of incentives in several specific environments, including R&D labs and non-profit organizations.",['This research was funded by the Harvard Business School Division of Research.']
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Moreover, the factors influencing the use of specific measures vary, suggesting that the aggregate performance measure classifications commonly used in compensation research provide somewhat misleading inferences regarding performance measurement choices. We thus circumvented the problematic instability of factor scores across different sub-samples as well as the problems attached to measurement model invariance (#CITATION_TAG) when comparing different groups, such as buyers and sellers.",,['This study examines the determinants of performance measure choices in worker incentive plans.']
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. ThePennsylvania State University does not discriminate against anypersonbecause of age, ancestry, color, disability or handicap, national origin, race, religious creed, sex, sexual orientation, or veteran status. Direct all inquiries regarding the nondiscrimination policy to the Affirmative Action Director, thePennsylvania Stat For instance, #CITATION_TAG shows that service providers with a relational approach to their customers deliver superior value and achieve greater financial returns than firms with a transactional approach.",,"['ThePennsylvania State University is committed to thepolicy that all persons shall have equal access to programs, facilities, admission, and employment Without regard to personal characteristics not related to ability, performance, or qualifications as determined by University policy or by state or federal authorities.']"
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Objective measures of performance are seldom perfect. In response, incentive contracts often include important subjective components that mitigate incentive distortions caused by imperfect objective measures. Naturally, objective and subjective measures often are substitutes, sometimes strikingly so: we show that if objective measures are sufficiently close to perfect then no implicit contracts are feasible (because the firm's fallback position after reneging on an implicit contact is too attractive). Overall, interfirm cooperation in buyer-seller relationships is associated with a number of positive economic and non-economic outcomes (Anderson et al., 1994; #CITATION_TAG).",,['This paper explores the combined use of subjective and objective performance measures in (respectively) implicit and explicit incentive contracts.']
"Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Do organizations plan precisely how the job is to be done ex ante, or ask workers to determine the process as they go? As predicted, job designs tend to be ""coherent"" across these attributes within the same job. Job designs also tend to follow similar patterns across jobs in the same firm, and especially in the same establishment: when one job is optimized ex ante, others are more likely to be also. There is evidence that firms segregate different types of job designs across different establishments. At the industry level, both computer usage and R&D spending are related to job design decisions. According to prospect theory, losses loom larger than gains of the same (monetary) amount (#CITATION_TAG).","We first model this decision and predict complementarity among these following job attributes: multitasking, discretion, skills, and interdependence of tasks. We argue that characteristics of the firm and industry (e.g., product and technology, organizational change) can explain observed patterns and trends in job design. We then use novel data on these job attributes to examine these issues.",['In this chapter we study job design.']
"The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. Islands in Oceania were some of the last habitable land masses on earth to be colonized by humans. Current archaeological evidence suggests that these islands were colonized episodically rather than continuously, and that bursts of migration were followed by longer periods of sedentism and population growth. The decision to colonize isolated, unoccupied islands and archipelagos was complex and dependent on a variety of social, technological and environmental variables. Unique among existing models, it can account for the episodic nature of certain aspects of the colonization process. We argue that intensive food production was one variable that contributed to decreasing suitability of island habitats, stimulating dispersal, and ultimately migrations to more distant islands in Oceania. One example that has entered the literature is costly signaling theory, as described above; other studies, more wide-ranging in their use of HBE theory, have begun to appear (e.g., Fitzhugh 2003, #CITATION_TAG.",This ecological model provides a framework that considers the dynamic character of island suitability along with density-dependent and density-independent variables influencing migratory behavior.,"['In this chapter we develop an integrative, multivariate approach to island colonization in Oceania based on a model from behavioral ecology known as the Ideal Free Distribution.']"
"The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. The term ""tipping point"" commonly refers to a critical threshold at which a tiny perturbation can qualitatively alter the state or development of a system. Moreover, not all the traits that characterize a complex object or entity will have had a common history (#CITATION_TAG).","An expert elicitation is used to help rank their sensitivity to global warming and the uncertainty about the underlying physical mechanisms. Then we explain how, in principle, early warning systems could be established to detect the proximity of some tipping points.","['Here we introduce the term ""tipping element"" to describe large-scale components of the Earth system that may pass a tipping point.']"
"The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. A critical issue in climate-change economics is the specification of the so-called ""damages function"" and its interaction with the unknown uncertainty of catastrophic outcomes. This is a very well-known technique with a long history and involves putting phenomena in a linear order on the basis of some measure of their similarity to one another (#CITATION_TAG, O'Brien & Lyman 2000.","In these examples, the primary reason for keeping GHG levels down is to insure against high-temperature catastrophic climate risks.",['This paper asks how much we might be misled by our economic assessment of climate change when we employ a conventional quadratic damages function and/or a thin-tailed probability distribution for extreme temperatures.']
"The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. A Climate Change Damage Function (CCDF) is a reduced form relationship linking macroeconomic aggregates (e.g., potential GDP) to climate indicators (e.g., average temperature levels). This function is used in a variety of studies about climate change impacts and policy analysis. However, despite the fact that this function is key in determining results in many integrated assessment models, it is not typically calibrated in a consistent and rigorous way. The developments in archaeology are part of broader trends in anthropology and psychology more generally (see e.g., #CITATION_TAG, Cronk et al. 2000, Dunbar & Barrett 2007, Durham 1991, Smith & Winterhalder 1992b, Sperber 1996, but the types of data dealt with by archaeologists and the diachronic questions they generally address have led to an emphasis on some theoretical perspectives rather than others and to the development of specifically archaeological methodologies for obtaining information relevant to testing evolutionary hypotheses.",,"['This paper presents a novel approach, in which several different impacts of climate change are first assessed by means of a full-fledged computable general equilibrium model of the world economy, then results are interpolated to get a simple relationship of the CCDF type.']"
"The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. Despite great progress in identifying genetic variants that influence human disease, most inherited risk remains unexplained. Neiman's original case study indicated that patterning in the rim attributes of eastern North American Woodland period pottery was a result of drift, but Shennan & Wilkinson (2001) showed that patterning in the frequency of decorative attributes of early Neolithic pottery from a small region of Germany indicated a pronovelty bias in the later periods and #CITATION_TAG in a case study from the U.S. Southwest were able to show a departure in the direction of conformity.","A more complete understanding requires genome-wide studies that fully examine less common alleles in populations with a wide range of ancestry. To inform the design and interpretation of such studies, we genotyped 1.6 million common single nucleotide polymorphisms (SNPs) in 1,184 reference individuals from 11 global populations, and sequenced ten 100-kilobase regions in 692 of these individuals. This integrated data set of common and rare alleles, called 'HapMap 3', includes both SNPs and copy number polymorphisms (CNPs). We characterized population-specific differences among low-frequency variants, measured the improvement in imputation accuracy afforded by the larger reference panel, especially in imputing SNPs with a minor allele frequency of <=5%, and demonstrated the feasibility of imputing newly discovered CNPs and SNPs.","['This expanded public resource of genome variants in global populations supports deeper interrogation of genomic variation and its role in human disease, and serves as a step towards a high-resolution map of the landscape of human genetic variation.']"
"The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. Word representation is a means of representing a word as mathematical entities that can be read, reasoned and manipulated by computational models. The representation is required for input to any new modern data models and in many cases, the accuracy of a model depends on it. Because animal body size is correlated with handling costs and is readily assessable using archaeological faunal data, the proportion of large-bodied vs. small-bodied animal bones has very frequently been used as a diet-breadth measure (e.g., Broughton 1994; see also Ugan 2005; for within-species size variation see, e.g., #CITATION_TAG).","In this paper, we analyze various methods of calculating vector space for Nepali words and postulate a word to vector model based on the Skip-gram model with NCE loss capturing syntactic and semantic word relationships.",['This is an attempt to implement a paper by Mikolov on Nepali words.']
"Part 1 signposts classic work from cultural/media studies, marketing and sociology, which has been centrally concerned with meanings of popular culture designed for children and young people (e.g. via critiques of the gendered content of iconic popular cultural phenomena). Halfway through the paper is a 'commercial break'. With reference to a specific popular cultural artefact (the Toys 'Ia' Us Christmas toy catalogue), I argue that both meanings and matterings are crucial for geographers engaging with children and young people's popular cultures. This paper calls for more direct, careful, sustained research on geographies of children, young people and popular culture. I present three sets of empirical and conceptual resources for researchers developing work in this area. I argue that these conceptualisations can extend and unsettle classic work on popular culture, by questioning how popular cultural texts, objects and phenomena matter. Here, I present some personal ref lections on working at the intersection between the ideas discussed in Parts 1 and 2. Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. Collecting a large corpus of training data for this task would be prohibitively expensive. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. Many key studies have highlighted the sophisticated, globalised and aggressive corporate systems of cultural production, marketing and commodification, which have constituted a distinctive 'child market' for popular cultural products (Seiter 1992, McNeal 1992, #CITATION_TAG, Roedder 1999, Cook 2004.","Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. To this end, we propose a novel method - supervised distant supervision (SDS) - that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.",['We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements.']
"Part 1 signposts classic work from cultural/media studies, marketing and sociology, which has been centrally concerned with meanings of popular culture designed for children and young people (e.g. via critiques of the gendered content of iconic popular cultural phenomena). Halfway through the paper is a 'commercial break'. With reference to a specific popular cultural artefact (the Toys 'Ia' Us Christmas toy catalogue), I argue that both meanings and matterings are crucial for geographers engaging with children and young people's popular cultures. This paper calls for more direct, careful, sustained research on geographies of children, young people and popular culture. I present three sets of empirical and conceptual resources for researchers developing work in this area. I argue that these conceptualisations can extend and unsettle classic work on popular culture, by questioning how popular cultural texts, objects and phenomena matter. Here, I present some personal ref lections on working at the intersection between the ideas discussed in Parts 1 and 2. Perhaps most inf luentially, a great deal of research within Anglo-American cultural/media studies, marketing and sociology has considered the way in which children and young people have increasingly been targeted as a market segment within contemporary consumer capitalism (#CITATION_TAG, Steinberg and Kincheloe 1997, Gunter and Furnham 1998, Langer 2002, Marshall 2010.","We develop a novel distant supervised model that integrates the results from open information extraction techniques to perform relation extraction task from biomedical literature. Unlike state-of-the-art models for relation extraction in biomedical domain which are mainly based on supervised methods, our approach does not require manually-labeled instances. In addition, our model incorporates a grouping strategy to take into consideration the coordinating structure among entities co-occurred in one sentence.",['We apply our approach to extract gene expression relationship between genes and brain regions from literature.']
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Apart from being affected by noise in parsing which reduces their effect, the syntactic features are likely to be useful relatively rarely, since in many cases simpler features suffice. However in the sentence ""Endogenous CED-4 is normally localized. "", the feature that ""CED-4"" is the subject of the verb ""localize"" in passive voice is crucial in recognizing the former as a gene name. Therefore, it was natural to attempt to combine the strengths of these two systems. This occurs because seen and unseen gene names may appear in the same sentence and choosing the predictions of the latter system could result in more errors on the seen gene names. Annotating full papers automatically, as performed in Section 2.2 for abstracts, is unlikely to provide us with training data of adequate quality given the more complex and variable language used in them. BIOMEDICAL NAMED ENTITY RECOGNITION 31 Figure 2.2: Screenshot of the interface used to collect the feedback from the users. This process presented us with an opportunity to obtain feedback from the users. A screenshot of the interface appears in Figure 2.2. For example, a source of errors for the CRF+syntax system was that identifiers of image panels in parentheses were commonly mistaken for gene names, especially when they followed a gene name, since in abstracts, the pattern ""gene name (token)"" is a very strong indication that the token in parentheses is in fact a gene name as well. An incentive plan's goal is not merely to motivate an employee to work harder, but also to balance motivation across different tasks (#CITATION_TAG).","The CRF+syntax system uses a complex but more general representation of the context based on the features extracted from the output of the syntactic parser, namely the lemmas, the part-of-speech tags and the grammatical relationships, while the HMM-based system uses a simple morphological rule-based classifier. Also, the former system takes the two previous labels into account, while the latter only the previous one. In order to assess the contribution of the syntactic features, we trained the CRF+syntax system excluding the features extracted from the GR output of RASP, i.e. For example, in the sample sentence of Figure 2.1 the link between the gene name ""UAS-D-mib"" and the rather strongly indicative token ""transgene"" is also obtained by adding the latter as the token following the former. In particular, since the HMM-based system is performing very well on seen gene names, for each sentence we check whether it has recognized any gene names unseen in the training data (potential unseen precision errors) or if it considered as ordinary English words any tokens not seen as such in the training data (potential unseen recall errors). If either of these is true, then we pass the sentence to the CRF+syntax system, which has better performance on unseen gene names. Such a strategy is expected to trade some of the performance of the seen gene names of the HMM-based system for improved performance on the unseen gene names by using the predictions of the CRF+syntax system. Out of the 1,220 sentences of the corpus, 759 were passed on to the CRF+syntax system for tagging. 2.7 Using annotation from the users In this section we attempt to improve the performance of the BioNER systems described in the previous sections by incorporating user feedback. as part of a pipeline which was used to preprocess full papers. They were then curated by FlyBase staff using a specialized curation interface which was developed under a usercentered approach and was shown to improve curation performance (Karamanis et al., 2007, 2008). For each article, the curators were asked to give feedback on the annotation performed by the HMM-based BioNER system. The tokens to be corrected were chosen according to the uncertainty of the system, which was estimated as the conditional entropy of the decision on the current token given the decision on the previous one. These tokens were chosen independently and no attempt was made to identify sequences of tokens to be corrected by the users. Each chosen token was highlighted and the curators had to confirm whether it was a gene name (by clicking on the ""Mark as Gene"" button) or not. They were not requested to distinguish between tokens that were gene names on their own and tokens being part of multi-token gene names. If needed, they could also modify the boundaries of the gene name by selecting the appropriate textual string. The curators provided feedback on 677 tokens, resulting in 549 sentences containing 1,668 gene names and 19,449 tokens in total. This feedback was requested once the curators had finished the curation of the paper. The data collected contained noise, since the curators were asked to make decisions on particular tokens, without correcting any other errors found in the sentences containing these tokens. As a consequence, errors that were not related to the token in question were not corrected. We add these sentences to the automatically annotated training data described in Section 2.2. By adding data from the full papers to the training material, such errors were avoided. Therefore we combine them, keeping the tagging provided by the former for sentences containing only seen gene names and the tagging of the latter for the rest. This is due to the fact that the CRF+syntax system took","['This strategy is likely to improve the performance on datasets where there are more unseen gene names, since the CRF+syntax system is substantially better than the HMM-based one on them.', 'The purpose is to provide the systems with training data from the exact domain they would be applied to, which is the full papers rather than their abstracts.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. To address this issue, the U.S. Environmental Protection Agency (EPA) and other organizations are developing chemical screening and prioritization programs. As part of these efforts, it is important to catalog, from widely dispersed sources, the toxicology information that is available. These include high-and medium-production-volume chemicals, pesticide active and inert ingredients, and drinking water contaminants. About one-quarter have been assessed in at least one highly curated toxicology evaluation database such as the U.S. EPA Toxicology Reference Database, U.S. EPA Integrated Risk Information System, and the National Toxicology Program. Several excellent surveys of this research are available (e.g., Gibbons 1998; #CITATION_TAG; Prendergast 1999; Bushman & Smith 2001).","Data sources We are developing ACToR (Aggregated Computational Toxicology Resource), which combines information for hundreds of thousands of chemicals from > 200 public sources, including the U.S. EPA, National Institutes of Health, Food and Drug Administration, corresponding agencies in Canada, Europe, and Japan, and academic sources. Data extraction ACToR contains chemical structure information; physical-chemical properties; in vitro assay data; tabular in vivo data; summary toxicology calls (e.g., a statement that a chemical is considered to be a human carcinogen); and links to online toxicology summaries. Here, we use data from ACToR to assess the toxicity data landscape for environmental chemicals.","['The main objective of this analysis is to define a list of environmental chemicals that are candidates for the U.S. EPA screening and prioritization process, and to catalog the available toxicology information.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. 3 Many observed problems with incentive systems can be attributed to imbalanced multitask incentives (#CITATION_TAG).",We applied unsupervised learning since the data sets did not have sentiment annotations. Note that unsupervised learning is a more realistic scenario than supervised learning which requires an access to a training set of sentiment-annotated data. We used SentiWordNet to establish a gold sentiment standard for the data sets and evaluate performance of Word2Vec and Doc2Vec methods.,"['In this study, we explored application of Word2Vec and Doc2Vec for sentiment analysis of clinical discharge summaries.', 'We aim to detect if there exists any underlying bias towards or against a certain disease.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. The phenomenal growth of global pharmaceutical sales and the quest for innovation are driving an unprecedented search for human test subjects, particularly in middle- and low-income countries. Our hope for medical progress increasingly depends on the willingness of the world's poor to participate in clinical drug trials. While these experiments often provide those in need with vital and previously unattainable medical resources, the outsourcing and offshoring of trials also create new problems. Moving between corporate and scientific offices in the United States and research and public health sites in Poland and Brazil, When Experiments Travel documents the complex ways that commercial medical science, with all its benefits and risks, is being integrated into local health systems and emerging drug markets. How are experiments controlled and how is drug safety ensured? When Experiments Travel challenges conventional understandings of the ethics and politics of transnational science and changes the way we think about global medicine and the new infrastructures of our lives. Several excellent surveys of this research are available (e.g., Gibbons 1998; Murphy 1999; #CITATION_TAG; Bushman & Smith 2001).",,"['In this groundbreaking book, anthropologist Adriana Petryna takes us deep into the clinical trials industry as it brings together players separated by vast economic and cultural differences.', 'Providing a unique perspective on globalized clinical trials, When Experiments Travel raises central questions: Are such trials exploitative or are they social goods?']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Biobanks and archived data sets collecting samples and data have become crucial engines of genetic and genomic research. Unresolved, however, is what responsibilities biobanks should shoulder to manage incidental findings and individual research results of potential health, reproductive, or personal importance to individual contributors (using ""biobank"" here to refer both to collections of samples and collections of data). This article reports recommendations from a 2-year project funded by the National Institutes of Health. This article specifies 10 concrete recommendations, addressing new biobanks as well as those already in existence.Genet Med 2012:14(4):361-384 This illustrates a key theme in the early theoretical literature on incentives: the tradeoff between uncontrollable risk and incentives (#CITATION_TAG; Banker & Datar 1989).","We analyze the responsibilities involved in managing the return of incidental findings and individual research results in a biobank research system (primary research or collection sites, the biobank itself, and secondary research sites).","['When reidentification of individual contributors is possible, the biobank should work to enable the biobank research system to discharge four core responsibilities to (1) clarify the criteria for evaluating findings and the roster of returnable findings, (2) analyze a particular finding in relation to this, (3) reidentify the individual contributor, and (4) recontact the contributor to offer the finding.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Introduction There is a growing need for largescale data and biobanks for biomedical research. The Radboud Biobank was conceived from the standards that were laid down in the String of Pearls Initiative (PSI), a unique partnership between the eight University Medical Centers (UMCs) in the Netherlands, that contributes to innovation in health care by facilitating biomedical research. The establishment of the RadboudBiobank creates an efficient and high quality facility for scientific research and medical innovation. Ratchet effects have received surprisingly little empirical study beyond Roy's (1952) famous description (#CITATION_TAG).","These procedures are generic and established with a view on standardization, quality and efficiency, transcending the interests of single departments. Furthermore, (quality) standards are set in the field of ICT, legal and ethical aspects, communication and distribution.","['Therefore, a central biobank facility at the Radboud university medical center, Nijmegen, the Netherlands, was established to contribute to biomedical research and innovation by creating an infrastructure for collecting, storing and managing biomaterial and associated clinical data.', 'The aim of the Radboud Biobank is to offer researchers an infrastructure wherein sub-collections are professionally secured for the long-term according to high quality standards.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. The development of genomics has dramatically expanded the scope of genetic research, and collections of genetic biosamples have proliferated in countries with active genomics research programs. In this essay, we consider a particular kind of collection, national biobanks. National biobanks are often presented by advocates as an economic ''resource'' that will be used by both basic researchers and academic biologists, as well as by pharmaceutical diagnostic and clinical genomics companies. Although national biobanks have been the subject of intense interest in recent social science literature, most prior work on this topic focuses either on bioethical issues related to biobanks, such as the question of informed consent, or on the possibilities for scientific citizenship that they make possible. Relative evaluation may also distort incentives if the employee can take actions to affect the group against which he is compared or if he can cooperate with or sabotage colleagues (#CITATION_TAG).","We emphasize, by contrast, the economic aspect of biobanks, focusing specifically on the way in which national biobanks create biovalue.","['Our emphasis on the economic aspect of biobanks allows us to recognize the importance of what we call clinical labor--that is, the regularized, embodied work that members of the national population are expected to perform in their role as biobank participants--in the creation of biovalue through biobanks.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. A nerve cell receives multiple inputs from upstream neurons by way of its synapses. Neuron processing functions are thus influenced by changes in the biophysical properties of the synapse, such as long-term potentiation (LTP) or depression (LTD). One major obstacle is the high dimensionality of the neuronal input-output space, which makes it unfeasible to perform a thorough computational analysis of a neuron with multiple synaptic inputs. Granule cells have a small dendritic tree (on average, they receive only four mossy fiber afferents), which greatly bounds the input combinatorial space, reducing the complexity of information-theoretic calculations. These selective mechanisms may have important consequences on the encoding of cerebellar mossy fiber inputs and the plasticity and computation at the next circuit stage, including the parallel fiberPurkinje cell synapses. Compensation systems have several roles beyond creating incentives (e.g., #CITATION_TAG; Ittner & Larcker 2002).",Numerical simulations and LTP experiments quantified how changes in neurotransmitter release probability (p) modulated information transmission of a cerebellar granule cell.,"['In this work, information theory was employed to characterize the information transmission of a cerebellar granule cell over a region of its excitatory input space following synaptic changes.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Understanding the transmission of sensory information at individual synaptic connections requires knowledge of the properties of presynaptic terminals and their patterns of firing evoked by sensory stimuli. Such information has been difficult to obtain because of the small size and inaccessibility of nerve terminals in the central nervous system. #CITATION_TAG find employees place a large value (compensating differential) on the extent to which they trust management.","Here we show, by making direct patch-clamp recordings in vivo from cerebellar mossy fibre boutons--the primary source of synaptic input to the cerebellar cortex--that sensory stimulation can produce bursts of spikes in single boutons at very high instantaneous firing frequencies (more than 700 Hz).",['This endows the cerebellar mossy fibre system with remarkable sensitivity and high fidelity in the transmission of sensory information.']
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. The cerebellar granular layer has been suggested to perform a complex spatiotemporal reconfiguration of incoming mossy fiber signals. This characteristic connectivity has recently been investigated in great detail and been correlated with specific functional properties of these neurons. Important advances have also been made in terms of determining the membrane and synaptic properties of the neuron, and clarifying the mechanisms of activation by input bursts. #CITATION_TAG find that selection improves when a firm uses performance measures that are less distorted and have less uncontrollable risk.","Central to this role is the inhibitory action exerted by Golgi cells over granule cells: Golgi cells inhibit granule cells through both feedforward and feedback inhibitory loops and generate a broad lateral inhibition that extends beyond the afferent synaptic field. These include theta-frequency pacemaking, network entrainment into coherent oscillations and phase resetting.","['These investigations have highlighted the critical role of Golgi cells in: generating dense clusters of granule cell activity organized in center-surround structures, implementing combinatorial operations on multiple mossy fiber inputs, regulating transmission gain, and cut-off frequency, controlling spike timing and burst transmission, and determining the sign, intensity and duration of long-term synaptic plasticity at the mossy fiber-granule cell relay.', 'This review considers recent advances in the field, highlighting the functional implications of Golgi cells for granular layer network computation and indicating new challenges for cerebellar research.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Long-term potentiation (LTP) is a synaptic change supposed to provide the cellular basis for learning and memory in brain neuronal circuits. Although specific LTP expression mechanisms could be critical to determine the dynamics of repetitive neurotransmission, this important issue remained largely unexplored. A mathematical model of mossy fiber-granule cell neurotransmission showed that increasing release probability efficiently modulated the first-spike delay. Several excellent surveys of this research are available (e.g., Gibbons 1998; Murphy 1999; Prendergast 1999; #CITATION_TAG).","In agreement with a presynaptic expression caused by increased release probability, similar changes were observed by raising extracellular [Ca(2+)]. Glutamate spillover, by causing tonic NMDA and AMPA receptor activation, accelerated excitatory postsynaptic potential (EPSP) temporal summation and maintained a sustained spike discharge. Independent regulation of spike burst initiation and frequency during LTP may provide mechanisms for temporal recoding and gain control of afferent signals at the input stage of cerebellar cortex.","['In this paper, we have performed whole cell patch-clamp recordings of mossy fiber-granule cell LTP in acute rat cerebellar slices and studied its computational implications with a mathematical model.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Information Theory enables the quantification of how much information a neuronal response carries about external stimuli and is hence a natural analytic framework for studying neural coding. The main difficulty in its practical application to spike train analysis is that estimates of neuronal information from experimental data are prone to a systematic error (called ""bias""). This bias is an inevitable consequence of the limited number of stimulus-response samples that it is possible to record in a real experiment. An interesting implication of distortion and manipulability is that numeric performance measures may degrade over time (Courty & Marschke 2004, #CITATION_TAG.","In this paper, we first explain the origin and the implications of the bias problem in spike train analysis. We then review and evaluate some recent general-purpose methods to correct for sampling bias: the Panzeri-Treves, Quadratic Extrapolation, Best Universal Bound, Nemenman-Shafee-Bialek procedures, and a recently proposed shuffling bias reduction procedure. This provides information estimates with acceptable variance and which are unbiased even when the number of trials per stimulus is as small as the number of possible discrete neuronal responses.",['Our main recommendation is to estimate information using the shuffling bias reduction procedure in combination with one of the other four general purpose bias reduction procedures mentioned in the preceding text.']
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Objective measures of performance are seldom perfect. In response, incentive contracts often include important subjective components that mitigate incentive distortions caused by imperfect objective measures. Naturally, objective and subjective measures often are substitutes, sometimes strikingly so: we show that if objective measures are sufficiently close to perfect then no implicit contracts are feasible (because the firm's fallback position after reneging on an implicit contact is too attractive). Finally, subjective evaluations have their own form of uncontrollable risk for the employee: they are difficult to verify and enforce contractually, so they require relational contracting and adequate trust of the supervisor (#CITATION_TAG).",,['This paper explores the combined use of subjective and objective performance measures in (respectively) implicit and explicit incentive contracts.']
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. An additional argument is suggested by #CITATION_TAG, who also provide evidence of a strong positive correlation between delegation and multitasking (but do not have data on incentives).","The study sample consisted of 1600 purposively selected SSS III students from 15 selected secondary schools. A questionnaire with four sections was developed and administered on the subjects. It is a test battery with section A containing the demographic data and the remaining three sections containing twenty items each. The collected data were analyzed using simple percentages, Pearson Product Moment Correlation and chi-square statistics to test the three hypotheses generated in the study.","[""The study investigated the relationship/effect of students' perception of teachers' knowledge of subject matter, attitude to work and teaching skills on students' academic performance.""]"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. This effect would be most severe for employees performing below T, because reducing effort would push them further from T, whereas the opposite would occur for those above T. For these reasons, supervisors may give less informative evaluations, and particularly avoid giving negative feedback (#CITATION_TAG).","METHODS A cross-sectional survey of adolescents aged 14-19 years studying at different educational institutions of Karachi, Pakistan was conducted through multistage sampling on a pre-tested self-administered questionnaire. They were divided into high, middle and lower socioeconomic strata on the basis of monthly fee structure.","['OBJECTIVE To study the impact of educational intervention on knowledge, attitude and practices with regard to water pipe smoking among adolescents (14-19 years old) in Karachi.']"
"Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. The poor knowledge of epilepsy among traditional healers is due to cultural prejudices and environment. The resultant deep-rooted misconceptions and myths negatively affect the attitudes and encourage traditional care with high morbidity and mortality. There were prevalent negative attitudes and perception about epilepsy among the healers, as 146 (88.0%) of them viewed it as contagious; 149 (89.8%) would decline either marrying or eating with epileptic persons. Although traditional healers are frequently involved in the care of epilepsy in our environment, they have little or no scientific knowledge about the condition. Adequate knowledge about epilepsy is essential for diagnosis and treatment. Only a small empirical literature exists on the use of subjective evaluation or discretion in incentive systems (e.g., #CITATION_TAG; Ittner, Larcker & Meyer 2003; Murphy & Oyer 2003; Gibbs et al. 2004 Gibbs et al., 2009, presumably because quantifying the concepts is difficult.","One hundred and seventy three traditional healers from villages/communities in Uyo were assessed for knowledge; attitude and perception of epilepsy, using an interviewer assisted Attitude Questionnaire.","['The objectives of the study were to assess knowledge of epilepsy among traditional healers and to determine the modalities used in the care.', 'Therefore, there is need to improve the knowledge about epilepsy in order to encourage positive attitudes and care.']"
"Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. The Stern Review on the Economics of Climate Change concluded that there can be ""no doubt"" the economic risks of business-as-usual (BAU) climate change are ""very severe"" [Stern, 2006. The Economics of Climate Change. Subsequently, a number of critiques have appeared, arguing that discounting is the principal explanation for this discrepancy. Together, the issues of risk and uncertainty on the one hand, and 'dangerous' climate change on the other, raise very strongly questions about the limits of a welfare-economic approach, where the loss of natural capital might be irreversible and impossible to compensate. There will always be an imperative to carry out integrated assessment modelling, bringing together scientific 'fact' and value judgement systematically. Ironically, the Stern Review is one of those voices. They also deal with intergenerational fairness, income regional distribution and, some of them, at least to a certain extent, risk and uncertainty management (#CITATION_TAG).","A fixation with cost-benefit analysis misses the point that arguments for stabilisation should, and are, built on broader foundations","[""Discounting is important, but in this paper we emphasise that how one approaches the economics of risk and uncertainty, and how one attempts to model the very closely related issue of low-probability/high-damage scenarios (which we connect to the recent discussion of 'dangerous' climate change), can matter just as much.""]"
"Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. BackgroundSexually Transmitted Infections (STI's), including HIV (Human Immunodeficiency Virus) mainly affects sexually active young people. Young adults aged 15-29 years, account for 32% of AIDS (Acquired Immunodeficiency Syndrome) cases reported in India and the number of young women living with HIV/AIDS is twice that of young men. Inevitably, IAMs rely on a series of simplifying assumptions, 3 using highly aggregated variables and data (#CITATION_TAG; Patt et al., 2010), and the limitations of the methodology have been noted (e.g. Warren et al., 2006, Dietz et al., 2007.",,"[""The aim of the study was to evaluate adolescent school girls' knowledge, perceptions and attitudes towards STIs/HIV and safer sex practice and sex education and to explore their current sexual behaviour in India.MethodsA cross sectional study was carried out in 2007 in South Delhi, India to investigate the perception, knowledge and attitude of adolescent urban schoolgirls towards sexually transmitted Infections (STIs), HIV/AIDS, safer sex practice and sex education.""]"
"Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. A future sustainable electricity supply will be characterised by distributed electricity generation structure and will be based on the integration and use of renewable energy sources. Since many renewable energy sources like wind energy and solar energy are intermittent or vary in intensity throughout the day, the balancing problem between energy supply and energy consumption will increase. With an increased penetration of renewable energy sources into the grids it could become necessary to integrate more energy storage simultaneously with further renewable energy integration. PAGE09 uses a simple economic module (Hope et al., 1993; #CITATION_TAG; Hope, 2006; Hope, 2008; Hope, 2011) and expands it to consider climate issues and the linkages between the economic and the climate systems through some stylized equations within the climate module.",,['This paper discusses the necessity of energy storage integration into the electricity supplies with increased renewable energy penetration and how the balancing problem between supply and demand could be solved.']
"Identifying ancestry along each chromosome in admixed individuals provides a wealth of information for understanding the population genetic history of admixture events and is valuable for admixture mapping and identifying recent targets of selection. We present PCAdmix (available at https://sites.google.com/site/ pcadmix/home), a Principal Componentsbased algorithm for determining ancestry along each chromosome from a high-density, genome-wide set of phased single-nucleotide polymorphism (SNP) genotypes of admixed individuals. We describe the location, allele frequency and local haplotype structure of approximately 15 million single nucleotide polymorphisms, 1 million short insertions and deletions, and 20,000 structural variants, most of which were previously undescribed. Phasing errors are becoming less common as phasing methods improve and efforts such as the 1,000 Genomes Project (#CITATION_TAG) produce larger pools of genotypes that can be used as references during phasing; however, it would be valuable to extend PCAdmix to unphased data, as well as to investigate the effectiveness of iterative phasing and ancestry assignment, employing reference panels conditional on estimated local ancestry.","We undertook three projects: low-coverage whole-genome sequencing of 179 individuals from four populations; high-coverage sequencing of two mother-father-child trios; and exon-targeted sequencing of 697 individuals from seven populations. From the two trios, we directly estimate the rate of de novo germline base substitution mutations to be approximately 10[superscript -8] per base pair per generation.","['April 1, 2011The 1000 Genomes Project aims to provide a deep characterization of human genome sequence variation as a foundation for investigating the relationship between genotype and phenotype.']"
"Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. More specifically, we utilize representation learning methods (#CITATION_TAG), where words or phrases are embedded into the same vector space.","The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.",['We propose two novel model architectures for computing continuous vector representations of words from very large data sets.']
"Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. Extracting data elements such as study descriptors from publication full texts is an essential step in a number of tasks including systematic review preparation (#CITATION_TAG), construction of reference databases (Kleinstreuer et al., 2016), and knowledge discovery (Smalheiser, 2012).","We estimate and partition genetic variation for height, body mass index (BMI), von Willebrand factor and QT interval (QTi) using 586,898 SNPs genotyped on 11,586 unrelated individuals. We show that the variance explained by each chromosome is proportional to its length, and that SNPs in or near genes explain more variation than SNPs between genes.",['We propose a new approach to estimate variation due to cryptic relatedness and population stratification.']
"Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. However, as it is very difficult and expensive to obtain annotated material for languages different from English, we only consider unsupervised approaches, where no annotated training set is necessary. (#CITATION_TAG) have introduced a system for unsupervised extraction of entities and relations between these entities from clinical texts written in Italian, which utilized a thesaurus for extraction of entities and clustering methods for relation extraction.","We therefore propose a complete system that is structured in two steps. In the first one domain entities are extracted from the clinical records by means of a metathesaurus and standard natural language processing tools. The second step attempts to discover relations between the entity pairs extracted from the whole set of clinical records. For this last step we investigate the performance of unsupervised methods such as clustering in the space of entity pairs, represented by an ad hoc feature vector. The resulting clusters are then automatically labelled by using the most significant features.",['This paper proposes and discusses the use of text mining techniques for the extraction of information from clinical records written in Italian.']
"Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. This book represents the first asymptotic analysis, via completely integrable techniques, of the initial value problem for the focusing nonlinear Schrodinger equation in the semiclassical asymptotic regime. This problem is a key model in nonlinear optical physics and has increasingly important applications in the telecommunications industry. (#CITATION_TAG) have shown that an important feature of Word2Vec embeddings is that similar words will have similar vectors because they appear in similar contexts.","The authors exploit complete integrability to establish pointwise asymptotics for this problem's solution in the semiclassical regime and explicit integration for the underlying nonlinear, elliptic, partial differential equations suspected of governing the semiclassical behavior. To achieve this, the authors have extended the reach of two powerful analytical techniques that have arisen through the asymptotic analysis of integrable systems: the Lax-Levermore-Venakides variational approach to singular limits in integrable systems, and Deift and Zhou's nonlinear Steepest-Descent/Stationary Phase method for the analysis of Riemann-Hilbert problems. In particular, they introduce a systematic procedure for handling certain Riemann-Hilbert problems with poles accumulating on curves in the plane.","['In doing so they also aim to explain the observed gradient catastrophe for the underlying nonlinear elliptic partial differential equations, and to set forth a detailed, pointwise asymptotic description of the violent oscillations that emerge following the gradient catastrophe.', 'This book, which includes an appendix on the use of the Fredholm theory for Riemann-Hilbert problems in the Holder class, is intended for researchers and graduate students of applied mathematics and analysis, especially those with an interest in integrable systems, nonlinear waves, or complex analysis.']"
"Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. We describe the integrable structure of solutions of the nonlinear Schrodinger (NLS) equation under periodic and quasiperiodic boundary conditions. linearized instabilities), and the effects of slow modulations in space and time, perhaps in the presence of external perturbations. Distant supervision has been applied to relation extraction (#CITATION_TAG), extraction of gene interactions (Mallory et al., 2015), PPI extraction, and identification of PICO elements (Wallace et al., 2016).",We focus on those aspects of the exact theory which reveal the behavior of these solutions under perturbations of initial conditions (i.e.,['Our purpose here is to document the corresponding features of NLS solutions; the rigorous analysis that underlies this paper derives from [1-7] and will appear in the thesis of Lee [8].']
"Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. Mencion Internacional en el titulo de doctorThe main hypothesis of this PhD dissertation is that novel Deep Learning algorithms can outperform classical Machine Learning methods for the task of Information Extraction in the Biomedical Domain. Contrary to classical systems, Deep Learning models can learn the representation of the data automatically without an expert domain knowledge and avoid the tedious and time-consuming task of defining relevant features. A Drug-Drug Interaction (DDI), which is an essential subset of Adverse Drug Reaction (ADR),  represents the alterations in the effects of drugs that were taken simultaneously. The early  recognition of interacting drugs is a vital process that prevents serious health problems that can  cause death in the worst cases. Health-care professionals and researchers in this domain find the  task of discovering information about these incidents very challenging due to the vast number  of pharmacovigilance documents. In the present document, the DDI corpus, which is an annotated  dataset of DDIs, is used with Deep Learning architectures without any external information  for the tasks of Name Entity Recognition and Relation Extraction in order to validate the  hypothesis. Many methods for biomedical data annotation and extraction exist which utilize labeled data and supervised learning approaches (#CITATION_TAG (Gonzalez et al., 2015) provided a good overview of a number of these methods); however, unsupervised approaches in this area are much scarcer.","For this reason, several shared tasks and datasets have been  developed in order to solve this issue with automated annotation systems with the capability  to extract this information.","['As a final goal, a complete architecture that  covers the two tasks is developed to structure the named entities and their relationships from  raw pharmacological texts.This thesis has been supported by:  Pre-doctoral research training scholarship of the Carlos III University of Madrid (PIF UC3M 02-1415) for four years.']"
"Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. More recently, (#CITATION_TAG) have utilized Word2Vec and Doc2Vec embeddings for unsupervised sentiment classification in medical discharge summaries.",We applied unsupervised learning since the data sets did not have sentiment annotations. Note that unsupervised learning is a more realistic scenario than supervised learning which requires an access to a training set of sentiment-annotated data. We used SentiWordNet to establish a gold sentiment standard for the data sets and evaluate performance of Word2Vec and Doc2Vec methods.,"['In this study, we explored application of Word2Vec and Doc2Vec for sentiment analysis of clinical discharge summaries.', 'We aim to detect if there exists any underlying bias towards or against a certain disease.']"
"Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. Named entity recognition is a crucial component of biomedical natural language processing, enabling information extraction and ultimately reasoning over and knowledge discovery from text. Much progress has been made in the design of rule-based and supervised tools, but they are often genre and task dependent. As such, adapting them to different genres of text or identifying new types of entities requires major effort in re-annotation or rule development. One such approach has been introduced by (#CITATION_TAG), who have proposed a model for unsupervised Named En-tity Recognition.",A noun phrase chunker followed by a filter based on inverse document frequency extracts candidate entities from free text. Classification of candidate entities into categories of interest is carried out by leveraging principles from distributional semantics. Detailed error analysis provides a road map for future work.,"['In this paper, we propose an unsupervised approach to extracting named entities from biomedical text.', 'We describe a stepwise solution to tackle the challenges of entity boundary detection and entity type classification without relying on any handcrafted rules, heuristics, or annotated data.']"
"Biobanking, the large-scale, systematic collection of data and tissue for open-ended research purposes, is on the rise, particularly in clinical research. However, the positioning of biobanking infrastructures and transfer of tissue and data between research and care is not an innocuous go-between. Instead, it involves changes in both domains and raises issues about how distinctions between research and care are drawn and policed. Based on an analysis of the emergence and development of clinical biobanking in the Netherlands, this article explores how processes of bio-objectification associated with biobanking arise, redefining the ways in which distinctions between research and clinical care are governed. Biobanks and archived data sets collecting samples and data have become crucial engines of genetic and genomic research. Unresolved, however, is what responsibilities biobanks should shoulder to manage incidental findings and individual research results of potential health, reproductive, or personal importance to individual contributors (using ""biobank"" here to refer both to collections of samples and collections of data). This article reports recommendations from a 2-year project funded by the National Institutes of Health. For instance, Wolf and others consider that ""findings that are analytically valid, reveal an established and substantial risk of a serious health condition, and are clinically actionable should generally be offered to consenting contributors"" (#CITATION_TAG).","We analyze the responsibilities involved in managing the return of incidental findings and individual research results in a biobank research system (primary research or collection sites, the biobank itself, and secondary research sites).","['When reidentification of individual contributors is possible, the biobank should work to enable the biobank research system to discharge four core responsibilities to (1) clarify the criteria for evaluating findings and the roster of returnable findings, (2) analyze a particular finding in relation to this, (3) reidentify the individual contributor, and (4) recontact the contributor to offer the finding.', 'This article specifies 10 concrete recommendations, addressing new biobanks as well as those already in existence.']"
"Biobanking, the large-scale, systematic collection of data and tissue for open-ended research purposes, is on the rise, particularly in clinical research. However, the positioning of biobanking infrastructures and transfer of tissue and data between research and care is not an innocuous go-between. Instead, it involves changes in both domains and raises issues about how distinctions between research and care are drawn and policed. Based on an analysis of the emergence and development of clinical biobanking in the Netherlands, this article explores how processes of bio-objectification associated with biobanking arise, redefining the ways in which distinctions between research and clinical care are governed. The development of genomics has dramatically expanded the scope of genetic research, and collections of genetic biosamples have proliferated in countries with active genomics research programs. In this essay, we consider a particular kind of collection, national biobanks. National biobanks are often presented by advocates as an economic ""resource"" that will be used by both basic researchers and academic biologists, as well as by pharmaceutical diagnostic and clinical genomics companies. Although national biobanks have been the subject of intense interest in recent social science literature, most prior work on this topic focuses either on bioethical issues related to biobanks, such as the question of informed consent, or on the possibilities for scientific citizenship that they make possible. At the same time, the labour performed by most donors is also minimized and made invisible by integrating it into routine aspects of care (Mitchell and Waldby 2010; #CITATION_TAG).","We emphasize, by contrast, the economic aspect of biobanks, focusing specifically on the way in which national biobanks create biovalue.","['Our emphasis on the economic aspect of biobanks allows us to recognize the importance of what we call clinical labor-that is, the regularized, embodied work that members of the national population are expected to perform in their role as biobank participants-in the creation of biovalue through biobanks.']"
"Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. A nerve cell receives multiple inputs from upstream neurons by way of its synapses. Neuron processing functions are thus influenced by changes in the biophysical properties of the synapse, such as long-term potentiation (LTP) or depression (LTD). One major obstacle is the high dimensionality of the neuronal input-output space, which makes it unfeasible to perform a thorough computational analysis of a neuron with multiple synaptic inputs. Granule cells have a small dendritic tree (on average, they receive only four mossy fiber afferents), which greatly bounds the input combinatorial space, reducing the complexity of information-theoretic calculations. These selective mechanisms may have important consequences on the encoding of cerebellar mossy fiber inputs and the plasticity and computation at the next circuit stage, including the parallel fiber-Purkinje cell synapses This peculiarity of GrCs response patterns lead to a low output variability [#CITATION_TAG] which, in turn, greatly reduces the complexity of calculations and the duration of recording sessions.",Numerical simulations and LTP experiments quantified how changes in neurotransmitter release probability (p) modulated information transmission of a cerebellar granule cell.,"['In this work, information theory was employed to characterize the information transmission of a cerebellar granule cell over a region of its excitatory input space following synaptic changes.']"
"Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. Understanding the transmission of sensory information at individual synaptic connections requires knowledge of the properties of presynaptic terminals and their patterns of firing evoked by sensory stimuli. Such information has been difficult to obtain because of the small size and inaccessibility of nerve terminals in the central nervous system. The cerebellar granular layer microcircuit allows to obtain MI quantification between mf and GrCs because: i) mf inputs are conveyed through high-frequency bursts (up to 500Hz [38, #CITATION_TAG]) that have been well characterized, enabling a proper representation of the input space [30, 40]; ii) in cerebellar slices GrCs are silent when mf are severed, reducing spontaneous firing and output variability; iii) in response to mf activation, GrCs generate only few spikes (typically less than 3-4) with a maximum output frequency of 100-150 Hz, reducing the output response space.","Here we show, by making direct patch-clamp recordings in vivo from cerebellar mossy fibre boutons-the primary source of synaptic input to the cerebellar cortex-that sensory stimulation can produce bursts of spikes in single boutons at very high instantaneous firing frequencies (more than 700 Hz).",['This endows the cerebellar mossy fibre system with remarkable sensitivity and high fidelity in the transmission of sensory information.']
"Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. The cerebellar granular layer has been suggested to perform a complex spatiotemporal reconfiguration of incoming mossy fiber signals. This characteristic connectivity has recently been investigated in great detail and been correlated with specific functional properties of these neurons. Important advances have also been made in terms of determining the membrane and synaptic properties of the neuron, and clarifying the mechanisms of activation by input bursts. In response to mossy fibers (mf) inputs, cerebellar (GrCs) respond with stereotyped patterns displaying a limited number of spikes (typically two or less [19, 20]) which are confined in a restricted time window, by the intervention of Golgi cells inhibition [21, #CITATION_TAG].","Central to this role is the inhibitory action exerted by Golgi cells over granule cells: Golgi cells inhibit granule cells through both feedforward and feedback inhibitory loops and generate a broad lateral inhibition that extends beyond the afferent synaptic field. These include theta-frequency pacemaking, network entrainment into coherent oscillations and phase resetting.","['These investigations have highlighted the critical role of Golgi cells in: generating dense clusters of granule cell activity organized in center-surround structures, implementing combinatorial operations on multiple mossy fiber inputs, regulating transmission gain, and cut-off frequency, controlling spike timing and burst transmission, and determining the sign, intensity and duration of long-term synaptic plasticity at the mossy fiber-granule cell relay.', 'This review considers recent advances in the field, highlighting the functional implications of Golgi cells for granular layer network computation and indicating new challenges for cerebellar research.']"
"Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. Long-term potentiation (LTP) is a synaptic change supposed to provide the cellular basis for learning and memory in brain neuronal circuits. Although specific LTP expression mechanisms could be critical to determine the dynamics of repetitive neurotransmission, this important issue remained largely unexplored. A mathematical model of mossy fiber-granule cell neurotransmission showed that increasing release probability efficiently modulated the first-spike delay. Independent regulation of spike burst initiation and frequency during LTP may provide mechanisms for temporal recoding and gain control of afferent signals at the input stage of cerebellar cortex Whole-cell recordings from GrCs were obtained with patch-clamp technique [20, 29, #CITATION_TAG] by using an Axopatch 200B amplifier (Molecular Devices, Union City, CA, USA) (-3dB; cut-off frequency = 2 kHz).","In agreement with a presynaptic expression caused by increased release probability, similar changes were observed by raising extracellular [Ca(2+)]. Glutamate spillover, by causing tonic NMDA and AMPA receptor activation, accelerated excitatory postsynaptic potential (EPSP) temporal summation and maintained a sustained spike discharge.","['In this paper, we have performed whole cell patch-clamp recordings of mossy fiber-granule cell LTP in acute rat cerebellar slices and studied its computational implications with a mathematical model.']"
"Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. Information Theory enables the quantification of how much information a neuronal response carries about external stimuli and is hence a natural analytic framework for studying neural coding. The main difficulty in its practical application to spike train analysis is that estimates of neuronal information from experimental data are prone to a systematic error (called ""bias""). This bias is an inevitable consequence of the limited number of stimulus-response samples that it is possible to record in a real experiment. MI descends directly from response entropy and noise entropy [13], which are correlated to the variability of responses to separate inputs [13] or to the same input [14] [15] [#CITATION_TAG], respectively.","In this paper, we first explain the origin and the implications of the bias problem in spike train analysis. We then review and evaluate some recent general-purpose methods to correct for sampling bias: the Panzeri-Treves, Quadratic Extrapolation, Best Universal Bound, Nemenman-Shafee-Bialek procedures, and a recently proposed shuffling bias reduction procedure. This provides information estimates with acceptable variance and which are unbiased even when the number of trials per stimulus is as small as the number of possible discrete neuronal responses.",['Our main recommendation is to estimate information using the shuffling bias reduction procedure in combination with one of the other four general purpose bias reduction procedures mentioned in the preceding text.']
"Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. We study the general structure of formal perturbative solutions to the Hamiltonian perturbations of spatially one-dimensional systems of hyperbolic PDEs vt + [ph(v)]x = 0. Under certain genericity assumptions it is proved that any bi-Hamiltonian perturbation can be eliminated in all orders of the perturbative expansion by a change of coordinates on the infinite jet space depending rationally on the derivatives. Total charge transfer was calculated by measuring IPSCs area [#CITATION_TAG].","We also describe, following [35], the invariants of such bi-Hamiltonian structures with respect to the group of Miura-type transformations depending polynomially on the derivatives.","['The main tool is in constructing the so-called quasi-Miura transformation of jet coordinates, eliminating an arbitrary deformation of a semisimple bi-Hamiltonian structure of hydrodynamic type (the quasi-triviality theorem).']"
"Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. For the systems of order one or two we describe the local structure of singularities of a generic solution to the unperturbed system near the point of ""gradient catastrophe"" in terms of standard objects of the classical singularity theory; we argue that their perturbed companions must be given by certain special solutions of Painleve' equations and their generalizations. MI descends directly from response entropy and noise entropy [#CITATION_TAG], which are correlated to the variability of responses to separate inputs [13] or to the same input [14] [15] [16], respectively.",,"['Our main goal is the comparative study of singularities of solutions to the systems of first order quasilinear PDEs and their perturbations containing higher derivatives.', 'The study is focused on the subclass of Hamiltonian PDEs with one spatial dimension.']"
"The chronological development of universities ranges from the state at which universities are considered to be knowledge accumulators followed by knowledge factories and finally the knowledge hubs. The various national systems of innovations are aligned with the knowledge hubs and it involves a substantial amount of research activities. The newly established Mbeya University of Science and Technology is recognised as a knowledge hub in some particular niches. However, there are a limited number of research activities conducted at the university and this study is an attempt to identify the reasons that limit research activities. Universities have assumed an expanded role in science and technology-based economic development that has become of interest to catch-up regions as well as to leading innovation locales. Central to the transformation of Georgia Tech as a knowledge hub is the emergence of new institutional leadership, programs, organizational forms and boundary-spanning roles that meditate among academic, educational, entrepreneurial, venture capital, industrial, and public spheres. The evolving university context and missions discussed by #CITATION_TAG show a timeline of three models of universities.",,"['This paper examines how the role of the university has evolved from performing conventional research and education functions to serving as an innovation-promoting knowledge hub though the case of Georgia Institute of Technology (Georgia Tech).', 'This case is discussed in the context of state efforts to shift the region from an agricultural to an industrial to an innovation-driven economy.']"
"The chronological development of universities ranges from the state at which universities are considered to be knowledge accumulators followed by knowledge factories and finally the knowledge hubs. The various national systems of innovations are aligned with the knowledge hubs and it involves a substantial amount of research activities. The newly established Mbeya University of Science and Technology is recognised as a knowledge hub in some particular niches. However, there are a limited number of research activities conducted at the university and this study is an attempt to identify the reasons that limit research activities. The rapid growth of the literature on neuroimaging in humans has led to major advances in our understanding of human brain function but has also made it increasingly difficult to aggregate and synthesize neuroimaging findings. Several studies that have involved the knowledge, attitude and perception of various issues have been conducted worldwide (Abasiubong et al., 2009; Adediwura & Tayo, 2007; #CITATION_TAG; Erhum et al., 2008; Tjakraatmadja et al., 2008).",,"['Here we describe and validate an automated brain-mapping framework that uses text-mining, meta-analysis and machine-learning techniques to generate a large database of mappings between neural and cognitive states.']"
"The chronological development of universities ranges from the state at which universities are considered to be knowledge accumulators followed by knowledge factories and finally the knowledge hubs. The various national systems of innovations are aligned with the knowledge hubs and it involves a substantial amount of research activities. The newly established Mbeya University of Science and Technology is recognised as a knowledge hub in some particular niches. However, there are a limited number of research activities conducted at the university and this study is an attempt to identify the reasons that limit research activities. The poor knowledge of epilepsy among traditional healers is due to cultural prejudices and environment. The resultant deep-rooted misconceptions and myths negatively affect the attitudes and encourage traditional care with high morbidity and mortality. There were prevalent negative attitudes and perception about epilepsy among the healers, as 146 (88.0%) of them viewed it as contagious; 149 (89.8%) would decline either marrying or eating with epileptic persons. Although traditional healers are frequently involved in the care of epilepsy in our environment, they have little or no scientific knowledge about the condition. Adequate knowledge about epilepsy is essential for diagnosis and treatment. Several studies that have involved the knowledge, attitude and perception of various issues have been conducted worldwide (#CITATION_TAG; Adediwura & Tayo, 2007; Anjum et al., 2008; Erhum et al., 2008; Tjakraatmadja et al., 2008).","One hundred and seventy three traditional healers from villages/communities in Uyo were assessed for knowledge; attitude and perception of epilepsy, using an interviewer assisted Attitude Questionnaire.","['The objectives of the study were to assess knowledge of epilepsy among traditional healers and to determine the modalities used in the care.', 'Therefore, there is need to improve the knowledge about epilepsy in order to encourage positive attitudes and care.']"
"The chronological development of universities ranges from the state at which universities are considered to be knowledge accumulators followed by knowledge factories and finally the knowledge hubs. The various national systems of innovations are aligned with the knowledge hubs and it involves a substantial amount of research activities. The newly established Mbeya University of Science and Technology is recognised as a knowledge hub in some particular niches. However, there are a limited number of research activities conducted at the university and this study is an attempt to identify the reasons that limit research activities. This issue of the Malta Medical Journal contains a historical perspective on medical publications in Malta over the years and it is a tribute to the medical community that over the last one hundred and seventy years, dedicated members of that profession have published articles of relevance to the practice of medicine in the Maltese Islands. On the other hand, in academic arena as discussed in several studies (#CITATION_TAG; Jones, 1997; Ali, 2012) the phrase ""Publish or perish"" has been used to researchers and academicians to describe the need to publish their research works.",,"['The aim has been to highlight problems particular to the epidemiology, pathophysiology and management of disease endemic in Malta.peer-reviewe']"
"The chronological development of universities ranges from the state at which universities are considered to be knowledge accumulators followed by knowledge factories and finally the knowledge hubs. The various national systems of innovations are aligned with the knowledge hubs and it involves a substantial amount of research activities. The newly established Mbeya University of Science and Technology is recognised as a knowledge hub in some particular niches. However, there are a limited number of research activities conducted at the university and this study is an attempt to identify the reasons that limit research activities. Sexually Transmitted Infections (STI's), including HIV (Human Immunodeficiency Virus) mainly affects sexually active young people. Young adults aged 15-29 years, account for 32% of AIDS (Acquired Immunodeficiency Syndrome) cases reported in India and the number of young women living with HIV/AIDS is twice that of young men. About 30% of respondents considered HIV/AIDS could be cured, 49% felt that condoms should not be available to youth, 41% were confused about whether the contraceptive pill could protect against HIV infection and 32% thought it should only be taken by married women.Though controversial, there is an immense need to implement gender-based sex education regarding STIs, safe sex options and contraceptives in schools in India. For instance, in India the knowledge, perception, attitude of adolescent girls towards STIs/HIV and safer sex education were done and indicated the need to implement gender-based sex education (#CITATION_TAG).",the self-administered questionnaire was completed by 251 female students from two senior secondary schools.More than one third of students in this study had no accurate understanding about the signs and symptoms of STIs other than HIV/AIDS.,"[""The aim of the study was to evaluate adolescent school girls' knowledge, perceptions and attitudes towards STIs/HIV and safer sex practice and sex education and to explore their current sexual behaviour in India.A cross sectional study was carried out in 2007 in South Delhi, India to investigate the perception, knowledge and attitude of adolescent urban schoolgirls towards sexually transmitted Infections (STIs), HIV/AIDS, safer sex practice and sex education.""]"
"Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. Rational models of cognition typically consider the abstract computational problems posed by the environment, assuming that people are capable of optimally solving those problems. DSM can further provide a means of accommodating growing power generation from fluctuating renewable sources (#CITATION_TAG) and may also help to address carbon emissions constraints (Bergaentzlé et al. 2014).","This differs from more traditional formal models of cognition, which focus on the psychological processes responsible for behavior. In particular, we argue that Monte Carlo methods provide a source of rational process models that connect optimal solutions to psychological processes. We support this argument through a detailed example, applying this approach to Anderson's (1990, 1991) rational model of categorization (RMC), which involves a particularly challenging computational problem. Drawing on a connection between the RMC and ideas from nonparametric Bayesian statistics, we propose 2 alternative algorithms for approximate inference in this model. The algorithms we consider include Gibbs sampling, a procedure appropriate when all stimuli are presented simultaneously, and particle filters, which sequentially approximate the posterior distribution with a small number of samples that are updated as new data become available.","['A basic challenge for rational models is thus explaining how optimal solutions can be approximated by psychological processes.', 'We outline a general strategy for answering this question, namely to explore the psychological plausibility of approximation algorithms developed in computer science and statistics.']"
"Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. Demand response (DR) measures typically aim at an improved utilization of power plant and grid capacities. In energy systems mainly relying on photovoltaic and wind power, DR may furthermore contribute to system stability and increase the renewable energy share. EPRI ( 2009) present an extensive review for the U.S., and #CITATION_TAG carries out a comprehensive comparative study on DSM potentials for 40 European countries.",Special attention is given to temporal availability and geographic distribution of flexible loads.,"['In this paper, an assessment of the theoretical DR potential in Europe is presented.']"
"Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. Such theories yield different classes of explanation, depending on the extent to which they emphasize adaptation to bounds, and adaptation to some ecology that differs from the immediate local environment. #CITATION_TAG provide an early detailed assessment for Germany.","The framework is based on the idea that behaviors are generated by cognitive mechanisms that are adapted to the structure of not only the environment but also the mind and brain itself. We call the framework computational rationality to emphasize the incorporation of computational mechanism into the definition of rational action. Theories are specified as optimal program problems, defined by an adaptation environment, a bounded machine, and a utility function. We illustrate this variation with examples from three domains: visual attention in a linguistic task, manual response ordering, and reasoning. We explore the relation of this framework to existing ""levels"" approaches to explanation, and to other optimality-based modeling approaches.","['We propose a framework for including information-processing bounds in rational analyses.', 'It is an application of bounded optimality (Russell & Subramanian, 1995) to the challenges of developing theories of mechanism and behavior.']"
"Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. For example, #CITATION_TAG model flexible operation of heat pumps combined with various types of thermal storage.","The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks.","['This leads to the development of a computational model of word recognition, the Bayesian reader.']"
"Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. Environmental policies, reduced manufacturing costs, and technology improvements have all contributed to the growing installation of wind turbines and solar photovoltaic arrays in the electric grid. While these new sources of renewable electrical power provide environmental and economic benefits to the electric grid, they also complicate the balancing of supply and demand required to reliably operate the grid. The seasonal, daily, and sub-hourly fluctuations in the energy output of wind and solar generators must be compensated by operating the existing power plant fleet more flexibly or by providing more flexible sources of electricity demand. Wind and solar impact flexibility requirements at different times of the day: wind tends to intensify demand-driven flexibility events by ramping up energy production at night when demand is decreasing and ramping down energy production in the morning when demand is increasing, while solar tends to intensify flexibility requirements due to its quick changes in energy output driven by the rising and setting sun. Adding wind to a system with large amounts of solar does not tend to increase flexibility requirements except for the daily volatility. This renewable mix produces 110 TWh of energy per year, 34% of the total electricity demand. Chapter 6 develops a mixed-integer linear program for modeling the optimal equipment capacity and dispatch of a central utility plant (CUP) in a residential neighborhood and its ability to improve rooftop solar integration. Building a clean, resilient, and reliable electric grid for the future is a worthwhile endeavor that will require innovative supply-side and demand-side solutions for integrating the intermittent power output of renewable generation into the electric grid. There are many opportunities to expand these analyses and explore new sources of grid flexibility in future work.Mechanical Engineerin DSM may help to increase power system efficiency by reducing peak generation capacity requirements and by improving the utilization of both generation and network assets (#CITATION_TAG).","These topics are covered in the four chapters described below. Chapter 3 utilizes a unit commitment and dispatch (UC&D) model to simulate large solar generation assets with different geographic locations and orientations. The west-located, west-oriented solar simulation required greater system flexibility, but utilized more low-cost generators and fewer high-cost generators for energy production than other simulated scenarios. Chapter 4 develops a quantitative framework for calculating flexibility requirements and performs a statistical analysis of load, wind, and solar data from the Electric Reliability Council of Texas (ERCOT) to show how wind and solar capacity impacts these grid flexibility requirements. It also presents a framework for choosing CO2 prices by balancing increasing system cost and flexibility requirements with CO2 emissions reductions. The CUP equipment includes a microturbine, battery, chiller plant, and cooling storage. The CUP model is exposed to a variety of electricity rate structures to see how they influence its operation. The model finds the optimal capacity for each piece of CUP equipment, optimizing their hourly dispatch to meet neighborhood cooling and electric demand while maximizing profit. As a cohesive document, this dissertation communicates the scale and severity of the flexibility requirements that will be required to operate systems with large amounts of wind and solar generation and explores one demand-side method for providing that needed flexibility.","['This dissertation categorizes and quantifies this compensation by studying the ""flexibility requirements\'\' imposed by wind and solar generation, approximates the economically optimal capacities of regional wind and solar resources in the grid, and explores the ability of a central utility plant to add a flexible source of demand to the electric grid system.', ""Chapter 5 develops a model for calculating the optimal amount of transmission, wind, and solar capacity that should be built in a grid's different regions.""]"
"Leading Edge Essay Distilling Pathophysiology from Complex Disease Genetics Aravinda Chakravarti,1,* Andrew G. Clark,2 and Vamsi K. Mootha3 1Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 2Cornell University, Ithaca, NY 14850, USA 3Massachusetts General Hospital, Boston, MA 02114, USA *Correspondence: aravinda@jhmi.edu http://dx.doi.org/10.1016/j.cell.2013.09.001 Technologies for genome-wide sequence interrogation have dramatically improved our ability to identify loci associated with complex human disease. However, a chasm remains between correlations and causality that stems, in part, from a limiting theoretical framework derived fromMendelian genetics and an incomplete understanding of disease physiology. It does not take much perspicacity to see that what really makes this difference is not the tall hat and the umbrella, but the wealth and nourishment of which they are evidence, and that a gold watch or membership of a club in Pall Mall might be proved in the same way to have the like sovereign virtues.. George Bernard Shaw, The Doctor's Dilemma (Preface), 1909 Distinguishing correlation from causality is the essence of experimental science. Nowhere is the need for this distinction greater today than in complex disease genetics, where proof that specific genes have causal effects on human disease phenotypes remains an enormous burden and challenge. This is particularly so in this age of routine -omic surveys, which can produce more false-positive than true-positive findings (Kohane et al., 2006). Moreover, genomic mapping and sequencing approaches that are invaluable for producing a list of unbiased candidates are, by themselves, insufficient for implicating specific gene(s) in a disease or biological process. We admit at the outset that the answers are not straightforward, and that there are serious technical and intellectual impediments to demonstrating causality for the common complex disorders of man where multiple interacting genes are involved. Nevertheless, the casual conflation of ''mapped locus'' to ''proven gene'' is a constant source of confusion and obfuscation in biology and medicine that requires remedy. Consider that two types of genomic surveys, one horizontal and the other vertical, are now routine for attempting to understand human biology and disease. In contrast, in vertical or deep surveys, we examine the effects of the genome as the DNA information gets processed, and its encoded functions get executed through its transcriptome, proteome, and effectors such as the metabolome. In turn, this implies that proving a gene's specific role in a biological process, either in wild-type or mutant form, may not be straightforward because its role may only be evident when examined in relation to its eptember 26, 2013 a2013 Elsevier Inc. 21 Box 1. biochemical partners, and in particular contexts of diet, pathogen exposure, etc. This is a particular problem in genetic studies of any outbred nonexperimental organism, such as the human, and studies of human disease, where investigations are observational not experimental. It is the strong belief of contemporary human geneticists that uncovering the genetic underpinnings of any disease, however complex, is the surest unbiased route to understanding its pathophysiology and, thus, enabling its future rational therapies (Brooke et al., 2008). Consequently, for this view to prevail, we should require experimental evidence, be it in cells, tissues, experimental models, or the rare patient, for the role of a specific gene in a disease process. We know that even in a simple model organism, budding yeast, synthetic lethality-- where death or some other phenotype occurs only through the conspiracy of mutations at two different genes--is widely prevalent (Costanzo et al., 2010). Interactions of greater complexity and involving more than two genes are also known in yeast (Hartman et al., 2001) and must be true for humans as well. A human genome will typically harbor 20 genes that are fully inactivated, without 22 Cell 155, September 26, 2013 a2013 Else any overt disease phenotype, presumably due to the buffering by other genes (MacArthur et al., 2012). Acknowledging this complexity, there are two general ways forward. The question then is how ''complex'' are complex traits and diseases? The New Genetics: Understanding the Function of Variation With the rediscovery of Mendel's rules of transmission more than 100 years ago, there was a vicious debate on the relative importance of single-gene versus multifactorial inheritance (Provine, 1971). Geneticists quickly, and successfully, focused on deciphering the specific mechanisms of gene inheritance and understanding the physiology of the gene in lieu of answering why some phenotypes had complex etiology and transmission. Nevertheless, the rare examples of deciphering the genetic basis of complex phenotypes, such as for truncate (wing) in Drosophila (Altenburg and Muller, 1920), clearly emphasized that traits were more than the additive properties of multiple genes. Today, it is quite clear that Mendelian inheritance of traits, including diseases, is the exception not the rule. Nevertheless, the entire language of genetics is in terms of individual genes for individual phenotypes, with one function, rather than the ensemble and emergent properties of genomes. This absence of a specific genetics language for the proper description of the multigenic architecture of traits (the ensemble) remains as an impediment to our understanding of the nature and degree of genetic complexity of the phenotype. The case of amyotrophic lateral sclerosis (ALS), a devastating, progressive motor neuron disease, illustrates this point (Ludolph et al., 2012). Despite the lack of evidence, we largely describe ALS as being ''heterogeneous'' and comprised of single-gene mutations that can individually lead to disease. In 1993, mutations in superoxide dismutase 1 (SOD1) were identified in an autosomal-dominant form of the disease; subsequently, the disorder has become synonymous with aberrant clearance of free radicals as its central pathology. What is often not appreciated, however, is that fewer than 10% of all cases of ALS are familial and even fewer follow an apparent Mendelian pattern. Even within this subset of cases, more than 20 distinct genes, spanning other pathways including RNA homeostasis, have been identified, and SOD1 represents a minority of cases. The molecular etiology for the majority of the sporadic forms of the disease remains unclear, and the scientific problem in understanding ALS is more than simply identification of additional genes. Are these the key rate-limiting steps to ALS or simply one of several required in concert? Is the aberrant clearance of free radicals the fundamental defect or one of many such pathologies or a common downstream consequence? Given the diversity and number of deleterious, even loss-of-function, genetic variants in all of our genomes (Abecasis et al., 2012; MacArthur et al., 2012) and, in the absence of stronger evidence bearing on these questions, it is fair to assume that ALS patients harbor multiple mutations with a plurality of molecular defects and that free radical metabolism is only one of a set of canonical pathophysiologies that define the disease. No doubt, this plurality is the case for cancer (Vogelstein et al., 2013), Crohn's disease (Jostins et al., 2012), and even rare developmental disorders such as Hirschsprung disease (McCallion et al., 2003). Molecular biology, genetics' twin, on the other hand, appears to have been far more successful in deciphering and describing not only its individual components (e.g., DNA, RNA, protein) but also their mutual relationships (e.g., DNAprotein interaction) and ensembles (e.g., transcriptional complex), although this is also far from complete (Watson et al., 2007). The consequences of the primary and interaction effects are often well understood, even though not completely described, at both the molecular and cellular levels (Alberts et al., 2007). Although the use of genetic tools and genetic perspectives are fundamental to this progress, these advances have not as yet led to a major revision of our understanding of trait or disease variation. The major reason for this discrepancy is that, with few exceptions (Raj et al., 2010), molecular and cell biology has focused on the impact of deleting or overexpressing genes and not grappled with the consequences of allelic variation. Classical Mendelian genetics has been a boon to uncovering biology from yeast to humans whenever a mutation with a simple inheritance pattern can be isolated. This approach has been revolutionary in the unicellular yeast, particularly because genetics (and gene manipulation), biochemistry, and cell biology were melded to understand function at a variety of levels. This kind of multilevel approach has been less straightforward, but still largely successful, for a metazoan such asDrosophilawheremore genes andmultiple specialized cells often rescue the effects of a mutation or enhance its minor effect. Success in this endeavor will require a synthesis of many biological disciplines that includes the role of genetic variation as intrinsic to the biological process, not an aspect to be ignored. Consequently, melding variation-based genetic and molecular biological thinking is of critical importance for both fields and is central to our understanding of mechanisms of trait variation, including interindividual variation in disease risk. If most disease, in most humans, is the consequence of the effects of variation at many genes, then knowledge of their functional relationships, rather than merely their identities, is central to understanding the phenotype. This is clearly a problem of ''Systems Biology'' but one that incorporates genetic variation directly. The ability to integrate the realities of such widespread genetic variation, which are ultimately at the causal root of disease mechanisms, with systems biology approaches to understand functional contingencies is central to the challenge of deciphering complex human disease. Genetic Dissection of Complex Phenotypes Genetic transmission rules imply that, even in an intractable species such as us, one can map genomic segments that must contain a disease or trait gene. Such mapping requires identification of the segregation of common sites of variation across the genome, now easy to identify through sequencing, and recognition of a genomic segment identical-by-descent in affected individuals, both within and between families. This task has become easier and more powerful as sequencing technology has improved to provide a nearly complete catalog of variants above 1% frequency in the population; further improvements to sample rarer variants are ongoing (Abecasis et al., 2012). Consequently, genetic mapping, once the province of rare Mendelian disorders, Cell 155, S is now applicable to any human trait or disease. For most complex traits examined, many such loci have been mapped, but the vast majority of the specific genes remain unidentified. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (Moon et al., 2004), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. In fact, most of the mapped loci underlying complex traits remain unresolved at the gene or mechanistic level. Despite the beginning clues to human disease pathophysiology that complex disease mapping is providing, and the slow identification of individual genes, it appears highly unlikely that we can understand traits and diseases this way. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (Yang et al., 2011). For Mendelian disorders, gene identification within a locus is made possible by each mutation being necessary and sufficient for the phenotype, being functionally deleterious and rare, and having an inheritance pattern consistent with the phenotype. It's the mutation that eventually reveals the biology and explains the phenotype. Any component locus for a complex disease has no such restriction, as the causal variants are neither necessary nor sufficient, nor coding (in fact, they are frequently noncoding and regulatory) nor rare (Emison et al., 2010; Jostins et al., 2012). Currently, the major attempts to overcome this impediment involve reliance on single severe mutations at the very same component genes and eptember 26, 2013 a2013 Elsevier Inc. 23 Genetic association studies in humans can synergize with prior knowledge and systems-level quantitative analysis to generate predictions of what pathways and modules are disrupted, where (anatomically), and when (developmentally) to yield a specificmorphological or biochemical phenotype. Consequently, these strategies themselves depend on the hidden biology we seek and are applicable only to the most common human diseases. It appears to us that ignorance of biology has become rate limiting for understanding disease pathophysiology, except perhaps for the Mendelian disorders. There are two ways to get out of this vicious cycle (Figure 1). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (Ilsley et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the vier Inc. complex inheritance problem (Yosef et al., 2013). One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013). Proving Causality: Molecular Koch's Postulates The evidence that a specific gene is involved in a particular human disease has historically been nonstatistical and based on our experience with identifying mutations in Mendelian diseases. Unfortunately, as already mentioned, all of these rules break down in complex phenotypes where neither cosegregation nor exclusivity to affecteds nor obviously deleterious alleles are likely; moreover, many mutations are suspected to be noncoding and in a diversity of regulatory RNA molecules. Consequently, statistical evidence of enrichment has been the mainstay, but this has two negative consequences: first, scanning across the genome or multiple loci covering tens to hundreds of megabases requires very large sample sizes and very strict levels of significance to guard against themany expected falsepositive findings; second, genetic effects that are small or genes with only a few causal alleles are notoriously difficult to detect, although they may be very important to understanding pathogenesis. This difficulty translates into a low power of detection, as common disease alleles cannot be distinguished from bystander associated alleles, whereas rare alleles are observed too infrequently to provide statistical significance. Consequently, although many genes are ''named'' as being responsible in a complex disease or disease process, proof of their involvement is either absent or circumstantial and not direct. In the late 19th century when bacteria were first shown to cause human disease, they were indiscriminately implicated in all manner of disease with little proof (Brown and Goldstein, 1992). One particularly embarrassing example was alcaptonuria, which Sir Archibald Garrod subsequently showed was inherited and which was his first ''inborn error of metabolism.'' We are likely to repeat this ''witch-hunt'' unless we are careful to note that mapping a locus is not equivalent to identifying the gene, and that identifying a gene and its mutations at a locus depends on numerous untested assumptions (mutational type, mutational frequency in cases and controls, coding or regulatory, cell autonomy). Inmicrobiology, Robert Koch set out three postulates that had to be satisfied to connect a specific bacterium (among the multitudes encountered, not unlike current genome analysis) to a disease: the agent had to be isolated from an affected subject, the agent had to produce disease when transmitted to an animal, and the agent had to be recoverable from an animal's lesion (Falkow, 1988). Simply because we cannot follow Koch to the letter in human patients does not absolve us from the responsibility of demonstrating a rigorous level of proof. This is particularly true if we are to pursue therapeutic targets for these diseases. It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-a-vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families Cell 155, S shows how environmental factors beyond diet can be examined even for congenital disorders (Sparrow et al., 2012). Despite these successes, pursuit of Koch's postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates. Ultimately, a lack of understanding of fundamental physiology is the biggest impediment to our understanding of genetically complex human disease. A unique aspect of genetics research seldom appreciated is that genetic effects are chronic biological exposures and as such can pinpoint the earliest stages of disease not readily studied otherwise. In reality, we still do not fully understand the pathogenesis stemming from some of the earliest identified human disease genes. With better understanding of disease mechanism, it seems likely that many disorders that we think of as ''genetic'' may have ameliorative diet, exercise, or other benign environmental ''treatments.'' Given the potential scientific and medical payoffs of disease gene discovery (Chakravarti, 2001), we argue in this Essay of the need for a rigorous examination of the assumptions under which we connect genes to phenotypes. Below we discuss the nature of the ''proof'' that we desire in order to make fundamental discoveries in human pathophysiology. Success in this difficult task requires us to solve a logical conundrum: how can we understand the genes underlying a phenotype if some of these component factors, in isolation, do not have recognizable phenotypes on their own? Both of these goals are approachable, particularly with recent advances in genome-editing technologies that allow the creation of multiple mutations within a single experimental organism (Wang et al., 2013). The second approach is to focus research on why the disease is complex in the first place. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. The chief criteria have been to demonstrate cosegregation with the phenotype in families, exclusivity of the mutation to affected individuals (rare alleles absent in controls), and the nature of themutation (a plausibly deleterious allele at a conserved site within a protein). We need to move beyond lists of plausible genes, to provide rigorous proof for their role in disease. But this goal is unlikely to be achieved in the absence of a superior understanding of the biology of hierarchical function within genomes, how variation alters these functions, and how these altered functions lead to human disease. Congenital scoliosis, a lateral curvature of the spine caused by vertebral defects, occurs in approximately 1 in 1,000 live births. A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families shows how environmental factors beyond diet can be examined even for congenital disorders (#CITATION_TAG).",,['Here we demonstrate that haploinsufficiency of Notch signaling pathway genes in humans can cause this congenital abnormality.']
"Leading Edge Essay Distilling Pathophysiology from Complex Disease Genetics Aravinda Chakravarti,1,* Andrew G. Clark,2 and Vamsi K. Mootha3 1Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 2Cornell University, Ithaca, NY 14850, USA 3Massachusetts General Hospital, Boston, MA 02114, USA *Correspondence: aravinda@jhmi.edu http://dx.doi.org/10.1016/j.cell.2013.09.001 Technologies for genome-wide sequence interrogation have dramatically improved our ability to identify loci associated with complex human disease. However, a chasm remains between correlations and causality that stems, in part, from a limiting theoretical framework derived fromMendelian genetics and an incomplete understanding of disease physiology. It does not take much perspicacity to see that what really makes this difference is not the tall hat and the umbrella, but the wealth and nourishment of which they are evidence, and that a gold watch or membership of a club in Pall Mall might be proved in the same way to have the like sovereign virtues.. George Bernard Shaw, The Doctor's Dilemma (Preface), 1909 Distinguishing correlation from causality is the essence of experimental science. Nowhere is the need for this distinction greater today than in complex disease genetics, where proof that specific genes have causal effects on human disease phenotypes remains an enormous burden and challenge. This is particularly so in this age of routine -omic surveys, which can produce more false-positive than true-positive findings (Kohane et al., 2006). Moreover, genomic mapping and sequencing approaches that are invaluable for producing a list of unbiased candidates are, by themselves, insufficient for implicating specific gene(s) in a disease or biological process. We admit at the outset that the answers are not straightforward, and that there are serious technical and intellectual impediments to demonstrating causality for the common complex disorders of man where multiple interacting genes are involved. Nevertheless, the casual conflation of ''mapped locus'' to ''proven gene'' is a constant source of confusion and obfuscation in biology and medicine that requires remedy. Consider that two types of genomic surveys, one horizontal and the other vertical, are now routine for attempting to understand human biology and disease. In contrast, in vertical or deep surveys, we examine the effects of the genome as the DNA information gets processed, and its encoded functions get executed through its transcriptome, proteome, and effectors such as the metabolome. In turn, this implies that proving a gene's specific role in a biological process, either in wild-type or mutant form, may not be straightforward because its role may only be evident when examined in relation to its eptember 26, 2013 a2013 Elsevier Inc. 21 Box 1. biochemical partners, and in particular contexts of diet, pathogen exposure, etc. This is a particular problem in genetic studies of any outbred nonexperimental organism, such as the human, and studies of human disease, where investigations are observational not experimental. It is the strong belief of contemporary human geneticists that uncovering the genetic underpinnings of any disease, however complex, is the surest unbiased route to understanding its pathophysiology and, thus, enabling its future rational therapies (Brooke et al., 2008). Consequently, for this view to prevail, we should require experimental evidence, be it in cells, tissues, experimental models, or the rare patient, for the role of a specific gene in a disease process. We know that even in a simple model organism, budding yeast, synthetic lethality-- where death or some other phenotype occurs only through the conspiracy of mutations at two different genes--is widely prevalent (Costanzo et al., 2010). Interactions of greater complexity and involving more than two genes are also known in yeast (Hartman et al., 2001) and must be true for humans as well. A human genome will typically harbor 20 genes that are fully inactivated, without 22 Cell 155, September 26, 2013 a2013 Else any overt disease phenotype, presumably due to the buffering by other genes (MacArthur et al., 2012). Acknowledging this complexity, there are two general ways forward. The question then is how ''complex'' are complex traits and diseases? The New Genetics: Understanding the Function of Variation With the rediscovery of Mendel's rules of transmission more than 100 years ago, there was a vicious debate on the relative importance of single-gene versus multifactorial inheritance (Provine, 1971). Geneticists quickly, and successfully, focused on deciphering the specific mechanisms of gene inheritance and understanding the physiology of the gene in lieu of answering why some phenotypes had complex etiology and transmission. Nevertheless, the rare examples of deciphering the genetic basis of complex phenotypes, such as for truncate (wing) in Drosophila (Altenburg and Muller, 1920), clearly emphasized that traits were more than the additive properties of multiple genes. Today, it is quite clear that Mendelian inheritance of traits, including diseases, is the exception not the rule. Nevertheless, the entire language of genetics is in terms of individual genes for individual phenotypes, with one function, rather than the ensemble and emergent properties of genomes. This absence of a specific genetics language for the proper description of the multigenic architecture of traits (the ensemble) remains as an impediment to our understanding of the nature and degree of genetic complexity of the phenotype. The case of amyotrophic lateral sclerosis (ALS), a devastating, progressive motor neuron disease, illustrates this point (Ludolph et al., 2012). Despite the lack of evidence, we largely describe ALS as being ''heterogeneous'' and comprised of single-gene mutations that can individually lead to disease. In 1993, mutations in superoxide dismutase 1 (SOD1) were identified in an autosomal-dominant form of the disease; subsequently, the disorder has become synonymous with aberrant clearance of free radicals as its central pathology. What is often not appreciated, however, is that fewer than 10% of all cases of ALS are familial and even fewer follow an apparent Mendelian pattern. Even within this subset of cases, more than 20 distinct genes, spanning other pathways including RNA homeostasis, have been identified, and SOD1 represents a minority of cases. The molecular etiology for the majority of the sporadic forms of the disease remains unclear, and the scientific problem in understanding ALS is more than simply identification of additional genes. Are these the key rate-limiting steps to ALS or simply one of several required in concert? Is the aberrant clearance of free radicals the fundamental defect or one of many such pathologies or a common downstream consequence? Given the diversity and number of deleterious, even loss-of-function, genetic variants in all of our genomes (Abecasis et al., 2012; MacArthur et al., 2012) and, in the absence of stronger evidence bearing on these questions, it is fair to assume that ALS patients harbor multiple mutations with a plurality of molecular defects and that free radical metabolism is only one of a set of canonical pathophysiologies that define the disease. No doubt, this plurality is the case for cancer (Vogelstein et al., 2013), Crohn's disease (Jostins et al., 2012), and even rare developmental disorders such as Hirschsprung disease (McCallion et al., 2003). Molecular biology, genetics' twin, on the other hand, appears to have been far more successful in deciphering and describing not only its individual components (e.g., DNA, RNA, protein) but also their mutual relationships (e.g., DNAprotein interaction) and ensembles (e.g., transcriptional complex), although this is also far from complete (Watson et al., 2007). The consequences of the primary and interaction effects are often well understood, even though not completely described, at both the molecular and cellular levels (Alberts et al., 2007). Although the use of genetic tools and genetic perspectives are fundamental to this progress, these advances have not as yet led to a major revision of our understanding of trait or disease variation. The major reason for this discrepancy is that, with few exceptions (Raj et al., 2010), molecular and cell biology has focused on the impact of deleting or overexpressing genes and not grappled with the consequences of allelic variation. Classical Mendelian genetics has been a boon to uncovering biology from yeast to humans whenever a mutation with a simple inheritance pattern can be isolated. This approach has been revolutionary in the unicellular yeast, particularly because genetics (and gene manipulation), biochemistry, and cell biology were melded to understand function at a variety of levels. This kind of multilevel approach has been less straightforward, but still largely successful, for a metazoan such asDrosophilawheremore genes andmultiple specialized cells often rescue the effects of a mutation or enhance its minor effect. Success in this endeavor will require a synthesis of many biological disciplines that includes the role of genetic variation as intrinsic to the biological process, not an aspect to be ignored. Consequently, melding variation-based genetic and molecular biological thinking is of critical importance for both fields and is central to our understanding of mechanisms of trait variation, including interindividual variation in disease risk. If most disease, in most humans, is the consequence of the effects of variation at many genes, then knowledge of their functional relationships, rather than merely their identities, is central to understanding the phenotype. This is clearly a problem of ''Systems Biology'' but one that incorporates genetic variation directly. The ability to integrate the realities of such widespread genetic variation, which are ultimately at the causal root of disease mechanisms, with systems biology approaches to understand functional contingencies is central to the challenge of deciphering complex human disease. Genetic Dissection of Complex Phenotypes Genetic transmission rules imply that, even in an intractable species such as us, one can map genomic segments that must contain a disease or trait gene. Such mapping requires identification of the segregation of common sites of variation across the genome, now easy to identify through sequencing, and recognition of a genomic segment identical-by-descent in affected individuals, both within and between families. This task has become easier and more powerful as sequencing technology has improved to provide a nearly complete catalog of variants above 1% frequency in the population; further improvements to sample rarer variants are ongoing (Abecasis et al., 2012). Consequently, genetic mapping, once the province of rare Mendelian disorders, Cell 155, S is now applicable to any human trait or disease. For most complex traits examined, many such loci have been mapped, but the vast majority of the specific genes remain unidentified. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (Moon et al., 2004), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. In fact, most of the mapped loci underlying complex traits remain unresolved at the gene or mechanistic level. Despite the beginning clues to human disease pathophysiology that complex disease mapping is providing, and the slow identification of individual genes, it appears highly unlikely that we can understand traits and diseases this way. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (Yang et al., 2011). For Mendelian disorders, gene identification within a locus is made possible by each mutation being necessary and sufficient for the phenotype, being functionally deleterious and rare, and having an inheritance pattern consistent with the phenotype. It's the mutation that eventually reveals the biology and explains the phenotype. Any component locus for a complex disease has no such restriction, as the causal variants are neither necessary nor sufficient, nor coding (in fact, they are frequently noncoding and regulatory) nor rare (Emison et al., 2010; Jostins et al., 2012). Currently, the major attempts to overcome this impediment involve reliance on single severe mutations at the very same component genes and eptember 26, 2013 a2013 Elsevier Inc. 23 Genetic association studies in humans can synergize with prior knowledge and systems-level quantitative analysis to generate predictions of what pathways and modules are disrupted, where (anatomically), and when (developmentally) to yield a specificmorphological or biochemical phenotype. Consequently, these strategies themselves depend on the hidden biology we seek and are applicable only to the most common human diseases. It appears to us that ignorance of biology has become rate limiting for understanding disease pathophysiology, except perhaps for the Mendelian disorders. There are two ways to get out of this vicious cycle (Figure 1). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (Ilsley et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the vier Inc. complex inheritance problem (Yosef et al., 2013). One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013). Proving Causality: Molecular Koch's Postulates The evidence that a specific gene is involved in a particular human disease has historically been nonstatistical and based on our experience with identifying mutations in Mendelian diseases. Unfortunately, as already mentioned, all of these rules break down in complex phenotypes where neither cosegregation nor exclusivity to affecteds nor obviously deleterious alleles are likely; moreover, many mutations are suspected to be noncoding and in a diversity of regulatory RNA molecules. Consequently, statistical evidence of enrichment has been the mainstay, but this has two negative consequences: first, scanning across the genome or multiple loci covering tens to hundreds of megabases requires very large sample sizes and very strict levels of significance to guard against themany expected falsepositive findings; second, genetic effects that are small or genes with only a few causal alleles are notoriously difficult to detect, although they may be very important to understanding pathogenesis. This difficulty translates into a low power of detection, as common disease alleles cannot be distinguished from bystander associated alleles, whereas rare alleles are observed too infrequently to provide statistical significance. Consequently, although many genes are ''named'' as being responsible in a complex disease or disease process, proof of their involvement is either absent or circumstantial and not direct. In the late 19th century when bacteria were first shown to cause human disease, they were indiscriminately implicated in all manner of disease with little proof (Brown and Goldstein, 1992). One particularly embarrassing example was alcaptonuria, which Sir Archibald Garrod subsequently showed was inherited and which was his first ''inborn error of metabolism.'' We are likely to repeat this ''witch-hunt'' unless we are careful to note that mapping a locus is not equivalent to identifying the gene, and that identifying a gene and its mutations at a locus depends on numerous untested assumptions (mutational type, mutational frequency in cases and controls, coding or regulatory, cell autonomy). Inmicrobiology, Robert Koch set out three postulates that had to be satisfied to connect a specific bacterium (among the multitudes encountered, not unlike current genome analysis) to a disease: the agent had to be isolated from an affected subject, the agent had to produce disease when transmitted to an animal, and the agent had to be recoverable from an animal's lesion (Falkow, 1988). Simply because we cannot follow Koch to the letter in human patients does not absolve us from the responsibility of demonstrating a rigorous level of proof. This is particularly true if we are to pursue therapeutic targets for these diseases. It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-a-vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families Cell 155, S shows how environmental factors beyond diet can be examined even for congenital disorders (Sparrow et al., 2012). Despite these successes, pursuit of Koch's postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates. Ultimately, a lack of understanding of fundamental physiology is the biggest impediment to our understanding of genetically complex human disease. A unique aspect of genetics research seldom appreciated is that genetic effects are chronic biological exposures and as such can pinpoint the earliest stages of disease not readily studied otherwise. In reality, we still do not fully understand the pathogenesis stemming from some of the earliest identified human disease genes. With better understanding of disease mechanism, it seems likely that many disorders that we think of as ''genetic'' may have ameliorative diet, exercise, or other benign environmental ''treatments.'' Given the potential scientific and medical payoffs of disease gene discovery (Chakravarti, 2001), we argue in this Essay of the need for a rigorous examination of the assumptions under which we connect genes to phenotypes. Below we discuss the nature of the ''proof'' that we desire in order to make fundamental discoveries in human pathophysiology. Success in this difficult task requires us to solve a logical conundrum: how can we understand the genes underlying a phenotype if some of these component factors, in isolation, do not have recognizable phenotypes on their own? Both of these goals are approachable, particularly with recent advances in genome-editing technologies that allow the creation of multiple mutations within a single experimental organism (Wang et al., 2013). The second approach is to focus research on why the disease is complex in the first place. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. The chief criteria have been to demonstrate cosegregation with the phenotype in families, exclusivity of the mutation to affected individuals (rare alleles absent in controls), and the nature of themutation (a plausibly deleterious allele at a conserved site within a protein). We need to move beyond lists of plausible genes, to provide rigorous proof for their role in disease. But this goal is unlikely to be achieved in the absence of a superior understanding of the biology of hierarchical function within genomes, how variation alters these functions, and how these altered functions lead to human disease. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (#CITATION_TAG).","We estimate and partition genetic variation for height, body mass index (BMI), von Willebrand factor and QT interval (QTi) using 586,898 SNPs genotyped on 11,586 unrelated individuals. We show that the variance explained by each chromosome is proportional to its length, and that SNPs in or near genes explain more variation than SNPs between genes.",['We propose a new approach to estimate variation due to cryptic relatedness and population stratification.']
"Leading Edge Essay Distilling Pathophysiology from Complex Disease Genetics Aravinda Chakravarti,1,* Andrew G. Clark,2 and Vamsi K. Mootha3 1Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 2Cornell University, Ithaca, NY 14850, USA 3Massachusetts General Hospital, Boston, MA 02114, USA *Correspondence: aravinda@jhmi.edu http://dx.doi.org/10.1016/j.cell.2013.09.001 Technologies for genome-wide sequence interrogation have dramatically improved our ability to identify loci associated with complex human disease. However, a chasm remains between correlations and causality that stems, in part, from a limiting theoretical framework derived fromMendelian genetics and an incomplete understanding of disease physiology. It does not take much perspicacity to see that what really makes this difference is not the tall hat and the umbrella, but the wealth and nourishment of which they are evidence, and that a gold watch or membership of a club in Pall Mall might be proved in the same way to have the like sovereign virtues.. George Bernard Shaw, The Doctor's Dilemma (Preface), 1909 Distinguishing correlation from causality is the essence of experimental science. Nowhere is the need for this distinction greater today than in complex disease genetics, where proof that specific genes have causal effects on human disease phenotypes remains an enormous burden and challenge. This is particularly so in this age of routine -omic surveys, which can produce more false-positive than true-positive findings (Kohane et al., 2006). Moreover, genomic mapping and sequencing approaches that are invaluable for producing a list of unbiased candidates are, by themselves, insufficient for implicating specific gene(s) in a disease or biological process. We admit at the outset that the answers are not straightforward, and that there are serious technical and intellectual impediments to demonstrating causality for the common complex disorders of man where multiple interacting genes are involved. Nevertheless, the casual conflation of ''mapped locus'' to ''proven gene'' is a constant source of confusion and obfuscation in biology and medicine that requires remedy. Consider that two types of genomic surveys, one horizontal and the other vertical, are now routine for attempting to understand human biology and disease. In contrast, in vertical or deep surveys, we examine the effects of the genome as the DNA information gets processed, and its encoded functions get executed through its transcriptome, proteome, and effectors such as the metabolome. In turn, this implies that proving a gene's specific role in a biological process, either in wild-type or mutant form, may not be straightforward because its role may only be evident when examined in relation to its eptember 26, 2013 a2013 Elsevier Inc. 21 Box 1. biochemical partners, and in particular contexts of diet, pathogen exposure, etc. This is a particular problem in genetic studies of any outbred nonexperimental organism, such as the human, and studies of human disease, where investigations are observational not experimental. It is the strong belief of contemporary human geneticists that uncovering the genetic underpinnings of any disease, however complex, is the surest unbiased route to understanding its pathophysiology and, thus, enabling its future rational therapies (Brooke et al., 2008). Consequently, for this view to prevail, we should require experimental evidence, be it in cells, tissues, experimental models, or the rare patient, for the role of a specific gene in a disease process. We know that even in a simple model organism, budding yeast, synthetic lethality-- where death or some other phenotype occurs only through the conspiracy of mutations at two different genes--is widely prevalent (Costanzo et al., 2010). Interactions of greater complexity and involving more than two genes are also known in yeast (Hartman et al., 2001) and must be true for humans as well. A human genome will typically harbor 20 genes that are fully inactivated, without 22 Cell 155, September 26, 2013 a2013 Else any overt disease phenotype, presumably due to the buffering by other genes (MacArthur et al., 2012). Acknowledging this complexity, there are two general ways forward. The question then is how ''complex'' are complex traits and diseases? The New Genetics: Understanding the Function of Variation With the rediscovery of Mendel's rules of transmission more than 100 years ago, there was a vicious debate on the relative importance of single-gene versus multifactorial inheritance (Provine, 1971). Geneticists quickly, and successfully, focused on deciphering the specific mechanisms of gene inheritance and understanding the physiology of the gene in lieu of answering why some phenotypes had complex etiology and transmission. Nevertheless, the rare examples of deciphering the genetic basis of complex phenotypes, such as for truncate (wing) in Drosophila (Altenburg and Muller, 1920), clearly emphasized that traits were more than the additive properties of multiple genes. Today, it is quite clear that Mendelian inheritance of traits, including diseases, is the exception not the rule. Nevertheless, the entire language of genetics is in terms of individual genes for individual phenotypes, with one function, rather than the ensemble and emergent properties of genomes. This absence of a specific genetics language for the proper description of the multigenic architecture of traits (the ensemble) remains as an impediment to our understanding of the nature and degree of genetic complexity of the phenotype. The case of amyotrophic lateral sclerosis (ALS), a devastating, progressive motor neuron disease, illustrates this point (Ludolph et al., 2012). Despite the lack of evidence, we largely describe ALS as being ''heterogeneous'' and comprised of single-gene mutations that can individually lead to disease. In 1993, mutations in superoxide dismutase 1 (SOD1) were identified in an autosomal-dominant form of the disease; subsequently, the disorder has become synonymous with aberrant clearance of free radicals as its central pathology. What is often not appreciated, however, is that fewer than 10% of all cases of ALS are familial and even fewer follow an apparent Mendelian pattern. Even within this subset of cases, more than 20 distinct genes, spanning other pathways including RNA homeostasis, have been identified, and SOD1 represents a minority of cases. The molecular etiology for the majority of the sporadic forms of the disease remains unclear, and the scientific problem in understanding ALS is more than simply identification of additional genes. Are these the key rate-limiting steps to ALS or simply one of several required in concert? Is the aberrant clearance of free radicals the fundamental defect or one of many such pathologies or a common downstream consequence? Given the diversity and number of deleterious, even loss-of-function, genetic variants in all of our genomes (Abecasis et al., 2012; MacArthur et al., 2012) and, in the absence of stronger evidence bearing on these questions, it is fair to assume that ALS patients harbor multiple mutations with a plurality of molecular defects and that free radical metabolism is only one of a set of canonical pathophysiologies that define the disease. No doubt, this plurality is the case for cancer (Vogelstein et al., 2013), Crohn's disease (Jostins et al., 2012), and even rare developmental disorders such as Hirschsprung disease (McCallion et al., 2003). Molecular biology, genetics' twin, on the other hand, appears to have been far more successful in deciphering and describing not only its individual components (e.g., DNA, RNA, protein) but also their mutual relationships (e.g., DNAprotein interaction) and ensembles (e.g., transcriptional complex), although this is also far from complete (Watson et al., 2007). The consequences of the primary and interaction effects are often well understood, even though not completely described, at both the molecular and cellular levels (Alberts et al., 2007). Although the use of genetic tools and genetic perspectives are fundamental to this progress, these advances have not as yet led to a major revision of our understanding of trait or disease variation. The major reason for this discrepancy is that, with few exceptions (Raj et al., 2010), molecular and cell biology has focused on the impact of deleting or overexpressing genes and not grappled with the consequences of allelic variation. Classical Mendelian genetics has been a boon to uncovering biology from yeast to humans whenever a mutation with a simple inheritance pattern can be isolated. This approach has been revolutionary in the unicellular yeast, particularly because genetics (and gene manipulation), biochemistry, and cell biology were melded to understand function at a variety of levels. This kind of multilevel approach has been less straightforward, but still largely successful, for a metazoan such asDrosophilawheremore genes andmultiple specialized cells often rescue the effects of a mutation or enhance its minor effect. Success in this endeavor will require a synthesis of many biological disciplines that includes the role of genetic variation as intrinsic to the biological process, not an aspect to be ignored. Consequently, melding variation-based genetic and molecular biological thinking is of critical importance for both fields and is central to our understanding of mechanisms of trait variation, including interindividual variation in disease risk. If most disease, in most humans, is the consequence of the effects of variation at many genes, then knowledge of their functional relationships, rather than merely their identities, is central to understanding the phenotype. This is clearly a problem of ''Systems Biology'' but one that incorporates genetic variation directly. The ability to integrate the realities of such widespread genetic variation, which are ultimately at the causal root of disease mechanisms, with systems biology approaches to understand functional contingencies is central to the challenge of deciphering complex human disease. Genetic Dissection of Complex Phenotypes Genetic transmission rules imply that, even in an intractable species such as us, one can map genomic segments that must contain a disease or trait gene. Such mapping requires identification of the segregation of common sites of variation across the genome, now easy to identify through sequencing, and recognition of a genomic segment identical-by-descent in affected individuals, both within and between families. This task has become easier and more powerful as sequencing technology has improved to provide a nearly complete catalog of variants above 1% frequency in the population; further improvements to sample rarer variants are ongoing (Abecasis et al., 2012). Consequently, genetic mapping, once the province of rare Mendelian disorders, Cell 155, S is now applicable to any human trait or disease. For most complex traits examined, many such loci have been mapped, but the vast majority of the specific genes remain unidentified. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (Moon et al., 2004), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. In fact, most of the mapped loci underlying complex traits remain unresolved at the gene or mechanistic level. Despite the beginning clues to human disease pathophysiology that complex disease mapping is providing, and the slow identification of individual genes, it appears highly unlikely that we can understand traits and diseases this way. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (Yang et al., 2011). For Mendelian disorders, gene identification within a locus is made possible by each mutation being necessary and sufficient for the phenotype, being functionally deleterious and rare, and having an inheritance pattern consistent with the phenotype. It's the mutation that eventually reveals the biology and explains the phenotype. Any component locus for a complex disease has no such restriction, as the causal variants are neither necessary nor sufficient, nor coding (in fact, they are frequently noncoding and regulatory) nor rare (Emison et al., 2010; Jostins et al., 2012). Currently, the major attempts to overcome this impediment involve reliance on single severe mutations at the very same component genes and eptember 26, 2013 a2013 Elsevier Inc. 23 Genetic association studies in humans can synergize with prior knowledge and systems-level quantitative analysis to generate predictions of what pathways and modules are disrupted, where (anatomically), and when (developmentally) to yield a specificmorphological or biochemical phenotype. Consequently, these strategies themselves depend on the hidden biology we seek and are applicable only to the most common human diseases. It appears to us that ignorance of biology has become rate limiting for understanding disease pathophysiology, except perhaps for the Mendelian disorders. There are two ways to get out of this vicious cycle (Figure 1). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (Ilsley et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the vier Inc. complex inheritance problem (Yosef et al., 2013). One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013). Proving Causality: Molecular Koch's Postulates The evidence that a specific gene is involved in a particular human disease has historically been nonstatistical and based on our experience with identifying mutations in Mendelian diseases. Unfortunately, as already mentioned, all of these rules break down in complex phenotypes where neither cosegregation nor exclusivity to affecteds nor obviously deleterious alleles are likely; moreover, many mutations are suspected to be noncoding and in a diversity of regulatory RNA molecules. Consequently, statistical evidence of enrichment has been the mainstay, but this has two negative consequences: first, scanning across the genome or multiple loci covering tens to hundreds of megabases requires very large sample sizes and very strict levels of significance to guard against themany expected falsepositive findings; second, genetic effects that are small or genes with only a few causal alleles are notoriously difficult to detect, although they may be very important to understanding pathogenesis. This difficulty translates into a low power of detection, as common disease alleles cannot be distinguished from bystander associated alleles, whereas rare alleles are observed too infrequently to provide statistical significance. Consequently, although many genes are ''named'' as being responsible in a complex disease or disease process, proof of their involvement is either absent or circumstantial and not direct. In the late 19th century when bacteria were first shown to cause human disease, they were indiscriminately implicated in all manner of disease with little proof (Brown and Goldstein, 1992). One particularly embarrassing example was alcaptonuria, which Sir Archibald Garrod subsequently showed was inherited and which was his first ''inborn error of metabolism.'' We are likely to repeat this ''witch-hunt'' unless we are careful to note that mapping a locus is not equivalent to identifying the gene, and that identifying a gene and its mutations at a locus depends on numerous untested assumptions (mutational type, mutational frequency in cases and controls, coding or regulatory, cell autonomy). Inmicrobiology, Robert Koch set out three postulates that had to be satisfied to connect a specific bacterium (among the multitudes encountered, not unlike current genome analysis) to a disease: the agent had to be isolated from an affected subject, the agent had to produce disease when transmitted to an animal, and the agent had to be recoverable from an animal's lesion (Falkow, 1988). Simply because we cannot follow Koch to the letter in human patients does not absolve us from the responsibility of demonstrating a rigorous level of proof. This is particularly true if we are to pursue therapeutic targets for these diseases. It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-a-vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families Cell 155, S shows how environmental factors beyond diet can be examined even for congenital disorders (Sparrow et al., 2012). Despite these successes, pursuit of Koch's postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates. Ultimately, a lack of understanding of fundamental physiology is the biggest impediment to our understanding of genetically complex human disease. A unique aspect of genetics research seldom appreciated is that genetic effects are chronic biological exposures and as such can pinpoint the earliest stages of disease not readily studied otherwise. In reality, we still do not fully understand the pathogenesis stemming from some of the earliest identified human disease genes. With better understanding of disease mechanism, it seems likely that many disorders that we think of as ''genetic'' may have ameliorative diet, exercise, or other benign environmental ''treatments.'' Given the potential scientific and medical payoffs of disease gene discovery (Chakravarti, 2001), we argue in this Essay of the need for a rigorous examination of the assumptions under which we connect genes to phenotypes. Below we discuss the nature of the ''proof'' that we desire in order to make fundamental discoveries in human pathophysiology. Success in this difficult task requires us to solve a logical conundrum: how can we understand the genes underlying a phenotype if some of these component factors, in isolation, do not have recognizable phenotypes on their own? Both of these goals are approachable, particularly with recent advances in genome-editing technologies that allow the creation of multiple mutations within a single experimental organism (Wang et al., 2013). The second approach is to focus research on why the disease is complex in the first place. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. The chief criteria have been to demonstrate cosegregation with the phenotype in families, exclusivity of the mutation to affected individuals (rare alleles absent in controls), and the nature of themutation (a plausibly deleterious allele at a conserved site within a protein). We need to move beyond lists of plausible genes, to provide rigorous proof for their role in disease. But this goal is unlikely to be achieved in the absence of a superior understanding of the biology of hierarchical function within genomes, how variation alters these functions, and how these altered functions lead to human disease. What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and interconnexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. Daniel & Whitteiidge (1959) have recently extended this work in the primate. As the case for Marfan syndrome demonstrates, the identification of fibrillin 1 mutations was insufficient to identify therapies without the concomitant understanding of the pathophysiology (#CITATION_TAG).","In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to","['This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours.']"
"Leading Edge Essay Distilling Pathophysiology from Complex Disease Genetics Aravinda Chakravarti,1,* Andrew G. Clark,2 and Vamsi K. Mootha3 1Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 2Cornell University, Ithaca, NY 14850, USA 3Massachusetts General Hospital, Boston, MA 02114, USA *Correspondence: aravinda@jhmi.edu http://dx.doi.org/10.1016/j.cell.2013.09.001 Technologies for genome-wide sequence interrogation have dramatically improved our ability to identify loci associated with complex human disease. However, a chasm remains between correlations and causality that stems, in part, from a limiting theoretical framework derived fromMendelian genetics and an incomplete understanding of disease physiology. It does not take much perspicacity to see that what really makes this difference is not the tall hat and the umbrella, but the wealth and nourishment of which they are evidence, and that a gold watch or membership of a club in Pall Mall might be proved in the same way to have the like sovereign virtues.. George Bernard Shaw, The Doctor's Dilemma (Preface), 1909 Distinguishing correlation from causality is the essence of experimental science. Nowhere is the need for this distinction greater today than in complex disease genetics, where proof that specific genes have causal effects on human disease phenotypes remains an enormous burden and challenge. This is particularly so in this age of routine -omic surveys, which can produce more false-positive than true-positive findings (Kohane et al., 2006). Moreover, genomic mapping and sequencing approaches that are invaluable for producing a list of unbiased candidates are, by themselves, insufficient for implicating specific gene(s) in a disease or biological process. We admit at the outset that the answers are not straightforward, and that there are serious technical and intellectual impediments to demonstrating causality for the common complex disorders of man where multiple interacting genes are involved. Nevertheless, the casual conflation of ''mapped locus'' to ''proven gene'' is a constant source of confusion and obfuscation in biology and medicine that requires remedy. Consider that two types of genomic surveys, one horizontal and the other vertical, are now routine for attempting to understand human biology and disease. In contrast, in vertical or deep surveys, we examine the effects of the genome as the DNA information gets processed, and its encoded functions get executed through its transcriptome, proteome, and effectors such as the metabolome. In turn, this implies that proving a gene's specific role in a biological process, either in wild-type or mutant form, may not be straightforward because its role may only be evident when examined in relation to its eptember 26, 2013 a2013 Elsevier Inc. 21 Box 1. biochemical partners, and in particular contexts of diet, pathogen exposure, etc. This is a particular problem in genetic studies of any outbred nonexperimental organism, such as the human, and studies of human disease, where investigations are observational not experimental. It is the strong belief of contemporary human geneticists that uncovering the genetic underpinnings of any disease, however complex, is the surest unbiased route to understanding its pathophysiology and, thus, enabling its future rational therapies (Brooke et al., 2008). Consequently, for this view to prevail, we should require experimental evidence, be it in cells, tissues, experimental models, or the rare patient, for the role of a specific gene in a disease process. We know that even in a simple model organism, budding yeast, synthetic lethality-- where death or some other phenotype occurs only through the conspiracy of mutations at two different genes--is widely prevalent (Costanzo et al., 2010). Interactions of greater complexity and involving more than two genes are also known in yeast (Hartman et al., 2001) and must be true for humans as well. A human genome will typically harbor 20 genes that are fully inactivated, without 22 Cell 155, September 26, 2013 a2013 Else any overt disease phenotype, presumably due to the buffering by other genes (MacArthur et al., 2012). Acknowledging this complexity, there are two general ways forward. The question then is how ''complex'' are complex traits and diseases? The New Genetics: Understanding the Function of Variation With the rediscovery of Mendel's rules of transmission more than 100 years ago, there was a vicious debate on the relative importance of single-gene versus multifactorial inheritance (Provine, 1971). Geneticists quickly, and successfully, focused on deciphering the specific mechanisms of gene inheritance and understanding the physiology of the gene in lieu of answering why some phenotypes had complex etiology and transmission. Nevertheless, the rare examples of deciphering the genetic basis of complex phenotypes, such as for truncate (wing) in Drosophila (Altenburg and Muller, 1920), clearly emphasized that traits were more than the additive properties of multiple genes. Today, it is quite clear that Mendelian inheritance of traits, including diseases, is the exception not the rule. Nevertheless, the entire language of genetics is in terms of individual genes for individual phenotypes, with one function, rather than the ensemble and emergent properties of genomes. This absence of a specific genetics language for the proper description of the multigenic architecture of traits (the ensemble) remains as an impediment to our understanding of the nature and degree of genetic complexity of the phenotype. The case of amyotrophic lateral sclerosis (ALS), a devastating, progressive motor neuron disease, illustrates this point (Ludolph et al., 2012). Despite the lack of evidence, we largely describe ALS as being ''heterogeneous'' and comprised of single-gene mutations that can individually lead to disease. In 1993, mutations in superoxide dismutase 1 (SOD1) were identified in an autosomal-dominant form of the disease; subsequently, the disorder has become synonymous with aberrant clearance of free radicals as its central pathology. What is often not appreciated, however, is that fewer than 10% of all cases of ALS are familial and even fewer follow an apparent Mendelian pattern. Even within this subset of cases, more than 20 distinct genes, spanning other pathways including RNA homeostasis, have been identified, and SOD1 represents a minority of cases. The molecular etiology for the majority of the sporadic forms of the disease remains unclear, and the scientific problem in understanding ALS is more than simply identification of additional genes. Are these the key rate-limiting steps to ALS or simply one of several required in concert? Is the aberrant clearance of free radicals the fundamental defect or one of many such pathologies or a common downstream consequence? Given the diversity and number of deleterious, even loss-of-function, genetic variants in all of our genomes (Abecasis et al., 2012; MacArthur et al., 2012) and, in the absence of stronger evidence bearing on these questions, it is fair to assume that ALS patients harbor multiple mutations with a plurality of molecular defects and that free radical metabolism is only one of a set of canonical pathophysiologies that define the disease. No doubt, this plurality is the case for cancer (Vogelstein et al., 2013), Crohn's disease (Jostins et al., 2012), and even rare developmental disorders such as Hirschsprung disease (McCallion et al., 2003). Molecular biology, genetics' twin, on the other hand, appears to have been far more successful in deciphering and describing not only its individual components (e.g., DNA, RNA, protein) but also their mutual relationships (e.g., DNAprotein interaction) and ensembles (e.g., transcriptional complex), although this is also far from complete (Watson et al., 2007). The consequences of the primary and interaction effects are often well understood, even though not completely described, at both the molecular and cellular levels (Alberts et al., 2007). Although the use of genetic tools and genetic perspectives are fundamental to this progress, these advances have not as yet led to a major revision of our understanding of trait or disease variation. The major reason for this discrepancy is that, with few exceptions (Raj et al., 2010), molecular and cell biology has focused on the impact of deleting or overexpressing genes and not grappled with the consequences of allelic variation. Classical Mendelian genetics has been a boon to uncovering biology from yeast to humans whenever a mutation with a simple inheritance pattern can be isolated. This approach has been revolutionary in the unicellular yeast, particularly because genetics (and gene manipulation), biochemistry, and cell biology were melded to understand function at a variety of levels. This kind of multilevel approach has been less straightforward, but still largely successful, for a metazoan such asDrosophilawheremore genes andmultiple specialized cells often rescue the effects of a mutation or enhance its minor effect. Success in this endeavor will require a synthesis of many biological disciplines that includes the role of genetic variation as intrinsic to the biological process, not an aspect to be ignored. Consequently, melding variation-based genetic and molecular biological thinking is of critical importance for both fields and is central to our understanding of mechanisms of trait variation, including interindividual variation in disease risk. If most disease, in most humans, is the consequence of the effects of variation at many genes, then knowledge of their functional relationships, rather than merely their identities, is central to understanding the phenotype. This is clearly a problem of ''Systems Biology'' but one that incorporates genetic variation directly. The ability to integrate the realities of such widespread genetic variation, which are ultimately at the causal root of disease mechanisms, with systems biology approaches to understand functional contingencies is central to the challenge of deciphering complex human disease. Genetic Dissection of Complex Phenotypes Genetic transmission rules imply that, even in an intractable species such as us, one can map genomic segments that must contain a disease or trait gene. Such mapping requires identification of the segregation of common sites of variation across the genome, now easy to identify through sequencing, and recognition of a genomic segment identical-by-descent in affected individuals, both within and between families. This task has become easier and more powerful as sequencing technology has improved to provide a nearly complete catalog of variants above 1% frequency in the population; further improvements to sample rarer variants are ongoing (Abecasis et al., 2012). Consequently, genetic mapping, once the province of rare Mendelian disorders, Cell 155, S is now applicable to any human trait or disease. For most complex traits examined, many such loci have been mapped, but the vast majority of the specific genes remain unidentified. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (Moon et al., 2004), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. In fact, most of the mapped loci underlying complex traits remain unresolved at the gene or mechanistic level. Despite the beginning clues to human disease pathophysiology that complex disease mapping is providing, and the slow identification of individual genes, it appears highly unlikely that we can understand traits and diseases this way. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (Yang et al., 2011). For Mendelian disorders, gene identification within a locus is made possible by each mutation being necessary and sufficient for the phenotype, being functionally deleterious and rare, and having an inheritance pattern consistent with the phenotype. It's the mutation that eventually reveals the biology and explains the phenotype. Any component locus for a complex disease has no such restriction, as the causal variants are neither necessary nor sufficient, nor coding (in fact, they are frequently noncoding and regulatory) nor rare (Emison et al., 2010; Jostins et al., 2012). Currently, the major attempts to overcome this impediment involve reliance on single severe mutations at the very same component genes and eptember 26, 2013 a2013 Elsevier Inc. 23 Genetic association studies in humans can synergize with prior knowledge and systems-level quantitative analysis to generate predictions of what pathways and modules are disrupted, where (anatomically), and when (developmentally) to yield a specificmorphological or biochemical phenotype. Consequently, these strategies themselves depend on the hidden biology we seek and are applicable only to the most common human diseases. It appears to us that ignorance of biology has become rate limiting for understanding disease pathophysiology, except perhaps for the Mendelian disorders. There are two ways to get out of this vicious cycle (Figure 1). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (Ilsley et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the vier Inc. complex inheritance problem (Yosef et al., 2013). One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013). Proving Causality: Molecular Koch's Postulates The evidence that a specific gene is involved in a particular human disease has historically been nonstatistical and based on our experience with identifying mutations in Mendelian diseases. Unfortunately, as already mentioned, all of these rules break down in complex phenotypes where neither cosegregation nor exclusivity to affecteds nor obviously deleterious alleles are likely; moreover, many mutations are suspected to be noncoding and in a diversity of regulatory RNA molecules. Consequently, statistical evidence of enrichment has been the mainstay, but this has two negative consequences: first, scanning across the genome or multiple loci covering tens to hundreds of megabases requires very large sample sizes and very strict levels of significance to guard against themany expected falsepositive findings; second, genetic effects that are small or genes with only a few causal alleles are notoriously difficult to detect, although they may be very important to understanding pathogenesis. This difficulty translates into a low power of detection, as common disease alleles cannot be distinguished from bystander associated alleles, whereas rare alleles are observed too infrequently to provide statistical significance. Consequently, although many genes are ''named'' as being responsible in a complex disease or disease process, proof of their involvement is either absent or circumstantial and not direct. In the late 19th century when bacteria were first shown to cause human disease, they were indiscriminately implicated in all manner of disease with little proof (Brown and Goldstein, 1992). One particularly embarrassing example was alcaptonuria, which Sir Archibald Garrod subsequently showed was inherited and which was his first ''inborn error of metabolism.'' We are likely to repeat this ''witch-hunt'' unless we are careful to note that mapping a locus is not equivalent to identifying the gene, and that identifying a gene and its mutations at a locus depends on numerous untested assumptions (mutational type, mutational frequency in cases and controls, coding or regulatory, cell autonomy). Inmicrobiology, Robert Koch set out three postulates that had to be satisfied to connect a specific bacterium (among the multitudes encountered, not unlike current genome analysis) to a disease: the agent had to be isolated from an affected subject, the agent had to produce disease when transmitted to an animal, and the agent had to be recoverable from an animal's lesion (Falkow, 1988). Simply because we cannot follow Koch to the letter in human patients does not absolve us from the responsibility of demonstrating a rigorous level of proof. This is particularly true if we are to pursue therapeutic targets for these diseases. It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-a-vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families Cell 155, S shows how environmental factors beyond diet can be examined even for congenital disorders (Sparrow et al., 2012). Despite these successes, pursuit of Koch's postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates. Ultimately, a lack of understanding of fundamental physiology is the biggest impediment to our understanding of genetically complex human disease. A unique aspect of genetics research seldom appreciated is that genetic effects are chronic biological exposures and as such can pinpoint the earliest stages of disease not readily studied otherwise. In reality, we still do not fully understand the pathogenesis stemming from some of the earliest identified human disease genes. With better understanding of disease mechanism, it seems likely that many disorders that we think of as ''genetic'' may have ameliorative diet, exercise, or other benign environmental ''treatments.'' Given the potential scientific and medical payoffs of disease gene discovery (Chakravarti, 2001), we argue in this Essay of the need for a rigorous examination of the assumptions under which we connect genes to phenotypes. Below we discuss the nature of the ''proof'' that we desire in order to make fundamental discoveries in human pathophysiology. Success in this difficult task requires us to solve a logical conundrum: how can we understand the genes underlying a phenotype if some of these component factors, in isolation, do not have recognizable phenotypes on their own? Both of these goals are approachable, particularly with recent advances in genome-editing technologies that allow the creation of multiple mutations within a single experimental organism (Wang et al., 2013). The second approach is to focus research on why the disease is complex in the first place. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. The chief criteria have been to demonstrate cosegregation with the phenotype in families, exclusivity of the mutation to affected individuals (rare alleles absent in controls), and the nature of themutation (a plausibly deleterious allele at a conserved site within a protein). We need to move beyond lists of plausible genes, to provide rigorous proof for their role in disease. But this goal is unlikely to be achieved in the absence of a superior understanding of the biology of hierarchical function within genomes, how variation alters these functions, and how these altered functions lead to human disease. Probabilistic models of cognition characterize the abstract computational problems underlying inductive inferences and identify their ideal solutions. Recent research employing this strategy has focused on the possibility that the Monte Carlo principle--which concerns sampling from probability distributions in order to perform computations--provides a way to link probabilistic models of cognition to more concrete cognitive and neural processes. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (#CITATION_TAG), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier.","To evaluate the theoretical implications of probabilistic models and increase their predictive power, we must understand the relationships between theories at these different levels of analysis.","['This approach differs from traditional methods of investigating human cognition, which focus on identifying the cognitive or neural processes that underlie behavior and therefore concern alternative levels of analysis.', 'One strategy for bridging levels of analysis is to explore cognitive processes that have a direct link to probabilistic inference.']"
"Such theories provide the necessary computational means to explain the flexible nature of human behavior but in doing so introduce extreme degrees of freedom in accounting for data. More recently, the universality conjecture of Dubrovin (2006) has been proven in #CITATION_TAG for solutions to the KdV equation with analytic initial data vanishing at infinity.","The authors assume that individuals adapt rationally to a utility function given constraints imposed by their cognitive architecture and the local task environment. The new approach narrows the space of predicted behaviors through analysis of the payoff achieved by alternative strategies, rather than through fitting strategies and theoretical parameters to data. It extends and complements established approaches, including computational cognitive architectures, rational analysis, optimal motor control, bounded rationality, and signal detection theory. The authors illustrate the approach with a reanalysis of an existing account of psychological refractory period (PRP) dual-task performance and the development and analysis of a new theory of ordered dual-task responses.","['This assumption underlies a new approach to modeling and understanding cognition-cognitively bounded rational analysis-that sharpens the predictive acuity of general, integrated theories of cognition and action.']"
"Since Brehm's (1956) initial free-choice experiment, psychologists have interpreted the spreading of alternatives as evidence for choice-induced attitude change. It is widely assumed to occur because choosing creates cognitive dissonance, which is then reduced through rationalization. Properties of various classes of solutions to this equation have been extensively studied both analytically and numerically (Bronski and Kutz 2002; Buckingham and Venakides 2007; Carles 2007; Ceniceros and Tian 2002; Forest and Lee 1986; Grenier 1998; Kamvissis 1996; Kamvissis et al. 2003; Klein 2006; Lyng and Miller 2007; #CITATION_TAG; Tovbis et al. 2004 Tovbis et al., 2006.","After making a choice between 2 objects, people reevaluate their chosen item more positively and their rejected item more negatively (i.e., they spread the alternatives). Specifically, if people's ratings/rankings are an imperfect measure of their preferences and their choices are at least partially guided by their preferences, then the FCP will measure spreading, even if people's preferences remain perfectly stable. We show this, first by proving a mathematical theorem that identifies a set of conditions under which the FCP will measure spreading, even absent attitude change. We then experimentally demonstrate that these conditions appear to hold and that the FCP measures a spread of alternatives, even when this spreading cannot have been caused by choice. We discuss how the problem we identify applies to the basic FCP paradigm as well as to all variants that examine moderators and mediators of spreading.","[""In this article, we express concern with this interpretation, noting that the free-choice paradigm (FCP) will produce spreading, even if people's attitudes remain unchanged.""]"
"THE importance of the choice situation is reflected in the considerable amount of theory and research on conflict. Conflict theory has generally dealt, however, with the phenomena that lead up to the choice. What happens after the choice has received little attention. The present paper is concerned with some of the consequences of making a choice. Previous consideration of the consequences of choice have been limited to relatively unspecified hypotheses (1, 3) or to qualitative analysis (4). However, a recent theory by Festinger (2) makes possible several explicit predictions. When ""dissonance"" exists, the person will attempt to eliminate or reduce it. Although space limitations preclude further discussion of the theory, it may be said that several derivations are possible concerning the consequences of making a choice. Choosing between two alternatives creates dissonance and a consequent pressure to reduce it. The magnitude of the dissonance and the 1 This paper is based on a thesis offered in partial fulfillment of the requirements for the Ph.D. degree at the University of Minnesota. They have been studied both analytically and numerically in Ceniceros and Tian (2002), Forest and Lee (1986), Grenier (1998), #CITATION_TAG, Kamvissis (1996), Kamvissis et al. (2003), Miller and Kamvissis (1998), Tovbis et al. (2004 Tovbis et al. (, 2006.","According to this analysis of the choice situation, all cognitive elements (items of information) that favor the chosen alternative are ""consonant,"" and all cognitive elements that favor the unchosen alternative are ""dissonant"" with the choice behavior. The dissonance is reduced by making the chosen alternative more desirable and the unchosen alternative less desirable after the choice than they were before it. Exposing a person to new relevant cognitive elements, at least some of which are consonant, facilitates the reduction of dissonance.","['The present study was designed to test the following: 1.', 'The author wishes to thank his advisor, Dr. Leon Festinger, for his invaluable aid in the formulation and execution of the study.', 'He also wishes to thank the Laboratory for Research in Social Relations, which supported the study.']"
"Impulsive behavior in humans partly relates to inappropriate overvaluation of reward-associated stimuli. Hence, it is desirable to develop methods of behavioral modification that can reduce stimulus value. This phenomenon is similar to the gradient catastrophe of solutions to nonlinear hyperbolic PDEs (#CITATION_TAG).","Here, we tested whether one kind of behavioral modification--the rapid stopping of actions in the face of reward-associated stimuli--could lead to subsequent devaluation of those stimuli. We developed a novel paradigm with three consecutive phases: implicit reward learning, a stop-signal task, and an auction procedure. In the learning phase, we associated abstract shapes with different levels of reward. In the stop-signal phase, we paired half those shapes with occasional stop-signals, requiring the rapid stopping of an initiated motor response, while the other half of shapes was not paired with stop signals. In the auction phase, we assessed the subjective value of each shape via willingness-to-pay. This suggests that the requirement to try to rapidly stop a response decrements stimulus value. It also provides a novel behavioral paradigm with carefully operationalized learning, treatment, and valuation phases.",['This study makes a theoretical link between research on inhibitory control and value.']
"Examples include patterns in hydrodynamic systems such as thermal convection in pure fluids and binary mixtures, Taylor-Couette flow, parametric-wave instabilities, as well as patterns in solidification fronts, nonlinear optics, oscillatory chemical reactions and excitable biological media. The theoretical starting point is usually a set of deterministic equations of motion, typically in the form of nonlinear partial differential equations. These are sometimes supplemented by stochastic terms representing thermal or instrumental noise, but for macroscopic systems and carefully designed experiments the stochastic forces are often negligible. Near a continuous (or supercritical) instability, the dynamics may be accurately described via ""amplitude equations,"" whose form is universal for each type of instability. For many systems appropriate starting equations are either not known or too complicated to analyze conveniently. An important element in nonequilibrium systems is the appearance of deterministic chaos. A greal deal is known about systems with a small number of degrees of freedom displaying ""temporal chaos,"" where the structure of the phase space can be analyzed in detail. For spatially extended systems with many degrees of freedom, on the other hand, one is dealing with spatiotemporal chaos and appropriate methods of analysis need to be developed. These include Rayleigh-Benard convection in a pure fluid, convection in binary-fluid mixtures, electrohydrodynamic convection in nematic liquid crystals, Taylor-Couette flow between rotating cylinders, parametric surface waves, patterns in certain open flow systems, oscillatory chemical reactions, static and dynamic patterns in biological media, crystallization fronts, and patterns in nonlinear optics. is the phenomenon of modulation (or Benjamin-Feir) instability (Agrawal 2006; #CITATION_TAG; Forest and Lee 1986), that is, self-induced amplitude modulation of a continuous wave propagating in a nonlinear medium, with subsequent generation of localized structures.","A unified description is developed, based on the linear instabilities of a homogeneous state, which leads naturally to a classification of patterns in terms of the characteristic wave vector q0 and frequency o0 of the instability. Type Is systems (o0=0, q00) are stationary in time and periodic in space; type IIIo systems (o00, q0=0) are periodic in time and uniform in space; and type Io systems (o00, q00) are periodic in both space and time. The specifics of each system enter only through the nonuniversal coefficients. Far from the instability threshold a different universal description known as the ""phase equation"" may be derived, but it is restricted to slow distortions of an ideal pattern. It is thus useful to introduce phenomenological order-parameter models, which lead to the correct amplitude equations near threshold, and which may be solved analytically or numerically in the nonlinear regime away from the instability. The above theoretical methods are useful in analyzing ""real pattern effects"" such as the influence of external boundaries, or the formation and dynamics of defects in ideal structures. In addition to the general features of nonequilibrium pattern formation discussed above, detailed reviews of theoretical and experimental work on many specific systems are presented.","['A comprehensive review of spatiotemporal pattern formation in systems driven away from equilibrium is presented, with emphasis on comparisons between theory and quantitative experiments.', 'An aim of theory is to describe solutions of the deterministic equations that are likely to be reached starting from typical initial conditions and to persist at long times.']"
"Norm theory's prediction that actions are more mutable than inactions and, as a consequence, will elicit more regret following a negative outcome was tested in two experiments. However, many ideas and methods of Dubrovin (2006) (see also #CITATION_TAG) play an important role in our considerations.","Action and inaction mutation frequencies were assessed in a design where both types of behaviors were presented together and when information about only one type was described. In Experiment 1, Kahneman and Tversky's (1982a) original scenario, where the action or inaction was instrumentally related to the negative outcome, was employed.","['Discussion focuses on an alternative mechanism, besides mutability, that can account for differential emotion elicitation and on specifying the conditions under which the emotion effect will or will not occur.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. The rapid growth of the literature on neuroimaging in humans has led to major advances in our understanding of human brain function but has also made it increasingly difficult to aggregate and synthesize neuroimaging findings. An alternative approach to solving the reverse inference problem has been developed by Poldrack and colleagues (e.g., Lenartowicz, Kalar, Congdon, & Poldrack, 2010; #CITATION_TAG).",,"['Here we describe and validate an automated brain-mapping framework that uses text-mining, meta-analysis and machine-learning techniques to generate a large database of mappings between neural and cognitive states.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. 2 Word Learning as Bayesian Inference Learning even the simplest names for object categories presents a difficult induction proble To illustrate, consider the motivation given by #CITATION_TAG in developing their Bayesian account of word learning.","The theory explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents, by making rational inductive inferences that integrate prior knowledge about plausible word meanings with the statistical structure of the observed examples. The theory addresses shortcomings of the two best-known approaches to modeling word learning that are based on deductive hypothesis elimination or associative learning. Three experiments with adults and children test the Bayesian account's predictions in the context of learning words for object categories at multiple levels of a taxonomic hierarchy. Several extensions of the basic theory are discussed, illustrating the broader potential for Bayesian models of word learning.",['This paper presents a Bayesian framework for understanding how adults and children learn the meanings of words.']
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Perhaps just as critically, #CITATION_TAG argues that interpretation of the neural data from reasoning experiments requires a re-evaluation of psychological theories of reasoning, with the neuroscience evidence indicating that human reasoning involves separate systems for dealing with (a) familiar and unfamiliar material, (b) conflicting information and belief bias, and (c) certain and uncertain information.","Psychological type was assessed by the Francis Psychological Type Scales which provide classification in terms of orientation (extraversion or introversion), perceiving (sensing or intuition), judging (thinking or feeling) and attitude toward the outer world (extraverted judging or extraverted perceiving). Work-related psychological health was assessed by the Francis Burnout Inventory which distinguishes between positive affect (the Satisfaction in Ministry Scale) and negative affect (the Scale of Emotional Exhaustion in Ministry). Strategies are suggested for enabling introverted clergy to cope more effectively and more efficiently with the extraverted demands of ministry.",['This study examines the relationship between work-related psychological health and the Jungian model of psychological type among a sample of 748 clergy serving within The Presbyterian Church (USA).']
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Previous research examining the links between age and burnout has found that there was a significant negative correlation between age and both emotional exhaustion and depersonalization as measured by the Maslach Burnout Inventory. Two theories are often advanced to account for this: those who suffered from emotional exhaustion or depersonalization at a younger age may have left the job either on grounds of ill-health or to seek alternative employment; older workers may have learned how better to pace themselves in their work in order to minimize opportunities for burnout. For example, over the last decade Anderson and his colleagues have attempted to map the core modules of the ACT-R cognitive architecture onto brain regions via fMRI data relating to the BOLD response (e.g., Anderson, 2005; #CITATION_TAG Qin et al.,, 2004.","A sample of 340 Anglican clergy in England and Wales, who had all served one year in ordained ministry, completed a modified form of the Maslach Burnout Inventory, alongside a variety of socio-demographic questions, including age and years in ministry.",['The present study sought to evaluate these two theories.']
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Rational models of cognition typically consider the abstract computational problems posed by the environment, assuming that people are capable of optimally solving those problems. This approach has recently been adopted by Bayesian theorists in the form of ""rational process models"" (#CITATION_TAG; Sanborn, Griffiths, & Navarro, 2010; Shi, Griffiths, Feldman, & Sanborn, 2010).","This differs from more traditional formal models of cognition, which focus on the psychological processes responsible for behavior. In particular, we argue that Monte Carlo methods provide a source of rational process models that connect optimal solutions to psychological processes. We support this argument through a detailed example, applying this approach to Anderson's (1990, 1991) rational model of categorization (RMC), which involves a particularly challenging computational problem. Drawing on a connection between the RMC and ideas from nonparametric Bayesian statistics, we propose 2 alternative algorithms for approximate inference in this model. The algorithms we consider include Gibbs sampling, a procedure appropriate when all stimuli are presented simultaneously, and particle filters, which sequentially approximate the posterior distribution with a small number of samples that are updated as new data become available.","['A basic challenge for rational models is thus explaining how optimal solutions can be approximated by psychological processes.', 'We outline a general strategy for answering this question, namely to explore the psychological plausibility of approximation algorithms developed in computer science and statistics.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Such theories yield different classes of explanation, depending on the extent to which they emphasize adaptation to bounds, and adaptation to some ecology that differs from the immediate local environment. One move in this direction is the ""computational rationality"" approach (#CITATION_TAG; Lewis, Howes, & Singh, 2014), which applies Russell and Subramanian's (1995) notion of bounded optimality for artificial intelligence agents to the analysis of human behavior.","The framework is based on the idea that behaviors are generated by cognitive mechanisms that are adapted to the structure of not only the environment but also the mind and brain itself. We call the framework computational rationality to emphasize the incorporation of computational mechanism into the definition of rational action. Theories are specified as optimal program problems, defined by an adaptation environment, a bounded machine, and a utility function. We illustrate this variation with examples from three domains: visual attention in a linguistic task, manual response ordering, and reasoning. We explore the relation of this framework to existing ""levels"" approaches to explanation, and to other optimality-based modeling approaches.Copyright (c) 2014 Cognitive Science Society, Inc.","['We propose a framework for including information-processing bounds in rational analyses.', 'It is an application of bounded optimality (Russell & Subramanian, 1995) to the challenges of developing theories of mechanism and behavior.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Cognitive science has now produced a massive number of high-quality regularities with many microtheories that reveal important mechanisms. The need for integration is pressing and will continue to increase. Equally important, cognitive science now has the theoretical concepts and tools to support serious attempts at unified theories. Cognitive science is not ready yet for a single theory - there must be multiple attempts. But cognitive science must begin to work toward such unified theories. One can make many arguments for the utility of developing cognitive models within a cognitive architecture (see, e.g., Cassimatis, Bell, & Langley, 2008; #CITATION_TAG), but adopting the concept of cognitive architecture is in fact highly consistent with Marr's original approach to vision.","The argument is made entirely by presenting an exemplar unified theory of cognition both to show what a real unified theory would be like and to provide convincing evidence that such theories are feasible. The exemplar is SOAR, a cognitive architecture, which is realized as a software system. After a detailed discussion of the architecture and its properties, with its relation to the constraints on cognition in the real world and to existing ideas in cognitive science, SOAR is used as theory for a wide range of cognitive phenomena: immediate responses (stimulus-response compatibility and the Sternberg phenomena); discrete motor skills (transcription typing); memory and learning (episodic memory and the acquisition of skill through practice); problem solving (cryptarithmetic puzzles and syllogistic reasoning); language (sentence verification and taking instructions); and development (transitions in the balance beam task). The treatments vary in depth and adequacy, but they clearly reveal a single, highly specific, operational theory that works over the entire range of human cognition, SOAR is presented as an exemplar unified theory, not as the sole candidate.","['The book presents the case that cognitive science should turn its attention to developing theories of human cognition that cover the full range of human perceptual, cognitive, and action phenomena.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. Similar appeals to the utility of CL explanation are common in the Bayesian literature (see, e.g., #CITATION_TAG).","The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks.","['This leads to the development of a computational model of word recognition, the Bayesian reader.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. According to Aristotle, humans are the rational animal. The borderline between rationality and irrationality is fundamental to many aspects of human life including the law, mental health, and language interpretation. But what is it to be rational? One answer, deeply embedded in the Western intellectual tradition since ancient Greece, is that rationality concerns reasoning according to the rules of logic - the formal theory that specifies the inferential connections that hold with certainty between propositions. Piaget viewed logical reasoning as defining the end-point of cognitive development; and contemporary psychology of reasoning has focussed on comparing human reasoning against logical standards. Bayesian Rationality argues that rationality is defined instead by the ability to reason about uncertainty. Although people are typically poor at numerical reasoning about probability, human thought is sensitive to subtle patterns of qualitative Bayesian, probabilistic reasoning. In Chapters 1-4 of Bayesian Rationality (Oaksford & Chater 2007), the case is made that cognition in general, and human everyday reasoning in particular, is best viewed as solving probabilistic, rather than logical, inference problems. In Chapters 5-7 the psychology of ""deductive"" reasoning is tackled head-on: It is argued that purportedly ""logical"" reasoning problems, revealing apparently irrational behaviour, are better understood from a probabilistic point of view. Bayesian approaches have been particularly successful in providing putative accounts of behavior that may be broadly characterized as inductive, such as categorization (Tenenbaum & Griffiths, 2001) and reasoning (#CITATION_TAG), but it is important to recognize that there are alternative frameworks within which CL explanations might be developed.","Data from conditional reasoning, Wason's selection task, and syllogistic inference are captured by recasting these problems probabilistically. The probabilistic approach makes a variety of novel predictions which have been experimentally confirmed.","['The book considers the implications of this work, and the wider ""probabilistic turn"" in cognitive science and artificial intelligence, for understanding human rationality']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Different types of psychotic symptoms may exist, some being normal variants and some having implications for mental health and functioning. Intermittent, infrequent psychotic experiences were common, but frequent experiences were not. Magical Thinking was only weakly associated with these variables. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas may represent expressions of underlying vulnerability to psychotic disorder, but Magical Thinking may be a normal personality variant. An alternative approach to solving the reverse inference problem has been developed by Poldrack and colleagues (e.g., #CITATION_TAG; Yarkoni, Poldrack, Nichols, Van Essen, & Wager, 2011).","Method: Eight hundred and seventy-five Year 10 students from 34 schools participated in a cross-sectional survey that measured psychotic-like experiences using the Community Assessment of Psychic Experiences; depression using the Centre for Epidemiologic Studies Depression Scale; and psychosocial functioning using the Revised Multidimensional Assessment of Functioning Scale. Factor analysis was conducted to identify any subtypes of psychotic experiences. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas were strongly associated with distress, depression and poor functioning.","['Objective: Studies conducted in community samples suggest that psychotic-like experiences are common in the general population, leading to suggestions that they are either variations of normal personality or are different expressions of underlying vulnerability to psychotic disorder.', 'The aim of the present study was to determine if different subtypes of psychotic-like experiences could be identified in a community sample of adolescents and to investigate if particular subtypes were more likely to be associated with psychosocial difficulties, that is, distress, depression and poor functioning, than other subtypes.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Background Epidemiological research has shown that hallucinations and delusions, the classic symptoms of psychosis, are far more prevalent in the population than actual psychotic disorder. These symptoms are especially prevalent in childhood and adolescence. Longitudinal research has demonstrated that psychotic symptoms in adolescence increase the risk of psychotic disorder in adulthood. There has been a lack of research, however, on the immediate clinicopathological significance of psychotic symptoms in adolescence. Adolescents who reported psychotic symptoms were at particularly high risk of having multiple co-occurring diagnoses. Conclusions Psychotic symptoms are important risk markers for a wide range of non-psychotic psychopathological disorders, in particular for severe psychopathology characterised by multiple co-occurring diagnoses. This has resulted in recent demonstrations that the importance sampling algorithm can be implemented by exemplar-based memory mechanisms (#CITATION_TAG; Shi & Griffiths, 2009; Shi et al., 2010).",Method Data from four population studies were used: two early adolescence studies (ages 11-13 years) and two mid-adolescence studies (ages 13-16 years). Studies 1 and 2 involved school-based surveys of 2243 children aged 11-16 years for psychotic symptoms and for emotional and behavioural symptoms of psychopathology. Studies 3 and 4 involved in-depth diagnostic interview assessments of psychotic symptoms and lifetime psychiatric disorders in community samples of 423 children aged 11-15 years. These symptoms should be carefully assessed in all patients.,"['Aims To investigate the relationship between psychotic symptoms and non-psychotic psychopathology in community samples of adolescents in terms of prevalence, co-occurring disorders, comorbid (multiple) psychopathology and variation across early v. middle adolescence.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Computational models will play an important role in our understanding of human higher-order cognition. How can a model's contribution to this goal be evaluated? One can make many arguments for the utility of developing cognitive models within a cognitive architecture (see, e.g., #CITATION_TAG; Newell, 1990), but adopting the concept of cognitive architecture is in fact highly consistent with Marr's original approach to vision.","Further, using analogies with other sciences, the history of cognitive science, and examples from modern-day research programs, this article identifies five activities that have been demonstrated to play an important role in our understanding of human higher-order cognition. These include modeling within a cognitive architecture, conducting artificial intelligence research, measuring and expanding a model's ability, finding mappings between the structure of different domains, and attempting to explain multiple phenomena within a single model.2008 Cognitive Science Society, Inc.","['This article argues that three important aspects of a model of higher-order cognition to evaluate are (a) its ability to reason, solve problems, converse, and learn as well as people do; (b) the breadth of situations in which it can do so; and (c) the parsimony of the mechanisms it posits.', 'This article argues that fits of models to quantitative experimental data, although valuable for other reasons, do not address these criteria.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Self-reported auditory hallucinations in adolescents are markers of concurrent and future psychiatric impairment due to non-psychotic Axis 1 disorders and possibly Axis 2 disorders. It cannot be excluded that there was selective attrition of children and adolescents who developed Schizophrenic or other psychotic disorders later in life. Again, Marr (1982) makes this point, citing Chomsky's distinction between linguistic competence and linguistic performance within transformational grammar (#CITATION_TAG) as corresponding to the difference between a CL explanation and an ARL one, with performance factors modulating the competence theory.",Method. The sample consisted of 914 adolescents between ages 11-18 participating in an ongoing longitudinal study. Responses on the Youth Self-Report questionnaire were used to ascertain hallucinations in adolescents. No subjects were diagnosed with schizophreniform disorders or schizophrenia.,['We aimed to assess the diagnostic outcome of self-reported hallucinations in adolescents from the general population.']
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Popperian falsifiability is not necessarily a requirement for scientific theorizing (#CITATION_TAG), but the implications of the argument concerning performance factors are illustrated by the simulations of Cooper, Yule, and Fox (2003), who compared the behavior of three classes of model-Bayesian, associationist, and hypothesis testing-with that of subjects on a sequential category-learning task (medical diagnosis).",,['Objective: To examine the hypothesis that individuals from the general population who report childhood abuse are at increased risk of developing positive psychotic symptoms.']
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and inter-connexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat&apos;s visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel &amp; Wiesel, 1959). In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. Daniel &amp; Whitteiidge (1959) have recently extended this work in the primate. Notwithstanding Marr's analysis, some might argue that this approach has been successful in understanding the functioning of peripheral systems (with low-level visual processing being perhaps the most celebrated success; #CITATION_TAG).","In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). It was employed by Talbot &amp; Marshall (1941) and by Thompson, Woolsey &amp; Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to) by guest on February 28, 2014jp.physoc.orgDownloaded from J Physiol","['This approach is necessary in order to understand the behaviour of individual cells, but it fails to deal with the problem of the relationship of one cell to its neighbours.']"
"We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Such theories provide the necessary computational means to explain the flexible nature of human behavior but in doing so introduce extreme degrees of freedom in accounting for data. One move in this direction is the ""computational rationality"" approach (#CITATION_TAG; Lewis, Howes, & Singh, 2014), which applies Russell and Subramanian's (1995) notion of bounded optimality for artificial intelligence agents to the analysis of human behavior.","The authors assume that individuals adapt rationally to a utility function given constraints imposed by their cognitive architecture and the local task environment. The new approach narrows the space of predicted behaviors through analysis of the payoff achieved by alternative strategies, rather than through fitting strategies and theoretical parameters to data. It extends and complements established approaches, including computational cognitive architectures, rational analysis, optimal motor control, bounded rationality, and signal detection theory. The authors illustrate the approach with a reanalysis of an existing account of psychological refractory period (PRP) dual-task performance and the development and analysis of a new theory of ordered dual-task responses.","['This assumption underlies a new approach to modeling and understanding cognition-cognitively bounded rational analysis-that sharpens the predictive acuity of general, integrated theories of cognition and action.']"
"Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Since Brehm's (1956) initial free-choice experiment, psychologists have interpreted the spreading of alternatives as evidence for choice-induced attitude change. It is widely assumed to occur because choosing creates cognitive dissonance, which is then reduced through rationalization. One paradigm demonstrating how choice affects preferences is the free-choice paradigm wherein value judgements are gathered on options before and after subjects are forced to choose one and reject the other option [#CITATION_TAG, 7].","After making a choice between two objects, people evaluate their chosen item higher and their rejected item lower (i.e., they ""spread "" the alternatives). Specifically, if people's ratings/rankings are an imperfect measure of their preferences, and their choices are at least partially guided by their preferences, then the FCP will measure spreading, even if people's preferences remain perfectly stable. We show this, first, by proving a mathematical theorem that identifies a set of conditions under which the FCP will measure spreading, even absent attitude change. We then experimentally demonstrate that these conditions appear to hold, and that the FCP measures a spread of alternatives, even when this spreading cannot have been caused by choice. We discuss how the problem we identify applies to the basic FCP paradigm as well as to all variants that examine moderators and mediators of spreading.","[""In this paper, we express concern with this interpretation, noting that the free-choice paradigm (FCP) will produce spreading, even if people's attitudes remain unchanged.""]"
"Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Subjects often rated harmful omissions as less immoral, or less bad as decisions, than harmful commissions. One possibility is this is related to an increased sense of causality and personal responsibility associated with overt actions [#CITATION_TAG].","Subjects read scenarios concerning pairs of options. One option was an omission, the other, a commission. Intentions, motives, and consequences were held constant. Subjects either judged the morality of actors by their choices or rated the goodness of decision options. Such ratings were associated with judgments that omissions do not cause outcomes. The 'omission bias ' revealed in these experiments can be described as an overgeneralization of a useful heuristic to cases in which it is not justified.","['It is based in part on undergraduate honors theses by M. S. and E. M., supervised by J.']"
"Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Rationale: Decision-making involves two fundamental axes of control namely valence, spanning reward and punishment, and action, spanning invigoration and inhibition. We recently exploited a go/no-go task whose contingencies explicitly decouple valence and action to show that these axes are inextricably coupled during learning. Given that this bias is independent of valence it may reflect a general action bias observed in instrumental learning under uncertainty [24, [#CITATION_TAG] [31] [32].","Conversely, serotonin is implicated in motor inhibition and punishment processing. Methods: We combined computational modeling with pharmacological manipulation in 90 healthy human volunteers, using levodopa and citalopram to affect dopamine and serotonin, respectively.",['Objective: To investigate the role of dopamine and serotonin in the interaction between action and valence during learning.']
"Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Auditory verbal hallucinations (AVH) are complex experiences that occur in the context of various clinical disorders. However, their predictive value for specific psychiatric disorders is not entirely clear. This choice bias is especially interesting because it makes explanations of enhanced revaluation due to effort [33, #CITATION_TAG] less plausible for the effect of action on the dynamics of choice-induced preference change.","AVH also occur in individuals from the general population who have no identifiable psychiatric or neurological diagnoses. Longitudinal studies suggest that AVH are an antecedent of clinical disorders when combined with negative emotional states, specific cognitive difficulties and poor coping, plus family history of psychosis, and environmental exposures such as childhood adversity.",['This article reviews research on AVH in nonclinical individuals and provides a cross-disciplinary view of the clinical relevance of these experiences in defining the risk of mental illness and need for care.']
"Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Intentional inhibition refers to stopping oneself from performing an action at the last moment, a vital component of self-control. It has been suggested that intentional inhibition is associated with negative hedonic value, perhaps due to the frustration of cancelling an intended action. By contrast, a general bias towards action could be explained by a higher hedonic value of action itself if inaction is perceived as inhibition [#CITATION_TAG, 36].",Participants gave aesthetic ratings of arbitrary visual stimuli that immediately followed voluntary decisions to act or to inhibit action.,['Here we investigate hedonic implications of the free choice to act or inhibit.']
"Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Impulsive behavior in humans partly relates to inappropriate overvaluation of reward-associated stimuli. Hence, it is desirable to develop methods of behavioral modification that can reduce stimulus value. Furthermore, a recent study shows that devaluation of stimuli can be induced by stopping actions [#CITATION_TAG].","Here, we tested whether one kind of behavioral modification--the rapid stopping of actions in the face of reward-associated stimuli--could lead to subsequent devaluation of those stimuli. We developed a novel paradigm with three consecutive phases: implicit reward learning, a stop-signal task, and an auction procedure. In the learning phase, we associated abstract shapes with different levels of reward. In the stop-signal phase, we paired half those shapes with occasional stop-signals, requiring the rapid stopping of an initiated motor response, while the other half of shapes was not paired with stop signals. In the auction phase, we assessed the subjective value of each shape via willingness-to-pay. This suggests that the requirement to try to rapidly stop a response decrements stimulus value. It also provides a novel behavioral paradigm with carefully operationalized learning, treatment, and valuation phases.",['This study makes a theoretical link between research on inhibitory control and value.']
"Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Long chain n-3 polyunsaturated fatty acids (n-3 LCPUFA) lower risk of coronary heart disease (CHD), but mechanisms are not well understood. Fish oil supplementation caused a significant shift towards the larger, more cholesterol-rich HDL2 particle. In the negative valence group images were selected from the IAPS [#CITATION_TAG] including dangerous animals, mutilations and depictions of violence.","We used proteomics to identify human serum proteins that are altered by n-3 LCPUFA. Such proteins could identify pathways whereby they affect CHD. Eighty-one healthy volunteers entered a double blind randomised trial to receive 3.5 g of fish oil or 3.5 g of high oleic sunflower oil daily. Serum was collected before and after 6 wk of intervention. Serum was analysed by proteomics using 2-DE. Proteins that were differentially regulated were identified by MS. We also analysed serum apolipoprotein A1 (apo A1), high-density lipoprotein (HDL) particle size and haptoglobin.",['These proteins are potential diagnostic biomarkers to assess the mechanisms whereby fish oil protects against CHD in humans.']
"Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Several studies on value of road safety (VRS) has been widely applied in various countries .One approach that is commonly used to calculate the VRS is to measure public willingnes to pay ( WTP ) in order to increase the safety aspect. In this model preferences are based on the behavioral theory of economics perspective that explains that every human behavior based on the preferences and each preference has a utility value further in human behavior will select preferences that have a high utility value. This model has been criticized from several researchers in the field of psychology because many factors affect human behavior and the researchers believe the results of measurements of preferences in this model is still a bias that ultimately affect the validity of the results of the calculation of VRS. This asymmetry of regret for action and inaction has been linked to a number of possible explanations and mediating factors including sense of agency [1], cognitive accessibility [2], and differential mutability [#CITATION_TAG, 4].",,['For these reason the objective of this research is to develop a model of VRS by combining behavior forecasting from the economics perspective and psychology perspective so that the result is closer to reality .']
"The recent progress in molecule-based magnetic materials exhibiting a large magnetocaloric effect at liquid-helium temperatures is reviewed. Theory and examples are presented with the aim of identifying those parameters to be addressed for improving the design of new refrigerants belonging to this class of materials. A failure in the degradation of these modified proteins might induce their accumulation and, thus, participation in lipofuscin or age pigments formation. Since then, magnetic refrigeration is a standard technique in cryogenics, which has shown to be useful to cool down from a few Kelvin [6, #CITATION_TAG].","The epoxyalkenal modified the primary structure of BSA as determined by lysine losses and formation of oxidative stress product epsilon-N-pyrrolylnorleucine (Pnl), which depended on the concentration of the aldehyde and the incubation time. These changes also modified secondary and tertiary structures of the protein, which were determined by studying protein denaturation and polymerization. In addition, all these modifications were parallel to the development of color and fluorescence, which were produced as a consequence of the formation and polymerization of pyrrole amino acid residues.","['Bovine serum albumin (BSA) was incubated for different periods of time and in the presence of several concentrations of 4, 5(E)-epoxy-2(E)-heptenal, at pH 7.4 and 37 degrees C, in an effort to analyze the changes produced in its structure as a consequence of its reaction with this product of lipid oxidation.']"
"Drawing on the classic model of balanced affect, the Francis Burnout Inventory (FBI) conceptulises good work-related psychological health among clergy in terms of negative affect being balanced by positive affect. The conceptualisation, operationalisation and assessment of the work-related psychological health of clergy is a matter that seems to be commanding increasing attention within the fields of pastoral psychology (Charlton, Rolph, Francis, Rolph, & Robbins, 2009) and empirical theology (#CITATION_TAG).","Psychological type was assessed by the Francis Psychological Type Scales which provide classification in terms of orientation (extraversion or introversion), perceiving (sensing or intuition), judging (thinking or feeling) and attitude toward the outer world (extraverted judging or extraverted perceiving). Work-related psychological health was assessed by the Francis Burnout Inventory which distinguishes between positive affect (the Satisfaction in Ministry Scale) and negative affect (the Scale of Emotional Exhaustion in Ministry). Strategies are suggested for enabling introverted clergy to cope more effectively and more efficiently with the extraverted demands of ministry",['This study examines the relationship between work-related psychological health and the Jungian model of psychological type among a sample of 748 clergy serving within The Presbyterian Church (USA).']
"Drawing on the classic model of balanced affect, the Francis Burnout Inventory (FBI) conceptulises good work-related psychological health among clergy in terms of negative affect being balanced by positive affect. Introduction: There is a high prevalence of burnout among emergency medicine (EM) residents. The Maslach Burnout Inventory - Human Services Survey (MBI-HSS) is a widely used tool to measure burnout. In an attempt to bring greater scientific objectivity to the conceptualisation and assessment of clergy work-related psychological health, a number of studies have employed the Maslach Burnout Inventory (MBI) established by #CITATION_TAG.",A 2-Question Summative Score totaling &gt;3 correlated most closely with the primary definition of burnout (Spearman's rho 0.65 [95% confidence interval 0.62-0.68]).,"['The objective of this study was to compare the MBI-HSS and a two-question tool to determine burnout in the EM resident population.Methods: Based on data from the 2017 National Emergency Medicine Resident Wellness Survey study, we determined the correlation between two single-item questions with their respective MBI subscales and the full MBI-HSS.']"
"Drawing on the classic model of balanced affect, the Francis Burnout Inventory (FBI) conceptulises good work-related psychological health among clergy in terms of negative affect being balanced by positive affect. The high scorer on the psychoticism scale is characterised by Eysenck and Eysenck (1976), in their study of psychoticism as a dimension of personality, as being 'cold, impersonal, hostile, lacking in sympathy, unfriendly, untrustful, odd, unemotional, unhelpful, lacking in insight, strange, with paranoid ideas that people were against him.' Lie scales were originally introduced into personality inventories to detect the tendency of some respondents to 'fake good' and so to distort the resultant personality scores (#CITATION_TAG).","The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.","['Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation.']"
"Drawing on the classic model of balanced affect, the Francis Burnout Inventory (FBI) conceptulises good work-related psychological health among clergy in terms of negative affect being balanced by positive affect. This article reviews the external evidence bearing on the internal structure of words in Semitic languages. This evidence comes mostly from psycholinguistics, neurolinguistics and lexical studies. Most of these studies have concluded that the mental lexicon of Semitic decomposes words into consonantal roots combined with other morphological units. While some studies in formal linguistics have also reached this conclusion on the basis of internal evidence, others have argued that Semitic roots must contain both consonants and vowels. This article reviews the theoretical and empirical motivations behind these two conclusions. A series of studies has reported findings employing this modified form of the Maslach Burnout Inventory in the United Kingdom among Roman Catholic priests engaged in parochial ministry (Francis, Louden, & Rutledge, 2004; Francis, Turton, & ASSESSING WORK-RELATED PSYCHOLOGICAL HEALTH Louden, 2007) and among Anglican parochial clergy (Francis & Rutledge, 2000; Francis & Turton, 2004a, 2004b Randall, 2004 Randall,, 2007 Rutledge, 2006;. The modified Maslach Burnout Inventory for use among clergy has been further modified and refined by #CITATION_TAG.",,"['It is proposed that i) convergences between external and internal types of evidence are theoretically significant, and ii) such convergences currently favor formal models of morphology that regard the consonantal root as a legitimate morphological unit in Semitic languages.']"
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Different types of psychotic symptoms may exist, some being normal variants and some having implications for mental health and functioning. Intermittent, infrequent psychotic experiences were common, but frequent experiences were not. Magical Thinking was only weakly associated with these variables. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas may represent expressions of underlying vulnerability to psychotic disorder, but Magical Thinking may be a normal personality variant Other PLE dimensions have been labeled as persecutory ideas, bizarre experiences and magical thinking (#CITATION_TAG), or delusions, paranoia, grandiosity and paranormal beliefs (Wigman, Van Winkel, Raaijmakers et al., 2011a).","Eight hundred and seventy-five Year 10 students from 34 schools participated in a cross-sectional survey that measured psychotic-like experiences using the Community Assessment of Psychic Experiences; depression using the Centre for Epidemiologic Studies Depression Scale; and psychosocial functioning using the Revised Multidimensional Assessment of Functioning Scale. Factor analysis was conducted to identify any subtypes of psychotic experiences. Four subtypes of psychotic-like experiences were identified: Bizarre Experiences, Perceptual Abnormalities, Persecutory Ideas, and Magical Thinking. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas were strongly associated with distress, depression and poor functioning.","['The aim of the present study was to determine if different subtypes of psychotic-like experiences could be identified in a community sample of adolescents and to investigate if particular subtypes were more likely to be associated with psychosocial difficulties, that is, distress, depression and poor functioning, than other subtypes.']"
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. A comprehensive understanding of the phenomenology of auditory hallucinations (AHs) is essential for developing accurate models of their causes. Yet, only 1 detailed study of the phenomenology of AHs with a sample size of N >= 100 has been published. However, own-thought AVH are common in clinical samples (#CITATION_TAG).","We undertook the most comprehensive phenomenological study of AHs to date in a psychiatric population (N = 199; 81% people diagnosed with schizophrenia), using a structured interview schedule.","['We propose that there are likely to be different neurocognitive processes underpinning these experiences, necessitating revised AH models.']"
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Psychotic-like experiences (PLE) in the general community are common. In young adults (age 18-24), a similar tendency for women reporting higher levels of hallucinations has been found (#CITATION_TAG).",Subjects completed the Composite International Diagnostic Interview (CIDI) and the 21-item Peters Delusional Inventory (PDI).,['The aims of this study were to examine the prevalence and demographic correlates of PLE in young adults.The sample consisted of 2441 subjects aged 18-23 years.']
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. In clinical samples, rates of auditory hallucinations have been suggested to be higher in female patients (Rector & Seeman, 1992; #CITATION_TAG), although there are many confounding variables which may account for apparent gender differences (e.g., sampling biases, comorbid depression, etc. ).","Hallucinatory experiences were measured by the hallucination item of the Brief Psychiatric Rating Scale in a sample of 160 drug-free inpatients admitted for the treatment of an acute episode. Patients were categorized as either non-hallucinators or hallucinators. An additional analysis included only those who had 'severe' hallucinations (i.e., score > or = 5).","['Our objective was to test the hypothesis that gender, the age of illness onset, or the interaction between these two variables, would distinguish acutely ill schizophrenic and schizoaffective patients who hallucinated from those who did not.']"
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Apart from individuals with clinical psychosis, community surveys have shown that many otherwise well individuals endorse items designed to identify psychosis. Finally, socioeconomic variables have been suggested as possible modulatory variables on the expression of PLE (#CITATION_TAG; Scott, Chant, Andrews & McGrath, 2006).",,['The aim of this study was to characterize the demographic correlates of individuals who endorse psychosis screening items in a large general community sample']
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Audibility, the perceptual aspect of AVH, may result from a disinhibition of the auditory cortex in response to self-generated speech. In isolation, this aspect leads to audible thoughts: Gedankenlautwerden. This failure may be related to the fact that cerebral activity associated with AVH is predominantly present in the speech production area of the right hemisphere. Since normal inner speech is derived from the left speech area, an aberrant source may lead to confusion about the origin of the language fragments. When alienation is not accompanied by audibility, it will result in the experience of thought insertion. Further, #CITATION_TAG suggest that the audibility component, independent of content and appraisal, should be considered separately as an important perceptual component of AVHs and reflects the neural pathophysiology underlying AVH.","The second component is alienation, which is the failure to recognize the content of AVH as self-generated. The 2 hypothesized components are illustrated using case vignettes.Copyright 2010 S. Karger AG, Basel.",['This study proposes a theoretical framework which dissects auditory verbal hallucinations (AVH) into 2 essential components: audibility and alienation.']
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Previous research has suggested that psychosis is better described as a continuum rather than a dichotomous entity. In general non-clinical populations, women have been found to have higher incidence of positive psychotic symptoms (Maric, Krabbendam, Vollebergh, de Graaf & Van Os, 2003), and specifically auditory hallucinations (#CITATION_TAG; Tien, 1991).","Multinomial logistic regression models were used to interpret the nature of the latent classes, or groups, by estimating the associations with demographic factors, clinical variables, and experiences of traumatic events.The best fitting latent class model was a four-class solution: a psychosis class, a hallucinatory class, an intermediate class, and a normative class.",['This study aimed to describe the distribution of positive psychosis-like symptoms in the general population by means of latent class analysis.Latent class analysis was used to identify homogeneous sub-types of psychosis-like experiences.']
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Most shrinkage occurred during the first year of treatment. There was little change in pulmonary function. Females in this age group report higher levels of depression (#CITATION_TAG; Nolen-Hoeksema & Girgus, 1994; Welham, Scott, Williams et al., 2009) indicating that they expericence a higher stress level which could account for their having more stressful AVH.","Experimental Design: In this multicenter phase 2 nonrandomized open label trial, 16 patients with tuberous sclerosis or sporadic LAM and renal angiomyolipoma(s) were treated with oral sirolimus for up to 2 years. Summated angiomyolipoma diameters were reduced in all 16 patients and by 30% or more in eight (all from the per protocol group of 10).","['Purpose: Renal angiomyolipomas are a frequent manifestation of tuberous sclerosis and sporadic lymphangioleiomyomatosis (LAM).', 'This study investigated the efficacy and safety of the mTORC1 inhibitor sirolimus for treatment of renal angiomyolipomas in patients with these disorders.']"
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Current psychological theories state that the clinical outcome of hallucinatory experiences is dependent on the degree of associated distress, anxiety, and depression. Also, depressive mood has also been shown to increase the risk of transition to psychosis in individuals with hallucinatory experiences (#CITATION_TAG).",,['This study examined the hypothesis that the risk for onset of psychotic disorder in individuals with self-reported hallucinatory experiences would be higher in those who subsequently developed depressed mood than in those who did not.status: publishe']
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Autism spectrum disorders (ASDs) are highly prevalent neurodevelopmental disorders, but the underlying pathogenesis remains poorly understood. Recent studies have implicated the cerebellum in these disorders, with post-mortem studies in ASD patients showing cerebellar Purkinje cell (PC) loss, and isolated cerebellar injury has been associated with a higher incidence of ASDs. However, the extent of cerebellar contribution to the pathogenesis of ASDs remains unclear. Tuberous sclerosis complex (TSC) is a genetic disorder with high rates of comorbid ASDs that result from mutation of either TSC1 or TSC2, whose protein products dimerize and negatively regulate mammalian target of rapamycin (mTOR) signalling. However, the roles of Tsc1 and the sequelae of Tsc1 dysfunction in the cerebellum have not been investigated so far. However, the conversion to a psychiatric disorder and need for psychiatric care is probably mediated by environmental risk factors as well as factors such as coping style, cognitive biases and negative emotional affect (#CITATION_TAG).",,"['TSC is an intriguing model to investigate the cerebellar contribution to the underlying pathogenesis of ASDs, as recent studies in TSC patients demonstrate cerebellar pathology and correlate cerebellar pathology with increased ASD symptomatology.']"
"Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Over the years, the prevalence of auditory verbal hallucinations (AVHs) have been documented across the lifespan in varied contexts, and with a range of potential long-term outcomes. Initially the emphasis focused on whether AVHs conferred risk for psychosis. However, recent research has identified significant differences in the presentation and outcomes of AVH in patients compared to those in non-clinical populations. For this reason, it has been suggested that auditory hallucinations are an entity by themselves and not necessarily indicative of transition along the psychosis continuum. In children, need for care depends upon whether the child associates the voice with negative beliefs, appraisals and other symptoms of psychosis. For example, the existence of maladaptive coping strategies in patient populations is one significant difference between clinical and non-clinical groups which is associated with a need for care. Whether or not these mechanisms start out the same and have differential trajectories is not yet evidenced. Furthermore, childhood trauma and childhood sexual abuse is more often experienced by females (Janssen et al., 2004; Molnar et al., 2001) and is also one of the strongest risk factors for developing clinically impairing AVH and psychotic symptoms (Daalman et al., 2012; #CITATION_TAG; Janssen et al., 2004).","The stages described include childhood, adolescence, adult non-clinical populations, hypnagogic/hypnopompic experiences, high schizotypal traits, schizophrenia, substance induced AVH, AVH in epilepsy, and AVH in the elderly. This includes features of the voices such as the negative content, frequency, and emotional valence as well as anxiety and depression, independently or caused by voices presence.","['This review will examine the presentation of auditory hallucinations across the life span, as well as in various clinical groups.']"
"Protein-bound pyrroles are a sign of oxidative damage. Pyrraline (epsilon-2-(formyl-5-hydroxymethyl-pyrrol-1-yl)-L-norleucine) is an advanced Maillard reaction product formed from 3-deoxyglucosone in the non-enzymatic reaction between glucose and the epsilon-amino group of lysine residues on proteins. Although its presence in vivo as well as in in vitro incubations of proteins with sugars has been documented by immunochemical methods using polyclonal and monoclonal antibodies, its formation in proteins has recently been questioned by similar methodology. Time- and sugar concentration-dependent increase in pyrraline formation was noted in serum albumin incubated with either 100 mM glucose or 50 mM 3-deoxyglucosone. The initial stages of many glycation reactions are also mediated by pyrrole intermediates [#CITATION_TAG] and these may progress further to AGE products that may constitute cross-links within and between proteins.","Formation of pyrraline from 3-deoxyglucosone was rapid at slightly acidic pH, confirming its synthetic pathway through this Maillard reaction intermediate. Using a slightly different approach, pyrraline-like material was detected in human plasma proteins following enzyme digestion and analysis by high performance liquid chromatography.","['To clarify this issue, we investigated pyrraline formation in proteins following alkaline hydrolysis and quantitation by high-performance liquid chromatography on a C18 reverse-phase column.']"
"Protein-bound pyrroles are a sign of oxidative damage. Tuberous sclerosis complex (TSC) is an autosomal dominant genetic disorder that occurs upon mutation of either the TSC1 or TSC2 genes, which encode the protein products hamartin and tuberin, respectively. AGE and ALE increasingly accumulate during aging and in chronic diseases [#CITATION_TAG], suggesting that detecting pyrroles in proteins should be a good way to develop biomarkers for early stage disease.","First, coexpression of hamartin and tuberin repressed phosphorylation of 4E-BP1, resulting in increased association of 4E-BP1 with eIF4E; importantly, a mutant of TSC2 derived from TSC patients was defective in repressing phosphorylation of 4E-BP1. Third, hamartin and tuberin blocked the ability of amino acids to activate S6K1 within nutrient-deprived cells, a process that is dependent on mTOR.","['Here, we show that hamartin and tuberin function together to inhibit mammalian target of rapamycin (mTOR)-mediated signaling to eukaryotic initiation factor 4E-binding protein 1 (4E-BP1) and ribosomal protein S6 kinase 1 (S6K1).']"
"This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Abstract In recent years, our understanding of how tense systems vary across languages has been greatly advanced by formal semantic study of languages exhibiting fewer tense categories than the three commonly found in European languages. However, it has also often been reported that languages can sometimes distinguish more than three tenses. Such languages appear to have 'graded tense' systems, where the tense morphology serves to track how far into the past or future a reported event occurs. Like many languages of the Bantu family, Gikuyu  appears to exhibit a graded tense system, wherein four grades of past tense and three grades of future tense are distinguished. I argue that the prefixes traditionally labeled as 'tenses ' in Gikuyu exhibit important differences from (and similarities to) tenses in languages like English. Although, like tenses in English, these 'temporal remoteness prefixes ' in Gikuyu  introduce presuppositions regarding a temporal parameter of the clause, in Gikuyu  these presuppositions concern the 'event time ' of the clause directly, rather than the 'topic time'. Consequently, the key difference between Gikuyu  and lan-guages like English lies not in how many tenses are distinguished, but in whether tense-like features are able to modify other, lower verbal functional projections in the clause An alternative view differentiates between processing routes on the basis of word form morphology (#CITATION_TAG; Pinker & Ullman, 2002; Ullman, 2001; Ullman et al., 1997).",,"['This paper presents a formal semantic analysis of the tense-aspect system of Gikuyu  (Kikuyu), a Northeastern Bantu language of Kenya.']"
"This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Autism spectrum disorders (ASDs) are a group of clinically and genetically heterogeneous neurodevelopmental disorders characterized by impaired social interactions, repetitive behaviors and restricted interests (Baird et al., 2006; Zoghbi and Bear, 2012). The genetic defects in ASDs may interfere with synaptic protein synthesis. Synaptic dysfunction caused by aberrant protein synthesis is a key pathogenic mechanism for ASDs (Kelleher and Bear, 2008; Richter and Klann, 2009; Ebert and Greenberg, 2013). The mammalian target of the rapamycin (mTOR) pathway plays central roles in synaptic protein synthesis (Hay and Sonenberg, 2004; Hoeffer and Klann, 2010; Hershey et al., 2012). Recently, Gkogkas and colleagues published exciting data on the role of downstream mTOR pathway in autism (Gkogkas et al., 2013) (Figure  (Figure11). However, the authors used a theoretical linguistic approach (prosodic nonconcatenative morphology developed by #CITATION_TAG) rather than models of processing to account for their data.",,['Understanding the details about aberrant synaptic protein synthesis is important to formulate potential treatment for ASDs.']
"This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Eye movement data can provide an in-depth view of human reasoning and the decision-making process, and modern information retrieval (IR) research can benefit from the analysis of this type of data. In her study of regular and irregular morpho-syntax in English, #CITATION_TAG concluded that models which propose differences in processing regular and irregular morpho-syntax fail to account for the whole spectrum of regular morphology.","To address this objective, a multimethod research design was employed that involved observation of participants' eye movements, talk-aloud protocols, and postsearch interviews. We present a novel stepwise methodological framework for the analysis of relevance judgments and eye movements on the Web and show new patterns of relevance criteria use during predictive relevance judgment.",['The aim of this research was to examine the relationship between relevance criteria use and visual behavior in the context of predictive relevance judgments.']
"This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. DAFFODIL is a digital library system targeting at strategic support during the information search process. The visualisation of stratagems is based on a strictly object-oriented tool-based model. This discrepancy supports previous studies which point to the fact that linking agrammatism to effects of regularity in the dual mechanism account needs to be revised (De Diego Balaguer et al., 2004; Laiacona and Caramazza, 2004; #CITATION_TAG).",,['This paper presents the graphical user interface with a particular view on the integration of stratagems to enable strategic support.']
"This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Most roots in Arabic are made up of three consonants which are referred to as triliteral roots or consonantal roots (Zabbal, 2002; #CITATION_TAG).","The observed shifting behavior allows for examining the question of predictability. The observed shifting patterns lead to the development of a new model for information seeking behavior that describes multiple search stages, aspects and dimensions. The model represents a non-linear process of multiple re-iterative cycles that make up the overall flow of communication with the information system. The model expands significantly earlier theoretical and empirical models as it represents a series of re-iterative processes that characterize the information seeking interaction.","[""The research reported here focuses on the users' information seeking behavior."", 'Set in the framework of the interactive information retrieval paradigm, the research examines the micro-dynamics of the information seeking process, and explores search stages that users pursue as they proceed to solve their information problem.', ""The research analyzes the users' information seeking behavior as they move between search stages, and looks for patterns in the transition.""]"
"This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. When train-ing vocabulary reflected the English-like competition between regular (suffixed) and irregular verbs (e.g., go 3 went, hit 3 hit), the acquisition of regular verbs became increasingly sus-ceptible to injury, while the irregulars were learned quickly and were relatively impervious to damage. Patterns of gener-alization to novel forms conflicts with the assumption that this behavioral dissociation is indicative of selective impairment of the learning and generalization of the past tense rule, while the associative lexical-based mechanism is left intact. The single mechanism account maintains that one mechanism governs production of both regular and irregular forms (Bird et al., 2003; Braber et al., 2005; Bybee, 1995; Joanisse and Seidenberg, 1999; Juola and Plunkett, 2000; Lambon Ralph et al., 2005; #CITATION_TAG; Patterson et al., 2001; Rumelhart and McClelland, 1986; Stockall, & Marantz, 2006; Yang, 2000).","When networks were trained only on phonological encodings of stem-suhed pairs similar to English regular verbs (e.g., walk 3 wulkd), long-term deficits (i.e., ""critical period "" effects) were not observed, yet there were substantive short-term effects of injury.","[""This paper investigates constraints on dissociation and plas-ticity in a connectionist model undergoing random ''lesions'' both prior to and during training.""]"
"This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Published online: 27 Apr 2017Comparative research on aphasia and aphasia rehabilitation is challenged by the lack of comparable assessment tools across different languages. In English, a large array of tools is available, while in most other languages, the selection is more limited. These subtests were taken from two sources: translated subtests of the Comprehensive Aphasia Test (CAT) (#CITATION_TAG) and subtests that have been developed in speech and language clinics in Jordan.","Specifically, we focus on challenges and solutions related to the use of imageability, frequency, word length, spelling-to-sound regularity and sentence length and complexity as underlying properties in the selection of the testing material.For the work reported in this article, we were supported by various funding bodies.","['In this article, some key challenges encountered in the adaptation process and the solutions to ensure that the resulting assessment tools are linguistically and culturally equivalent, are proposed.']"
"We report a high-pressure single-crystal synchrotron x-ray diffraction on a LaAlO 3 single crystal. Lichens, a classic example of an obligate symbiosis between fungi and photobionts (which could be algae or cyanobacteria), are abundant in many terrestrial ecosystems. The genetic structure of the photobiont population found in association with a lichen-forming fungal species could be affected by fungal reproductive mode and by the spatial extent of gene flow in the photobiont. We had previously shown that the fungus exhibited no genetic structure among four local sites or three phorophyte species. The pressure-induced rhombohedral to cubic transition of LAO was revealed by a powder Raman spectroscopy and synchrotron diffraction study [#CITATION_TAG].","Using DNA sequences from one nuclear ribosomal and two chloroplast loci, we analyzed the genetic structure of the photobiont associated with the fungus Ramalina menziesii at an oak woodland study site in southern California. We hypothesize that R. menziesii is locally adapted to the phorophyte species through habitat specialization in the algal partner of the symbiosis.",['Our goals were to identify the photobiont species and assess its genetic structure.']
"We report a high-pressure single-crystal synchrotron x-ray diffraction on a LaAlO 3 single crystal. Abstract Algal and fungal symbionts of the lichenized genus Thamnolia typically co-disperse through thallus fragmentation, which may be expected to lead to fungal associations with a restricted range of algal symbionts. Phylogenetic analyses of internal transcribed spacer rDNA (ITS) sequences suggest that Trebouxia algae associated with T. vermicularis are not monophyletic. The pressure scale used was the equation given by Jacobsen [#CITATION_TAG].","Additionally, as a species, T. vermicularis associates with a range of algae equal to or greater than that of many other fungal taxa.",['Here we examine the range of algae that associate with the fungus Thamnolia vermicularis.']
"We report a high-pressure single-crystal synchrotron x-ray diffraction on a LaAlO 3 single crystal. Biological soil crusts (BSCs) are ubiquitous lichen-bryophyte microbial communities, which are critical structural and functional components of many ecosystems. However, BSCs are rarely addressed in the restoration literature. Because BSCs are ecosystem engineers in high abiotic stress systems, loss of BSCs may be synonymous with crossing degradation thresholds. The strong influence that BSCs exert on ecosystems is an underexploited opportunity for restorationists to return disturbed ecosystems to a desirable trajectory. Following Glazer's notations [25, #CITATION_TAG, 27], the tilt system in LAO is noted a − a − a − and gives rise to superstructure reflections that can be indexed in the doubled cubic cell with the general form (hkl) C where h, k and l are odd integers and one index at least is different from the others.","In practice, BSC rehabilitation has three major components: (1) establishment of goals; (2) selection and implementation of rehabilitation techniques; and (3) monitoring. Statistical predictive modeling is a useful method for estimating the potential BSC condition of a rehabilitation site. Various rehabilitation techniques attempt to correct, in decreasing order of difficulty, active soil erosion (e.g., stabilization techniques), resource deficiencies (e.g., moisture and nutrient augmentation), or BSC propagule scarcity (e.g., inoculation).","['The purposes of this review were to examine the ecological roles BSCs play in succession models, the backbone of restoration theory, and to discuss the practical aspects of rehabilitating BSCs to disturbed ecosystems.']"
"We report a high-pressure single-crystal synchrotron x-ray diffraction on a LaAlO 3 single crystal. The recent international surge in private equity markets has been accompanied by growing interest in its nature and effects. Private equity involves investment in unquoted companies and includes both early stage venture capital and later stage buyouts. Following Glazer's notations [#CITATION_TAG, 26, 27], the tilt system in LAO is noted a − a − a − and gives rise to superstructure reflections that can be indexed in the doubled cubic cell with the general form (hkl) C where h, k and l are odd integers and one index at least is different from the others.","Thereafter, the findings of a large cross section of empirical studies within a range of different national settings are compared and contrasted.",['This paper provides a review of the different theoretical approaches that have been deployed to understand this phenomenon.']
"We report a high-pressure single-crystal synchrotron x-ray diffraction on a LaAlO 3 single crystal. Research Issue: This study investigates the employment consequences of private equity acquisitions, in particular institutional buyouts (IBOs), in the UK. It involves a pre- and post-acquisition analysis of employment and performance characteristics for a sample of acquired firms and a matched sample of non-acquired firms. Two important theoretical issues emerge. The evolution of the tilt angles in LAO itself was investigated by single crystal diffraction under hydrostatic stress up to 8 GPa [7] and non-hydrostatic stress [#CITATION_TAG].","The first is a need to conceptualize skills and human capabilities on a collective dimension, specific to a particular organizational setting, and the extent to which they contribute to the organization's performance.",['This highlights the need for new management to better understand the link between employment and performance in the specific corporate setting of the acquired firm.']
We report a high-pressure single-crystal synchrotron x-ray diffraction on a LaAlO 3 single crystal. It appeared worthwhile to us to present a state-of-the-art look at the field of ferroelectrics. Explorations of pressure-temperature or pressure-substitution phase diagrams remain even rarer [#CITATION_TAG].,For each topic we will try to work out both current interesting approaches and an outlook into future challenges.,"['We are certainly not attempting to provide a complete review of all aspects of the field of ferroelectrics over the last years but we wish to transport a flavour of the current excitement in the field through the (subjective) choice of four specific examples of current interest: (i) Piezoelectrics and the morphotropic phase boundary, (ii) Multiferroics, (iii) The effect of high pressure on ferroelectrics and (iv) Strain-engineering in ferroelectric oxide thin films.']"
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Genetic disorders that present with a high incidence of autism spectrum disorders (ASD) offer tremendous potential both for elucidating the underlying neurobiology of ASD and identifying therapeutic drugs and/or drug targets. Tuberous sclerosis complex (TSC) is one such genetic disorder that presents with ASD, epilepsy, and intellectual disability. Cell culture and mouse model experiments have identified the mTOR pathway as a therapeutic target in this disease. Typical TSC lesions include hypomelanic macules and facial angiofibromas, as well as brain cortical tubers, subependymal nodules, and subependymal giant cell astrocytomas (SEGAs) (Holmes et al, 2007; Curatolo et al, 2008; #CITATION_TAG).",,['This review summarizes the advantages of using TSC as model of ASD and the recent advances in the translational and clinical treatment trials in TSC.Copyright (c) 2012 Elsevier Ltd. All rights reserved.']
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Autism spectrum disorders (ASDs) are highly prevalent neurodevelopmental disorders, but the underlying pathogenesis remains poorly understood. Recent studies have implicated the cerebellum in these disorders, with post-mortem studies in ASD patients showing cerebellar Purkinje cell (PC) loss, and isolated cerebellar injury has been associated with a higher incidence of ASDs. However, the extent of cerebellar contribution to the pathogenesis of ASDs remains unclear. Tuberous sclerosis complex (TSC) is a genetic disorder with high rates of comorbid ASDs that result from mutation of either TSC1 or TSC2, whose protein products dimerize and negatively regulate mammalian target of rapamycin (mTOR) signalling. However, the roles of Tsc1 and the sequelae of Tsc1 dysfunction in the cerebellum have not been investigated so far. Others have also provided evidence that autistic-like behavior can be prevented with mTOR treatment in mouse models of TSC (#CITATION_TAG; Talos et al, 2012; Reith et al. 2013).",,"['TSC is an intriguing model to investigate the cerebellar contribution to the underlying pathogenesis of ASDs, as recent studies in TSC patients demonstrate cerebellar pathology and correlate cerebellar pathology with increased ASD symptomatology.']"
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Tuberous sclerosis (TSC) is a neurocutaneous disorder with an autosomal-dominant pattern of inheritance and is caused by heterozygous mutations in the TSC1 or TSC2 gene. Neuropsychiatric conditions, including intellectual disability, autism and epilepsy, are highly prevalent in TSC populations. Emerging intervention studies in animal models show striking effects of mTORC1 inhibitors on TSC-related CNS manifestations. In addition to manifestations in the skin and nervous system, TSC is associated with hallmark tumors in the kidney, lung, heart and liver such as angiomyolipomas, lymphangioleiomyomatosis, and rhabdomyomas (Crino et al, 2006; Curatolo et al, 2008; Orlova and Crino, 2010; #CITATION_TAG).",,"['Here, I review recent findings that shed light on some of the neurobiological mechanisms that may contribute to the pathogenesis of TSC-associated neuropsychiatric impairments.']"
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. The increased adoption of AMR techniques in the past decade is driven in part by the public availability of AMR codes and frameworks. Others have also provided evidence that autistic-like behavior can be prevented with mTOR treatment in mouse models of TSC (Tsai et al, 2012; Talos et al, 2012; #CITATION_TAG).","Two basic techniques are in use to extend the dynamic range of Eulerian grid simulations in multi-dimensions: cell refinement, and patch refinement, otherwise known as block-structured adaptive mesh refinement (SAMR). I provide a partial list of resources for those interested in learning more about AMR simulations.",['I survey the use and impact of adaptive mesh refinement (AMR) simulations in numerical astrophysics and cosmology.']
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Tuberous sclerosis complex (TSC) is a multiorgan genetic disease caused by mutations in the TSC1 or TSC2 genes. TSC has been recognized for many years as an important cause of severe neurological disease with patients suffering from epilepsy, developmental delay, autism, and psychiatric problems. It is estimated that approximately 70 to 90% of all TSC patients have seizures at some point during their life (Holmes et al, 2007; #CITATION_TAG; Sahin, 2012).","In addition, I will discuss the development of new animal models, translational data, and recent clinical trials using mammalian target of rapamycin complex 1 inhibitors such as rapamycin.The past few years have seen spectacular advances that have energized TSC-related research and challenged existing symptomatic treatments.","['During the last year, there have been enormous advances in basic and translational research pertaining to TSC.In this review, I discuss the basic science findings that position the TSC1 and TSC2 genes as critical regulators of the mammalian target of rapamycin kinase within mammalian target of rapamycin complex 1.']"
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Storms are thought to be an important mechanism for transporting coarse sediment from shallow carbonate platforms to the deep-sea, and bank-edge sediments may offer an unexplored archive of long-term hurricane activity. Although the majority of TSC individuals with autism have a history of infantile spasms, there are also subjects who develop autism but with no history of seizures (#CITATION_TAG), suggesting that additional factors also play a role in this association.","High energy event layers within the resulting archive are (1) broadly correlated throughout an offbank transect of multi-cores, (2) closely matched with historic hurricane events, and (3) synchronous with previous intervals of heightened North Atlantic hurricane activity in overwash reconstructions from Puerto Rico and elsewhere in the Bahamas.","['Establishing longer records is essential for understanding mid-Holocene patterns of storminess and their climatic drivers, which will lead to better forecasting of how climate change over the next century may affect tropical cyclone frequency and intensity.', 'Here, we develop this new approach, reconstructing more than 7000 years of North Atlantic hurricane variability using coarse-grained deposits in sediment cores from the leeward margin of the Great Bahama Bank.']"
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Tuberous sclerosis complex (TSC) is an autosomal dominant genetic disorder that occurs upon mutation of either the TSC1 or TSC2 genes, which encode the protein products hamartin and tuberin, respectively. The discovery that the products of the TSC genes regulate mTOR signaling (#CITATION_TAG) has paved the way to current experimental mTORC inhibitor-based treatment approaches.","First, coexpression of hamartin and tuberin repressed phosphorylation of 4E-BP1, resulting in increased association of 4E-BP1 with eIF4E; importantly, a mutant of TSC2 derived from TSC patients was defective in repressing phosphorylation of 4E-BP1. Third, hamartin and tuberin blocked the ability of amino acids to activate S6K1 within nutrient-deprived cells, a process that is dependent on mTOR.","['Here, we show that hamartin and tuberin function together to inhibit mammalian target of rapamycin (mTOR)-mediated signaling to eukaryotic initiation factor 4E-binding protein 1 (4E-BP1) and ribosomal protein S6 kinase 1 (S6K1).']"
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Larger open-label studies of everolimus for SEGAs in TSC showed that the treatment was associated with a marked and sustained tumor reduction (#CITATION_TAG).",,"['Abstract Aim  We evaluate environmental and historical determinants of modern species composition for upland vegetation types across Cape Cod, Massachusetts, a region that supports numerous uncommon species assemblages that are conservation priorities.']"
"Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. The double-blind, placebo-controlled, phase 3 trial everolimus for angiomyolipoma in TSC where this patient took part, found an angiomyolipoma response rate of 42% for everolimus treatment, which contrasted with a 0% response for placebo (#CITATION_TAG).",,"['Aim  We analysed lake-sediment pollen records from eight sites in southern New England to address: (1) regional variation in ecological responses to post-glacial climatic changes, (2) landscape-scale vegetational heterogeneity at different times in the past, and (3) environmental and ecological controls on spatial patterns of vegetation.']"
"The data stems from studies of searchers interaction with an XML information retrieval system. In this paper we present findings from a comparative study of data collected from client and server side loggings. The purpose is to see what factors of effort can be captured from the two logging methods. Charcoal records from lake sediment and fire-scar networks from long-lived tree species have improved our understanding of long-term relationships between fire events and climate. This work has primarily focused on historically fire-prone ecosystems and regions. In the northeastern USA, where wildfire has been relatively infrequent in historical times, fire-risk assessments have incorporated little-to-no pre-historical data and little is known about long-term fire-climate relationships. However, although prolonged droughts were widespread and associated with higher probability of fire, the fire events were rarely synchronous among sites, with the exception of ~550 years before present (yr BP) when all three sites experienced both drought and fire. While fire has been relatively uncommon in the northeastern USA during the past century, our records clearly highlight the potential vulnerability of the region to future drought and fire impacts. Numerous studies have been performed on searchers' interaction with IR systems, in non-web systems [1] [2] [3], but in particular with the advent of the Web [#CITATION_TAG] [5] [6].","We developed coupled, high-resolution records of moisture variability and fire from three ombrotrophic peatlands in Maine using testate amoebae and analysis of microscopic charcoal.","['Understanding the role of fire in the Earth system, and particularly regional controls on its frequency and severity, is critical to risk assessment.']"
"The data stems from studies of searchers interaction with an XML information retrieval system. In this paper we present findings from a comparative study of data collected from client and server side loggings. The purpose is to see what factors of effort can be captured from the two logging methods. This paper presents the organization of the INEX 2008 interactive track. In order to understand what aspects of effort, as it is understood in our definition of search transitions, can optimally be identified from server logs, we have performed an analysis of a selection of server logs collected by the INEX 2008 interactive track [#CITATION_TAG], which also contains interaction with the documents.","Two research groups collected data from 29 test persons, each performing two tasks. We describe the methods used for data collection and the tasks performed by the participants.","[""In this year's iTrack we aimed at exploring the value of element retrieval for two different task types, fact-finding and research tasks.""]"
"The data stems from studies of searchers interaction with an XML information retrieval system. In this paper we present findings from a comparative study of data collected from client and server side loggings. The purpose is to see what factors of effort can be captured from the two logging methods. Searchers were asked to perform two search sessions, to solve one fact-finding task and one research task, each task was formulated as a simulated work task [#CITATION_TAG].",,['Aim  This study uses the combination of presettlement tree surveys and spatial analysis to produce an empirical reconstruction of tree species abundance and vegetation units at different scales in the original landscape.']
"The data stems from studies of searchers interaction with an XML information retrieval system. In this paper we present findings from a comparative study of data collected from client and server side loggings. The purpose is to see what factors of effort can be captured from the two logging methods. Throughout the eastern United States, plant species distributions and community patterns have developed in re- sponse to heterogeneous environmental conditions and a wide range of historical factors, including complex histories of natural and anthropogenic disturbance. Despite increased rec- ognition of the importance of disturbance in determining forest composition and structure, few studies have assessed the relative influence of current environment and historical factors on moder vegetation, in part because detailed knowl- edge of prior disturbance is often lacking. Similar to the forested uplands throughout the north- eastern United States, the site is physiographically heteroge- neous and has a long and complex history of natural and anthropogenic disturbance. Few species vary in accordance with ionic gradients, damage from the 1938 hurricane, or a 1957 fire. Such stages may be identified for instance in information seeking mediation, as in [#CITATION_TAG] where stages are identified as sets of cognitive and operational elements and transitions between stages are identified through vocabulary changes in dialogue.","Soil analyses and historical sources document four catego- ries of historical land use on areas that are all forested today: cultivated fields, improved pastures/mowings, unimproved pastures, and continuously forested woodlots. Ordination and logistic regressions indicate that although species have re- sponded individualistically to a wide range of environmental and disturbance factors, many species are influenced by three factors: soil drainage, land use history, and C:N ratios.","['In the present study, we investigate moder and historical factors that control veg- etation patterns at Harvard Forest in central Massachusetts, USA.']"
"The genetic diversity of green algal photobionts (chlorobionts) in soil crust forming lichens was studied as part of the SCIN-project (Soil Crust InterNational). The dominant lichen species at all four sites was Psora decipiens, often occurring with Buellia elegans, Fulgensia bracteata, F. fulgens and Peltigera rufescens. In arid lands, where vegetation is sparse or absent, the open ground is not bare but generally covered by a community of small, highly specialized organisms. Cyanobacteria, algae, microfungi, lichens, and bryophytes aggregate soil particles to form a coherent skin - the biological soil crust. Soil surface disturbance, such as heavy livestock grazing, human trampling or off-road vehicles, breaks up the fragile soil crust, thus compromising its stability, structure, and productivity. The species composition of BSCs mainly depends on water-availability, climate zone and soil-type (#CITATION_TAG).",,"['It stabilizes and protects the soil surface from erosion by wind and water, influences water runoff and infiltration, and contributes nitrogen and carbon to desert soils.', 'This book is the first synthesis of the biology of soil crusts and their importance as an ecosystem component.']"
"The genetic diversity of green algal photobionts (chlorobionts) in soil crust forming lichens was studied as part of the SCIN-project (Soil Crust InterNational). The dominant lichen species at all four sites was Psora decipiens, often occurring with Buellia elegans, Fulgensia bracteata, F. fulgens and Peltigera rufescens. Lichens, a classic example of an obligate symbiosis between fungi and photobionts (which could be algae or cyanobacteria), are abundant in many terrestrial ecosystems. The genetic structure of the photobiont population found in association with a lichen-forming fungal species could be affected by fungal reproductive mode and by the spatial extent of gene flow in the photobiont. We had previously shown that the fungus exhibited no genetic structure among four local sites or three phorophyte species. Because of soil crust related contaminations-mainly different eukaryotic algae-highly specific primers were developed for amplifying the target markers from Trebouxia sp. and Asterochloris sp. The primers psbF and psbR (#CITATION_TAG) were used to amplify and sequence the cp-marker (psbL-J for Trebouxia sp.) from Antarctic samples that were already known to have Trebouxia photobionts (Ruprecht et al. 2012) and from own Trebouxia cultures.","Using DNA sequences from one nuclear ribosomal and two chloroplast loci, we analyzed the genetic structure of the photobiont associated with the fungus Ramalina menziesii at an oak woodland study site in southern California. We hypothesize that R. menziesii is locally adapted to the phorophyte species through habitat specialization in the algal partner of the symbiosis.",['Our goals were to identify the photobiont species and assess its genetic structure.']
"The genetic diversity of green algal photobionts (chlorobionts) in soil crust forming lichens was studied as part of the SCIN-project (Soil Crust InterNational). The dominant lichen species at all four sites was Psora decipiens, often occurring with Buellia elegans, Fulgensia bracteata, F. fulgens and Peltigera rufescens. Regional droughts are common in North America, but pan-continental droughts extending across multiple regions, including the 2012 event, are rare relative to single-region events. During the Medieval Climate Anomaly (MCA), the central plains (CP), Southwest (SW), and Southeast (SE) regions experienced drier conditions and increased occurrence of droughts and the Northwest (NW) experienced several extended pluvials. Notably, megadroughts in these regions differed in their timing and persistence, suggesting that they represent regional events influenced by local dynamics rather than a unified, continental-scale phenomena. While relatively rare, pancontinental droughts are present in the paleo record and are linked to defined modes of climate variability, implying the potential for seasonal predictability. Peltigera rufescens, known to have a cyanobacterium as its primary photobiont (#CITATION_TAG), was also found to be associated with chlorobionts (Henskens et al. 2012).","Positive values of the Southern Oscillation index (La Ni~ conditions) are linked to SW, CP, and SE (SW1CP1SE) droughts and SW, CP, and NW (SW1CP1NW) droughts, whereas CP, NW, and SE (CP1NW1SE) droughts are associated with positive values of the Pacific decadal oscillation and Atlantic multidecadal oscillation.","['Here, the tree-ring-derived North American Drought Atlas is used to investigate drought variability in four regions over the last millennium, focusing on pan-continental droughts.']"
"ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. This paper reviews the existing literature on venture capital and private equity. Industry level issues relate to rivalry between firms, the power of suppliers and customers, and the threats from new entrants and substitutes. Private equity takeovers of mature firms can themselves be divided into several subcategories, although, once more, these are often conflated in the literature (#CITATION_TAG).","In order to understand current developments, the paper adopts a framework which combines industry/market and firm levels of analysis. Existing literature is reviewed using this framework. Firm level issues concern deal generation, initial and second screening, valuation and due diligence, deal approval and structuring, post-contractual monitoring, investment realisation, and entrepreneurs' exit and recontracting with venture capitalists. This is followed by a review of the evidence on the performance of venture capital firms.",['The paper emphasises the importance of examining venture capital in the light of recent developments in corporate finance and its distinctiveness from other forms of finance.']
"ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. Manuscript Type: Empirical    Research Issue: This study investigates the employment consequences of private equity acquisitions, in particular institutional buyouts (IBOs), in the UK. It involves a pre- and post-acquisition analysis of employment and performance characteristics for a sample of acquired firms and a matched sample of non-acquired firms. Two important theoretical issues emerge. This paper builds on an earlier pilot study conducted by the authors (#CITATION_TAG).","The first is a need to conceptualize skills and human capabilities on a collective dimension, specific to a particular organizational setting, and the extent to which they contribute to the organization's performance.",['This highlights the need for new management to better understand the link between employment and performance in the specific corporate setting of the acquired firm']
"ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. The report surveys the activity of private equity and other financial investors in the water, waste and healthcare sectors in Europe. Based on US evidence, #CITATION_TAG find an initial decline in jobs, but that this is not lasting.",,['It includes the appraisal of a WEF study on employment effects']
"ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. Despite the establishment of significant traditions of emotion research in many disciplines, there has been little discussion of the state and potential of the archaeological study of emotion. In archaeology, both the sociobiological approach and one based on empathy have serious problems. Finally, institutional buy outs (IBOs) involve private equity and other institutional investors; here managers do not hold any shares at all, unless this is part of their reward package (#CITATION_TAG).","After reviewing and rejecting the dichotomy between emotions as entirely biological, universal, and hard-wired, on one hand, and entirely social and constructed, on the other, a view of emotions as historically specific and experientially embedied is advanced.","['This paper aims to review archaeological approaches to emotion-to assess the significance of existing studies and outline the potential for the incorporation of emotion into archaeological research.', 'It argues that the study of emotion in the past is both necessary and possible ; it considers which understandings of emotion we might find most useful, how the archaeology of emotion might be carried out, and what are the most promising avenues to explore.']"
"ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. Abstract The World Archaeology volume 'The Cultural Biography of Objects' (Marshall and Gosden 1999) retains its currency ten years after its publication and the ideas highlighted in it continue to be developed. However, the relative success of biographical studies which rely on anthropological or historical information compared with biographical studies of prehistoric objects is evident. Indeed, both politicians and trade union representatives have raised serious concerns regarding the potential consequences of private equity acquisitions for the welfare of employees in acquired firms, calling for stronger regulation and greater transparency in respect of the activities of private equity acquirers (#CITATION_TAG; Treasury Select Committee, 2007).",,['Through the example of a British Iron Age mirror this paper explores ways of redressing the difficulties of applying a biographical approach to prehistoric objects.']
"ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. Caves in Ireland, as elsewhere, have been used for shelter and burial over much of recorded time. This consists of firms that have quadratic cost functions, Cobb-Douglas technologies as well as output constraints and that are price takers in the markets for production factors (see also #CITATION_TAG).",,"['The author here focuses on their use during the Neolithic, carefully isolating the available material and arguing from it that caves then had a primary role in the remembrance of the dead, and were used for excarnation, token deposition or inhumation.']"
"However, it is found that in the regime of strong supersonic flows an appropriate limiting condition, which depends on the Prandtl number, must be imposed on the artificial conductivity SPH coefficients in order to avoid an unphysical amount of heat diffusion. This paper investigates the hydrodynamic performances of an SPH code incorporating an artificial heat conductivity term in which the adopted signal velocity is applicable when gravity is present. Students enter physics classes often having an understanding of basic concepts which is incomplete or incorrect. In order to evaluate the relative effectiveness of heat and momentum transport, in the theory of heat transfer the Prandtl number Pr is defined as the ratio between the kinematic viscosity ν and the thermal diffusion coefficient D: Pr = ν/D (#CITATION_TAG).",,"['In this study, we investigate the performance difference between males and females on one particular question of the Thermodynamics Concept Survey (TCS).', 'The question evaluates student understanding of the heat (energy) that is transferred during change of state and change of temperature.']"
"However, it is found that in the regime of strong supersonic flows an appropriate limiting condition, which depends on the Prandtl number, must be imposed on the artificial conductivity SPH coefficients in order to avoid an unphysical amount of heat diffusion. This paper investigates the hydrodynamic performances of an SPH code incorporating an artificial heat conductivity term in which the adopted signal velocity is applicable when gravity is present. This article explores the relationship between the making of things and the making of people at the Bronze Age tell at Szazhalombatta, Hungary. Potters literally came into being as potters through repeated bodily enactment of potting skills. The creation of potters as a social category was essential to the ongoing creation of specific forms of material culture. This has motivated the development of adaptative mesh refinement (AMR) methods, in which the spatial resolution of the grid is locally refined according to some selection criterion (Berger & Colella 1989; Kravtsov, Klypin & Khokhlov 1997; #CITATION_TAG).",Potters also gained their identity in the social sphere through the connection between their potting performance and their audience. We trace degrees of skill in the ceramic record to reveal the material articulation of non-discursive knowledge and consider the ramifications of the differential acquisition of non-discursive knowledge for the expression of different kinds of potter's identities. We examine the implications of altered potters' performances and the role of non-discursive knowledge in the construction of social models of the Bronze Age.,"['Focusing on potters and potting, we explore how the performance of non-discursive knowledge was critical to the construction of social categories.']"
"Accurate astrometry is required to reliably cross-match 20th-century photographic catalogues against 21st-century digital surveys. Targets have been individually identified in digital images using the Armagh Atlas and, in most cases, unambiguously matched to entries in the UCAC astrometric catalogues. Astrometry with sub-arcsecond precision is now available for all the major photographic spectroscopic surveys of the LMC. The present work provides modern-era identifications and astrometry for the 801 emission-line objects ""of stellar appearance"" in the Armagh survey (the largest of its nature to date). The idea of prehistory dates from the nineteenth century, but Richard Bradley contends that it is still a vital area for research. He argues that it is only through a combination of oral tradition and the experience of encountering ancient material culture that people were able to formulate a sense of their own pasts without written records. The Past in Prehistoric Societies presents case studies which extend from the Palaeolithic to the early Middle Ages and from the Alps to Scandinavia. As a consequence, a number of extensive photographic searches for luminous LMC stars with Hα emission were conducted as technological advances allowed, first by Henize (1956; 172 'LHα120-S' stars), and subsequently at Armagh (Lindsay 1963, Andrews & Lindsay 1964 801 'L63' and 'AL' stars) and by Bohannan & Epps (1974; 625 'BE74' stars), alongside more general surveys, such as Sanduleak's (#CITATION_TAG).","It also investigates the ways in which ancient remains might have been invested with new meanings long after their original significance had been forgotten. Finally, the author compares the procedures of excavation and field survey in the light of these examples.",['It examines how archaeologists might study the origin of myths and the different ways in which prehistoric people would have inherited artefacts from the past.']
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009; #CITATION_TAG).","We demonstrate how to create artificial external non-Abelian gauge potentials acting on cold atoms in optical lattices. The method employs atoms with k internal states, and laser assisted state sensitive tunneling, described by unitary k x k matrices. The single-particle dynamics in the case of intense U2 vector potentials lead to a generalized Hofstadter butterfly spectrum which shows a complex mothlike structure.",['We discuss the possibility to realize non-Abelian interferometry (Aharonov-Bohm effect) and to study many-body dynamics of ultracold matter in external lattice gauge fields.']
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. The structure or composition of all vegetation types in the region have been shaped by past land-use, fire, or other disturbances, and vegetation patterns will con-tinue to change through time. Conservation efforts aimed at maintaining early succes-sional vegetation types may require intensive management comparable in intensity to the historical disturbances that allowed for their widespread development The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981; Rubertone, 1985; #CITATION_TAG; Eberhardt et al., 2003; Forman et al., 2008).","Methods Historical changes in land-use and land-cover across the study region were determined from historical maps and documentary sources. Modern vegetation and soils were sampled and land-use and fire history determined for 352 stratified-random study plots. Ordination and classification were used to assess vegetation variation, and G-tests of independence and Kruskal-Wallis tests were used to evaluate relationships among individual species distributions, past land-use, surficial landforms and edaphic condi-tions.","['Aim We evaluate environmental and historical determinants of modern species com-position for upland vegetation types across Cape Cod, Massachusetts, a region that supports numerous uncommon species assemblages that are conservation priorities.']"
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. Location The eight study sites are located in southern New England in the states of Massachusetts and Connecticut. Tsuga canadensis and Fagus grandifolia are abundant in the upland area, while Quercus, Carya and Pinus species have higher abundances in the lowlands. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp. ), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000; Shuman et al., 2001; Shuman and Donnelly, 2006; #CITATION_TAG; Marsicek et al., 2013).","The sites span a climatic and vegetational gradient from the lowland areas of eastern Massachusetts and Connecticut to the uplands of north-central and western Massachusetts. Methods We collected sediment cores from three lakes in eastern and north-central Massachusetts (Berry East, Blood and Little Royalston Ponds). Pollen records from those sites were compared with previously published pollen dat","['Aim We analysed lake-sediment pollen records from eight sites in southern New England to address: (1) regional variation in ecological responses to post-glacial climatic changes, (2) landscape-scale vegetational heterogeneity at different times in the past, and (3) environmental and ecological controls on spatial patterns of vegetation.']"
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. Hurricanes are a major factor controlling ecosystem structure, function, and dynamics in many coastal forests, but their ecological role can be understood only by assessing impacts in space and time over a period of centuries. Actual forest damage was strongly dependent on land use and natural disturbance history. Annual and decadal timing of hurricanes varied widely. There was no clear centuryscale trend in the number of major hurricanes. Other factors such as storminess, hurricane-force winds (#CITATION_TAG; Eberhardt et al., 2003) and forest fires (Motzkin et al., 2002; Parshall et al., 2003) may have contributed to historic disturbance of this dune landscape.","We present a new method for reconstructing hurricane disturbance regimes using a combination of historical research and computer modeling. Historical evidence of wind damage for each hurricane in the selected region is quantified using the Fujita scale to produce regional maps of actual damage. A simple meteorological model (HURRECON), parameterized and tested for selected recent hurricanes, provides regional estimates of wind speed, direction, and damage for each storm. Individual reconstructions are compiled to analyze spatial and temporal patterns of hurricane impacts. Long-term effects of topography on a landscape scale are then simulated with a simple topographic exposure model (EXPOS). We applied this method to the region of New England, USA, examining hurricanes since European settlement in 1620.",['The historical-modeling approach is applicable to any region with good historical records and will enable ecologists and land managers to incorporate insights on hurricane disturbance regimes into the interpretation and conservation of forests at landscape to regional scales.Organismic and Evolutionary Biolog']
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. The availability of sand for eolian transport is often mediated by the type and extent of vegetation cover on dunes, and may be further controlled by moisture availability, edaphic associations (cf. related to soil processes) and landscape disturbance (e.g., Sala et al., 1988; Mangan et al., 2004; Hugenholtz and Wolfe, 2005; #CITATION_TAG).","By controlling the lattice anisotropy, one can realize both massive and massless Dirac fermions and observe the phase transition between them. Through explicit calculations, we show that both the Bragg spectroscopy and the atomic density profile in a trap can be used to demonstrate the Dirac fermions and the associated phase transition.",['We propose an experimental scheme to simulate and observe relativistic Dirac fermions with cold atoms in a hexagonal optical lattice.']
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. Precision metrology and quantum measurement often demand that matter be prepared in well-defined quantum states for both internal and external degrees of freedom. Laser-cooled neutral atoms localized in a deeply confining optical potential satisfy this requirement. Proxy records of hurricane occurrence from coastal overwash timeseries (e.g., #CITATION_TAG; Scileppi and Donnelly, 2007) and from marine sediment cores extracted strategically along the Bahama Bank (Williams, 2013) to reflect wave climate indicate heightened hurricane activity in the North Atlantic Ocean between 4900 and 3600, 2500, and 1000, and 600 and 400 years BP (Mann et al., 2009; Toomey et al., 2013).","With an appropriate choice of wavelength and polarization for the optical trap, two electronic states of an atom can experience the same trapping potential, permitting coherent control of electronic transitions independent of the atomic center-of-mass motion. We also provide a brief survey of promising prospects for future work.","['Here, we review a number of recent experiments that use this approach to investigate precision quantum metrology for optical atomic clocks and coherent control of optical interactions of single atoms and photons within the context of cavity quantum electrodynamics.']"
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. The force exerted by optical-frequency radiation on neutral atoms can be quite substantial, particularly in the neighborhood of an atomic resonance line. Dune reactivation is associated with large-scale disturbance of the forest which is dominated by pitch pine (Pinus rigida) and a variety of oak species (e.g., Quercus alba, and Quercus coccinea), but white pine (Pinus strobes), red maple (Acer rubrum), hickory (Carya spp. ), and American beech (Fagus grandifolia) are locally common (#CITATION_TAG; Cogbill et al., 2002).",,"['In this paper we derive from quantum theory the optical force, its first-order velocity dependence, and its fluctuations for arbitrary light intensity, and apply the results to the problem of creating a stable optical trap for sodium atoms.']"
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. A U(1) adiabatic phase is created by two laser beams for the tunneling of atoms between neighbor lattice sites. Near these crossing points the quasiparticles and quasiholes can be considered as massless Dirac fermions. The quartz fraction was isolated by density separations (at 2.55 and 2.70 g/cc) using the heavy liquid Na-polytungstate and a 40-min immersion in HF (40%) was applied to etch the outer ∼10 µm of grains, which is affected by alpha radiation (#CITATION_TAG).","Furthermore, the anisotropic effects of massless Dirac fermions are obtained in the present square lattice model. The Dirac fermions as well as the anisotropic behaviors realized in our system can be experimentally detected with the Bragg spectroscopy technique.",['We propose a scheme to simulate and observe massless Dirac fermions with cold atoms in a square optical lattice.']
"The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. Regional droughts are common in North America, but pan-continental droughts extending across multiple regions, including the 2012 event, are rare relative to single-region events. During the Medieval Climate Anomaly (MCA), the central plains (CP), Southwest (SW), and Southeast (SE) regions experienced drier conditions and increased occurrence of droughts and the Northwest (NW) experienced several extended pluvials. Notably, megadroughts in these regions differed in their timing and persistence, suggesting that they represent regional events influenced by local dynamics rather than a unified, continental-scale phenomena. There is no trend in pan-continental drought occurrence, de-fined as synchronous droughts in three ormore regions. SW,CP, and SE (SW1CP1SE) droughts are themost common, occurring in 12 % of all years and peaking in prevalence during the twelfth and thirteenth centuries; patterns involving three other regions occur in about 8 % of years. Another factor that may influence eolian activity is periodic drought as documented by tree-ring time series that span the past years (e.g., #CITATION_TAG).",,"['Here, the tree-ring-derived North American Drought Atlas is used to investigate drought variability in four regions over the last millennium, focusing on pan-continental droughts.']"
"It draws upon recent social and archaeological theory around embodied memory; in particular, Connerton's (1989) division of memory claims into three kinds. It is argued that cognitive memory claims and habit-memory should be regarded as aspects of the same process of remembering; following Gell (1998) and Jones (2007), physical traces of past action are regarded as central to this act of memory. It is argued that all three share some of the attributes of a ritual performance. This article is concerned with archaeological evidence for the mechanisms by which group memory is transmitted. In general, a classical solution in a tensor model may be physically regarded as a background space, and small fluctuations about the solution as emergent fields on the space. The numerical analyses of the tensor models possessing Gaussian classical background solutions have shown that the low-lying long-wavelength fluctuations around the backgrounds are in one-to-one correspondence with the geometric fluctuations on flat spaces in the general relativity. It has also been shown that part of the orthogonal symmetry of the tensor model spontaneously broken by the backgrounds can be identified with the local translation symmetry of the general relativity. Human remains occur over a long timescale at many prehistoric cave sites, particularly from the Neolithic onwards (Chamberlain, 1996; #CITATION_TAG; Leach, 2008; Schulting, 2007).","Thus the tensor model provides an interesting model of simultaneous emergence of space, the general relativity, and its local gauge symmetry of translation.","[""This paper gives a summary of the author's works concerning the emergent general relativity in a particular class of tensor models, which possess Gaussian classical solutions.""]"
"It draws upon recent social and archaeological theory around embodied memory; in particular, Connerton's (1989) division of memory claims into three kinds. It is argued that cognitive memory claims and habit-memory should be regarded as aspects of the same process of remembering; following Gell (1998) and Jones (2007), physical traces of past action are regarded as central to this act of memory. It is argued that all three share some of the attributes of a ritual performance. This article is concerned with archaeological evidence for the mechanisms by which group memory is transmitted. It is argued that the account of Savage-Rumbaugh's ape language research in Savage-Rumbaugh, Shanker and Taylor (1998. Apes, Language and the Human Mind. Oxford University Press, Oxford) is profitably read in the terms of the theoretical perspective developed in Clark (1997. The authors, though, make heavy going of a critique of what they take to be standard approaches to understanding language and cognition in animals, and fail to offer a worthwhile theoretical position from which to make sense of their own data. Jones follows #CITATION_TAG in viewing memory as a process of pattern re-creation which involves the mind, the body and the world.","This model of 'distributed' cognition helps makes sense of the lexigram activity of Savage-Rumbaugh's subjects, and points to a re-evaluation of the language behaviour of humans","[""The contribution made by Clark's work is to show the range of ways in which cognition exploits bodily and environmental resources.""]"
"It draws upon recent social and archaeological theory around embodied memory; in particular, Connerton's (1989) division of memory claims into three kinds. It is argued that cognitive memory claims and habit-memory should be regarded as aspects of the same process of remembering; following Gell (1998) and Jones (2007), physical traces of past action are regarded as central to this act of memory. It is argued that all three share some of the attributes of a ritual performance. This article is concerned with archaeological evidence for the mechanisms by which group memory is transmitted. The rank-three tensor model may be regarded as a theory of dynamical fuzzy spaces, because a fuzzy space is defined by a three-index coefficient of the product between functions on it, fa*fb = Cabcfc. It is numerically shown that fuzzy flat tori and fuzzy spheres of various dimensions are classical solutions of the rank-three tensor model. The symmetry of the model under the general linear transformation can be identified with a fuzzy analogue of the general coordinate transformation symmetry in general relativity. These questions about social memory have received detailed consideration within archaeology over the last 20 years (e.g. Boric´, 2010; #CITATION_TAG; Jones, 2007; Whittle et al., 2007b).","Since these solutions are obtained for the same coupling constants of the tensor model, the cosmological constant and the dimensions are not fundamental but can be regarded as dynamical quantities. This symmetry of the tensor model is broken at the classical solutions. This feature may make the model a concrete finite setting for applying the old idea of obtaining gravity as Nambu-Goldstone fields of the spontaneous breaking of the local translational symmetry.","['In this paper, this proposal is applied to the dynamical generation of commutative non-associative fuzzy spaces.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. Tunnelling of material particles through a classically impenetrable barrier constitutes one of the hallmark effects of quantum physics. When interactions between the particles compete with their mobility through a tunnel junction, intriguing novel dynamical behaviour can arise where particles do not tunnel independently. In single-electron or Bloch transistors, for example, the tunnelling of an electron or Cooper pair can be enabled or suppressed by the presence of a second charge carrier due to Coulomb blockade. Here we report on the first direct and time-resolved observation of correlated tunnelling of two interacting atoms through a barrier in a double well potential. Similar second-order processes form the basis of superexchange interactions between atoms on neighbouring lattice sites of a periodic potential, a central component of quantum magnetism.Comment: 18 pages, 4 figures, accepted for publication in Natur In this paper, we extend the JZ proposal and discuss a new scheme based on an optical superlattice [25, #CITATION_TAG] to generate a gauge potential leading to (1).","We show that for weak interactions between the atoms and dominating tunnel coupling, individual atoms can tunnel independently, similar to the case in a normal Josephson junction. With strong repulsive interactions present, two atoms located on one side of the barrier cannot separate, but are observed to tunnel together as a pair in a second order co-tunnelling process. By recording both the atom position and phase coherence over time, we fully characterize the tunnelling process for a single atom as well as the correlated dynamics of a pair of atoms for weak and strong interactions. In addition, we identify a conditional tunnelling regime, where a single atom can only tunnel in the presence of a second particle, acting as a single atom switch.","['Our work constitutes the first direct observation of second order tunnelling events with ultracold atoms, which are the dominating dynamical effect in the strongly interacting regime.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. These fields are needed for using neutral atoms as an analog quantum computer for simulating the properties of many-body systems of charged particles. They allow for seemingly paradoxical geometries, such as a ring where atoms continuously reduce their potential energy while moving in a closed path. Recently alternative schemes to ""simulate"" artificial gauge fields for neutral atoms have been explored using two-dimensional (2D) optical lattices [8, 9, #CITATION_TAG, 11, 12, 13, 14].",We show how lasers may create fields which couple to neutral atoms in the same way that the electromagnetic fields couple to charged particles.,"['We propose neutral atom experiments which probe quantum Hall effects and the interplay between magnetic fields and periodic potentials.Comment: 4 pages, 1 color figure, RevTeX 4; v2 Revised introduction,   additional reference']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. This article reviews developments in the theory of rapidly rotating degenerate atomic gases. Rotation leads to the formation of quantized vortices which order into a vortex array, in close analogy with the behaviour of superfluid helium. Under conditions of rapid rotation, when the vortex density becomes large, atomic Bose gases offer the possibility to explore the physics of quantized vortices in novel parameter regimes. This article describes the equilibrium properties of rapidly rotating atomic Bose gases in both the mean-field and the strongly correlated regimes, and related theoretical developments for Bose gases in lattices, for multi-component Bose gases, and for atomic Fermi gases. Analogous quantum Hall phases should also arise in cold atomic gases when they are set into fast rotation (see [#CITATION_TAG] and references therein).","First, there is an interesting regime in which the vortices become sufficiently dense that their cores -- as set by the healing length -- start to overlap. In this regime, the theoretical description simplifies, allowing a reduction to single particle states in the lowest Landau level. Second, one can envisage entering a regime of very high vortex density, when the number of vortices becomes comparable to the number of particles in the gas. In this regime, theory predicts the appearance of a series of strongly correlated phases, which can be viewed as {\it bosonic} versions of fractional quantum Hall states.","['The main focus is on the equilibrium properties of a single component atomic Bose gas, which (at least at rest) forms a Bose-Einstein condensate.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. RESULTS While dopamine is critical for acute reward and initiation of addiction, end-stage addiction results primarily from cellular adaptations in anterior cingulate and orbitofrontal glutamatergic projections to the nucleus accumbens. Pathophysiological plasticity in excitatory transmission reduces the capacity of the prefrontal cortex to initiate behaviors in response to biological rewards and to provide executive control over drug seeking. Simultaneously, the prefrontal cortex is hyperresponsive to stimuli predicting drug availability, resulting in supraphysiological glutamatergic drive in the nucleus accumbens, where excitatory synapses have a reduced capacity to regulate neurotransmission. Recently alternative schemes to ""simulate"" artificial gauge fields for neutral atoms have been explored using two-dimensional (2D) optical lattices [8, 9, 10, 11, 12, 13, #CITATION_TAG].",,"['OBJECTIVE A primary behavioral pathology in drug addiction is the overpowering motivational strength and decreased ability to control the desire to obtain drugs.', 'In this review the authors explore how advances in neurobiology are approaching an understanding of the cellular and circuitry underpinnings of addiction, and they describe the novel pharmacotherapeutic targets emerging from this understanding.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. Non-electronic communication aids provide one form of augmentative and alternative communication (AAC) for people with complex communication needs. Such funding has usually failed to meet the total device costs and has not provided for adequate speech-language pathology support. Despite the high demand for non-electronic aids, the research literature has tended to focus on electronic communication aids, including within intervention studies and addressing design features and long-term outcomes. We note that this dispersion relation gives rise to two 'Dirac points"" for (k x, k y) = (±π/2d x, π/2d y) around which the dispersion relation is linear [13, 48] (see also [49, #CITATION_TAG, 51, 52, 53] for discussions of the Dirac points occurring in different settings, such as hexagonal lattices or non-Abelian gauge fields).",Concerns about ensuring that AAC systems are chosen according to the assessed needs of individuals are discussed within the context of limitations in outcomes research and appropriate outcome measures.,"['The aim here was to explore non-electronic communication aids as one AAC option and research challenges.', 'This aim was addressed by reviewing funding for the provision of AAC systems, data from an Australian pilot project providing non-electronic communication aids, an audit of aided AAC published studies (2000-2009), and discussion of the review literature.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. Dragon Dictate is time-consuming to learn and demands a high level of motivation, but can be beneficial to a person who has profound dysarthria and great difficulties in accessing the computer. Finally, one can calculate the spontaneous emission and associated heating rates for atoms in each sublattice [#CITATION_TAG].","Single subject design was used and measures of computer access system effectiveness and speech production were used before, during and after intervention. The users' original switch access system was compared to a combination of their switch access system and speech recognition, by counting the number of correct entries. The other participant did not complete the intervention protocol.","['This study investigated the use of the speech recognition system Dragon Dictate as an augmentative method of computer access for two individuals with cerebral palsy, including severe motor dysfunction and dysarthria.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. A U(1) adiabatic phase is created by two laser beams for the tunneling of atoms between neighbor lattice sites. Near these crossing points the quasiparticles and quasiholes can be considered as massless Dirac fermions. We note that this dispersion relation gives rise to two 'Dirac points"" for (k x, k y) = (±π/2d x, π/2d y) around which the dispersion relation is linear [13, #CITATION_TAG] (see also [49, 50, 51, 52, 53] for discussions of the Dirac points occurring in different settings, such as hexagonal lattices or non-Abelian gauge fields).","Furthermore, the anisotropic effects of massless Dirac fermions are obtained in the present square lattice model. The Dirac fermions as well as the anisotropic behaviors realizeded in our system can be experimentally detected with the Bragg spectroscopy technique",['We propose a novel scheme to simulate and observe massless Dirac fermions with cold atoms in a square optical lattice.']
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. In this context, g and e denoted two different hyperfine states in the ground state manifold, and the spin-dependent optical potential was obtained by exploiting the vector light-shift arising in a laser field with suitable polarization (see [57, #CITATION_TAG], for example).","Individuals with mild and moderate dysarthria trained and then used 45 command words to input text independently into the PSDD. The PSDD system also adapted to the speech of two participants with different degrees of severe dysarthria, but they were unable to use this system independently.","['This study describes the feasibility of using speech recognition as a text input method for speakers with different degrees of dysarthria.', 'The project investigated two different types of speech recognition systems: Prototype Swedish DragonDictate (PSDD), a speaker-adaptive phoneme-based system, and Infovox RA, a speaker-dependent, whole-word pattern-matching system.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. In this context, g and e denoted two different hyperfine states in the ground state manifold, and the spin-dependent optical potential was obtained by exploiting the vector light-shift arising in a laser field with suitable polarization (see [#CITATION_TAG, 58], for example).","Method Six typically developing peers were taught to implement peer-mediated naturalistic teaching, with and without a speech generating device (SGD), during play sessions with 3 classmates with autism in three preschools. Generalisation probes were conducted during mealtimes at the preschools. A multiple baseline design was used to assess the outcomes of the two intervention conditions.",['Abstract Background The aim of this study was to assess the effectiveness of two communication interventions for preschool-aged children with autism.']
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. Background: Increasingly, computerised communication aids are used by people with severe, chronic aphasia. Although the candidacy for these devices is relatively unknown, it has been hypothesised that cognitive deficits have a negative impact on the functional use of Augmentative and Alternative Communication (AAC). In a previous study (van de Sandt-Koenderman, Wiegers, Wielaert, Duivenvoorden, & Ribbers, in press) we reported the functional effect of TouchSpeak (TS), a computerised communication aid, in a group of stroke patients with severe aphasia. It has also inspired novel proposals for quantum information processing [39, 40, 41] or quantum simulation [42, #CITATION_TAG].","Deficits of executive functioning are assumed to be particularly important, but other functions, such as memory and semantic processing, may also be relevant. Methods & Procedures: The data of 30 patients with severe aphasia were analysed retrospectively. All were trained to use TS in two self-chosen communicative situations. Pre-training assessment included memory, executive functioning, semantic processing, and communication skills. The role of the cognitive variables was analysed with univariate ANCOVAs with contrast analysis, with correction for age, gender, aphasia type, or time post onset in case of significant differences between the groups on these variables. The broad concept of ""executive functioning"" needs to be studied in more detail in relation to aphasia.","['Aims: To find factors associated with the functional success of TS in people with severe aphasia, focusing on memory, executive functioning, semantic processing, and communication skills.', 'The importance of intact executive functioning is not supported in this study.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. BACKGROUND AND OBJECTIVES The expansion of evidence-based practice across sectors has lead to an increasing variety of review types. However, the diversity of terminology used means that the full potential of these review types may be lost amongst a confusion of indistinct and misapplied terms. A limited number of review types are currently utilized within the health information domain. Notwithstanding such limitations, this typology provides a valuable reference point for those commissioning, conducting, supporting or interpreting reviews, both within health information and the wider health care domain. of allowed states in the combined lattice plus external trap, in striking contrast with the uniform case [#CITATION_TAG].","METHODS Following scoping searches, an examination was made of the vocabulary associated with the literature of review and synthesis (literary warrant). A simple analytical framework -- Search, AppraisaL, Synthesis and Analysis (SALSA) -- was used to examine the main review types. A description of the key characteristics is given, together with perceived strengths and weaknesses.","['The objective of this study is to provide descriptive insight into the most common types of reviews, with illustrative examples from health and health information domains.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. Combined with interatomic interactions, an entirely new class of superfluid or strongly correlated systems becomes accessible with ultracold atoms [34, 53, #CITATION_TAG, 64, 65].",,"['In this paper we review published research describing the use of augmentative and alternative communication (AAC) to support societal participation by adolescents and adults with developmental disabilities who require AAC.', 'We focus on three major participation domains: post-secondary education and training, the workplace, and community living and social interaction opportunities.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. Speech output from speech-generating devices (SGD) and SGD software, such as talking word processors, has changed the landscape of options for aided communication. Combined with interatomic interactions, an entirely new class of superfluid or strongly correlated systems becomes accessible with ultracold atoms [34, 53, 63, #CITATION_TAG, 65].","Learner-oriented roles of speech output are summarized in terms of graphic symbol learning, communicative functions and social regulation, learner preference, challenging behaviors, natural speech production, comprehension, and literacy. Roles for the learner - partner dyad include changes to interaction patterns. Methodological issues are discussed and practical implications are drawn where appropriate.","['The purpose of this paper is to review and critique research into the roles of speech output for communication partners, learners, and learner-partner dyads.', 'Research on partner - oriented roles is reviewed in terms of attitudes and perceived communicative competence, and communicative behavior.']"
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. The question of what heats the solar corona remains one of the most important problems in astrophysics. Finding a definitive solution involves a number of challenging steps, beginning with an identification of the energy source and ending with a prediction of observable quantities that can be compared directly with actual observations. A variety of difficult issues must be addressed: highly disparate spatial scales, physical connections between the corona and lower atmosphere, complex microphysics, and variability and dynamics. Nearly all of the coronal heating mechanisms that have been proposed produce heating that is impulsive from the perspective of elemental magnetic flux strands. It is this perspective that must be adopted to understand how the plasma responds and radiates. Exciting new developments include the identification of the ""secondary instability"" as the likely mechanism of energy release and the demonstration that impulsive heating in sub-resolution strands can explain certain observed properties of coronal loops that are otherwise very difficult to understand. Whatever the detailed mechanism of energy release, it is clear that some form of magnetic reconnection must be occurring at significant altitudes in the corona (above the magnetic carpet), so that the tangling does not increase indefinitely. The fractional quantum Hall (FQH) phases realized by two-dimensional electron gases in very large magnetic fields are among the most intriguing states of matter (see, for instance, [#CITATION_TAG]).",Critical intermediate steps include realistic modeling of both the energy release process (the conversion of magnetic stress energy or wave energy into heat) and the response of the plasma to the heating.,['This article outlines the key elements of a comprehensive strategy for solving the coronal heating problem and warns of obstacles that must be overcome along the way.']
"We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. The very long lifetime of e (around 20 s for Yb [#CITATION_TAG]) allows one to operate optical atomic clocks at the resonance wavelength λ 0 ≈ 578 nm between g and e [36, 37, 38].","Observations are compared to numerical simulations of the response of the solar atmosphere to an energy perturbation of4 x 1024 ergs representing an energy release during magnetic reconnection in a 1-D semi-circular flux tube. The temporal evolution of the thermodynamic state of the loop is converted into Civ 1548, Ovi 1032 and Neviii 770 line profiles in non-equilibrium ionization.",['We explore the idea that the occurrence of nanoflares in a magnetic loop around the O vi formation temperature could explain the observed red-shift of mid-low transition region lines as well as the blue-shift observed in low coronal lines (T > 6 x 105 K).']
"Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. These generalizations are algebraic structures in which the two entries Lie bracket has been replaced by a bracket with n entries. Three-Lie algebras have surfaced recently in multi-brane theory in the context of the Bagger-Lambert-Gustavsson model. In the sequel, it is found that 3-ary algebras [#CITATION_TAG] [28] [29] describe the symmetries of the tensor models.","Each type of n-ary bracket satisfies a specific characteristic identity which plays the r\^ole of the Jacobi identity for Lie algebras. Particular attention will be paid to generalized Lie algebras, which are defined by even multibrackets obtained by antisymmetrizing the associative products of its n components and that satisfy the generalized Jacobi identity (GJI), and to Filippov (or n-Lie) algebras, which are defined by fully antisymmetric n-brackets that satisfy the Filippov identity (FI). Because of this, Filippov algebras will be discussed at length, including the cohomology complexes that govern their central extensions and their deformations (Whitehead's lemma extends to all semisimple n-Lie algebras). When the skewsymmetry of the n-Lie algebra is relaxed, one is led the n-Leibniz algebras. The standard Poisson structure may also be extended to the n-ary case. We shall review here the even generalized Poisson structures, whose GJI reproduces the pattern of the generalized Lie algebras, and the Nambu-Poisson structures, which satisfy the FI and determine Filippov algebras.",['This paper reviews the properties and applications of certain n-ary generalizations of Lie algebras in a self-contained and unified way.']
"Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. Tensor models can be regarded as theories of dynamical fuzzy spaces, and provide back-ground independent theories of space. Their classical solutions correspond to classical back-ground spaces, and the small fluctuations around them can be regarded as fluctuations of fields in them. Tensor models (or generalized matrix models)1)-8) were originally considered as the generalization of the matrix models, which describe the two-dimensional simpli-cial quantum gravity, to higher-dimensional cases. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19] [20] [21] [#CITATION_TAG] [23] [24] [25] [26].","Numerical analysis also suggests that the lowest-order effective action is composed of curvature square terms, which is consistent with general relativity in view of the form of the considered action of the tensor model. As in the matrix models, partition functions are given by a certain summation over simplicial manifolds, which are dua","['In this paper, I numerically study a tensor model around its classical back-grounds of two-, three- and four-dimensional fuzzy flat tori and show that the properties of low-lying low-momentum modes are in clear agreement with general relativity.']"
"Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. Tensor models can be interpreted as theory of dynamical fuzzy spaces. It is found that the momentum distribution of the low-lying low-momentum spectra is in agreement with that of the metric tensor modulo the general coordinate transformation in the general relativity at least in the dimensions studied numerically, i.e. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19] [#CITATION_TAG] [21] [22] [23] [24] [25] [26].",,"['In this paper, I study numerically the fluctuation spectra around a Gaussian classical solution of a tensor model, which represents a fuzzy flat space in arbitrary dimensions.']"
"Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. Rank-three tensor model may be regarded as theory of dynamical fuzzy spaces, because a fuzzy space is defined by a three-index coefficient of the product between functions on it, f_a*f_b=C_ab^cf_c. It is numerically shown that fuzzy flat torus and fuzzy spheres of various dimensions are classical solutions of the rank-three tensor model. The symmetry of the model under the general linear transformation can be identified with a fuzzy analog of the general coordinate transformation symmetry in general relativity. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [#CITATION_TAG] [20] [21] [22] [23] [24] [25] [26].","Since these solutions are obtained for the same coupling constants of the tensor model, the cosmological constant and the dimensions are not fundamental but can be regarded as dynamical quantities. This symmetry of the tensor model is broken at the classical solutions. This feature may make the model to be a concrete finite setting for applying the old idea of obtaining gravity as Nambu-Goldstone fields of the spontaneous breaking of the local translational symmetry.Comment: Adding discussions on effective geometry, a note added, four   references added, other minor changes, 27 pages, 17 figure","['In this paper, this previous proposal is applied to dynamical generation of commutative nonassociative fuzzy spaces.']"
"Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. Active region coronal loop observations with broadband X-ray instruments have often been found to be consistent with the predictions of static loop models. Recent observations in the EUV, however, have discovered a class of active region loops that are difficult to reconcile with static loop models. Despite various difficulties 8 and the rather slow development since the introduction of tensor models, some interesting results have been reported recently [9] [10] [#CITATION_TAG] [12] [13] [14] [15].","We select 67 loops with a large range of apex temperatures and half-lengths observed with either the Transition Region and Coronal Explorer or the Soft X-Ray Telescope. We compare these observations to static loop models using both uniform and nonuniform heating. We then consider the possibility that the disparity in the density could be due to steady, nonuniform heating along the loop and find that footpoint heating can increase densities only by a factor of 3 over density solutions with uniform heating while loop-top heating results in density solutions that are, at most, a factor of 2.5 smaller than the density solutions with uniform heating.","['In this paper, we take a comprehensive look at how coronal loops compare with static models.']"
"Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. Group field theories are higher dimensional generalizations of matrix models. Their Feynman graphs are fat and in addition to vertices, edges and faces, they also contain higher dimensional cells, called bubbles. Unlike its bosonic counterpart, the bubbles of the Feynman graphs of this theory are well defined and readily identified. Despite various difficulties 8 and the rather slow development since the introduction of tensor models, some interesting results have been reported recently [9] [10] [11] [#CITATION_TAG] [13] [14] [15].",We prove that this graphs are combinatorial cellular complexes. We define and study the cellular homology of this graphs. Furthermore we define a homotopy transformation appropriate to this graphs.,"['In this paper, we propose a new, fermionic Group Field Theory, posessing a color symmetry, and take the first steps in a systematic study of the topological properties of its graphs.']"
"Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. One of the paradigms about coronal heating has been the belief that the mean or summit temperature of a coronal loop is completely insensitive to the nature of the heating mechanisms. However, we point out that the temperature profile along a coronal loop is highly sensitive to the form of the heating. For example, when a steady state heating is balanced by thermal conduction, a uniform heating function makes the heat flux a linear function of distance along the loop, while T7/2 increases quadratically from the coronal footpoints; when the heating is concentrated near the coronal base, the heat flux is small and the T7/2 profile is flat above the base; when the heat is focused near the summit of a loop, the heat flux is constant and T7/2 is a linear function of distance below the summit. Despite various difficulties 8 and the rather slow development since the introduction of tensor models, some interesting results have been reported recently [9] [10] [11] [12] [13] [14] [#CITATION_TAG].","In particular, we apply this philosophy to a preliminary analysis of Yohkoh observations of the large-scale solar corona. In addition, we suggest that the decline in coronal intensity by a factor of 100 from solar maximum to solar minimum is a natural consequence of the observed ratio of magnetic field strength in active regions and the quiet Sun; the altitude of the maximum temperature in coronal holes may represent the dissipation height of Alfven waves by turbulent phase mixing; and the difference in maximum temperature in closed and open regimes may be understood in terms of the roles of the conductive flux there.","['It is therefore important to determine how the heat deposition from particular heating mechanisms varies spatially within coronal structures such as loops or arcades and to compare it to high-quality measurements of the temperature profiles.', 'We propose a new two-part approach to try and solve the coronal heating problem, namely, first of all to use observed temperature profiles to deduce the form of the heating, and second to use that heating form to deduce the likely heating mechanism.']"
"How do addictive drugs hijack the brain's reward system? Addictive drugs affect acute responses and plasticity in dopamine neurons and postsynaptic structures. Addictive drugs steepen neuronal temporal reward discounting and create temporal myopia that impairs the control of drug taking. Tonically enhanced dopamine levels may disturb working memory mechanisms necessary for assessing background rewards and thus may generate inaccurate neuronal reward predictions. Drug-induced working memory deficits may impair neuronal risk signaling, promote risky behaviors, and facilitate preaddictive drug use. Malfunctioning adaptive reward coding may lead to overvaluation of drug rewards. Many of these malfunctions may result in inadequate neuronal decision mechanisms and lead to choices biased toward drug rewards. This review speculates how normal, physiological reward processes may be affected by addictive drugs. It is thought likely that vast numbers of nanoflares are responsible for the corona having a temperature of millions of degrees. Current observational technologies lack the resolving power to confirm the nanoflare hypothesis. This paper presents the initial results generated by a coronal loop model that flares whenever it becomes unstable to an ideal MHD kink mode. Dissipation of the loop's magnetic energy begins during the nonlinear stage of the instability, which develops as a consequence of current sheet reconnection. There exists substantial variation in the radial magnetic twist profiles for the loop states along the instability threshold. Deviations from these optimal levels, including increases of local tonic dopamine concentrations and dopamine turnover, lead to impaired striatal and cortical mechanisms underlying working memory, sensory discrimination, and planning (Murphy et al., 1996; #CITATION_TAG; Liu et al., 2008), which may underlie the working memory deficits seen in drug abusers (Ornstein et al., 2000).","A feature of the model is that it predicts heating events with a range of sizes, depending on where the instability threshold for linear kink modes is encountered. Methods. The loop is represented as a straight line-tied cylinder. The twisting caused by random photospheric motions is captured by two parameters, representing the ratio of current density to field strength for specific regions of the loop. Instability onset is mapped as a closed boundary in the 2D parameter space. After flaring, the loop evolves to the state of lowest energy where, in accordance with relaxation theory, the ratio of current to field is constant throughout the loop and helicity is conserved. The model is applied such that the loop undergoes repeated episodes of instability followed by energy-releasing relaxation. Hence, an energy distribution of the nanoflares produced is collated. This paper also presents the calculated relaxation states and energy releases for all instability threshold points. The final energy distribution features two nanoflare populations that follow different power laws.","['An alternative approach is to construct a magnetohydrodynamic coronal loop model that has the ability to predict nanoflare energy distributions.', 'The aims are to calculate the distribution of event energies and to investigate whether kink instability can be predicted from a single parameter.']"
"requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Background: There has been a rapid growth in recent years of available technologies for individuals with communication difficulties. Research in the area is currently under-developed with practitioners having a limited body of work to draw on to guide the process of intervention. Concerns have been raised that this newly-developed technology may have limited functional usage. The synthesis of evidence describing views of users and providers, and the implementation of high tech AAC systems, can provide valuable data to inform intervention studies and functional outcome measures Findings regarding the qualitative studies are reported elsewhere [#CITATION_TAG].","Main Contribution: The review highlights the range of factors that can impact on provision and use of high tech AAC, which practitioners should consider and address as appropriate in the intervention process. The work outlines how qualitative synthesis review methods may be applied to the consideration of published material that is not reporting outcomes data, and how this may provide valuable information to inform future studies.","['Aims: This review aimed to investigate the potential barriers and facilitators to high tech AAC provision and its ongoing use.', 'The aim of the analysis was to explore factors underpinning use rather than effectiveness, thus it synthesised data from predominantly qualitative and survey studies reporting the views and perceptions of AAC users or staff providing the devices.']"
"requires a greater understanding of characteristics of clients who may or may not benefit from this technology. High-tech AAC systems are expensive to purchase and repair, and funding may fail to meet total device costs, or include adequate speech-language pathology support [#CITATION_TAG].",,"['We focus on new observational capabilities (Yohkoh, SoHO, TRACE), observations, modeling approaches, and insights into physical processes of the solar corona.']"
"requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Dragon Dictate is time-consuming to learn and demands a high level of motivation, but can be beneficial to a person who has profound dysarthria and great difficulties in accessing the computer. Another [#CITATION_TAG] assessing the use of DragonDictate reported that while 1 participant withdrew from the study, the other achieved gains in computer access efficiency of 40%.","Single subject design was used and measures of computer access system effectiveness and speech production were used before, during and after intervention. The users' original switch access system was compared to a combination of their switch access system and speech recognition, by counting the number of correct entries. The other participant did not complete the intervention protocol.","['This study investigated the use of the speech recognition system Dragon Dictate as an augmentative method of computer access for two individuals with cerebral palsy, including severe motor dysfunction and dysarthria.']"
"requires a greater understanding of characteristics of clients who may or may not benefit from this technology. The Extreme-Ultraviolet Imaging Spectrometer (EIS) on Hinode produces high resolution spectra that can be combined via rasters into monochromatic images of solar structures, such as active regions. One [#CITATION_TAG] which evaluated the introduction of Swedish DragonDictate TM and Infovox TM systems outlined gains in recognition accuracy of 26-60%.","Electron temperature and density maps of the structures can be obtained by imaging the structures in different spectral lines with ratios sensitive to either temperature or density. Doppler maps and ion temperature maps can be made from spectral line wavelengths and profiles, respectively.","['In this paper we discuss coronal temperature and density distributions within an active region, illustrating the power of EIS for solar plasma diagnostics.']"
"requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Other studies in this client group compared a SGD versus PECS and found little difference in outcomes between them [49], and compared peer-mediated naturalistic interventions with and without a SGD, reporting a significant increase in communicative behaviours using the SGD [#CITATION_TAG].","Generalisation probes were conducted during mealtimes at the preschools. A multiple baseline design was used to assess the outcomes of the two intervention conditions.All 3 children with autism increased their communicative behaviours immediately following the introduction of the two interventions, and generalised these increases to mealtime interactions with their peers.","['The aim of this study was to assess the effectiveness of two communication interventions for preschool-aged children with autism.Six typically developing peers were taught to implement peer-mediated naturalistic teaching, with and without a speech generating device (SGD), during play sessions with 3 classmates with autism in three preschools.']"
"requires a greater understanding of characteristics of clients who may or may not benefit from this technology. The expansion of evidence-based practice across sectors has lead to an increasing variety of review types. However, the diversity of terminology used means that the full potential of these review types may be lost amongst a confusion of indistinct and misapplied terms. A limited number of review types are currently utilized within the health information domain.Few review types possess prescribed and explicit methodologies and many fall short of being mutually exclusive. Notwithstanding such limitations, this typology provides a valuable reference point for those commissioning, conducting, supporting or interpreting reviews, both within health information and the wider health care domain. This review therefore was undertaken as a 'state of the art' review [#CITATION_TAG] to present an assessment of the current state of knowledge in the field.","A simple analytical framework -- Search, AppraisaL, Synthesis and Analysis (SALSA) -- was used to examine the main review types.Fourteen review types and associated methodologies were analysed against the SALSA framework, illustrating the inputs and processes of each review type. A description of the key characteristics is given, together with perceived strengths and weaknesses.","['The objective of this study is to provide descriptive insight into the most common types of reviews, with illustrative examples from health and health information domains.Following scoping searches, an examination was made of the vocabulary associated with the literature of review and synthesis (literary warrant).']"
requires a greater understanding of characteristics of clients who may or may not benefit from this technology. It is important to note the predominance of case series or case study designs in the field representing only level IV evidence [#CITATION_TAG].,"We simulate microflares as small (either randomic or periodic) heating episodes which account for all the ""stationary heating"". We synthesize the emission in some lines and bands observable by X-ray instruments.","['Our scope is both to explore feasibility of directly detecting microvariability, and to devise possible diagnostics of variable vs. steady heating.']"
requires a greater understanding of characteristics of clients who may or may not benefit from this technology. It has also been suggested [#CITATION_TAG] that practitioners and users may have limited access to available systems or services due to funding issues and limited specialist knowledge.,,"['In this paper we review published research describing the use of augmentative and alternative communication (AAC) to support societal participation by adolescents and adults with developmental disabilities who require AAC.', 'We focus on three major participation domains: post-secondary education and training, the workplace, and community living and social interaction opportunities.']"
"requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Speech output from speech-generating devices (SGD) and SGD software, such as talking word processors, has changed the landscape of options for aided communication. Schlosser [#CITATION_TAG] described how practitioners faced a difficult task when matching appropriate systems to individuals with disabilities.","Learner-oriented roles of speech output are summarized in terms of graphic symbol learning, communicative functions and social regulation, learner preference, challenging behaviors, natural speech production, comprehension, and literacy. Roles for the learner - partner dyad include changes to interaction patterns. Methodological issues are discussed and practical implications are drawn where appropriate.","['The purpose of this paper is to review and critique research into the roles of speech output for communication partners, learners, and learner-partner dyads.', 'Research on partner - oriented roles is reviewed in terms of attitudes and perceived communicative competence, and communicative behavior.']"
requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Recent analysis of the EUV emission of the solar Active Region observed with TRACE (Sakamoto et al. One further paper relating to adults with non-progressive disorders [#CITATION_TAG] described the use of a laptop computer with word processing software for a male following total glossectomy and laryngectomy.,This was considered as a signature of a multiple-strand structure of coronal loops that originates from numerous sporadic coronal heating events (nanoflares). Methods. Simultaneous TRACE and Yohkoh/SXT observations were interpreted by using theoretical predictions of the forward modelling of the nanoflare heating.,['The present Letter aims to put these findings into a broader context of the nanoflare heating scenario.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The nanoflare model was early applied to the heating of coronal loops observed by Yohkoh (#CITATION_TAG).","The solar validation is carried out by using spatially resolved X-ray observations of the Sun obtained from the Yohkoh satellite. In particular, we show how this analysis procedure can be used in the context of archival Einstein, ROSAT, and EUVE data, as well as for Chandra and XMM-Newton data, as a complementary analysis tool to existing multithermal component models.","['The aim of this paper is to validate a methodology for connecting the emission measure of individual solar coronal loops to the integrated emission measure of the entire solar corona and using this connection to deduce the energetic properties of the corona; we then show how this methodology can be applied to observations of solar-like stellar coronae.', 'This work is a further step in our effort to place the ""solar-stellar connection"" on a quantitative footing.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Previous solar observations have shown that coronal loops near 1 MK are difficult to reconcile with simple heating models. The electron densities in these loops, however, are too high to be consistent with thermodynamic equilibrium. Models proposed to explain these properties generally rely on the existence of smaller scale filaments within the loop that are in various stages of heating and cooling. In particular, as already mentioned in Section 3.2.2, steady hydrodynamic loop modeling (i.e., assuming equilibrium condition and, therefore, dropping the time-dependent terms in Eqs. ( 4), ( 5), and ( 6)), showed that flows may not be able to explain the evidence of isothermal loops (Patsourakos et al., 2004), as instead proposed by #CITATION_TAG.","These loops have lifetimes that are long relative to a radiative cooling time, suggesting quasi-steady heating. Such a framework implies that there should be a distribution of temperatures within a coronal loop. EIS is capable of observing active regions over a wide range of temperatures (Fe VIII-Fe XVII) at relatively high spatial resolution (1'' -->). We also derive volumetric filling factors in these loops of approximately 10%.",['In this paper we analyze new observations from the EUV Imaging Spectrometer (EIS) on Hinode.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Several items on the diagnostics and interpretation of coronal loop observations are under debate. The amount of background depends on the instrument characteristics, such as the passband and the point response function: it is most of the signal in TRACE UV filterbands, for instance, and its subtraction becomes a very delicate issue (e.g., Del Zanna and Reale and Ciaravella, 2006; Aschwanden et al., 2008; #CITATION_TAG).","We have taken as pixel-by-pixel background the latest TRACE, Yohkoh and CDS images where the loop has faded out. We examine the loop morphology evolution, the light curves, the TRACE filter ratio distribution and evolution, the images and emission measure from the CDS spectral lines.","['In this work, we analyze a well-defined loop system detected in a time-resolved observation in several spectral bands.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Smithsonian Astrophysical Observatory, 60 Garden St., Cambridge MA 02138, USAPreprint online version: December 14, 2011ABSTRACTContext.Recent studies carried out with SOHO and Hinode high-resolution spectrometers have shown that the plasma in the off-disksolar corona is close to isothermal. Some mechanisms have been proposed to explain these redshifts: downward propagating acoustic waves (Hansteen, 1993), downdrafts driven by radiativelycooling condensations in the solar transition region (Reale et al., 1996 (Reale et al.,, 1997b, nanoflares (#CITATION_TAG); the scenario is improving with the better and better definition of the observational framework.","We also test the effects of 4) atomic data uncertainties on the results, and 5) the number of ionswhose lines are available for the DEM reconstruction.Methods.We first use the CHIANTI database to calculate synthetic spec tra from different thermal distributions: single isothermalplasmas, multithermal plasmas made of multiple isothermal components, and multithermal plasmas with a Gaussian DEM distributionwith variable width. We then apply the MCMC technique on each of these synthetic spectra, so that the ability of the MCMC techniqueat reconstructing the original thermal distribution can be evaluated. Next, we add a random noise to the synthetic spectra, and repeatthe exercise, in order to determine the effects of random errors on the results. We also we repeat the exercise using a different set ofatomic data from those used to calculate synthetic line intensities, to understand the robustness of the results against atomic physicsuncertainties. Also, the DEMcurves obtained using lines calculated with an isothermal plasma and with a Gaussian distribution with FWHM of logT  0.05 arevery similar. Methods: data analysis -- Techniques: spectroscopic -- Sun: c orona -- Sun: UV radiation","['However, these studies have been carried out with diagnostic techniques whose ability to reconstruct the plasma distributionwith temperature has not been thoroughly tested.Aims.In this paper, we carry out tests on the Monte Carlo Markov Chain (MCMC) technique with the aim of determining: 1) itsability to retrieve isothermal plasmas from a set of spectral line intensities, with and without random noise; 2) to what extent can itdiscriminate between an isothermal solution and a narrow multithermal distribution; and 3) how well it can detect multiple isothermalcomponents along the line of sight.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The high temperatures could never be directly measured in the corona due to the small emission measure and the most promising signature of such heating is blue-shifted plasma from the loop footpoints. The brightening of a single coronal loop was analyzed in detail in an observation of more than 2 hours with a cadence of about 30 s (#CITATION_TAG).",Methods. We derive an analytical model in order to gain some simple physical insights into the system and use a one dimensional hydrodynamic model that treats the electrons and ions as a coupled fluid to simulate nanoflare heating with time-scales of 30 s. Our analytical model also provide a means of verifying our numerical results.,"['This paper addresses the impulsive heating of very diffuse coronal loops, such as can occur in a nanoflare-heated corona with low filling factor.', 'We study the physics associated with nanoflare heating in this scenario and aim to determine whether there exist any observable signatures.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The double filter ratio temperature analysis technique is applied to two sets of TRACE images having a 20 minute time gap resulting in temperatures between 1.0 and 1.3 MK. The first coronal loop structures were identified properly after a rocket launch in 1968, which provided for the first time an image of an X-ray flare (#CITATION_TAG), with a resolution of a few arcsec.",The CDS sparse raster line intensities are used to create emission measure (EM) loci plots determining temperature values over an overlapping 40 minute observing period.,['In this paper we aim to measure the temperature along a specific coronal loop feature which is located in an active region on the western solar limb.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Many studies of the solar corona have shown that the observed X-ray luminosity is well correlated with the total unsigned magnetic flux. Also, the peak of the coronal emission measure of active regions -where the loops are brightest -is above 2 MK, which is best observed in X-rays (e.g., #CITATION_TAG; Reale et al., 2009a; Warren et al., 2011).","We use potential field extrapolations to compute magnetic field lines and populate these field lines with solutions to the hydrostatic loop equations assuming steady, uniform heating. Our volumetric heating rates are of the form H ~ a/Lb, where is the magnetic field strength averaged along a field line and L is the loop length. Visualizations of the emission are also needed.","['In this paper we present results from the extensive numerical modeling of active regions observed with the Solar and Heliospheric Observatory (SOHO) EUV Imaging Telescope (EIT), the Yohkoh Soft X-Ray Telescope (SXT), and the SOHO Michelson Doppler Imager (MDI).']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Similar results but different conclusions were reached after the analysis of a loop observed with SoHO, invoking a non-constant cross-section to explain the evidence of isothermal loop (#CITATION_TAG; Landi and Feldman, 2004).","First, using the emission measure (Ec,) and the temperature (T Ca ) values derived for a set of hydrodynamic (HD) flare models discussed in previous papers, we have obtained and analysed the (e-T) counterparts of the (N-T) diagrams.",['The aim of the paper is to illustrate the application of the density-temperature diagrams discussed in the previous paper (Paper II) to interpretation of soft X-ray measurements namely calcium spectra recorded by the Solar Maximum Mission Bent Crystal Spectrometer.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Widespread evidence for outward propagation of Alfvén waves is reported from ground optical polarimetric observations (#CITATION_TAG), and non-thermal broadening has been shown to correlate with swaying motions detected in the corona from SDO/AIA data (speed of ∼ 20 km s -1 and periods of few minutes) (McIntosh et al., 2011).","Treatment of DC with intact IgG or Fab of mAb DF272 enhanced their T cell stimulatory capacity. By using a retrovirus-based cDNA expression library generated from DC, we cloned and sequenced the mAb DF272-defined cell surface receptor and could demonstrate that it is identical with B7-H1 (programmed death-1 ligand), a recently identified new member of the B7 family of costimulatory molecules.","['In an effort to identify immunoregulatory molecules on dendritic cells (DC), we generated and screened for mAbs capable of modulating the T cell stimulatory function of DC.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Context: It is thought likely that vast numbers of nanoflares are responsible for the corona having a temperature of millions of degrees. Current observational technologies lack the resolving power to confirm the nanoflare hypothesis. Dissipation of the loop's magnetic energy begins during the nonlinear stage of the instability, which develops as a consequence of current sheet reconnection. Loop twisting or braiding and kink instability can lead to magnetic reconnection with the formation and fragmentation of thin current sheets and their dissipation through resistivity (Hood et al., 2009b; Wilmot-Smith et al., 2010; Bareford et al., 2010; Pontin et al., 2011; Wilmot-Smith et al., 2011; Bareford et al., 2011 #CITATION_TAG.","It predicts heating events with a range of sizes, depending on where the instability threshold for linear kink modes is encountered. Methods: The loop is represented as a straight line-tied cylinder. The twisting caused by random photospheric motions is captured by two parameters, representing the ratio of current density to field strength for specific regions of the loop. After flaring, the loop evolves to the state of lowest energy where, in accordance with relaxation theory, the ratio of current to field is constant throughout the loop and helicity is conserved. The model is applied such that the loop undergoes repeated episodes of instability followed by energy-releasing relaxation. Hence, an energy distribution of the nanoflares produced is collated.","['An alternative approach is to construct a magnetohydrodynamic coronal loop model that has the ability to predict nanoflare energy distributions.', 'Aims: This paper presents the initial results generated by such a model.', 'The aims are to calculate the distribution of event energies and to investigate whether kink instability can be predicted from a single parameter.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The electron plasma frequency $\omega_{pe}$ and electron gyrofrequency $\Omega_e$ are two parameters that allow us to describe the properties of a plasma and to constrain the physical phenomena at play, for instance, whether a maser instability develops. The importance of the maser instability in coronal active regions depends on the complexity and topology of the magnetic field configurations.Comment: 10 pages, 7 figures, 1 appendix (with 1 figure This problem emerged dramatically when the analysis of the same large loop structure observed with Yohkoh/SXT on the solar limb led to three Living Reviews in Solar Physics http://www.livingreviews.org/lrsp-2014-4 different results depending mostly on the different ways to treat the background (Priest et al., 2000; #CITATION_TAG; Reale, 2002a).","We perform an in-depth analysis of the $\omega_{pe}$/$\Omega_e$ ratio for simple theoretical and complex solar magnetic field configurations. Using the combination of force-free models for the magnetic field and hydrostatic models for the plasma properties, we determine the ratio of the plasma frequency to the gyrofrequency for electrons. For the sake of comparison, we compute the ratio for bipolar magnetic fields containing a twisted flux bundle, and for four different observed active regions. We also study how $\omega_{pe}$/$\Omega_e$ is affected by the potential and non-linear force-free field models. We demonstrate that the ratio of the plasma frequency to the gyrofrequency for electrons can be estimated by this novel method combining magnetic field extrapolation techniques and hydrodynamic models.","['In this paper, we aim to show that the maser instability can exist in the solar corona.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Chronic hepatitis B was characterized by fluctuant immune response to infected hepatocytes resulting in hepatic inflammation and virus persistence. Recently, Programmed Death-1 (PD-1) and its ligand PD-L1 have been demonstrated to play an essential role in balancing antiviral immunity and inflammation in the livers of acute hepatitis B patients, significantly influencing disease outcome. PD-1 up-regulation in peripheral T cells is associated with immune dysfunction in chronic hepatitis B patients. In particular, whereas the loop total emission measure distribution should steepen above the canonical 1.5 (Jordan, 1980; #CITATION_TAG; Peres et al., 2001) dependence for temperature above 1 MK.",,"['Here, we report up-regulation of PD-1 and PD-L1 in liver biopsies from 32 chronic HBV patients compared to 4 healthy donors.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Some of the most successful pathogens of human, such as Mycobacterium tuberculosis (Mtb), HIV, and Leishmania donovani not only establish chronic infections but also remain a grave global threat. These pathogens have developed innovative strategies to evade immune responses such as antigenic shift and drift, interference with antigen processing/presentation, subversion of phagocytosis, induction of immune regulatory pathways, and manipulation of the costimulatory molecules. Costimulatory molecules expressed on the surface of various cells play a decisive role in the initiation and sustenance of immunity. Exploitation of the ""code of conduct"" of costimulation pathways provides evolutionary incentive to the pathogens and thereby abates the functioning of the immune system. Impairment by pathogens in the signaling events delivered by costimulatory molecules may be responsible for defective T-cell responses; consequently organisms grow unhindered in the host cells. In another study, hot monolithic loops visible with the Yohkoh/SXT were instead resolved as stranded cooler structures with TRACE at later times (#CITATION_TAG), although the large time delay (1 to 3 hours) is hardly compatible with the cooling time from SXT to TRACE sensitivity.",,"['Here we review how Mtb, HIV, Leishmania sp., and other pathogens manipulate costimulatory molecules to establish chronic infection.', 'This review summarizes the convergent devices that pathogens employ to tune and tame the immune system using costimulatory molecules.', 'Studying host-pathogen interaction in context with costimulatory signals may unveil the molecular mechanism that will help in understanding the survival/death of the pathogens.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The influenza A virus is one of the main causes of respiratory infection. Although influenza virus infection alone can result in pneumonia, secondary bacterial infection combined with the virus is the major cause of morbidity and mortality. Interestingly, while influenza infection increases susceptibility to some bacteria, including Streptococcus pneumoniae, Staphylococcus aureus (S. aureus), and Haemophilus influenzae, other bacteria such as Escherichia coli (E. coli) and Klebsiella pneumoniae are not associated with influenza infection. The reason for this discrepancy is not known. Density diagnostics through density-sensitive line ratio led to measure directly density values in active regions (e.g., #CITATION_TAG).","Here, the mechanism for this inhibition is elucidated: prior influenza virus infection strongly increases interferon gamma (IFN-g) production. Furthermore, it was shown that IFN-g differentially affects alveolar macrophage phagocytosis of S. aureus and E. coli.","['In this study, it was found that prior influenza virus infection inhibits murine alveolar macrophage phagocytosis of S. aureus but not of E. coli.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Secondary pneumococcal pneumonia is a serious complication during and shortly after influenza infection. This increased susceptibility to secondary bacterial pneumonia is at least in part caused by excessive IL-10 production and reduced neutrophil function in the lungs. Also, the peak of the coronal emission measure of active regions -where the loops are brightest -is above 2 MK, which is best observed in X-rays (e.g., Peres et al., 2000; Reale et al., 2009a; #CITATION_TAG).",C57BL/6 mice were intranasally inoculated with 10 median tissue culture infective doses of influenza A (A/PR/8/34) or PBS (control) on day 0.,['We established a mouse model to study postinfluenza pneumococcal pneumonia and evaluated the role of IL-10 in host defense against Streptococcus pneumoniae after recovery from influenza infection.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Recent analysis of the EUV emission of the solar Active Region observed with TRACE (Sakamoto et al. Studies based both on models and on analysis of observations independently suggest that elementary loop components should be very fine with typical cross-sections of the strands on the order of 10 -100 km (Beveridge et al., 2003; Cargill and Klimchuk, 2004; #CITATION_TAG).",This was considered as a signature of a multiple-strand structure of coronal loops that originates from numerous sporadic coronal heating events (nanoflares). Methods. Simultaneous TRACE and Yohkoh/SXT observations were interpreted by using theoretical predictions of the forward modelling of the nanoflare heating.,['The present Letter aims to put these findings into a broader context of the nanoflare heating scenario.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. In particular, whereas the loop total emission measure distribution should steepen above the canonical 1.5 (Jordan, 1980; Orlando et al., 2000; #CITATION_TAG) dependence for temperature above 1 MK.","The solar validation is carried out by using spatially resolved X-ray observations of the Sun obtained from the Yohkoh satellite. In particular, we show how this analysis procedure can be used in the context of archival Einstein, ROSAT and EUVE data, as well as Chandra and XMM Newton data, as a complementary analysis tool to existing multi-thermal component models","['The aim of this paper is to validate a methodology for connecting the emission measure of individual solar coronal loops to the integrated emission measure of the entire solar corona, and using this connection to deduce the energetic properties of the corona, and then to show how this methodology can be applied to observations of solar-like stellar coronae.', 'This work is a further step in our effort to place the ""solar-stellar connection"" on a quantitative footing.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Light curves in the EUV band have been analysed also with a different approach: they have been compared to simulated ones obtained from sequences of random pulses with powerlaw distribution (#CITATION_TAG).","It also investigated whether acoustic-phonetic modifications made to counteract the effects of a challenging listening condition are tailored to the condition under which communication occurs. Forty talkers were recorded in pairs while engaged in ""spot the difference"" picture tasks in good and challenging conditions. In the challenging conditions, one talker heard the other (1) via a three-channel noise vocoder (VOC); (2) with simultaneous babble noise (BABBLE).","['This study investigated whether speech produced in spontaneous interactions when addressing a talker experiencing actual challenging conditions differs in acoustic-phonetic characteristics from speech produced (a) with communicative intent under more ideal conditions and (b) without communicative intent under imaginary challenging conditions (read, clear speech).']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. However, it is unclear how often training should be performed to maximize its benefit. The present study investigated how the frequency of training contributed to normal-hearing listeners' adaptation to spectrally shifted speech. Although more frequent training may accelerate listeners' adaptation to spectrally shifted speech, there may be significant benefits from training as little as one session per week. It has been shown that time-dependent loop models must include a relatively thick, cool, and dense chromosphere and the transition region for a correct description of the mass transfer driven by transient heating (e.g., #CITATION_TAG) and to maintain the necessary numerical stability (Antiochos, 1979; Hood and Priest, 1980; Peres et al., 1982).","Methods: Eighteen normal-hearing listeners were trained with spectrally shifted and compressed speech via an 8-channel acoustic simulation of cochlear implant speech processing. Five short training sessions (1 hr per session) were completed by each subject; subjects were trained at one of three training rates: five sessions per week, three sessions per week, or one session per week. Subjects were trained to identify medial vowels presented in a cVc format; depending on the level of difficulty, the number of response choices was increased and/or the acoustic differences between vowels were reduced. Vowel and consonant recognition was measured before and after training as well as at regular intervals during the training period. Sentence recognition was measured before and after training only.",['Objective: Previous studies have shown that the protocol used for auditory training may significantly affect the outcome of training.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. We expect better resolution switching from narrow-band instruments with few channels (Weber et al., 2005) to spectrometers (#CITATION_TAG; Landi et al., 2012b), but it is difficult to achieve a temperature resolution better than Δ log ≈ 0.05 (Landi et al., 2012b).","We also test the effects of 4) atomic data uncertainties on the results, and 5) the number of ions whose lines are available for the DEM reconstruction. Also, the DEM curves obtained using lines calculated with an isothermal plasma and with a Gaussian distribution with FWHM of log T = 0.05 are very similar. The availability of small sets of lines also does not worsen the performance of the MCMC technique, provided these lines are formed in a wide temperature range.","['In this paper, we carry out tests on the Monte Carlo Markov Chain (MCMC) technique with the aim of determining: 1) its ability to retrieve isothermal plasmas from a set of spectral line intensities, with and without random noise; 2) to what extent can it discriminate between an isothermal solution and a narrow multithermal distribution; and 3) how well it can detect multiple isothermal components along the line of sight.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Context.This paper addresses the impulsive heating of very diffuse coronal loops, such as can occur in a nanoflare-heated corona with low filling factor. The high temperatures could never be directly measured in the corona due to the small emission measure and the most promising signature of such heating is blue-shifted plasma from the loop footpoints. These equations can be solved numerically and several specific codes have been used extensively to investigate the physics of coronal loops and of X-ray flares (e.g., Nagai, 1980; Peres et al., 1982; Doschek et al., 1982; Nagai and Emslie, 1984; Fisher et al., 1985a,a,a; MacNeice, 1986; Gan et al., 1991; Hansteen, 1993; Betta et al., 1997; Antiochos et al., 1999; Ofman and Wang, 2002; Müller et al., 2003; #CITATION_TAG; Sigalotti and Mendoza-Briceño, 2003; Bradshaw and Cargill, 2006).",Methods.We derive an analytical model in order to gain some simple physical insights into the system and use a one dimensional hydrodynamic model that treats the electrons and ions as a coupled fluid to simulate nanoflare heating with time-scales of 30 s. Our analytical model also provide a means of verifying our numerical results.,['Aims.We study the physics associated with nanoflare heating in this scenario and aim to determine whether there exist any observable signatures.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Performance at 22- and 23-mm simulated insertion depths was always poorer than normal, and performance at 25 mm simulated insertion depth was, most generally, the same as normal. Along a coronal loop in an active region on the solar limb, while TRACE double filter ratios led to temperatures between 1.0 and 1.3 MK, the emission measure loci from CDS data were consistent with a line-of-sight isothermal structure which increases in temperature from ∼ 1.20 to 1.75 MK along the loop, in contrast with the nearby multithermal background (#CITATION_TAG).","Normally hearing listeners were presented with vowels, consonants, and sentences for identification through an acoustic simulation of a five-channel cochlear implant with electrodes separated by 4 mm (as in the Ineraid implant). Insertion depth was simulated by outputting sine waves from each channel of the processor at a frequency determined by the cochlear place of electrodes inserted 22-25 mm into the cochlea.",['The aim of the experiment was to simulate the effect of depth of electrode insertion on identification accuracy.']
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Vowel recognition may be the limiting factor in recognizing basally shifted speech. In some of these studies, for instance, seismological techniques are used in order to measure flare-induced loop oscillations (Ballai et al., 2011) and waves and flows in active region loops (#CITATION_TAG; Uritsky et al., 2013).",,"['The purpose of this project was to assess the degree to which a patient, after 1 wk of experience, could adapt to 3.2-mm and 6.8-mm basal shifts in the representation of speech.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Undamped, or even growing waves were observed by SDO/AIA (#CITATION_TAG; Nisticò et al., 2013).","Special efforts are made to disentangle the effects of family size and birth order, since these effects have often been confounded in the past.","['This paper explores the relationship between sibship structure and educational outcomes, in the context of theories of dilution of parental time.']"
"Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The associations of family structure and reason for family disruption with outcomes for fifteen/sixteen-year-olds are similar in the West of Scotland to those in Britain as a whole. In typical coronal conditions, i.e., ratio of thermal and magnetic pressure ≪ 1, temperature of a few MK, density of 10 8 -10 10 cm −3, the plasma confined in coronal loops can be assumed as a compressible fluid moving and transporting energy only along the magnetic field lines, i.e., along the loop itself (e.g., #CITATION_TAG; Vesecky et al., 1979).","A wide range of measures were considered, using educational achievement to represent life-chances, smoking and drinking to represent life styles and psychological well-being (GHQ), and physical symptoms to represent health. The associations of family structure with these outcomes is estimated for each Study at three levels: the overall association, that after controlling for gender and family income, and, finally, after controlling in addition for family processes. Odds ratios with 95 percent confidence intervals are reported for those living in lone-parent households and reconstituted households each compared with intact families.","['The main aim of this paper is to compare the association of family structure with outcomes for young people in living in the West of Scotland (the Twenty-07 Study, N=1009) with their contemporaries living in Britain (the 1970 British Cohort Study N=11615) in the mid-1980s.']"
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Antigen-presenting cells (APC) are considered to play a critical role in promoting the (re)activation of potentially autoreactive T cells in multiple sclerosis (MS), an inflammatory demyelinating disorder of the central nervous system (CNS). B7-H1 (PD-L1) is a novel member of the B7 family proteins which exert costimulatory and immune regulatory functions. B7-H1-dependent immune inhibition is in part mediated by CD4/CD25+ regulatory T cells. Since type I IFN expression was observed in response to influenza infection and previous studies have suggested that these cytokines could induce PDL1 expression [18, #CITATION_TAG], we investigated whether rhIFNβ could induce PDL1 gene expression in our MDM model (Fig. 7A).","B7-H1 is constitutively expressed on monocytes and differentially matured DC, but not on B cells.",['Here we characterize the expression and functional activity of B7-H1 expressed on monocytes and dendritic cells (DC) of healthy donors and MS patients.']
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. In the Andean region of South America, understanding communities' water perceptions is particularly important for water management as many rural communities must decide by themselves if and how they will protect their micro-watersheds and distribute their water. On the one hand, observed changes in land cover match perceptions of deforestation as the primary cause of increasing water scarcity. In contrast, binding of PDL1 to PD1 causes inhibition of TCR-mediated phosphatidylinositol-3-kinase (PI3Kinase) activation leading to inhibition of T cell proliferation and cytokine release [#CITATION_TAG].","Furthermore, water scarcity was perceived in regions where seasonal rainfall variability is higher but not in regions where annual rainfall is lower.","['In this study we examine how Water User Associations in the Eastern Andes of Colombia perceive water scarcity and the relationship between this perception and observed climate, land use, and demographic changes.']"
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Abstract Smallholder livelihoods in the Peruvian Altiplanoare frequently threatened by weather extremes, includingdroughts, frosts and heavy rainfall. Its highlyvariable, semi-arid climate is closely linked to weatherextremes, such as droughts, frosts and heavy rainfall, whichfrequently challenge people's livelihoods. In 2007, severaldistricts in the Peruvian Altiplano declared a state of emer-gencycausedbyfrosts,hailandtransientdroughtsinthemidstof the agricultural season (INDECI 2009). Despite regionaldevelopment programmes to improve the smallholder sys-tems, poverty and undernourishment remain significant(FONCODES 2006). Produc-tive resources are heterogeneously distributed and liveli-hood options differ among the smallholders which requiresthat the respective vulnerability-creating mechanisms betackled appropriately. One further possibility is suggested by the fact that both CD80 and CD86 can induce signalling by the inhibitory CTLA-4 receptor on T cells as well as activating the CD28 pathway [#CITATION_TAG], therefore an inhibitory milieu may be propagated via the infected macrophage.","Given the persistence ofsignificant undernourishment despite regional developmentefforts,weproposeaclusterapproachtoevaluatesmallholders'vulnerability to weather extremes with regard to food security.We applied this approach to 268 smallholder households usinginformation from two existing regional assessments and fromour own household survey. The vulnerability patternswere then ranked according to the different amounts of pur-chase. A second validation aspect accounted for independentlyreported mechanisms explaining smallholders' sensitivity andadaptive capacity.",['Addressing the heterogeneity ofhuman-nature interactions at an intermediate functional']
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Acute exacerbations are the major cause of asthma morbidity, mortality, and health-care costs and are difficult to treat and prevent. The majority of asthma exacerbations are associated with rhinovirus (RV) infection, but evidence supporting a causal relationship is weak and mechanisms are poorly understood. Asthma has long been associated with a skewing of lung immunity away from a Th1 (i.e. IFNγ producing) response to a high Th2 (i.e. IL-4/13 producing) response [30] and more recent work has demonstrated no significant induction of IFNγ in BAL cells from asthma patients experimentally challenged with rhinovirus [#CITATION_TAG].","We hypothesized that in asthmatic, but not normal, subjects RV infection would induce clinical, physiologic, and pathologic lower airway responses typical of an asthma exacerbation and that these changes would be related to virus replication and impaired T helper 1 (Th1)/IL-10 or augmented Th2 immune responses.","['We investigated physiologic, virologic, and immunopathologic responses to experimental RV infection in blood, induced sputum, and bronchial lavage in 10 asthmatic and 15 normal volunteers.']"
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Chronic hepatitis B was characterized by fluctuant immune response to infected hepatocytes resulting in hepatic inflammation and virus persistence. Recently, Programmed Death-1 (PD-1) and its ligand PD-L1 have been demonstrated to play an essential role in balancing antiviral immunity and inflammation in the livers of acute hepatitis B patients, significantly influencing disease outcome. PD-1 up-regulation in peripheral T cells is associated with immune dysfunction in chronic hepatitis B patients. The question remains as to why this host response mechanism in response to virus has developed and what its functional significance is? Perhaps it is a means by which anti-viral responses can be restrained before the resulting inflammation can cause too much tissue damage, as has been suggested in the liver [#CITATION_TAG]. Or, as has been suggested for HIV, this mechanism has been subverted by the virus to prevent viral clearance [37].",,"['Here, we report up-regulation of PD-1 and PD-L1 in liver biopsies from 32 chronic HBV patients compared to 4 healthy donors.']"
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Some of the most successful pathogens of human, such as Mycobacterium tuberculosis (Mtb), HIV, and Leishmania donovani not only establish chronic infections but also remain a grave global threat. These pathogens have developed innovative strategies to evade immune responses such as antigenic shift and drift, interference with antigen processing/presentation, subversion of phagocytosis, induction of immune regulatory pathways, and manipulation of the costimulatory molecules. Costimulatory molecules expressed on the surface of various cells play a decisive role in the initiation and sustenance of immunity. Exploitation of the ""code of conduct"" of costimulation pathways provides evolutionary incentive to the pathogens and thereby abates the functioning of the immune system. Impairment by pathogens in the signaling events delivered by costimulatory molecules may be responsible for defective T-cell responses; consequently organisms grow unhindered in the host cells. The question remains as to why this host response mechanism in response to virus has developed and what its functional significance is? Perhaps it is a means by which anti-viral responses can be restrained before the resulting inflammation can cause too much tissue damage, as has been suggested in the liver [36]. Or, as has been suggested for HIV, this mechanism has been subverted by the virus to prevent viral clearance [#CITATION_TAG].",,"['Here we review how Mtb, HIV, Leishmania sp., and other pathogens manipulate costimulatory molecules to establish chronic infection.', 'This review summarizes the convergent devices that pathogens employ to tune and tame the immune system using costimulatory molecules.', 'Studying host-pathogen interaction in context with costimulatory signals may unveil the molecular mechanism that will help in understanding the survival/death of the pathogens.']"
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. The influenza A virus is one of the main causes of respiratory infection. Although influenza virus infection alone can result in pneumonia, secondary bacterial infection combined with the virus is the major cause of morbidity and mortality. Interestingly, while influenza infection increases susceptibility to some bacteria, including Streptococcus pneumoniae, Staphylococcus aureus (S. aureus), and Haemophilus influenzae, other bacteria such as Escherichia coli (E. coli) and Klebsiella pneumoniae are not associated with influenza infection. The reason for this discrepancy is not known. Subsequent studies have suggested that it is the cytokines released by lymphocytes, such as IFNγ and IL-10 that mediate the suppressive effects of virus [1, [22] [23] [#CITATION_TAG].","Here, the mechanism for this inhibition is elucidated: prior influenza virus infection strongly increases interferon gamma (IFN-g) production. Furthermore, it was shown that IFN-g differentially affects alveolar macrophage phagocytosis of S. aureus and E. coli.","['In this study, it was found that prior influenza virus infection inhibits murine alveolar macrophage phagocytosis of S. aureus but not of E. coli.']"
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Secondary pneumococcal pneumonia is a serious complication during and shortly after influenza infection. This increased susceptibility to secondary bacterial pneumonia is at least in part caused by excessive IL-10 production and reduced neutrophil function in the lungs. Subsequent studies have suggested that it is the cytokines released by lymphocytes, such as IFNγ and IL-10 that mediate the suppressive effects of virus [1, [#CITATION_TAG] [23] [24].",C57BL/6 mice were intranasally inoculated with 10 median tissue culture infective doses of influenza A (A/PR/8/34) or PBS (control) on day 0.,['We established a mouse model to study postinfluenza pneumococcal pneumonia and evaluated the role of IL-10 in host defense against Streptococcus pneumoniae after recovery from influenza infection.']
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Unreported cases were generally dealt with less carefully than reported cases: a written request for euthanasia was more often absent (87.7% v 17.6% verbal request only; P<0.001), other physicians and caregivers specialised in palliative care were consulted less often (54.6% v 97.5%; 33.0% v 63.9%; P<0.001 for both), the life ending act was more often performed with opioids or sedatives (92.1% v 4.4%; P<0.001), and the drugs were more often administered by a nurse (41.3% v 0.0%; P<0.001). Most non-reporting physicians do not perceive their act as euthanasia. Countries debating legalisation of euthanasia should simultaneously consider developing a policy facilitating the due care and reporting obligations of physicians. This work demonstrated an additional role for MHC class II expressing cells and T helper cell responses [#CITATION_TAG] highlighting the potential importance of macrophage-T cell interactions in the control of influenza infection.",Participants A stratified at random sample was drawn of people who died between 1 June 2007 and 30 November 2007. The certifying physician of each death was sent a questionnaire on end of life decision making in the death concerned.,['Objectives To estimate the rate of reporting of euthanasia cases to the Federal Control and Evaluation Committee and to compare the characteristics of reported and unreported cases of euthanasia.']
"Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. The airway epithelium forms a continuous barrier from the nose to the alveoli and serves a variety of functions. Multiple functionally distinct cell types are involved in these processes. The innate defence functions require a patent airway epithelium, with infections often associated with epithelial defects and phenotypic alterations that are themselves associated with multiple lung diseases. Non-typeable Haemophilus influenzae (NTHi) and respiratory syncytial virus (RSV) are frequently identified in the airways in a range of respiratory diseases These pathogens often trigger exacerbations and worsening symptoms that often result in hospitalisation. This is particularly true in paediatric populations. Although mortality for NTHi and RSV infections alone are themselves low it remains unclear what role these infections play in mortality rates in complex chronic respiratory infections. These markers are representative of different epithelial cell types within the cultures. Macrophages are not only a key line of defence in the respiratory tract, responsible for phagocytosis and clearance of infectious organisms but are also orchestrators of the adaptive immune response through presentation of antigen and by the cytokines they release [4, #CITATION_TAG].","In vitro airway models were established using lung derived cell lines, undifferentiated primary human bronchial epithelial (uHBE) cells and air-liquid interface (ALI) differentiated uHBE cell cultures. Following establishment of differentiation we validated ALI cultures using a number of markers, including for the putative innate defence PLUNC family proteins, gel-forming mucins and tubulin. Cultures were infected with NTHi or RSV for periods of time ranging from 1 hour to 7 days with a view to establishing chronic infections and allowing biofilm formation. Neutrophil products and trypsin were shown to degrade PLUNC proteins in ALI cell secretions.","['These studies aimed to establish NTHi and RSV infections within airway epithelium models, and use them as tools to study pulmonary innate defence mechanisms in order to understand the role of these infections in respiratory disease.']"
CDT encourages the use of naturalistic idioms and allows the training talker to adapt their speaking style to perceptual difficulties (#CITATION_TAG).,"It also investigated whether acoustic-phonetic modifications made to counteract the effects of a challenging listening condition are tailored to the condition under which communication occurs. Forty talkers were recorded in pairs while engaged in ""spot the difference"" picture tasks in good and challenging conditions. In the challenging conditions, one talker heard the other (1) via a three-channel noise vocoder (VOC); (2) with simultaneous babble noise (BABBLE).","['This study investigated whether speech produced in spontaneous interactions when addressing a talker experiencing actual challenging conditions differs in acoustic-phonetic characteristics from speech produced (a) with communicative intent under more ideal conditions and (b) without communicative intent under imaginary challenging conditions (read, clear speech).']"
"Performance at 22- and 23-mm simulated insertion depths was always poorer than normal, and performance at 25 mm simulated insertion depth was, most generally, the same as normal. When vocoded speech is spectrally shifted upward to simulate relatively shallow CI electrode insertions, shifts in excess of 3 mm of basilar membrane distance have large acute effects on speech perception (#CITATION_TAG; Shannon et al., 1998).","Normally hearing listeners were presented with vowels, consonants, and sentences for identification through an acoustic simulation of a five-channel cochlear implant with electrodes separated by 4 mm (as in the Ineraid implant). Insertion depth was simulated by outputting sine waves from each channel of the processor at a frequency determined by the cochlear place of electrodes inserted 22-25 mm into the cochlea.",['The aim of the experiment was to simulate the effect of depth of electrode insertion on identification accuracy.']
"Vowel recognition may be the limiting factor in recognizing basally shifted speech. Cochlear implant listeners show comparable adaptation to changes of frequency mapping (#CITATION_TAG; Fu et al., 2002), and several studies indicate that explicit speech-based training can facilitate this Fu and Galvin, 2008).",,"['The purpose of this project was to assess the degree to which a patient, after 1 wk of experience, could adapt to 3.2-mm and 6.8-mm basal shifts in the representation of speech.']"
"While often included as a control, this is rarely a focus of UK studies, #CITATION_TAG excepted.","Special efforts are made to disentangle the effects of family size and birth order, since these effects have often been confounded in the past.","['This paper explores the relationship between sibship structure and educational outcomes, in the context of theories of dilution of parental time.']"
"These arguments have been described extensively in Kimsma and Van Leeuwen (Asking to die. Inside the Dutch debate about euthanasia, Kluwer Academic Publishers, Dordrecht, 1998). Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008; Kuha and Goldthorpe, 2010) still tend to focus on father's occupation or the 'dominant' parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents' characteristics (#CITATION_TAG; Lampard, 2007a; Marks, 2009; Schoon, 2008).","After some general introductory descriptions, by way of formulating a frame of reference, I shall describe the effects of this practice on patients, physicians and families, followed by a more philosophical reflection on the significance of these effects for the assessment of the authenticity of a request and the nature of unbearable suffering, two key concepts in the procedure towards euthanasia or physician-assisted suicide.","['In this article I intend to describe an issue of the Dutch euthanasia practice that is not common knowledge.', 'This article does not focus on the arguments for or against euthanasia and the ethical justification of physician-assisted dying.']"
"The associations of family structure and reason for family disruption with outcomes for fifteen/sixteen year olds are similar in the West of Scotland to those in Britain as a whole Furthermore, studies of occupational outcomes in Britain have examined family-type effects surprisingly rarely, given the prominence of family 'disruption' in numerous educational attainment studies, often using longitudinal data (Cusworth, 2009; #CITATION_TAG; Ermisch and Francesconi, 2001; Ermisch et al., 2004; Kiernan, 1992 Kiernan,, 1996 Kiernan,, 1997 Lampard, 2007b; Ní Bhrolcháin et al., 2000; Scott, 2004).","A wide range of measures were considered using educational achievement to represent life-chances, smoking and drinking to represent life styles and psychological well-being (GHQ) and physical symptoms to represent health. The associations of family structure with these outcomes is estimated for each Study at three levels: the overall association, that after controlling for gender and family income, and finally after controlling in addition for family processes. Odds ratios with 95 per cent confidence intervals are reported for those living in lone-parent households and reconstituted households each compared with intact families.","['The main aim of this paper is to compare the association of family structure with outcomes for young people in living in the West of Scotland (the Twenty-07 Study, N=1009) with their contemporaries living in Britain (the 1970 British Cohort Study N=11615) in the mid-1980s.']"
"Setting Tertiary referral centre for oncology patients in Utrecht, the Netherlands. Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008; #CITATION_TAG) still tend to focus on father's occupation or the 'dominant' parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents' characteristics (Beller, 2009; Lampard, 2007a; Marks, 2009; Schoon, 2008).",Participants 189 bereaved family members and close friends of terminally ill cancer patients who died by euthanasia and 316 bereaved family members and close friends of comparable cancer patients who died a natural death between 1992 and 1999.,['Abstract Objective To assess how euthanasia in terminally ill cancer patients affects the grief response of bereaved family and friends.']
"Physicians must report each euthanasia case to the Federal Control and Evaluation Committee. Cases describing other ELDs were sometimes also labelled 'euthanasia'. Mislabelling of ELDs could impede societal control over euthanasia. Schoon (2008: 74) views them as indicators of 'family social position', citing the view that they can act as indicators of socio-economic resources and cultural characteristics (#CITATION_TAG).","METHODS Five hypothetical cases of ELDs: intensified pain alleviation, palliative/terminal sedation, euthanasia with neuromuscular relaxants, euthanasia with morphine and life-ending without patient request were presented in a cross-sectional survey of 914 physicians in Belgium in 2009.","[""This study examines which end-of-life decisions (ELDs) Belgian physicians label 'euthanasia', which ELDs they think should be reported and the physician characteristics associated with correct labelling of euthanasia cases, the awareness that they should be reported and the reporting of them.""]"
"Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. The most frequently mentioned reasons for requesting EAS were ""pointless suffering,"" ""loss of dignity,"" and ""weakness."" The decisions physicians make, the reasons they have for their decisions, and the way they arrived at their decisions seem to be based on patient evaluations. Physicians report compliance with the official requirements for accepted practice. Glacier contribution definitely has a considerable effect on the runoff of the Río Santa during the dry season (Bury et al., 2013; #CITATION_TAG) and even more so on the tributaries draining the Cordillera Blanca.","METHODS All general practitioners in 18 of the 23 Dutch general practitioner districts received a written questionnaire in which they were asked to describe the most recent request for EAS they received. Patients' most prominent symptoms were ""feeling bad,"" ""tiredness,"" and ""lack of appetite.""","[""BACKGROUND The aims of this study were to obtain information about the characteristics of requests for euthanasia and physician-assisted suicide (EAS) and to distinguish among different types of situations that can arise between the request and the physician's decision.""]"
"Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. The total glacial area of the Cordillera Blanca, Peru, has shrunk by more than 30% in the period of 1930 to the present with a marked glacier retreat also in the recent decades. The strong increase in precipitation in the last 30 years probably did not balance the increase of temperature before the 1980s. It is suggested that recent changes in temperature and precipitation alone may not explain the glacial recession within the thirty years from the early 1980s to 2012. Glaciers in the Cordillera Blanca may be still reacting to the positive air temperature rise before 1980. Especially small and low-lying glaciers are characterised by a serious imbalance and may disappear in the near future Precipitation increases from August towards the October to April core wet season and is close to nil during June and July (e.g., Kaser and Osmaston, 2002; Mark et al., 2010; #CITATION_TAG).",We documented a cooling trend for maximum daily air temperatures and an increase in precipitation of about 60 mm/decade since the early 1980s.,['The aim of this paper is to assess local air temperature and precipitation changes in the Cordillera Blanca and to discuss how these variables could have affected the observed glacier retreat between the 1980s and present.']
"Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. Smallholder agriculture in the Central Andes of Peru is based to large extent on rainfed cropping systems, is exposed to climatic risks and is expected to respond sensitively to increasing temperatures and shifts in the precipitation regime. Criterion 3a follows data presented in Table 1 in #CITATION_TAG which is the only study we know that presents typical precipitation values required for planting of different crop types in the region.",A simple crop model is used to assess the effects of these changes on crop phenology and development.,"[""Here, we examine the potential implications of early twenty-first century climate change scenarios for the cultivation of potato, maize, wheat, barley and broad bean, five annual crops that account for 50 % of the cultivated area in the Department of Cusco and  Apuri'mac and provide the dietary backbone for a large share of the local population.""]"
"Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. Patients and physicians seemed to agree about this. In cases in which patients said they suffered unbearably there was less agreement about what constitutes unbearable suffering; patients put more emphasis on psychosocial suffering, such as dependence and deterioration, whereas physicians referred more often to physical suffering. In some cases the physician thought that the suffering was not unbearable because the patient's behaviour seemed incompatible with unbearable suffering--for instance, because the patient was still reading books. Conclusions Patients do not always think that their suffering is unbearable, even if they have a lasting wish to die. Physicians seem to have a narrower perspective on unbearable suffering than patients and than case law suggests. despite no detectable trends in the total amount of precipitation during the wet seasons (Fig. 4b) nor any other trend, the high inter-annual variability of (1) the timing of the onset of the agricultural year (as determined by the first pronounced precipitation event) and (2) dry spells during the wet season, especially during the very sensible early phase of plant growing, kept rain-fed farming constantly challenging and likely favored perceptions of water scarcity (#CITATION_TAG).","Design In-depth interviews with a topic list. Setting Patients' homes and physicians' offices. Participants 10 patients who explicitly requested euthanasia but whose request was not granted or performed and eight physicians of these patients; and eight physicians of patients who had requested euthanasia but had died before the request had been granted or performed or had died after the request was refused by the physician or after the patient had withdrawn his or her request. In an attempt to solve the problem of different perspectives, physicians should take into account the different aspects of suffering as described in the literature and a framework for assessing the suffering of patients who ask for euthanasia.",['Objective To obtain in-depth information about the views of patients and physicians on suffering in patients who requested euthanasia in whom the request was not granted or granted but not performed.']
"Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. Smallholder livelihoods in the Peruvian Altiplano are frequently threatened by weather extremes, including droughts, frosts and heavy rainfall. The peasant families of the Río Auqui watershed cultivate an average area of around 3 hectares per family which are distributed in small plots over different altitudes of the valley (Fig. 1) in order to guarantee diversified production for each family (#CITATION_TAG; Vos, 2010; Zimmerer, 2011).","We applied this approach to 268 smallholder households using information from two existing regional assessments and from our own household survey. The cluster analysis revealed four vulnerability patterns that depict typical combinations of household attributes, including their harvest failure risk, agricultural resources, education level and non-agricultural income. The vulnerability patterns were then ranked according to the different amounts of purchase. A second validation aspect accounted for independently reported mechanisms explaining smallholders' sensitivity and adaptive capacity.","[""Given the persistence of significant undernourishment despite regional development efforts, we propose a cluster approach to evaluate smallholders' vulnerability to weather extremes with regard to food security.""]"
"Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. Climatologically defined wet seasons (e.g., #CITATION_TAG) might typically end a few weeks earlier.",,"['Purpose: In the legal performance of the euthanasia procedure, unbearable suffering, one of the requirements of due care, is difficult to assess.']"
"Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. ABSTRACT Background: Although advance directives may seem useful instruments in decision-making regarding incompetent patients, their validity in cases of dementia has been a much debated subject and little is known about their effectiveness in practice. Insight into the experiences and wishes of people with dementia regarding advance directives is totally lacking in empirical research. It is clear, however, that the use of advance directives in practice remains problematic, above all in cases of advance euthanasia directives, but to a lesser extent also when non-treatment directives are involved. The peasant families of the Río Auqui watershed cultivate an average area of around 3 hectares per family which are distributed in small plots over different altitudes of the valley (Fig. 1) in order to guarantee diversified production for each family (Sietz et al., 2012; #CITATION_TAG; Zimmerer, 2011).",Methods: The relevant problems from the ethical debate on advance directives in cases of dementia are summarized and we discuss how these relate to what is known from empirical research on the validity and effectiveness of advance directives in the clinical practice of dementia care.,"['This paper assesses the contribution of advance directives to decision-making in the care of people with dementia, with a special focus on non-treatment directives and directives for euthanasia.']"
"Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. BackgroundFollowing the 2002 enactment of the Belgian law on euthanasia, which requires the consultation of an independent second physician before proceeding with euthanasia, the Life End Information Forum (LEIF) was founded which provides specifically trained physicians who can act as mandatory consultants in euthanasia requests. They tended to more often discuss the request with the attending physician (100% vs 95%) and with the family (76% vs 69%), and also more frequently helped the attending physician with performing euthanasia (44% vs 24%). Irrespective of whether euthanasia is a legal practice within a country, similar services may prove useful to also improve quality of consultations in various other difficult end-of-life decision-making situations. Climate change is mainly seen in view of environmental justice (#CITATION_TAG) with causes in both the industrialization in ""the First World"" on a global and air pollution from mining as well as air and car traffic on the regional scale.","This study assesses quality of consultations in Flanders and Brussels and compares these between LEIF and non-LEIF consultants.MethodsA questionnaire was sent in 2009 to a random sample of 3,006 physicians in Belgium from specialties likely involved in the care of dying patients. Several questions about the last euthanasia request of one of their patients were asked.",['As LEIF serves the Flemish speaking community (i.e.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. The European Association for Palliative Care Task (EAPC) Force on the Development of Palliative Care in Europe was created in 2003 and the results of its work are now being reported in full, both here and in several other publications. Different models of service delivery have been developed and implemented throughout the countries of Europe. For example, in addition to the UK, the countries of Germany, Austria, Poland and Italy have a well-developed and extensive network of hospices. The model for mobile teams or hospital support teams has been adopted in a number of countries, most notably in France. Day Centres are a development that is characteristic of the UK with hundreds of these services currently in operation. The number of beds per million inhabitants ranges between 45--75 beds in the most advanced European countries, to only a few beds in others. The countries with the highest development of palliative care in their respective subregions as measured in terms of ratio of services per one million inhabitants are: Western Europe -- UK (15); Central and Eastern Europe -- Poland (9); Commonwealth of Independent States -- Armenia (8). To date, worldwide, this is the only professional PC organisation to have done so (Federatie Palliatieve Zorg Vlaanderen 2003; #CITATION_TAG).","Four studies, each with different working methods, made up the study protocol: a literature review, a review of all the existing palliative care directories in Europe, a qualitative `Eurobarometer' survey and a quantitative `Facts Questionnaire' survey.","['The objective of the Task Force is to assess the degree of palliative care development in the European Region as defined by the World Health Organization (WHO).', 'The Task Force is a collaboration between EAPC, the International Observatory on End of Life Care, Help the Hospices and the International Association for Hospice and Palliative Care.', 'The work of the Task Force covers the entire WHO European Region of 52 countries.', 'In this article we present a comparative study on the development of palliative care in Europe, drawing on all four elements of the study.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Physicians who deny the existence of a transcendent power and hardly attend religious services are more likely to approve of euthanasia even in the case of minors or demented patients. This was not at all so in Belgium: An initial major motive for the introduction of palliative care was also to promote the acceptability of euthanasia (Bernheim 1990 (#CITATION_TAG Distelmans 2010).","Materials and Methods: An anonymous self administered questionnaire approved by Flemish Palliative Care Federation and its ethics steering group was sent to all physicians(n-147) working in Flemish Palliative Care. Questionnaire consisted of three parts. In first part responded were requested to provide demographic information. In second part the respondents were asked to provide information concerning their religion or world view through several questions enquiring after religious or ideological affiliation, religious or ideological self-definition, view on life after death, image of God, spirituality, importance of rituals in their life, religious practice, and importance of religion in life. The third part consisted of a list of attitudinal statements regarding different treatment decisions in advanced disease on which the respondents had to give their opinion using a five-point Likert scale.99 physician responded.",['Aims: To Study the religious and ideological views and practice of Palliative Care physician towards Euthanasia.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. This book is a successor to J Griffiths, A Bood and H Weyers, Euthanasia and Law in the Netherlands (Amsterdam University Press 1998) which was widely praised for its thoroughness, clarity, and accuracy. Does the Dutch experience with legalised euthanasia support the idea of a 'slippery slope' toward a situation in which life-especially of the more vulnerable members of society-is less effectively protected? Is it possible to explain and to predict when a society will decide to legalise euthanasia? Similarly, illegal clandestine end-of-life practices, performed without peer control, as documented in Belgium before the euthanasia law and elsewhere (Kuhse et al. 1997; Deliens et al. 2000; #CITATION_TAG) can be considered more worrying than even imperfectly regulated legal euthanasia.","The new book emphasises recent legal developments and new research, and has been expanded to include a full treatment of Belgium, where since 2002 euthanasia has also become legal. The book also includes descriptions written by local specialists of the legal situation and what is known about actual practice in a number of other European countries (England and Wales, France, Italy, Scandinavia, Spain, Switzerland). It covers in detail: - the substantive law applicable to euthanasia, physician-assisted suicide, withholding and withdrawing treatment, use of pain relief in potentially lethal doses, palliative and terminal sedation, and termination of life without a request (in particular in the case of newborn babies); -the process of legal development that has led to the current state of the law; -the system of legal control and its operation in practice; -the results of empirical research concerning actual medical practice. A concluding part deals with some general questions that arise out of the material presented: Is the legalisation of euthanasia an example of the decline of law or should it, on the contrary, be seen as part and parcel of the increasing juridification of the doctor-patient relationship?",['The book strives for as complete and dispassionate a description of the situation as possible.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Public and healthcare professionals differ in their attitudes towards euthanasia and physician-assisted suicide (PAS), the legal status of which is currently in the spotlight in the UK. The unexpected direction of association between religiosity and attitudes may reflect a broader cultural shift in attitudes since earlier research in this area. The palliative care budget has continued to rise annually by 10 percent, and Belgian palliative care, according to European Association for Palliative Care (EAPC) indicators (#CITATION_TAG), is on a par with the reference United Kingdom.","One hundred and fifty-one undergraduate students (early-stage nursing training, late-stage nursing training and non-nursing controls) were approached on a UK university campus and asked to complete a self-report questionnaire. Participants were of mixed gender and were on average 25.5 years old.",['Previous research tends toward basic designs reporting on attitudes in the context of just one or two potentially influencing factors; we aimed to test the comparative importance of a larger range of variables in a sample of nursing trainees and non-nursing controls.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. We study the graviton propagator in Euclidean loop quantum gravity. Some took issue with the data and pragmatic considerations, arguing that the legal safeguards against abuse are insufficiently enforced (Materstvedt 2009; #CITATION_TAG; Cohen-Almagor 2013), and others aimed to refute the conclusions on fundamental ideological grounds (Jaspers, Müller-Busch, and Nauck 2009; Kettler and Nauck 2010; Johnstone 2012; Materstvedt 2012).","We use spin foam, boundary-amplitude, and group-field-theory techniques. We compute a component of the propagator to first order, under some approximations, obtaining the correct large-distance behavior.",['This indicates a way for deriving conventional spacetime quantities from a background-independent theory.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Unreported cases were generally dealt with less carefully than reported cases: a written request for euthanasia was more often absent (87.7% v 17.6% verbal request only; P<0.001), other physicians and caregivers specialised in palliative care were consulted less often (54.6% v 97.5%; 33.0% v 63.9%; P<0.001 for both), the life ending act was more often performed with opioids or sedatives (92.1% v 4.4%; P<0.001), and the drugs were more often administered by a nurse (41.3% v 0.0%; P<0.001).One out of two euthanasia cases is reported to the Federal Control and Evaluation Committee. Most non-reporting physicians do not perceive their act as euthanasia. Countries debating legalisation of euthanasia should simultaneously consider developing a policy facilitating the due care and reporting obligations of physicians. In Flanders in 2007, slightly more than half the estimated total number of euthanasia cases (as according to the technical definition of the death certificate studies) were reported (#CITATION_TAG).","Setting Flanders, Belgium.A stratified at random sample was drawn of people who died between 1 June 2007 and 30 November 2007.",['To estimate the rate of reporting of euthanasia cases to the Federal Control and Evaluation Committee and to compare the characteristics of reported and unreported cases of euthanasia.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Death is often preceded by medical decisions that potentially shorten life (end-of-life decisions [ELDs]), for example, the decision to withhold or withdraw treatment. Almost all of the incompetent patients had previously stated that they wanted their family involved in case of incompetence, but half did not achieve this.In half of the cases, advanced lung cancer patients-or their families in cases of incompetence-were not involved in ELD making, despite the wishes of most of them. Physicians should openly discuss ELDs and involvement preferences with their advanced lung cancer patients.Copyright A(c) 2012 U.S. Cancer Pain Relief Committee. Some patients express wishes of abbreviation of suffering at the end of their life, but when the time comes, they do not want to be informed of the imminence of their death (Bernheim 1996 (Bernheim, 2001 #CITATION_TAG).","When the patient died, the specialist and general practitioner were asked to fill in a questionnaire.Eighty-five patients who died within 18 months of diagnosis were studied.",['Respect for patient autonomy requires physicians to involve their patients in this decision making.The objective of this study was to examine the involvement of advanced lung cancer patients and their families in ELD making and compare their actual involvement with their previously stated preferences for involvement.Patients with Stage IIIb/IV non-small cell lung cancer were recruited by physicians in 13 hospitals and regularly interviewed between diagnosis and death.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Design Two year nationwide retrospective study, 2005-6 (SENTI-MELC study).Data collection via the sentinel network of general practitioners, an epidemiological surveillance system representative of all general practitioners in Belgium.1690 non-sudden deaths in practices of the sentinel general practitioners.Non-sudden deaths of patients (aged >1 year) reported each week. To a large extent receiving spiritual care was associated with higher frequencies of euthanasia or physician assisted suicide than receiving little spiritual care (18.5, 2.0 to 172.7).End of life decisions that shorten life, including euthanasia or physician assisted suicide, are not related to a lower use of palliative care in Belgium and often occur within the context of multidisciplinary care. The relationship between euthanasia and spiritual or existential caregiving to the patient has been examined in a study involving the last three months of life of patients of a representative panel of Belgian general practitioners (#CITATION_TAG).","Multivariable regression analysis controlled for age, sex, cause, and place of death.Use of specialist multidisciplinary palliative care services was associated with intensified alleviation of symptoms (odds ratio 2.1, 95% confidence interval 1.6 to 2.6), continuous deep sedation forgoing food/fluid (2.9, 1.7 to 4.9), and the total of decisions explicitly intended to shorten life (1.5, 1.1 to 2.1) but not with euthanasia or physician assisted suicide in particular.",['To explore the relation between the care provided in the final three months of life and the prevalence and types of end of life decisions in Belgium.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. The Netherlands, Belgium, and Luxembourg have adopted laws decriminalizing euthanasia under strict conditions of prudent practice. These laws stipulate, among other things, that the attending physician should consult an independent colleague to judge whether the substantive criteria of due care have been met. articles of bye-laws, inventories of activities, training books, consultation protocols).In both countries, the consultation services are delivered by trained physicians who can be consulted in cases of a request for euthanasia and who offer support and information to attending physicians. The context in which the two organisations were founded, as well as the way they are organised and regulated, is different in each country. In countries where legalising physician-assisted death is being contemplated, the development of such a consultation provision could also be considered in order to safeguard the practice of euthanasia (as it can provide safeguards to adequate performance of euthanasia and assisted suicide). In both the French-speaking Belgium Forum EOL (End of Life) and the Netherlands SCEN (Steun en Consultatie voor Euthanasie Nederland) doctors are available for this service (#CITATION_TAG, 2009b.","However, SCEN-training puts more emphasis on the consultation report, whereas LEIF-training primarily emphasizes the ethical framework of end-of-life decisions.In case of a request for euthanasia, in the Netherlands as well as in Belgium similar consultation services by independent qualified physicians have been developed.","['In this context initiatives were taken in the Netherlands and Belgium to establish specialized services providing such consultants: Support and Consultation for Euthanasia in the Netherlands (SCEN) and Life End Information Forum (LEIF) in Belgium.', 'The aim of this study is to describe and compare these initiatives.We studied and compared relevant documents concerning the Dutch and Belgian consultation service (e.g.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. However, the most popular of these, the Barrett-Crane model, does not have the good boundary state space and there are indications that it fails to yield good low-energy n-point functions. Some patients express wishes of abbreviation of suffering at the end of their life, but when the time comes, they do not want to be informed of the imminence of their death (#CITATION_TAG (Bernheim, 2001 Pardon et al. 2012a).","We present an alternative dynamics that can be derived as a quantization of a Regge discretization of Euclidean general relativity, where second class constraints are imposed weakly. Its state space matches the SO(3) loop gravity one and it yields an SO(4)-covariant vertex amplitude for Euclidean loop gravity.",['Spin foam models are hoped to provide the dynamics of loop-quantum gravity.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. In recent years, palliative care and related organizations have increasingly adopted a stance of ""studied neutrality"" on the question of whether euthanasia should be legalized as a bona fide medical regimen in palliative care contexts. This stance, however, has attracted criticism from both opponents and proponents of euthanasia. Pro-euthanasia activists see the stance as an official position of indecision that is fundamentally disrespectful of a patient's right to ""choose death"" when life has become unbearable. Some palliative care constituents, in turn, are opposed to the stance, contending that it reflects an attitude of ""going soft"" on euthanasia and as weakening the political resistance that has hitherto been successful in preventing euthanasia from becoming more widely legalized. It is argued that although palliative care and related organizations have an obvious stake in the outcome of the euthanasia debate, it is neither unreasonable nor inconsistent for such organizations to be unwilling to take a definitive stance on the issue. Some took issue with the data and pragmatic considerations, arguing that the legal safeguards against abuse are insufficiently enforced (Materstvedt 2009; Pereira 2011; Cohen-Almagor 2013), and others aimed to refute the conclusions on fundamental ideological grounds (Jaspers, Müller-Busch, and Nauck 2009; Kettler and Nauck 2010; #CITATION_TAG; Materstvedt 2012).","It is further contended that, given the long-standing tenets of palliative care, palliative care organizations have both a right and a responsibility to defend the integrity of the principles and practice of palliative care and to resist demands for euthanasia to be positioned either as an integral part or logical extension of palliative care.Copyright (c) 2012 U.S. Cancer Pain Relief Committee.","['In this article, attention is given to examining critically the notion and possible unintended consequences of adopting a stance of studied neutrality on euthanasia in palliative care.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. In loop quantum gravity we now have a clear picture of the quantum geometry of space, thanks in part to the theory of spin networks. In general, a spin network is a graph with edges labeled by represen- tations and vertices labeled by intertwining operators. In a 'spin foam model' we describe states as linear combina- tions of spin networks and compute transition amplitudes as sums over spin foams. A reasonable hypothesis is that actual practices are not so different and that these are countries where the culture is more conducive to using the doctrine of double effect #CITATION_TAG","Similarly, a spin foam is a 2-dimensional complex with faces labeled by representations and edges labeled by intertwining operators.","[""The concept of 'spin foam' is intended to serve as a similar picture for the quantum geometry of spacetime."", 'This paper aims to provide a self-contained introduction to spin foam models of quantum gravity and a simpler field theory called BF theory.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Background: Legalizing euthanasia or physician-assisted suicide (PAS) is a current topic of debate in many countries. The Netherlands is the only country where legislation covers both. Two thirds of physicians thought that PAS underlines the autonomy and responsibility of the patient and considered this a reason to choose PAS. Patients receiving PAS more often experienced psychosocial suffering in comparison with patients receiving euthanasia. In vignettes of patients with a request for assistance in dying due to psychosocial suffering, physicians agreed more often with the performance of PAS than with euthanasia. Although they believe PAS underlines patient autonomy and responsibility, the option of PAS is rarely discussed with the patient. The more psychosocial in nature the patient's suffering, the more physicians choose PAS. Paradoxically, the choice for PAS is predominantly a physician's one. However, though 34 percent of Dutch general practitioners prefer assisted suicide, only 22 percent offer the two options to the patients requesting assisted dying (#CITATION_TAG).","Methods: A questionnaire including vignettes was sent to a random sample of 1955 Dutch general practitioners, elderly care physicians and medical specialists.","[""Objectives: To study physicians' experiences and attitudes concerning the choice between euthanasia and PAS.""]"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. There is increasing recognition that the development of evidence-informed health policy is not only a technical problem of knowledge exchange or translation, but also a political challenge. This said, the ""translation"" of scientific evidence into health policy is a complex process that is subject to inertia and cultural impediments (#CITATION_TAG).","Yet, while political scientists have long considered the nature of political systems, the role of institutional structures, and the political contestation of policy issues as central to understanding policy decisions, these issues remain largely unexplored by scholars of evidence-informed policy making.We conducted a systematic review of empirical studies that examined the influence of key features of political systems and institutional mechanisms on evidence use, and contextual factors that may contribute to the politicisation of health evidence. Eligible studies were identified through searches of seven health and social sciences databases, websites of relevant organisations, the British Library database, and manual searches of academic journals. Relevant political and institutional aspects affecting the use of health evidence included the level of state centralisation and democratisation, the influence of external donors and organisations, the organisation and function of bureaucracies, and the framing of evidence in relation to social norms and values.","['However, our understanding of such influences remains piecemeal given the limited number of empirical analyses on this subject, the paucity of comparative works, and the limited consideration of political and institutional theory in these studies.This review highlights the need for a more explicit engagement with the political and institutional factors affecting the use of health evidence in decision-making.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. These arguments have been described extensively in Kimsma and Van Leeuwen (Asking to die. Inside the Dutch debate about euthanasia, Kluwer Academic Publishers, Dordrecht, 1998). This experience is confirmed by a Dutch qualitative study of the psychological and philosophical aspects of euthanasia requests (#CITATION_TAG).","After some general introductory descriptions, by way of formulating a frame of reference, I shall describe the effects of this practice on patients, physicians and families, followed by a more philosophical reflection on the significance of these effects for the assessment of the authenticity of a request and the nature of unbearable suffering, two key concepts in the procedure towards euthanasia or physician-assisted suicide.","['In this article I intend to describe an issue of the Dutch euthanasia practice that is not common knowledge.', 'This article does not focus on the arguments for or against euthanasia and the ethical justification of physician-assisted dying.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Patients and physicians seemed to agree about this. In cases in which patients said they suffered unbearably there was less agreement about what constitutes unbearable suffering; patients put more emphasis on psychosocial suffering, such as dependence and deterioration, whereas physicians referred more often to physical suffering. In some cases the physician thought that the suffering was not unbearable because the patient's behaviour seemed incompatible with unbearable suffering-for instance, because the patient was still reading books. Conclusions Patients do not always think that their suffering is unbearable, even if they have a lasting wish to die. Physicians seem to have a narrower perspective on unbearable suffering than patients and than case law suggests. Dutch qualitative research has shown that some people who considered ""their life completed"" and requested euthanasia did not themselves call their suffering ""unbearable"" (e.g., they could still read a book or watch television), yet did not want to go on living in their condition (#CITATION_TAG).","Design In-depth interviews with a topic list. Setting Patients' homes and physicians' offices. Participants 10 patients who explicitly requested euthanasia but whose request was not granted or performed and eight physicians of these patients; and eight physicians of patients who had requested euthanasia but had died before the request had been granted or performed or had died after the request was refused by the physician or after the patient had withdrawn his or her request. In an attempt to solve the problem of different perspectives, physicians should take into account the different aspects of suffering as described in the literature and a framework for assessing the suffering of patients who ask for euthanasia.",['To obtain in-depth information about the views of patients and physicians on suffering in patients who requested euthanasia in whom the request was not granted or granted but not performed.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Since its first publication in 2003, the World Health Organization's ""Safe abortion: technical and policy guidance for health systems"" has had an influence on abortion policy, law, and practice worldwide. To reflect significant developments in the clinical, service delivery, and human rights aspects of abortion care, the Guidance was updated in 2012. These themes not only connect the chapters into a coherent whole. They reflect the research and advocacy efforts of a growing field in women's health and human rights.Copyright (c) 2012 International Federation of Gynecology and Obstetrics. FIGO now states ""neither society, nor members of the health care team responsible for counseling women, have the right to impose their religious or cultural convictions regarding abortion on those whose attitudes are different"" (FIGO 2013, 104) and concludes that ""the Committee recommend[s] that after appropriate counselling, a woman [has] the right to have access to medical or surgical induced abortion, and that the health care service [has] an obligation to provide such services as safely as possible"" (FIGO 2013, 105; #CITATION_TAG). But also here, in addition to philosophical motives, pragmatic motives have been operating: Another important objective of FIGO was to lift abortion out of clandestine illegal activity, to let it be performed in medically correct conditions and so ensure that abortion is safe and accessible.",,"['This article reviews select recommendations of the updated Guidance, highlighting 3 key themes that run throughout its chapters: evidence-based practice and assessment, human rights standards, and a pragmatic orientation to safe and accessible abortion care.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. There has been much debate regarding the 'double-effect' of sedatives and analgesics administered at the end-of-life, and the possibility that health professionals using these drugs are performing 'slow euthanasia.' On the one hand analgesics and sedatives can do much to relieve suffering in the terminally ill. On the other hand, they can hasten death. According to a standard view, the administration of analgesics and sedatives amounts to euthanasia when the drugs are given with an intention to hasten death. Some were explicit in describing a 'grey' area between palliation and euthanasia, or a continuum between the two. The distinction between the intention of life-ending and compassionate intensification of symptom treatment is often blurred (#CITATION_TAG).","In this paper we report a small qualitative study based on interviews with 8 Australian general physicians regarding their understanding of intention in the context of questions about voluntary euthanasia, assisted suicide and particularly the use of analgesic and sedative infusions (including the possibility of voluntary or non-voluntary 'slow euthanasia').","[""A major theme was that 'slow euthanasia' may be more psychologically acceptable to doctors than active voluntary euthanasia by bolus injection, partly because the former would usually only result in a small loss of 'time' for patients already very close to death, but also because of the desirable ambiguities surrounding causation and intention when an infusion of analgesics and sedatives is used.""]"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. It is now well established that quantities such as energy dissipation, scalar dissipation and enstrophy possess huge fluctuations in turbulent flows, and that the fluctuations become increasingly stronger with increasing Reynolds number of the flow. The effects of this small-scale ""intermittenc"" on various aspects of reacting flows have not been addressed fully. The law provides a reassuring procedural framework that helps with observing the ethical requirements to respect patient autonomy, to act beneficently, and to do no harm (#CITATION_TAG; De Keyser 2003).","We also discuss the likelihood that large-amplitude events in a given class of shear flows are characteristic of that class, and that, plausible estimates of such quantities cannot be made, in general, on the hypothesis that large and small scales are independent.","['This paper draws brief attention to a few possible effects on reaction rates, flame extinction, flamelet approximation, conditional moment closure methods, and so forth, besides commenting on possible effects on the resolution requirements of direct numerical simulations of turbulence.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Although advance directives may seem useful instruments in decision-making regarding incompetent patients, their validity in cases of dementia has been a much debated subject and little is known about their effectiveness in practice. Insight into the experiences and wishes of people with dementia regarding advance directives is totally lacking in empirical research.Ethics and actual practice are two ""different worlds"" when it comes to approaching advance directives in cases of dementia. It is clear, however, that the use of advance directives in practice remains problematic, above all in cases of advance euthanasia directives, but to a lesser extent also when non-treatment directives are involved. is hotly debated (#CITATION_TAG).",,"[""This paper assesses the contribution of advance directives to decision-making in the care of people with dementia, with a special focus on non-treatment directives and directives for euthanasia.The relevant problems from the ethical debate on advance directives in cases of dementia are summarized and we discuss how these relate to what is known from empirical research on the validity and effectiveness of advance directives in the clinical practice of dementia care.The ethical debate focuses essentially on how to respond to the current wishes of a patient with dementia if these contradict the patient's wishes contained in an advance directive.""]"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Following the 2002 enactment of the Belgian law on euthanasia, which requires the consultation of an independent second physician before proceeding with euthanasia, the Life End Information Forum (LEIF) was founded which provides specifically trained physicians who can act as mandatory consultants in euthanasia requests. They tended to more often discuss the request with the attending physician (100% vs 95%) and with the family (76% vs 69%), and also more frequently helped the attending physician with performing euthanasia (44% vs 24%). Irrespective of whether euthanasia is a legal practice within a country, similar services may prove useful to also improve quality of consultations in various other difficult end-of-life decision-making situations. The value of specific training for the quality of second physician consultation was recently documented (#CITATION_TAG).","This study assesses quality of consultations in Flanders and Brussels and compares these between LEIF and non-LEIF consultants.A questionnaire was sent in 2009 to a random sample of 3,006 physicians in Belgium from specialties likely involved in the care of dying patients. Several questions about the last euthanasia request of one of their patients were asked.",['As LEIF serves the Flemish speaking community (i.e.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. The European Association for Palliative Care Task (EAPC) Force on the Development of Palliative Care in Europe was created in 2003 and the results of its work are now being reported in full, both here and in several other publications. Different models of service delivery have been developed and implemented throughout the countries of Europe. For example, in addition to the UK, the countries of Germany, Austria, Poland and Italy have a well-developed and extensive network of hospices. The model for mobile teams or hospital support teams has been adopted in a number of countries, most notably in France. Day Centres are a development that is characteristic of the UK with hundreds of these services currently in operation. The number of beds per million inhabitants ranges between 45-75 beds in the most advanced European countries, to only a few beds in others. The countries with the highest development of palliative care in their respective subregions as measured in terms of ratio of services per one million inhabitants are: Western Europe - UK (15); Central and Eastern Europe - Poland (9); Commonwealth of Independent States - Armenia (8). MA: Why have the development of palliative care and the drive for legal euthanasia been synergistic only in Belgium? How did Belgian ""exceptionalism"" come to be? That Belgium is one of the countries with the most developed palliative care systems (Centeno et al. 2007; #CITATION_TAG;) and the third country to have legally regulated euthanasia 5 is not paradoxical, as we have tried to make clear above, but it may also not be accidental.","Four studies, each with different working methods, made up the study protocol: a literature review, a review of all the existing palliative care directories in Europe, a qualitative ;Eurobarometer' survey and a quantitative ;Facts Questionnaire' survey.","['The objective of the Task Force is to assess the degree of palliative care development in the European Region as defined by the World Health Organization (WHO).', 'The Task Force is a collaboration between EAPC, the International Observatory on End of Life Care, Help the Hospices and the International Association for Hospice and Palliative Care.', 'The work of the Task Force covers the entire WHO European Region of 52 countries.', 'In this article we present a comparative study on the development of palliative care in Europe, drawing on all four elements of the study.']"
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Physicians who deny the existence of a transcendent power and hardly attend religious services are more likely to approve of euthanasia even in the case of minors or demented patients. As in the United Kingdom (Seale 2010) and other countries (Bülow et al. 2012), in Belgium practicing religious palliative care physicians are more critical of euthanasia than others (#CITATION_TAG).","Questionnaire consisted of three parts. In first part responded were requested to provide demographic information. In second part the respondents were asked to provide information concerning their religion or world view through several questions enquiring after religious or ideological affiliation, religious or ideological self-definition, view on life after death, image of God, spirituality, importance of rituals in their life, religious practice, and importance of religion in life. The third part consisted of a list of attitudinal statements regarding different treatment decisions in advanced disease on which the respondents had to give their opinion using a five-point Likert scale.99 physician responded.WE WERE ABLE TO DISTINGUISH FOUR CLUSTERS: Church-going physicians, infrequently church-going physicians, atheists and doubters.",['To Study the religious and ideological views and practice of Palliative Care physician towards Euthanasia.An anonymous self administered questionnaire approved by Flemish Palliative Care Federation and its ethics steering group was sent to all physicians(n-147) working in Flemish Palliative Care.']
"This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Public and healthcare professionals differ in their attitudes towards euthanasia and physician-assisted suicide (PAS), the legal status of which is currently in the spotlight in the UK. The unexpected direction of association between religiosity and attitudes may reflect a broader cultural shift in attitudes since earlier research in this area. Perhaps surprisingly, in a survey of British students there was a significant positive correlation between religious belief and a positive attitude towards euthanasia, a finding suggesting a cultural shift to #CITATION_TAG.","One hundred and fifty-one undergraduate students (early-stage nursing training, late-stage nursing training and non-nursing controls) were approached on a UK university campus and asked to complete a self-report questionnaire. Participants were of mixed gender and were on average 25.5 years old.",['Previous research tends toward basic designs reporting on attitudes in the context of just one or two potentially influencing factors; we aimed to test the comparative importance of a larger range of variables in a sample of nursing trainees and non-nursing controls.']
"We study the graviton propagator in Euclidean loop quantum gravity. More recently, we have seen significant progress toward extraction of their semiclassical behavior and its favorable comparison to the expected weak field limit of gravity, starting with Rovelli and collaborators' calculation of the graviton propagator [30, #CITATION_TAG].","We use spin foam, boundary-amplitude, and group-field-theory techniques. We compute a component of the propagator to first order, under some approximations, obtaining the correct large-distance behavior.",['This indicates a way for deriving conventional spacetime quantities from a background-independent theory.']
"This is seen by the growing number of terminologies used to define subprojects concerning particular classes of bioactive carbohydrates. Sulfated fucans (SFs) and sulfated galactans (SGs) are relatively new classes of sulfated polysaccharides (SPs) that occur mostly in marine organisms, and exhibit a broad range of medicinal effects. Their structures are taxonomically dependent, and their therapeutic actions include benefits in inflammation, coagulation, thrombosis, angiogenesis, cancer, oxidation, and infections. Some red algae, marine angiosperm and invertebrates express SPs of unique structures composed of regular repeating oligomeric units of well-defined sulfation patterns. Seeing that, fucanomics and galactanomics may comprise distinguished glycomics subprojects. The evaluation of this vertex amplitude is discussed in several papers [8, 9, 16, #CITATION_TAG], where variations on the face and edge amplitudes have also been considered.",,['We hereby discuss the relevance that justifies the international recognition of these subprojects in the current glycomics age associated with the beneficial outcomes that these glycans may offer in drug development.']
"OBJECTIVE Small hyaluronan (HA) oligosaccharides serve as competitive receptor antagonists to displace HA from the cell surface and induce cell signaling events. In articular chondrocytes, this cell signaling is mediated by the HA receptor CD44 and induces stimulation of genes involved in matrix degradation, such as matrix metalloproteinases (MMPs) as well as matrix repair genes including type II collagen, aggrecan, and HA synthase 2. The overall spin network conventions and normalizations used in this paper are those of [#CITATION_TAG, 24].","METHODS Bovine articular chondrocytes or bovine cartilage tissue was pretreated with a variety of inhibitors of major signaling pathways prior to the addition of HA oligosaccharides. Changes in aggrecanase were monitored by real-time reverse transcription-polymerase chain reaction and Western blot analyses of ADAMTS-4, ADAMTS-5, and aggrecan proteolytic fragments. To test the interactions between ADAMTS-4 and membrane type 4 MMP (MT4-MMP), protein lysates purified from stimulated chondrocytes were subjected to coimmunoprecipitation. The association of glycosyl phosphatidylinositol-anchored MT4-MMP with ADAMTS-4 was also induced in articular chondrocytes by HA oligosaccharides. Inhibition of the NF-kB pathway blocked HA oligosaccharide-mediated stimulation of aggrecanases. CONCLUSION Disruptive changes in chondrocyte-matrix interactions by HA oligosaccharides induce matrix degradation and elevate aggrecanases via the activation of the NF-kB signaling pathway.",['The objective of this study was to determine changes in the expression and function of aggrecanases after disruption of chondrocyte CD44-HA interactions.']
"Lionfish are representative venomous fish, having venomous glandular tissues in dorsal, pelvic and anal spines. Some properties and primary structures of proteinaceous toxins from the venoms of three species of lionfish, Pterois antennata, Pterois lunulata and Pterois volitans, have so far been clarified. Nevertheless, the lionfish hyaluronidases as well as the stonefish hyaluronidases almost maintain structural features (active site, glyco_hydro_56 domain and cysteine location) observed in other hyaluronidases. The first proposal, by Engle, Pereira and Rovelli (EPR) [20, #CITATION_TAG], aimed also to identify the spin foam boundary state space with that of loop quantum gravity spin networks; this model is also referred to as the ""flipped"" vertex model.","The hyaluronidases of P. antennata and P. volitans were shown to be optimally active at pH 6.6, 37 degC and 0.1 M NaCl and specifically active against hyaluronan. The primary structures (483 amino acid residues) of the lionfish hyaluronidases were elucidated by a cDNA cloning strategy using degenerate primers designed from the reported amino acid sequences of the stonefish hyaluronidases.",['This prompted us to examine enzymatic properties and primary structures of lionfish hyaluronidases.']
"But a number of their features remain elusive. These difficulties can be traced to the SO(4) -> SU(2) gauge fixing and the way certain second class constraints are imposed, arguably incorrectly, strongly. The first proposal, by Engle, Pereira and Rovelli (EPR) [#CITATION_TAG, 21], aimed also to identify the spin foam boundary state space with that of loop quantum gravity spin networks; this model is also referred to as the ""flipped"" vertex model.","We present an alternative model, that can be derived as a bona fide quantization of a Regge discretization of euclidean general relativity, and where the constraints are imposed weakly. Its state space is a natural subspace of the SO(4) spin-network space and matches the SO(3) hamiltonian spin network space. The model provides a long sought SO(4)-covariant vertex amplitude for loop quantum gravity.Comment: 6page",['Spinfoam theories are hoped to provide the dynamics of non-perturbative loop quantum gravity.']
The connection between spin foams and gravity is motivated by results from loop quantum gravity [#CITATION_TAG].,"Furthermore, AOSC blocked the fibril formation of Abeta, which may be responsible for its anti-cytotoxic effects.","['In this paper, we investigated interactions of the acidic oligosaccharide sugar chain (AOSC), derived from brown algae Echlonia kurome OKAM, with amyloid beta protein (Abeta).']"
"A deeper exploration of the biodiversity richness and ecophysiological properties of microalgae is crucial for enhancing their use for applicative purposes. Moreover, for BF theory, quantization and discretization commute [#CITATION_TAG].","After describing the actual biotechnological use of microalgae, we consider the multiple faces of taxonomical, morphological, functional and ecophysiological biodiversity of these organisms, and investigate how these properties could better serve the biotechnological field. Lastly, we propose new approaches to enhancing microalgal growth, photosynthesis, and synthesis of valuable products used in biotechnological fields, mainly focusing on culture conditions, especially light manipulations and genetic modifications.","['In this review, we aim to explore the potential of microalgal biodiversity and ecology for biotechnological use.']"
"Marine microalgae have been used for a long time as food for humans, such as Arthrospira (formerly, Spirulina), and for animals in aquaculture. The biomass of these microalgae and the compounds they produce have been shown to possess several biological applications with numerous health benefits. Considering the four earthworm papers together [10][11][12]#CITATION_TAG, there seems to be a good basis for saying that the first benefit of transcription profiling, specificity, is real","It goes through the most studied activities of sulphated polysaccharides (sPS) or their derivatives, but also highlights lesser known applications as hypolipidaemic or hypoglycaemic, or as biolubricant agents and drag-reducers.","['The present review puts up-to-date the research on the biological activities and applications of polysaccharides, active biocompounds synthesized by marine unicellular algae, which are, most of the times, released into the surrounding medium (exo- or extracellular polysaccharides, EPS).', 'Therefore, the great potentials of sPS from marine microalgae to be used as nutraceuticals, therapeutic agents, cosmetics, or in other areas, such as engineering, are approached in this review.']"
"Chitin is one the most abundant polymers in nature and interacts with both carbon and nitrogen cycles. Processes controlling chitin degradation are summarized in reviews published some 20 years ago, but the recent use of culture-independent molecular methods has led to a revised understanding of the ecology and biochemistry of this process and the organisms involved. Considering the four earthworm papers together [10][11]#CITATION_TAG[13], there seems to be a good basis for saying that the first benefit of transcription profiling, specificity, is real",Principal environmental drivers of chitin degradation are identified which are likely to influence both community composition of chitin degrading bacteria and measured chitin hydrolysis activities.,['This review summarizes different mechanisms and the principal steps involved in chitin degradation at a molecular level while also discussing the coupling of community composition to measured chitin hydrolysis activities and substrate uptake.']
"A factor u of a word w is a cover of w if every position in w lies within some occurrence of u in w. A word w covered by u thus generalizes the idea of a repetition, that is, a word composed of exact concatenations of u. A factor u of a string y is a cover of y if every letter of y lies within some occurrence of u in y; thus every cover u is also a border--both prefix and suffix--of y. Covers and seeds are two formalisations of quasiperiodicity, and there exist linear-time algorithms for computing all the covers and seeds of y. A string y covered by u thus generalises the idea of a repetition; that is, a string composed of exact concatenations of u. Even though a string is coverable somewhat more frequently than it is a repetition, still a string that can be covered by a single u is rare. As a result, seeking to find a more generally applicable and descriptive notion of cover, many articles were written on the computation of a minimum k-cover of y; that is, the minimum cardinality set of strings of length k that collectively cover y. Unfortunately, this computation turns out to be NP-hard. We introduce the notion of partial covers, that is, factors covering at least a given number of positions in w. Recently, Flouri et al. [#CITATION_TAG] suggested a related notion of enhanced covers which are additionally required to be borders of the word.",If u is a cover of a superstring of y then u is a seed of y.,"['Therefore, in this article, we propose new, simple, easily-computed, and widely applicable notions of string covering that provide an intuitive and useful characterisation of a string: the enhanced cover; the enhanced left cover; and the enhanced left seed']"
"A factor u of a word w is a cover of w if every position in w lies within some occurrence of u in w. A word w covered by u thus generalizes the idea of a repetition, that is, a word composed of exact concatenations of u. Normal organogenesis requires co-ordinate development and interaction of multiple cell types, and is seemingly governed by tissue specific factors. Lymphoid organogenesis during embryonic life is dependent on molecules the temporal expression of which is tightly regulated. A subset of these cells expresses the receptor tyrosine kinase RET, which is essential for mammalian enteric nervous system formation. CST (w) is similar to the data structure named Minimal Augmented Suffix Tree (see [#CITATION_TAG, 5]).","During this process, haematopoietic 'inducer' cells interact with stromal 'organizer' cells, giving rise to the lymphoid organ primordia. Here we show that the haematopoietic cells in the gut exhibit a random pattern of motility before aggregation into the primordia of Peyer's patches, a major component of the gut-associated lymphoid tissue. To support this hypothesis, we show that the RET ligand ARTN is a strong attractant of gut haematopoietic cells, inducing the formation of ectopic Peyer's patch-like structures.","['Our work strongly suggests that the RET signalling pathway, by regulating the development of both the nervous and lymphoid system in the gut, has a key role in the molecular mechanisms that orchestrate intestine organogenesis.']"
"A factor u of a word w is a cover of w if every position in w lies within some occurrence of u in w. A word w covered by u thus generalizes the idea of a repetition, that is, a word composed of exact concatenations of u. Secondary lymphoid organs develop during embryogenesis or in the first few weeks after birth according to a highly coordinated series of interactions between newly emerging hematopoietic cells and immature mesenchymal or stromal cells. Lymphotoxin signaling also maintains the expression of adhesion molecules and chemokines that govern the ultimate structure and function of secondary lymphoid organs. Li & Smyth [#CITATION_TAG] provided a linear-time algorithm for computing the maximal cover array of a word, and showed that, analogous to the border array [8], it actually determines the structure of all the covers of every prefix of the word.","These interactions are orchestrated by homeostatic chemokines, cytokines, and growth factors that attract hematopoietic cells to sites of future lymphoid organ development and promote their survival and differentiation. In turn, lymphotoxin-expressing hematopoietic cells trigger the differentiation of stromal and endothelial cells that make up the scaffolding of secondary lymphoid organs.","['Here we describe the current paradigm of secondary lymphoid organ development and discuss the subtle differences in the timing, molecular interactions, and cell types involved in the development of each secondary lymphoid organ.']"
"A factor u of a word w is a cover of w if every position in w lies within some occurrence of u in w. A word w covered by u thus generalizes the idea of a repetition, that is, a word composed of exact concatenations of u. We consider the challenges of lack of data, incomplete knowledge and modelling in the context of a rapidly changing knowledge base. There is a mismatch in scale between these cellular models and tissue structures that are affected by tumours, and bridging this gap requires substantial computational resource. We present concurrent programming as a technology to link scales without losing important details through model simplification. Recall that the locus of a factor v of w, given by its start and end position in w, can be found in O(log log |v|) time [#CITATION_TAG].","Computer simulation can be used to inform in vivo and in vitro experimentation, enabling rapid, low-cost hypothesis generation and directing experimental design in order to test those hypotheses. Here, we outline a framework that supports developing simulations as scientific instruments, and we select cancer systems biology as an exemplar domain, with a particular focus on cellular signalling models. Our framework comprises a process to clearly separate scientific and engineering concerns in model and simulation development, and an argumentation approach to documenting models for rigorous way of recording assumptions and knowledge gaps.","['We propose interactive, dynamic visualisation tools to enable the biological community to interact with cellular signalling models directly for experimental design.']"
"The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [#CITATION_TAG].","The use of complex system simulation as a research tool is facilitated by principled development; software quality assurance, an important part of fitness for purpose, can be assisted by use of model driven engineering (MDE) techniques.","['The paper addresses two key aspects of MDE for simulation development: the choice of appropriate modelling languages, and language adaptation, illustrated from a cell division and differentiation simulation development, for use in research on prostate conditions.']"
"The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . In modeling turbulent reactive flows based on the transport equation for the joint probability density function (jpdf) of velocity and composition, the change in fluid composition due to convection and reaction is treated exactly, while molecular mixing has to be modeled. In this model the change in particle composition is determined by particle interactions along the edges of a Euclidean minimum spanning tree (EMST) constructed in composition space. One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [#CITATION_TAG].",The model is applied to the diffusion flame test model problem proposed by Norris and Pope [Combust.,"['A new mixing model is proposed, which is local in composition space and which seeks to address problems encountered in flows with simultaneous mixing and reaction.']"
"The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . They are a valuable complement and precursor to simulation specifications and implementations, focusing purely on thoroughly exploring the biology, recording hypotheses and assumptions, and serve as a communication medium detailing exactly how a simulation relates to the real biology. For example, the thinning method [#CITATION_TAG] that is used to sample ODT map occurrences will likewise be advantageous for sampling swap occurrences.","The framework comprises three levels of modelling, ranging in scope from the dynamics of individual model entities to system-level emergent properties. By way of an immunological case study of the mouse disease experimental autoimmune encephalomyelitis, we show how the framework can be used to produce models that capture and communicate the biological system, detailing how biological entities, interactions and behaviours lead to higher-level emergent properties observed in the real world. We show how specialized, well-explained diagrams with less formal semantics can be used where no suitable UML formalism exists. We highlight UML's lack of expressive ability concerning cyclic feedbacks in cellular networks, and the compounding concurrency arising from huge numbers of stochastic, interacting agents. To compensate for this, we propose several additional relationships for expressing these concepts in UML's activity diagram. We also demonstrate the ambiguous nature of class diagrams when applied to complex biology, and question their utility in modelling such dynamic systems. Models created through our framework are non-executable, and expressly free of simulation implementation concerns.",['We present a framework to assist the diagrammatic modelling of complex biological systems using the unified modelling language (UML).']
"The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . Considerable research effort has provided mathematical and computational models of the human immune response under viral infection. However, the quality of simulated results are highly dependent on the choice of modeling strategy. Both approaches are however, not directly accessible to immunologists due to the need for programming knowledge; hence, closer collaboration between computer scientists and immunologists is necessary. Effects of inertial-range phenomenology, especially small-scale intermittency, on turbulent reacting flows have been addressed in numerous modeling studies, and opportunities for further progress have been noted [#CITATION_TAG, 25].","The latter has relatively wider Model Scope due to the agent-rule specification method. Mathematical Models employ Parameter and Population/Subpopulation Level entity granularities with equation-based interaction, while MA Models specify entities at Individual Level, implemented with agents to describe interactions via IF-THEN rules. Compared to the former, MA Models naturally handles entity heterogeneity and spatial non-uniformity, and suffers less from the issue of directly designed dynamics.",['We examine two modeling approaches of HIV pathogenesis: Mathematical and Multi-Agent (or MA) Models.']
"The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . THE AREA UNDER CULTIVATION IN SOUTH Africa more than tripled during the twentieth century, while plantation area increased more than tenfold. Both domestic and global population growth partly underlie the increased demand for crop products over the past century. Increased production was initially achieved mainly by expanding the area under cultivation, and, from the 1960s onwards, principally through enhanced yields per hectare. In the latter period, nationally averaged productivity in a given year was related to fertilizer use, irrigation and the proportion of the country experiencing dry conditions. Tree geometry has been used for reduced modeling of turbulence intermittency [2, 4, 5, 14, 22], in some instances formulated as generalizations of the zero-dimensional shell models [6, #CITATION_TAG] in which the scale space of turbulence is parameterized by wavenumber modulus.","Independent estimates of historical cultivated area at the national level were derived from estimates of production and productivity per hectare, presenting a method that could be used to obtain improved historical land-cover estimates in data-poor countries.","['This paper describes the changes that have occurred in relation to the production and yields per hectare of major crops (maize, wheat, sorghum and sugar cane) and discusses the factors that contributed to the changes.']"
"The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . The dynamics of fully developed hydrodynamic turbulence still is a basically unsolved theoretical problem, due to the strong-coupling long-range nonlinearities in the Navier-Stokes equations. Averaging the response spectrum over all possible orientational configurations and sweep velocities results in a novel self-consistency integral for the 4D energy spectrum function. These two features are manifestations of a fundamental cascade property, the so-called sweeping of the small scales by the large scales [#CITATION_TAG].","After taking a (2+1)D spatiotemporal spectral transform of the fluctuating vorticity fields, care is taken of large-scale sweeping which arises as a collective zero mode from the nonlinear flow terms. The ""unswept"" small-scale nonlinearities are then shown to be asymptotically locally isotropic (i.e., for wave numbers k-[?]) by internal consistency, which allows to close the nonlinear hierarchy. The Navier-Stokes equations (without external forcing) are integrated to give the spectral response of the fluctuating small-scale velocity fields on the presence of a locally isotropic blob of turbulence while it is being swept around over an arbitrary steady state mean velocity profile, using viscous boundary conditions at y=0. The distribution of turbulence sweep velocities is modeled by means of Levy-type densities, having an algebraic tail with power p>1. The generic case (which includes Von Karman's logarithmic mean velocity profile) is found to correspond to 1<p<3. Asymptotic analysis of the self-consistency integral leads to a differential equation which fixes the scaling exponent l of the unswept frequency D and admits a nonempty, integrable and positive definite Airy-type frequency spectrum E(i)(k,D/k(l))~k(m) with so-called ""normal"" Kolmogorov scaling, that is, m=-7/3 and l=2/3. Anomalous scaling is possible for one special mean profile.",['The present analysis focuses on the small-scale fluctuations in a turbulent boundary layer with one external length scale y(o).']
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Case-based reasoning is a recent approach to problem solving and learning that has got a lot of attention over the last few years. Originating in the US, the basic idea and underlying theories have spread to other continents, and we are now within a period of highly active research in case-based reasoning in Europe, as well. Others stressed on the anti-UV radiation potential of both alginate-derived oligosaccharides and chito-oligosaccharides and discussed the potential for development of UV radiation protector agent in the area of functional foods (#CITATION_TAG).","Initially, a general framework is defined, to which the subsequent descriptions and discussions will refer. The framework is influenced by recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval, reuse, solution testing, and learning are summarized, and their actual realization is discussed in the light of a few example systems that represent different CBR approaches. We also discuss the role of case-based methods as one type of reasoning and learning method within an integrated system architecture.","['This paper gives an overview of the foundational issues related to case-based reasoning, describes some of the leading methodological approaches within the field, and exemplifies the current state through pointers to some systems.']"
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. There has been significant recent interest in the commercial utilisation of algae based on their valuable chemical constituents many of which exhibit multiple bioactivities with applications in the food, cosmetic, agri- and horticultural sectors and in human health. Compounds of particular commercial interest include pigments, lipids and fatty acids, proteins, polysaccharides and phenolics which all display considerable diversity between and within taxa. The chemical composition of natural algal populations is further influenced by spatial and temporal changes in environmental parameters including light, temperature, nutrients and salinity, as well as biotic interactions. As reported bioactivities are closely linked to specific compounds it is important to understand, and be able to quantify, existing chemical diversity and variability. The high biodiversity of the latter and how they can serve the biotechnological field has been recently pointed out (#CITATION_TAG; Barra et al., 2014).",,"['This review outlines the taxonomic, ecological and chemical diversity between, and within, different algal groups and the implications for commercial utilisation of algae from natural populations.', 'The biochemical diversity and complexity of commercially important types of compounds and their environmental and developmental control are addressed.']"
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Web services technology has generated a lot interest, but its adoption rate has been slow. In an attempt to obtain low molecular-weight derivatives from sulfated fuco-oligosaccharides, results based on mild acid hydrolysis were also reported (#CITATION_TAG).",quality of services) are taken into account for the service discovery.,"['This paper discusses issues related to this slow take up and argues that quality of services is one of the contributing factors.', 'The paper proposes a new Web services discovery model in which the functional and non-functional requirements (i.e.', 'The proposed model should give Web services consumers some confidence about the quality of service of the discovered Web services.']"
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. This is seen by the growing number of terminologies used to define subprojects concerning particular classes of bioactive carbohydrates. Sulfated fucans (SFs) and sulfated galactans (SGs) are relatively new classes of sulfated polysaccharides (SPs) that occur mostly in marine organisms, and exhibit a broad range of medicinal effects. Their structures are taxonomically dependent, and their therapeutic actions include benefits in inflammation, coagulation, thrombosis, angiogenesis, cancer, oxidation, and infections. Some red algae, marine angiosperm and invertebrates express SPs of unique structures composed of regular repeating oligomeric units of well-defined sulfation patterns. Seeing that, fucanomics and galactanomics may comprise distinguished glycomics subprojects. In the current -omic age the proposal of disseminating fucanomics and galactanomics, related to interesting sulfated fucans and sulfated galactan of marine origin was advanced (#CITATION_TAG) in accord to an expected increase of research in projects of such nature.",,['We hereby discuss the relevance that justifies the international recognition of these subprojects in the current glycomics age associated with the beneficial outcomes that these glycans may offer in drug development.']
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Small hyaluronan (HA) oligosaccharides serve as competitive receptor antagonists to displace HA from the cell surface and induce cell signaling events. In articular chondrocytes, this cell signaling is mediated by the HA receptor CD44 and induces stimulation of genes involved in matrix degradation, such as matrix metalloproteinases (MMPs) as well as matrix repair genes including type II collagen, aggrecan, and HA synthase 2. Inhibition of the NF-kB pathway blocked HA oligosaccharide-mediated stimulation of aggrecanases.Disruptive changes in chondrocyte-matrix interactions by HA oligosaccharides induce matrix degradation and elevate aggrecanases via the activation of the NF-kB signaling pathway.Copyright (c) 2012 by the American College of Rheumatology. Natural quick action characterizing these biocatalysts is a very interesting feature for biocatalytic manipulation of these carbohydrate-related molecules (Madokoro et al., 2011; Kiriake et al., 2014) also in view of bioactivity expressed by the polymer and HA oligosaccharides (#CITATION_TAG).","Changes in aggrecanase were monitored by real-time reverse transcription-polymerase chain reaction and Western blot analyses of ADAMTS-4, ADAMTS-5, and aggrecan proteolytic fragments. To test the interactions between ADAMTS-4 and membrane type 4 MMP (MT4-MMP), protein lysates purified from stimulated chondrocytes were subjected to coimmunoprecipitation.Disruption of chondrocyte CD44-HA interactions with HA oligosaccharides induced the transcription of ADAMTS-4 and ADAMTS-5 in a time- and dose-dependent manner. The association of glycosyl phosphatidylinositol-anchored MT4-MMP with ADAMTS-4 was also induced in articular chondrocytes by HA oligosaccharides.",['The objective of this study was to determine changes in the expression and function of aggrecanases after disruption of chondrocyte CD44-HA interactions.Bovine articular chondrocytes or bovine cartilage tissue was pretreated with a variety of inhibitors of major signaling pathways prior to the addition of HA oligosaccharides.']
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. The popularity of service oriented computing (SOC) brings a large number of distributed, well-encapsulated and reusable services all over Internet, and makes it possible to create value-added services by means of service composition. Current composition styles are too professional to those end users when building their own applications. Intriguingly, the results showed that these oligosaccharides exhibited different activities in various assays (#CITATION_TAG) in relation to their structures.","In our approach, similar candidate services are aggregated together as a unified resource, whose wide QoS spectrum can be easily manipulated by the end users to satisfy their requirements. Then they can personalize the services and, the composition occurs only at the presentation layer. The main contributions of the approach are: (i) enabling the end users to personalize the composite application with more powerful presentation; (ii) supporting the end users to dynamically customize the service composition in terms of QoS; (iii) alleviating the end users from the time-consuming task of selecting service to compose.",['We propose an end user service composition approach for reducing the composition complexity and difficulty from the end user perspective.']
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Lionfish are representative venomous fish, having venomous glandular tissues in dorsal, pelvic and anal spines. Some properties and primary structures of proteinaceous toxins from the venoms of three species of lionfish, Pterois antennata, Pterois lunulata and Pterois volitans, have so far been clarified. Nevertheless, the lionfish hyaluronidases as well as the stonefish hyaluronidases almost maintain structural features (active site, glyco_hydro_56 domain and cysteine location) observed in other hyaluronidases. Natural quick action characterizing these biocatalysts is a very interesting feature for biocatalytic manipulation of these carbohydrate-related molecules (Madokoro et al., 2011; #CITATION_TAG) also in view of bioactivity expressed by the polymer and HA oligosaccharides (Ariyoshi et al., 2012).","The hyaluronidases of P. antennata and P. volitans were shown to be optimally active at pH 6.6, 37degC and 0.1 M NaCl and specifically active against hyaluronan. The primary structures (483 amino acid residues) of the lionfish hyaluronidases were elucidated by a cDNA cloning strategy using degenerate primers designed from the reported amino acid sequences of the stonefish hyaluronidases.",['This prompted us to examine enzymatic properties and primary structures of lionfish hyaluronidases.']
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Computer science as an engineering discipline has been spectacularly successful. Yet it is also a philosophical enterprise in the way it represents the world and creates and manipulates models of reality, people, and action. The phenomenological tradition emphasizes the primacy of natural practice over abstract cognition in everyday activity. The attracting interest in developing potential drugs for chronic diseases has been evidenced in a recent review on biofunction of marine oligosaccharides (#CITATION_TAG).",,"['In this book, Paul Dourish addresses the philosophical bases of human-computer interaction.', 'He looks at how what he calls ""embodied interaction"" -- an approach to interacting with software systems that emphasizes skilled, engaged practice rather than disembodied rationality -- reflects the phenomenological approaches of Martin Heidegger, Ludwig Wittgenstein, and other twentieth-century philosophers.', 'He looks in particular at how tangible and social approaches to interaction are related, how they can be used to analyze and understand embodied interaction, and how they could affect the design of future interactive systems.']"
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Alginate-derived oligosaccharides of 373-571 Da and chitooligosaccharides of 855-1671 Da obtained by enzymolysis with alginate lyase and chitosanase respectively, were investigated for cell regulation, erythrocytes haemolysis inhibition and antioxidant capacity (#CITATION_TAG).","Furthermore, AOSC blocked the fibril formation of Abeta, which may be responsible for its anti-cytotoxic effects.","['In this paper, we investigated interactions of the acidic oligosaccharide sugar chain (AOSC), derived from brown algae Echlonia kurome OKAM, with amyloid beta protein (Abeta).']"
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. A deeper exploration of the biodiversity richness and ecophysiological properties of microalgae is crucial for enhancing their use for applicative purposes. The high biodiversity of the latter and how they can serve the biotechnological field has been recently pointed out (Stengel et al., 2011; #CITATION_TAG).","After describing the actual biotechnological use of microalgae, we consider the multiple faces of taxonomical, morphological, functional and ecophysiological biodiversity of these organisms, and investigate how these properties could better serve the biotechnological field. Lastly, we propose new approaches to enhancing microalgal growth, photosynthesis, and synthesis of valuable products used in biotechnological fields, mainly focusing on culture conditions, especially light manipulations and genetic modifications.","['In this review, we aim to explore the potential of microalgal biodiversity and ecology for biotechnological use.']"
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Marine microalgae have been used for a long time as food for humans, such as Arthrospira (formerly, Spirulina), and for animals in aquaculture. The biomass of these microalgae and the compounds they produce have been shown to possess several biological applications with numerous health benefits. Scientific knowledge present in literature for each of these sources seems to differ, from the well-known seaweed (Rinaudo, 2007) to relatively new sources of polysaccharides such as extremophiles and microalgae (#CITATION_TAG).","It goes through the most studied activities of sulphated polysaccharides (sPS) or their derivatives, but also highlights lesser known applications as hypolipidaemic or hypoglycaemic, or as biolubricant agents and drag-reducers.","['The present review puts up-to-date the research on the biological activities and applications of polysaccharides, active biocompounds synthesized by marine unicellular algae, which are, most of the times, released into the surrounding medium (exo- or extracellular polysaccharides, EPS).', 'Therefore, the great potentials of sPS from marine microalgae to be used as nutraceuticals, therapeutic agents, cosmetics, or in other areas, such as engineering, are approached in this review']"
"It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Chitin is one the most abundant polymers in nature and interacts with both carbon and nitrogen cycles. Processes controlling chitin degradation are summarized in reviews published some 20 years ago, but the recent use of culture-independent molecular methods has led to a revised understanding of the ecology and biochemistry of this process and the organisms involved. The case of chitinases is illustrative (#CITATION_TAG).",Principal environmental drivers of chitin degradation are identified which are likely to influence both community composition of chitin degrading bacteria and measured chitin hydrolysis activities.,['This review summarizes different mechanisms and the principal steps involved in chitin degradation at a molecular level while also discussing the coupling of community composition to measured chitin hydrolysis activities and substrate uptake.']
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Much human behaviour can be seen as decision-making, and so understanding and inuencing those decision-making processes could be an important component in design for behaviour change. Areas covered include a number of specic cognitive biases in detail, and the alternative perspective of Gigerenzer and others, who contend (following Herbert Simon) that many heuristics potentially leading to biases are actually ecologically rational, and part of humans' adaptive responses to situations. The application of scientific software continues to be discussed in journals [5, #CITATION_TAG], with concerns that researchers place too much trust in published software, and suggestions that code, as well as methods and results, should be subject to peer review [7].",,"[""This paper examines the 'heuristics and biases' approach to modelling decision-making, and attempts to extract insights which are relevant to designers working to inuence user behaviour for social or environmental benet|either by exploiting biases, or helping to counter those which lead to undesirable behaviour.""]"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Influencing more environmentally friendly and sustainable behaviour is a current focus of many projects, ranging from government social marketing campaigns, education and tax structures to designers' work on interactive products, services and environments. These approaches make different assumptions about 'what people are like': how users will respond to behavioural interventions, and why, and in the process reveal some of the assumptions that designers and other stakeholders, such as clients commissioning a project, make about human nature. While much focus has been given to the release of software tools that aid researchers in developing and analysing computational models [#CITATION_TAG][9][10][11][12][13], the same attention has not been given to providing researchers with a means of showing that their developed tool can adequately support the investigation of a specific biological research question: that the tool is fit for purpose.","There is a wide variety of techniques and methods used, intended to work via different sets of cognitive and environmental principles. The models are characterised using systems terminology and the application of each model to design for sustainable behaviour is examined via a series of examples.","[""This paper discusses three simple models of user behaviour - the pinball, the shortcut and the thoughtful - which emerge from user experience designers' statements about users while focused on designing for behaviour change.""]"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Simulation models that describe autonomous individual organisms (individual based models, IBM) or agents (agent-based models, ABM) have become a widely used tool, not only in ecology, but also in many other disciplines dealing with complex systems made up of autonomous entities. However, there is no standard protocol for describing such simulation models, which can make them difficult to understand and to duplicate. In ecology, the ODD (overview, design concepts, details) protocol is starting to address this, through application of a standard protocol completed while implementing a computational model, with the aim of ensuring reproducibility of results [#CITATION_TAG].","This protocol consists of three blocks (Overview, Design concepts, and Details), which are subdivided into seven elements: Purpose, State variables and scales, Process overview and scheduling, Design concepts, Initialization, Input, and Submodels. We explain which aspects of a model should be described in each element, and we present an example to illustrate the protocol in use. We consider ODD as a first step for establishing a more detailed common format of the description of IBMs and ABMs.","['This paper presents a proposed standard protocol, ODD, for describing IBMs and ABMs, developed and tested by 28 modellers who cover a wide range of fields within ecology.']"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Normal organogenesis requires co-ordinate development and interaction of multiple cell types, and is seemingly governed by tissue specific factors. Lymphoid organogenesis during embryonic life is dependent on molecules the temporal expression of which is tightly regulated. A subset of these cells expresses the receptor tyrosine kinase RET, which is essential for mammalian enteric nervous system formation. Although a basic model of tissue formation has been developed through laboratory experimentation [#CITATION_TAG, [34] [35] [36], the reasons for this emergent behaviour are not fully understood.","During this process, haematopoietic 'inducer' cells interact with stromal 'organizer' cells, giving rise to the lymphoid organ primordia. Here we show that the haematopoietic cells in the gut exhibit a random pattern of motility before aggregation into the primordia of Peyer's patches, a major component of the gut-associated lymphoid tissue. To support this hypothesis, we show that the RET ligand ARTN is a strong attractant of gut haematopoietic cells, inducing the formation of ectopic Peyer's patch-like structures.","['Our work strongly suggests that the RET signalling pathway, by regulating the development of both the nervous and lymphoid system in the gut, has a key role in the molecular mechanisms that orchestrate intestine organogenesis.']"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. People use metaphors every time they speak. Some of those metaphors are literary - devices for making thoughts more vivid or entertaining. But most are much more basic than that - they're ""metaphors we live by"", metaphors we use without even realizing we're using them. Toxicology and human risk assessment studies, for example, use adverse outcome pathway (AOP) tools to demonstrate existing understanding of how molecular, cellular, organ and organism interactions link a molecular initiating event with a particular adverse outcome, such as skin inflammation [#CITATION_TAG].",,"['In this book, George Lakoff and Mark Johnson suggest that these basic metaphors not only affect the way we communicate ideas, but actually structure our perceptions and understandings from the beginning.', 'Bringing together the perspectives of linguistics and philosophy, Lakoff and Johnson offer an intriguing and surprising guide to some of the most common metaphors and what they can tell us about the human mind.']"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. We consider the challenges of lack  of data, incomplete knowledge and modelling in the context of a rapidly changing knowledge base. There is a mismatch in scale between  these cellular models and tissue structures that are affected by tumours, and bridging this gap requires substantial  computational resource. We present concurrent programming as a technology to link scales without losing  important details through model simplification. An effect of the uncovering of incomplete knowledge is viewed as necessarily making a model unfit for purpose: instead, the process of argumentation highlights where biological experimentation might be focused to improve understanding [#CITATION_TAG].","Computer simulation can be used to inform in vivo and in vitro experimentation, enabling rapid, low-cost  hypothesis generation and directing experimental design in order to test those hypotheses. Here, we outline a  framework that supports developing simulations as scientific instruments, and we select cancer systems biology  as an exemplar domain, with a particular focus on cellular signalling models. Our  framework comprises a process to clearly separate scientific and engineering concerns in model and simulation  development, and an argumentation approach to documenting models for rigorous way of recording assumptions  and knowledge gaps.","['We propose interactive, dynamic visualisation tools to enable the biological community to  interact with cellular signalling models directly for experimental design.']"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Individual or agent-based simulation is an important tool for research involving understanding of complex systems. For a research tool to be useful, its use must be understood, and it must be possible to interpret the results of using the tool in the context of the research. In common with acceptable safety, our argumentation approach aims to capture and expose reasoning, via an argumentation structure, to critical scrutiny, in order to establish trust in simulations [24, #CITATION_TAG].",,['This paper presents the partial validity argument for ongoing work on prostate cell simulation (a companion paper describes the models and implementation of the simulation).']
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. However, many existing safety cases, in their attempt to manage potentially complex arguments, are poorly structured, presented and understood. This creates problems in developing and maintaining safety cases, and in capturing successful safety arguments for use on future projects. Drawing on safety-case argumentation, we create a diagrammatic summary of the structured argument of fitness for purpose, using a visual notation closely based on the standard safety-critical argumentation notation, goal structuring notation (GSN) [#CITATION_TAG, 29].","A safety case should present a clear, comprehensive and defensible argument that a system is acceptably safe to operate within a particular context. This approach is based upon a graphical technique -- the Goal Structuring Notation (GSN) -- and has three strands. Firstly, a method for the use of GSN is defined together with an approach to supporting incremental safety case development. Thirdly, the concept of `Safety Case Patterns&apos; is defined as a means of supporting and promoting the reuse of successful safety arguments between safety cases. Examples of the approach are provided throughout.","['This thesis defines and demonstrates a coherent approach to the development, presentation, maintenance and reuse of the safety arguments within a safety case.']"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. The papers were initially produced for an interdisciplinary workshop that sought to explore the relationship between financialization, space and place and, in particular, the relationship between debates about financialization and geographical approaches to money and finance. This debate is now moving on apace (for example, CHRISTOPHERS, 2012; FRENCH et al., 2009, 2011; PIKE and POLLARD, 2010) and the papers here are a further development of these arguments. The crisis broke in late 2007 and it continues to evolve in new ways. At one level, the financial crisis was 'solved' at the moment that governments intervened to underwrite the losses of banks and other financial institutions, but in absorbing these losses and debts, and making their repayment the liability of present and future taxpayers over many generations, states converted a financial crisis into a sovereign debt crisis. The level of national debt born by many developed economies is such that it will cast a shadow over economic development for years to come, and it brings to the fore two issues which are directly addressed by papers in this collection. As WAINWRIGHT (2011) has argued, tax avoidance has been a central concern of the financial sector over a long period of time and helps explain the geography of financial centres, as money flows to spaces that enable it to escape the clutches of fiscal authorities (SHAXSON, 2012). However, sovereign debt crises are also fiscal crises, in that to pay down the debt states must ensure that their taxation strategies are as effective as possible, and which may involve increasing taxes or ensuring that taxes levied are recovered as fully as possible. But at this current conjuncture, the present needs of the state are hampered by earlier rounds of regulatory and financial restructuring that have facilitated the emergence of a large and complex network of financial technologies that are dedicated to lowering the tax burden corporations and individuals capable of hiring them. The ability of financial centres to mobilize money and capital in the interest of its owners and to direct it away from fiscal authorities is illustrated by a report for the Tax Justice Network which estimated that at least US$21 trillion and possibly up to US$32 trillion is channelled into tax havens to protect wealth from taxation (HENRY, 2012). The ability of rich individuals to avoid progressive taxation has encouraged a further polarization of wealth on a global scale, and also means that the impact of tax increases introduced to generate income to reduce budget deficits is not only less effective than it could be, but also has important equity effects. The financial crisis tipped the global economy into recession, and the recovery from that recession has been fitful and uneven. Some economies suffered only a slowing of growth, others went into recession and recovered, while some recovered only to fall back into recession once more. How growth and recovery might be secured is highly contested, and a range of policy measures are being experimented with. Unfortunately, in economies like Britain where this policy has been enthusiastically adopted, it does not appear to be working, and the UK sank back into a double-dip recession in 2012. Meanwhile, the policy of quantitative easing (QE), which has been enthusiastically pursued by the United States and the UK since 2007, has done relatively little to stimulate growth, and is better seen as an additional support to the financial system Regional Studies, 2013 We have previously developed a computational model, or simulation, of pre-natal lymphoid tissue formation to help direct and understand results of laboratory experimentation [#CITATION_TAG]","The first is the issue of tax. and by Engelen and Glasmacher reveal how central tax avoidance has been to the competitive success of the financial centres of both London and Amsterdam over a long period time, and which continues to be important in the ways that such centres market themselves to hyper mobile flows of capital. Public sector spending cuts are justified both to reconcile budget deficits, but also to prevent the 'crowding out' of the private sector.","['This themed section of Regional Studies contains three papers that, between them, address issues that are of pressing concern within the unfolding of the postfinancial crisis economy.', 'To the right of the political spectrum, the crisis is seen as an opportunity to continue the neo-liberal project of hollowing out the state.', 'This will have the effect of replacing expensive public services with more cost-effective, income-generating businesses.']"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Research Question/Issue: Conventional regulatory reforms of the financial system focus on standard economic assumptions of self-interested, rational actors. The Global Financial Crisis (GFC) and similar financial failures highlight that there are limits to this approach. Fueled by sector-wide remuneration practices, these norms created information asymmetries that fundamentally undermined the integrity of the market. This presents an information asymmetry problem whereby they can exploit the market norm of caveat emptor (buyer beware) when developing innovative financial transactions. For the description of computational models, both unified modelling language, adopted in the development of computer software, and systems biology mark-up language (SBML) may be applied [#CITATION_TAG] [19] [20]; the latter possessing the benefit of allowing model execution by a number of SBML-supported software tools.","Instead we use a norm-based (or soft law) perspective to examine how the systemic problems underlying the GFC lay not so much in neo-classical economic assumptions of self-interest, but in unchecked financial innovation exploited by norms of buyer beware and ratings agency reliance among market participants. Our approach provides a re-examination of the often unquestioned use of universal norms for differing market transactions in the financial sector. We contend that a mismatch between norms and market mechanisms can lead to significant unintended outcomes. Our approach of combining soft law (norms) and hard law (regulation) approaches to regulation provides added insights into agency, stewardship, and institutional theories. Specifically, differentiating between transaction types in financial markets will address the problems associated with information and search costs facing buyers of flawed financial innovation. We also provide proposals for policy makers seeking to embed accountability for risk taking across the key participants in the financial system to minimize market distortions in the majority of the financial sector.","['We propose a model highlighting how flawed financial innovation can lead to widespread, systemic problems of assessing and pricing risk because market participants can actively develop and promote flawed transactions.', 'Researchers need to explore the interaction between social norms and market contexts (such as financial innovation) to better understand the behavior of financial markets.']"
"This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. They are a valuable complement and precursor to simulation specifications and implementations, focusing purely on thoroughly exploring the biology, recording hypotheses and assumptions, and serve as a communication medium detailing exactly how a simulation relates to the real biology. Both however are limited by restrictions in the extent of the system they can capture: UML lacks the formalism to capture some biological features (such as cyclicfeedback) [#CITATION_TAG], and SBML cannot currently describe complex agent-based models.","The framework comprises three levels of modelling, ranging in scope from the dynamics of individual model entities to system-level emergent properties. By way of an immunological case study of the mouse disease experimental autoimmune encephalomyelitis, we show how the framework can be used to produce models that capture and communicate the biological system, detailing how biological entities, interactions and behaviours lead to higher-level emergent properties observed in the real world. We show how specialized, well-explained diagrams with less formal semantics can be used where no suitable UML formalism exists. We highlight UML's lack of expressive ability concerning cyclic feedbacks in cellular networks, and the compounding concurrency arising from huge numbers of stochastic, interacting agents. To compensate for this, we propose several additional relationships for expressing these concepts in UML's activity diagram. We also demonstrate the ambiguous nature of class diagrams when applied to complex biology, and question their utility in modelling such dynamic systems. Models created through our framework are non-executable, and expressly free of simulation implementation concerns.",['We present a framework to assist the diagrammatic modelling of complex biological systems using the unified modelling language (UML).']
"Biofuels and biodiversity in South Africa. v107i5/6.186 The South African government, as part of its efforts to mitigate the effects of the ongoing energy crisis, has proposed that biofuels should form an important part of the country's energy supply. The contribution of liquid biofuels to the national fuel supply is expected to be at least 2% by 2013. The Biofuels Industrial Strategy of the Republic of South Africa of 2007 outlines key incentives for reaching this target and promoting the development of a sustainable biofuels industry. The available and proposed processing technologies have important implications for land use and the use of different non-native plant species as desired feedstocks. South Africa has a long history of planting non-native plant species for commercial purposes, notably for commercial forestry. This paper discusses issues relating to this strategy as well as key drivers in biofuel processing with reference to potential impacts on South Africa's rich biological heritage. Today, the principal focus of anti-globalizers is not the effect of globalization on economic prosperity but its harm to social agendas such as the reduction of child labour and poverty, the maintenance of rich-country labour and environmental standards, the exercise of national sovereignty, the maintenance of local culture, and womenaEUR(tm)s rights and welfare. The contrary view, which I defend in this essay, is that economic globalization advances the achievement of that social agenda. 4, 5, #CITATION_TAG, 7 ch of the global debate on biofuels has focused on policy, economics, social issues (such as competition with food crops), and the potential of biofuels to reduce greenhouse gas emissions.",,['But we must ask: what institutional and policy framework is necessary to improve on the benign outcomes that globalization fetches?']
"In previous work the authors considered the asymmetric simple exclusion process on the integer lattice in the case of step initial condition, particles beginning at the positive integers. There it was shown that the probability distribution for the position of an individual particle is given by an integral whose integrand involves a Fredholm determinant. In one an apparently new distribution function arises and in another the distribution function F 2 arises. CONTEXT Previous studies may have underestimated the contribution of health behaviors to social inequalities in mortality because health behaviors were assessed only at the baseline of the study. DESIGN, SETTING, AND PARTICIPANTS Established in 1985, the British Whitehall II longitudinal cohort study includes 10 308 civil servants, aged 35 to 55 years, living in London, England. In previous work [#CITATION_TAG] the authors considered the asymmetric simple exclusion process (ASEP) on the integer lattice Z in the case of step initial condition, particles beginning at the positive integers Z +.","Analyses are based on 9590 men and women followed up for mortality until April 30, 2009. Socioeconomic position was derived from civil service employment grade (high, intermediate, and low) at baseline.",['OBJECTIVE To examine the role of health behaviors in the association between socioeconomic position and mortality and compare whether their contribution differs when assessed at only 1 point in time with that assessed longitudinally through the follow-up period.']
"In previous work the authors considered the asymmetric simple exclusion process on the integer lattice in the case of step initial condition, particles beginning at the positive integers. There it was shown that the probability distribution for the position of an individual particle is given by an integral whose integrand involves a Fredholm determinant. In one an apparently new distribution function arises and in another the distribution function F 2 arises. Although differential mortality decline for cardiovascular diseases has been suggested as an important contributory factor, it is not known what its quantitative contribution was, and to what extent other causes of death have contributed to the widening gap in total mortality. In most countries, mortality from cardiovascular diseases declined proportionally faster in the upper socioeconomic groups. In all countries with the exception of Italy (Turin), changes in cardiovascular disease mortality contributed about half of the widening relative gap for total mortality. For these causes, widening inequalities were sometimes due to increasing mortality rates in the lower socioeconomic groups. We found rising rates of mortality from lung cancer, breast cancer, respiratory disease, gastrointestinal disease, and injuries among men and/or women in lower socioeconomic groups in several countries. In this case the probability equals a probability in a unitary Laguerre random matrix ensemble #CITATION_TAG","METHODS We collected data on mortality by educational level and occupational class among men and women from national longitudinal studies in Finland, Sweden, Norway, Denmark, England/Wales, and Italy (Turin), and analysed age-standardized death rates in two recent time periods (1981-1985 and 1991-1995), both total mortality and by cause of death. For simplicity, we report on inequalities in mortality between two broad socioeconomic groups (high and low educational level, non-manual and manual occupations).",['OBJECTIVES During the past decades a widening of the relative gap in death rates between upper and lower socioeconomic groups has been reported for several European countries.']
"In previous work the authors considered the asymmetric simple exclusion process on the integer lattice in the case of step initial condition, particles beginning at the positive integers. There it was shown that the probability distribution for the position of an individual particle is given by an integral whose integrand involves a Fredholm determinant. In one an apparently new distribution function arises and in another the distribution function F 2 arises. Socioeconomic inequalities in premature mortality in Britain increased over the second half of the 20th century, particularly from the early 1970s onwards.1 The magnitude of mortality differentials reflects the trend in income inequality, which has also undergone a dramatic increase over the past quarter century.1 The present British government have emphasised their commitment to reducing health inequalities. For example the Minister of Health, Alan Milburn, has stated that ""Our ambition is to do something that no government--Tory or Labour--has ever done. The mortality data are the Office for National Statistics digital records of all deaths in England and Wales, and equivalent records from the General Register Office (Scotland). In the second result an apparently new distribution function arises and in the third the distribution function F 2 of random matrix theory [#CITATION_TAG] arises.","Not only to improve the health of the nation, but also to improve the health of the worst off at a faster rate"".2 A set of targets for the reduction of health inequalities has been presented. The full postcode of the usual residence of the deceased was used to assign each death to the parliamentary constituency in which the deceased usually lived.",['To monitor progress in this regard we have produced updated analyses of premature mortality rates running through to the end of 1999.']
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. #CITATION_TAG have investigated first approaches towards large-scale requirements elicitation using social networks.",A description of what they intend to measure is given together with how data are elicited and the advantages and limitation of the indicators. The glossary is divided into two parts for journal publication but the intention is that it should be used as one piece. The second part highlights a life course approach and will be published in the next issue of the journal.,['This glossary presents a comprehensive list of indicators of socioeconomic position used in health research.']
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. Requirements engineering for market-driven software development entails special challenges. These approaches complement classical market-driven requirements elicitation methods (#CITATION_TAG).","A number of challenging issues were found, including bridging communication gaps between marketing and development, selecting the right level of process support, basing the release plan on uncertain estimates, and managing the constant flow of requirements","['This paper presents results from an empirical study that investigates these challenges, taking a qualitative approach using interviews with fourteen employees at eight software companies and a focus group meeting with practitioners.', 'The objective of the study is to increase the understanding of the area of market-driven requirements engineering and provide suggestions for future research by describing encountered challenges.']"
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. This is a study of social mobility within the developing class structures of modern industrial societies based on a unique data-set constructed by John Goldthorpe and Robert Erikson. Approaches which allow end-user to give feedback on current context-aware services (#CITATION_TAG) and which allow them to document their ideas on services in situ (Seyff et al., 2010) build a basis to satisfy some of the depicted issues.",The authors combine historical and statistical approaches in their analysis of both trends in mobility and of cross-national similarities and differences.,"[""The focus is on the experience of European nations - western and eastern - in the period of the `long boom' following the Second World War; but the book also devotes separate chapters to examining the experience of the USA, Australia, and Japan."", 'This book is intended for teachers and postgraduates in sociology, social and economic history, social stratification, and the sociology of industrial societies.']"
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. Educational level is most often used to identify social groups with increased prevalence of smoking. Other indicators of socioeconomic position (SEP) might, however, be equally or even more discriminatory. Approaches for service discovery (Ran, 2003), service composition (#CITATION_TAG and Su, 2005) (in particular using AI techniques (Beauche and Poizat, 2008)) and service adaptation (di Nitto et al., 2008) are of application to satisfy some of the envisaged challenges.Social collaboration","We selected data for 45,765 respondents aged 25-60 years from nine European countries. The association between six different SEP indicators and smoking prevalence was examined using prevalence rate ratios (RRs) estimated through log linear regression analyses. In multivariate analyses, educational level, occupational class, accumulated wealth (measured by household assets), and housing tenure retained independent effects on smoking (RRs about 1.20). These measures should be used in addition to educational level to identify groups at increased risk for smoking.",['This study examined the extent to which smoking behavior is related to other socioeconomic indicators in addition to educational level.']
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. Web services technology has generated a lot interest, but its adoption rate has been slow. Approaches for service discovery (#CITATION_TAG), service composition (Rao and Su, 2005) (in particular using AI techniques (Beauche and Poizat, 2008)) and service adaptation (di Nitto et al., 2008) are of application to satisfy some of the envisaged challenges.",quality of services) are taken into account for the service discovery.,"['This paper discusses issues related to this slow take up and argues that quality of services is one of the contributing factors.', 'The paper proposes a new Web services discovery model in which the functional and non-functional requirements (i.e.', 'The proposed model should give Web services consumers some confidence about the quality of service of the discovered Web services']"
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. Abstract--Despite the recent advances in test generation, fully automatic software testing remains a dream: Ultimately, any generated test input depends on a test oracle that deter-mines correctness, and, except for generic properties such as ""the program shall not crash"", such oracles require human input in one form or another. CrowdSourcing is a recently popular technique to automate computations that cannot be performed by machines, but only by humans. Crowdsourcing for addressing the Oracle problem in software testing has been also recently investigated (#CITATION_TAG).","A problem is split into small chunks, that are then solved by a crowd of users on the Internet. If the crowd determines that an assertion does not match the behavior described in the code documentation, then a bug has been found.",['In this paper we investigate whether it is possible to exploit CrowdSourcing to solve the oracle problem: We produce tasks asking users to evaluate CrowdOracles - assertions that reflect the current behavior of the program.']
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. The respective contribution of occupational and behavioural factors to social disparities in all-cause mortality has been studied very seldom. Occupational factors played a substantial role in explaining social disparities in mortality, especially for premature mortality and men. The key value of requirements in this context was also recently highlighted by #CITATION_TAG who demonstrated that requirements are socially constructed in a political context.",Mortality was derived from register-based information and linked to the baseline data. Socioeconomic status was measured using occupation.,['The objective of this study was to evaluate the role of occupational and behavioural factors in explaining social inequalities in premature and total mortality in the French working population.']
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. Network-based software application services are receiving a lot of attention in recent years, as observed in developments as Internet of Services, Software as a Service and Cloud Computing. A service-oriented computing ecosystem is being created where the end-user is having an increasingly more active role in the service creation process. However, supporting end-users in the creation process, at runtime, is a difficult undertaking. Users have different requirements and preferences towards application services, use services in different situations and expect highly abstract mechanisms in the creation process. Furthermore, there are different types of end-users: some can deliver more detailed requirements or can be provided with more advanced request interface, while others can not. Another noteworthy work (#CITATION_TAG) studies end-user service composition from the perspective of users.","To tackle these issues and provide end-users with personalised service delivery, we claim that runtime automated service composition mechanisms are required.","['In this paper we present the DynamiCoS framework, which aims at supporting the different phases required to provide end-users with automatic service discovery, selection and composition process.']"
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. Recommender systems (#CITATION_TAG and Tuzhilin, 2005) may provide (even automatically execute) recommendations on which services to apply; some applications in the marketing context.","This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multcriteria ratings, and a provision of more flexible and less intrusive types of recommendations","['This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches.']"
"The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. For instance, recent findings dispute the idea that people are rational decision-makers (#CITATION_TAG).","Methods: The data derive from the surveys of the Helsinki health study, collected in 2000, 2001, and 2002 from 40-60 year old employees working for the City of Helsinki (n = 8970, response rate 67%). The study measured occupation based social class and Karasek's demand-control model. Age adjusted prevalence percentages and fitted logistic regression models were calculated. The relation between social class and both health outcomes considerably attenuated when job control was controlled for, but was reinforced when controlling for job demands. Controlling for both job control and job demands attenuated the relation between social class and self rated health and limiting longstanding illness among women, however, was reinforced among men.","['Background: The aim of the study was to investigate (1) how much of the association between health and social class is accounted by psychosocial working conditions, and (2) whether health is related to working conditions after controlling for social class.']"
"This paper reviews a diverse set of social and interpersonal inuence approaches and techniques which could be relevant to designers seeking to inuence behaviour change for social and environmental benet. En este articulo se plantea una psicologia social especializada en contextos etnicos. Para llevar a cabo este trabajo, se requieren nuevas bases teoricas que requieran abordar y comprender la realidad de las comunidades indigenas. Se presenta un argumento epistemologico para fundamentar la disciplina, basado en que el conocimiento cientifico moderno, ha sido un productor de sujetos invisibilizados, con la finalidad de la explotacion de recursos que permiten sustentar el poder hegemonico. Dentro de los ""otros"" generados, se posiciona al ""indigena latinoamericano"", en oposicion al colonizador. Se plantean fundamentos epistemologicos de este enfoque, basados en la Sociologia de las Ausencias de Sousa y en la Psicologia Social Comunitaria, que permitirian un estudio de las comunidades indigenas y de la subjetividad etnica. Social psychology especially involves the scientic study of the behaviour of individuals as a function of social stimuli (#CITATION_TAG).","Within the 'other' generated, is positioned at the ""Latin American indigenous"", in opposition to the colonizer. There are arise epistemological foundations of this approach, based on the Sociology of the Absences of Sousa and Community Social Psychology, which would allow a study of indigenous communities and ethnic subjectivity","['This article proposes a social psychology specializing in ethnic contexts.', 'To carry out this work, requires new theoretical bases that require understand the reality of the indigenous communities.', 'It presents an epistemological argument to base the discipline, based on which modern scientific knowledge, has been a producer of invisible subjects, with the purpose of the exploitation of resources that sustain the hegemonic power.']"
"This paper reviews a diverse set of social and interpersonal inuence approaches and techniques which could be relevant to designers seeking to inuence behaviour change for social and environmental benet. Abstract : The results of observational studies are often disputed because of nonrandom treatment assignment. For example, patients at greater risk may be overrepresented in some treatment group. The propensity score is the (estimated) conditional probability of assignment to a particular treatment given a vector of observed covariates. Not all of these are directly applicable in a design context, but most have parallels with other strategies encountered in design for behaviour changee.g.`reason' is seen in Petty and Cacioppo's central route persuasion (#CITATION_TAG);`friendliness' sums up a number of Carnegie's recommendations;`higher authority' is seen in Cialdini's`authority' (Lockton, 2012b).","Applications include: matched sampling on the univariate propensity score which is equal percent bias reducing under more general conditions than required for discriminant matching, multivariate adjustment by subclassification on balancing scores where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and visual representation of multivariate adjustment by a two-dimensional plot.",['This paper discusses the central role of propensity scores and balancing scores in the analysis of observational studies.']
"This paper reviews a diverse set of social and interpersonal inuence approaches and techniques which could be relevant to designers seeking to inuence behaviour change for social and environmental benet. Nos  concentramos en el analisis de metaforas con colores en relacion con conceptos diferentes a  los de las emociones, en contextos no literarios y donde la sinestesia no es la unica  motivacion. Nuestro corpus consiste en elementos lexicos, frases hechas y colocaciones  donde el color contribuye al significado, tomado del BNC (ingles) y del CREA (espanol). El  estudio demuestra a) que el continuo literal-metaforico no siempre puede verse dentro de la  misma expresion; b) la importancia del punto medio del continuo compuesto de cadenas de  implicaturas predominantemente basadas en el conocimiento cultural y los valores asignados  a los colores por los hablantes de cada comunidad The academic study of cognitive linguistics, including the investigation of language and metaphor usage to explore people's mental models (e.g. #CITATION_TAG) is an interesting and potentially extremely useful eld for designers, but NLP's claims about the existence of preferred representational systems and the validity of eye accessing cues have not been supported by empirical study (Sharpley, 1987; Wiseman et al, 2012).","Our  corpus consists of lexical items, idioms and collocations where colour contributes to  meaning, taken from the BNC (English) and the CREA (Spanish).","['This paper aims to deepen into the nature of motivation and into the literal and  metaphorical continuum of colour expressions for red and green in English and Spanish.', 'We  focus on the analysis of colour metaphors in relation to concepts different from those of  emotions, in non-literary texts, and where synaesthesia is not the only motivation.']"
"This paper reviews a diverse set of social and interpersonal inuence approaches and techniques which could be relevant to designers seeking to inuence behaviour change for social and environmental benet. In previous literature, George Stigler asserts a law of diminishing returns to group size in politics: Beyond some point it becomes counterproductive to dilute the per capita transfer. NLP was developed by Richard Bandler and John Grinder in the context of understanding the patterns used by`successful' psychotherapists, e.g. the psychiatrist and hypnotist Milton H. Erickson (#CITATION_TAG; Grinder, Delozier and Bandler, 1977), and putting them into a form where they could be useful to others.","Since the total transfer is endogenous, there is a corollary that dirninishing returns apply to the transfer as well, due both to the opposition provoked by the transfer and to the demand this opposition exerts on resources to quiet it. This is at one level, a detail, which is the way Stigler treated it, but a detail with some important implications -- for entry into regulation, and for the price-output structure that emerges from regulation.","['Stigler does not himself formalize this model, and my first task will be to do just this.', ""The main task of the paper is to derive these implications from a generalization of Stigler's model.""]"
"The 2007-9 period saw an unprecedented crisis emerge in global financial markets with the collapse of several large western financial institutions, and the nearest moment of systemic crisis yet witnessed in the globalised financial system. The crisis has thus provoked a significant questioning of market theories, and in particular understandings of market within orthodox neoclassical economics. Within the social sciences, a significant element of this response has built on a growing heterodox socioeconomic literature which is heavily critical of hegemonic conceptions of the market within economics. However, whilst a small body of work in economic geography has begun to engage with this literature, geographical thinking has not directly sought to conceptualise the nature and significance of market spatiality. Utilising a cultural economy approach, this paper therefore argues that economic geographical theories need to foreground the concept of market rather than treat markets as a 'component' of wider processes. Drawing on the growing heterodox socioeconomic literature on markets, it thus proposes a practice-oriented 'socio-spatial approach' for framing conceptions of market spatiality, arguing that such a spatial epistemology opens up a range of theoretical possibilities for further contesting hegemonic neoclassical theories of the market beyond current socioeconomic critiques. It seeks to illustrate the utility of such a framework through a case study analysis of the limitations inherent in existing policy practices surrounding the early phase of the recent global financial crisis. Real estate is, by definition, local as it is spatially fixed. Mortgage lending, however, has developed from a local to a national market and is increasingly a global market today. The article looks at different states, different cities, different neighbourhoods and different financial centres. Investors in many places had invested in residential mortgage backed securities and have seen their value drop. Housing bubbles, faltering economies and regulation together have shaped the geography of the financial crisis on the state and city level in the US. Subprime and predatory lending have affected low-income and minority communities more than others and we therefore not only see a concentration of foreclosures in certain cities, but also in certain neighbourhoods. On an international level, the long-term economical and political consequences of this are still mostly unknown, but it is clear that some financial centres in Asia (including the Middle East) will become more important now that globalisation is coming full circle. The crisis was understood to have spread geographically from the US economy to Europe, and the worldwide (although some financial markets were less affected such as those in Asia) (#CITATION_TAG).",An understanding of the financial crisis is ultimately a spatialised understanding of the linkages between local and global.,"['This article looks at the geographies of the mortgage crisis and credit crunch and asks the question: how are different places affected by the crisis?', 'This article does not present new empirical research, but brings together work from different literatures that all in some way have a specific angle on the financial crisis.', 'The aim of this article is to make the geographical dimensions of the financial crisis understandable to geographers that are not specialists in all - or even any - of these literatures, so that they can comprehend the spatialisation of this crisis']"
"The 2007-9 period saw an unprecedented crisis emerge in global financial markets with the collapse of several large western financial institutions, and the nearest moment of systemic crisis yet witnessed in the globalised financial system. The crisis has thus provoked a significant questioning of market theories, and in particular understandings of market within orthodox neoclassical economics. Within the social sciences, a significant element of this response has built on a growing heterodox socioeconomic literature which is heavily critical of hegemonic conceptions of the market within economics. However, whilst a small body of work in economic geography has begun to engage with this literature, geographical thinking has not directly sought to conceptualise the nature and significance of market spatiality. Utilising a cultural economy approach, this paper therefore argues that economic geographical theories need to foreground the concept of market rather than treat markets as a 'component' of wider processes. Drawing on the growing heterodox socioeconomic literature on markets, it thus proposes a practice-oriented 'socio-spatial approach' for framing conceptions of market spatiality, arguing that such a spatial epistemology opens up a range of theoretical possibilities for further contesting hegemonic neoclassical theories of the market beyond current socioeconomic critiques. It seeks to illustrate the utility of such a framework through a case study analysis of the limitations inherent in existing policy practices surrounding the early phase of the recent global financial crisis. This paper presents a theory of competition among pressure groups for political influence. Political equilibrium depends on the efficiency of each group in producing pressure, the effect of additional pressure on their influence, the number of persons in different groups, and the deadweight cost of taxes and subsidies. An increase in deadweight costs discourages pressure by subsidized groups and encourages pressure by taxpayers. The financial crisis that gripped the global financial system in the latter half of 2007 has prompted had significant and far-reaching re-evaluation of orthodox market theories and their associated neoliberal policy prescriptions (#CITATION_TAG; Kay, 2009; Norfield, 2010), and this has persisted -if not deepened -with the sovereign debt crisis in the EU since 2010 (Lane, 2010; Mody and Sandri,2012).",,['This analysis unifies the view that governments correct market failures with the view that they favor the politically powerful: both are produced by the competition for political favors.']
"The 2007-9 period saw an unprecedented crisis emerge in global financial markets with the collapse of several large western financial institutions, and the nearest moment of systemic crisis yet witnessed in the globalised financial system. The crisis has thus provoked a significant questioning of market theories, and in particular understandings of market within orthodox neoclassical economics. Within the social sciences, a significant element of this response has built on a growing heterodox socioeconomic literature which is heavily critical of hegemonic conceptions of the market within economics. However, whilst a small body of work in economic geography has begun to engage with this literature, geographical thinking has not directly sought to conceptualise the nature and significance of market spatiality. Utilising a cultural economy approach, this paper therefore argues that economic geographical theories need to foreground the concept of market rather than treat markets as a 'component' of wider processes. Drawing on the growing heterodox socioeconomic literature on markets, it thus proposes a practice-oriented 'socio-spatial approach' for framing conceptions of market spatiality, arguing that such a spatial epistemology opens up a range of theoretical possibilities for further contesting hegemonic neoclassical theories of the market beyond current socioeconomic critiques. It seeks to illustrate the utility of such a framework through a case study analysis of the limitations inherent in existing policy practices surrounding the early phase of the recent global financial crisis. Four political systems (dictatorship, one-party, dominant party, democracy) and an index of political rights account for differences in political institutions. Pluralistic systems are associated with higher agricultural protection levels, although in a nonlinear fashion. The financial crisis that gripped the global financial system in the latter half of 2007 has prompted had significant and far-reaching re-evaluation of orthodox market theories and their associated neoliberal policy prescriptions (Cooper, 2008; #CITATION_TAG; Norfield, 2010), and this has persisted -if not deepened -with the sovereign debt crisis in the EU since 2010 (Lane, 2010; Mody and Sandri,2012).",,"['This paper analyzes the influence of political systems and rights in patterns of agricultural protection across commodities, countries and over time.']"
"The 2007-9 period saw an unprecedented crisis emerge in global financial markets with the collapse of several large western financial institutions, and the nearest moment of systemic crisis yet witnessed in the globalised financial system. The crisis has thus provoked a significant questioning of market theories, and in particular understandings of market within orthodox neoclassical economics. Within the social sciences, a significant element of this response has built on a growing heterodox socioeconomic literature which is heavily critical of hegemonic conceptions of the market within economics. However, whilst a small body of work in economic geography has begun to engage with this literature, geographical thinking has not directly sought to conceptualise the nature and significance of market spatiality. Utilising a cultural economy approach, this paper therefore argues that economic geographical theories need to foreground the concept of market rather than treat markets as a 'component' of wider processes. Drawing on the growing heterodox socioeconomic literature on markets, it thus proposes a practice-oriented 'socio-spatial approach' for framing conceptions of market spatiality, arguing that such a spatial epistemology opens up a range of theoretical possibilities for further contesting hegemonic neoclassical theories of the market beyond current socioeconomic critiques. It seeks to illustrate the utility of such a framework through a case study analysis of the limitations inherent in existing policy practices surrounding the early phase of the recent global financial crisis. The location of agricultural production is fixed, but ionopolistcally competitive manufacturing finns choose their location to maximize profits. If transportation costs are high, returns to scale weak, and the share of spending on manufactured goods low, the incentive to produce close to the market leads to an equal division of manufacturing between the regions. With lower transport costs, stronger scale economies, or a higher manufacturing share, circular causation sets in: the more manufacturing is located in one region, the larger that region&apos;s share of demand, and this provides an incentive to locate still more manufacturing there. In an earlier period economic geographers sought to spatialise analysis of markets around orthodox neoclassical conceptions, and of course this branch of the subdiscipline has continued to develop within economics as the 'new economic geography' (Krugman, 1991 (#CITATION_TAG Fujita et al., 1999; Fujita and Thisse, 2008).",,"['This paper develops a two--region, two--sector general equilibriun model of location.']"
"The 2007-9 period saw an unprecedented crisis emerge in global financial markets with the collapse of several large western financial institutions, and the nearest moment of systemic crisis yet witnessed in the globalised financial system. The crisis has thus provoked a significant questioning of market theories, and in particular understandings of market within orthodox neoclassical economics. Within the social sciences, a significant element of this response has built on a growing heterodox socioeconomic literature which is heavily critical of hegemonic conceptions of the market within economics. However, whilst a small body of work in economic geography has begun to engage with this literature, geographical thinking has not directly sought to conceptualise the nature and significance of market spatiality. Utilising a cultural economy approach, this paper therefore argues that economic geographical theories need to foreground the concept of market rather than treat markets as a 'component' of wider processes. Drawing on the growing heterodox socioeconomic literature on markets, it thus proposes a practice-oriented 'socio-spatial approach' for framing conceptions of market spatiality, arguing that such a spatial epistemology opens up a range of theoretical possibilities for further contesting hegemonic neoclassical theories of the market beyond current socioeconomic critiques. It seeks to illustrate the utility of such a framework through a case study analysis of the limitations inherent in existing policy practices surrounding the early phase of the recent global financial crisis. Leadership turnover is managed by a selectorate - a group of individuals on whom the leader depends to hold onto power. This requires that the selectorate's hold on power is not too dependent on a specific leader being in office. In this view, the epistemological starting point is a recognition that markets 'do not simply fall out of thin air' (#CITATION_TAG Boeckler, 2007, 2009) but rather are phenomenon that are 'continually produced and constructed socially with the help of actors who are interlinked in dense and extensive webs of social relations' (Berndt and Boeckler, 2007: 536).",The paper develops a simple theoretical model of accountability in the absence of regularized elections. Good policy is institutionalized when the selectorate removes poorly performing leaders from office. We use these case studies to identify the selectorate in specific instances of successful autocracy.,"['One of the key goals of political economy is to understand how institutional arrangements shape policy outcomes.', 'This paper studies a comparatively neglected aspect of this - the forces that shape heterogeneous performance of autocracies.', 'The paper looks empirically at spells of autocracy to establish cases where it has been successful according to various objective criteria.']"
"The main contributing causes to these larger inequalities differed strongly between countries (e.g., cancer in France, all other causes in Denmark). This study analyses occupational class inequalities in all-cause mortality and four specific causes of death among men, in Europe in the early 2000s, and is the most extensive comparative analysis of occupational class inequalities in mortality in Europe so far. Inequalities in mortality from conditions amenable to health care were found to explain a substantial part of mortality inequalities in Central and Eastern Europe [#CITATION_TAG, 50], and inequalities in access to health care could thus partly account for the inequalities in mortality in Lithuania.","Analyses are based on 9590 men and women followed up for mortality until April 30, 2009. Socioeconomic position was derived from civil service employment grade (high, intermediate, and low) at baseline.","['Context Previous studies may have underestimated the contribution of health behaviors to social inequalities in mortality because health behaviors were assessed only at the baseline of the study.Objective To examine the role of health behaviors in the association between socioeconomic position and mortality and compare whether their contribution differs when assessed at only 1 point in time with that assessed longitudinally through the follow-up period.Design, Setting, and Participants Established in 1985, the British Whitehall II longitudinal cohort study includes 10 308 civil servants, aged 35 to 55 years, living in London, England.']"
"The main contributing causes to these larger inequalities differed strongly between countries (e.g., cancer in France, all other causes in Denmark). This study analyses occupational class inequalities in all-cause mortality and four specific causes of death among men, in Europe in the early 2000s, and is the most extensive comparative analysis of occupational class inequalities in mortality in Europe so far. These mostly focused on the 1970s, 1980s, and 1990s, and not always took economically inactive persons into account, due to lacking information on their occupational class [#CITATION_TAG] [9] [10].","Methods. We develop and apply an enhanced regularization algorithm, used in RHESSI X-ray spectral analysis, to constrain the ill-posed inverse problem that is determining the DEM from solar observations. We demonstrate this computationally fast technique applied to a range of DEM models simulating broadband imaging data from SDO/AIA and high resolution line spectra from Hinode/EIS, as well as actual active region observations with Hinode/EIS and XRT. As this regularization method naturally provides both vertical and horizontal (temperature resolution) error bars we are able to test the role of uncertainties in the data and response functions. The regularization method is able to successfully recover the DEM from simulated data of a variety of model DEMs (single Gaussian, multiple Gaussians and CHIANTI DEM models). The combination of horizontal and vertical error bars and the regularized solution matrix allows us to easily determine the accuracy and robustness of the regularized DEM. When applied to real active region observations with Hinode/EIS and XRT the regularization method is able to recover a DEM similar to that found via a MCMC method but in considerably less computational time. Regularized inversion quickly determines the DEM from solar observations and provides reliable error estimates (both horizontal and vertical) which allows the temperature spread of coronal plasma to be robustly quantified.",['To demonstrate the capabilities of regularized inversion to recover differential emission measures (DEMs) from multiwavelength observations provided by telescopes such as Hinode and SDO.']
"The main contributing causes to these larger inequalities differed strongly between countries (e.g., cancer in France, all other causes in Denmark). This study analyses occupational class inequalities in all-cause mortality and four specific causes of death among men, in Europe in the early 2000s, and is the most extensive comparative analysis of occupational class inequalities in mortality in Europe so far. Education reflects knowledge attainment, intellectual resources, and cognitive functioning, and may also reflect the ability to take up health education and health innovations [#CITATION_TAG].",A description of what they intend to measure is given together with how data are elicited and the advantages and limitation of the indicators. The glossary is divided into two parts for journal publication but the intention is that it should be used as one piece.,['This glossary presents a comprehensive list of indicators of socioeconomic position used in health research.']
"The main contributing causes to these larger inequalities differed strongly between countries (e.g., cancer in France, all other causes in Denmark). This study analyses occupational class inequalities in all-cause mortality and four specific causes of death among men, in Europe in the early 2000s, and is the most extensive comparative analysis of occupational class inequalities in mortality in Europe so far. Educational level is most often used to identify social groups with increased prevalence of smoking. Other indicators of socioeconomic position (SEP) might, however, be equally or even more discriminatory. Not only during adolescence, but also in adulthood healthy behaviours may be encouraged or discouraged by the social environment, and health-related behaviours may therefore be as strongly or even more strongly related to occupational class as it is to education [3, #CITATION_TAG].","We selected data for 45,765 respondents aged 25-60 years from nine European countries. The association between six different SEP indicators and smoking prevalence was examined using prevalence rate ratios (RRs) estimated through log linear regression analyses. In multivariate analyses, educational level, occupational class, accumulated wealth (measured by household assets), and housing tenure retained independent effects on smoking (RRs about 1.20). These measures should be used in addition to educational level to identify groups at increased risk for smoking.",['This study examined the extent to which smoking behavior is related to other socioeconomic indicators in addition to educational level.']
"The main contributing causes to these larger inequalities differed strongly between countries (e.g., cancer in France, all other causes in Denmark). This study analyses occupational class inequalities in all-cause mortality and four specific causes of death among men, in Europe in the early 2000s, and is the most extensive comparative analysis of occupational class inequalities in mortality in Europe so far. There is no evidence that mortality differences are smaller in countries with more egalitarian socio-economic and other policie Occupational class was classified following the EGP scheme which was initially developed for international comparisons and has already been used in several studies on occupational class inequalities in mortality [5, #CITATION_TAG].","A common social class scheme was applied to most data sets. The magnitude of mortality differences was quantified by three summary indices. Three major data problems were identified and their potential effect on inequality estimates was quantified for each country individually. Relatively large ratios were only observed for France. Data problems were found to have the potential to bias inequality estimates, substantially especially those for Ireland, Spain and Portugal.","['This study compares eleven countries with respect to the magnitude of mortality differences by occupational class, paying particular attention to problems with the reliability and comparability of the data that are available for different countries.', 'This study underlines the similarities rather than the dissimilarities between European countries.']"
"The main contributing causes to these larger inequalities differed strongly between countries (e.g., cancer in France, all other causes in Denmark). This study analyses occupational class inequalities in all-cause mortality and four specific causes of death among men, in Europe in the early 2000s, and is the most extensive comparative analysis of occupational class inequalities in mortality in Europe so far. The linear programing algorithms available for optimizing the routing of shipments in multi-plant, multi-destination systems cannot, in the current state of knowledge, be applied directly to the more general problem of determining the number and location of regional warehouses in large-scale distribution networks. Substantial differences in mortality between socioeconomic groups, however, are still observed and might even be increasing in Europe [3] [#CITATION_TAG] [5].",,['This paper outlines a heuristic computer program for locating warehouses and compares it with recently published efforts at solving the problem either by means of simulation or as a variant of linear programing.']
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Specifically, we study the democratic reforms effects using a difference-in-differences technique, as well as by combining it with propensity score matching methods as in #CITATION_TAG.","A total of 198 cases (with at least two of three symptoms: wheezing, rhinitis, eczema) and 202 healthy controls, living in 390 homes, were examined by physicians. Ventilation rates were measured by a passive tracer gas method, and inspections were carried out in the homes. Families with allergic children should be given the advice to have good ventilation in the home. In investigations, of associations between environmental factors and allergies, the air change rate in homes has to be considered.",['UNLABELLED The aim of the study was to test the hypothesis that a low-ventilation rate in homes is associated with an increased prevalence of asthma and allergic symptoms among children.']
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . >  > European Centre for Health Policy1  The long tradition of never considering the impact on health of public investment has ended. The white paper Saving Lives: Our Healthier Nation exhorted local decision makers to ""think about the effects which their policies have on health, and in particular, how they can reduce health inequality,""2 a recommendation that echoes statements made in the Acheson report, by the European Union, and by the World Health Organization.3-5  In the United Kingdom, government support for assessment of the health impact of policies has continued with the recent call for proposals for health impact assessment projects under the inequalities in health research programme, the establishment of a cross departmental health impact assessment group in central government, and the organisation and publication of methodological seminars and reports.6-8 The European Centre for Health Policy of the World Health Organization has produced a series of publications that includes the ""Gothenberg paper,"" regarded by many as the key document in stating the aims, objectives, and methods for health impact assessment.1  Increasing recognition of the effects of the socioeconomic and physical environment on health may, on the face of it, make it difficult to question the philosophy of health impact assessment--indeed the hype currently surrounding health impact assessment would imply that it is the indispensable condition of policy investment. But what is health impact assessment, and can it in its present format reliably inform better decision making? Giavazzi and Tabellini (2005) and #CITATION_TAG, exploiting the within country variation in trade policies, find that regime change towards democracy is associated with more trade liberalization in developing countries.",,"['> ""The general objective of such assessments is to improve knowledge about the potential impact of a policy or programme, inform decision-makers and affected people, and facilitate adjustment of the proposed policy in order to mitigate the negative and maximize the positive impacts.""']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . First, the #CITATION_TAG 'openness index' to measure trade liberalization used by, among others, Giavazzi and Tabellini (2005), Milner and Kubota (2005) and Persson (2005) has been criticized by Rodriguez and Rodrik (2000) 3 who show that it is a poor measure of trade barriers.",It also enables other researchers to use a realistic Halo 2 traffic model in network simulations.,"['This paper analyses the traffic characteristics of, and proposes a traffic model for, the Xbox game Halo 2.', 'Our goal is to help players and network providers to estimate the amount of traffic caused by the game and the impact on access links or provider networks.', 'We focus on the following characteristics: bandwidth, packet rate and distribution of packet inter-arrival times and packet lengths.']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . For similar conclusions based on a more structured model, see also #CITATION_TAG.","We distinguish between confined dyadic change and network change and show how change itself may be incremental or radical, using the punctuated equilibrium model of change. We propose an analytical framework where the ideas of mechanism, nature and forces of change are integrated. Two circles of network change (incremental and radical) are presented and transfers from one circle to the other are discussed.","['This paper investigates the dynamics of business networks.', 'The concept of the critical event is introduced to highlight radical change.']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . In early December 2006, the Fijian military seized power in a coup led by the Armed Forces commander Commodore Frank Bainimarama. It was a coup long expected, and Fiji's fourth since 1987. Internationally, the response was swift imposing sanctions and removing or delaying international aid programmes. This has a potentially significant impact on Fiji because it is one of the largest per capita recipients of developmental aid funding in the world. However, it may also have little impact because, despite such assistance, the Fijian GDP has stagnated with an average growth of under 1% for the last 20 years. However, to review all international developmental programmes across all sectors of Fijian society, while maintaining contemporary relevance and coherency, is untenable. The EU is one of the most influential partners for Fiji and is often overlooked by scholars, allowing this thesis to make a valuable contribution to developmental studies in the pacific region. This is because they are the sectors that the European Union is presently devoting most attention. Similar mechanisms that imply a different fiscal policy between a democracy and an autocracy are proposed by #CITATION_TAG and McGuire and Olson (1996).",,"[""This thesis thus examines the dichotomy between Fiji's ODA and its apparent inability to arrest the decline of the Fijian lifestyle and economy."", 'Therefore, the thesis will focus on the European Union and its external relations with Fiji.', 'The thesis has selected and examines four sectors of Fijian society, that of the Economy, Governance, Sugar, and Education sectors.']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . In previous literature, George Stigler asserts a law of diminishing returns to group size in politics: Beyond some point it becomes counterproductive to dilute the per capita transfer. A different view can be found in the 'Chicago school' of political economy (Stigler, 1971; #CITATION_TAG; Becker, 1983) and studies such as Wittman (1989).","Since the total transfer is endogenous, there is a corollary that dirninishing returns apply to the transfer as well, due both to the opposition provoked by the transfer and to the demand this opposition exerts on resources to quiet it. This is at one level, a detail, which is the way Stigler treated it, but a detail with some important implications -- for entry into regulation, and for the price-output structure that emerges from regulation.","['Stigler does not himself formalize this model, and my first task will be to do just this.', ""The main task of the paper is to derive these implications from a generalization of Stigler's model.""]"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . A different view can be found in the 'Chicago school' of political economy (Stigler, 1971; Peltzman, 1976; #CITATION_TAG) and studies such as Wittman (1989).","The conceptual discussion is based on the analytical framework for change developed by Halinen, Salmi and Havila. This framework, compiling the mechanism, nature and forces of change in business networks, distinguishes between confined and connected change. It is suggested that mergers and acquisitions (M&As) may cause changes that spread in the business networks, and M&As are investigated as triggers of radical network change in particular.","['This paper investigates the spread of change in business networks with focus on critical events as triggers of radical change.', 'The paper includes an empirical analysis of M&As in the Nordic graphic industry.']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . The link between the political system of a country and its economic growth has attracted a great deal of attention in recent years (for an extensive survey of the literature see Alesina and Perotti 1994). Authors from Friedman (1962) to Scully (1988) have argued that politi-cally open societies grow at faster rates than societies where freedom is restricted. Others such as Kormendi and Meguire (1985) have found a negative relation-ship between civil liberty and growth, and Barro (1994) has argued that more political freedom may encourage a greater role for interest groups in the legisla-tive process, thereby retarding growth. Landell-Mills and Serageldin (1992) sum-marize this literature by concluding that benevolent dictators are rare and that democracies often resort to populist policies that are inimical to growth. The focus of this article is more narrow. We consider labor policies and, to the extent that they influence labor policies, policies affecting the openness of the economy. What, if any, is the link between such policies and the nature of the political regime? Many governments in developing countries have adopted labor policies--including high minimum wages, public sector overemployment, and job security guarantees--that benefit a small group of &quot;insiders. Other countries in the developing world have resisted the tempta For example, #CITATION_TAG show cross-country evidence supporting the view that authoritarian regimes are associated with higher trade protectionism (as well as greater labor market distortions).","&quot; But these same policies limit the opportunities of &quot;outsiders, &quot; thereby aggravating income inequality, generating efficiency losses, and possibly discouraging investment and growth.",['This article uses cross-country data to examine the link between a country&apos;s type of political regime and its degree of openness and labor market distortion.']
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . We study both the case of two ""classes"" (workers and capitalists) and the case of a continuum distribution of agents, characterized by different capital/labor shares. For example, it is shown that policies which maximize growth are optimal only for a government that cares only about the ""capitalists."" As a consequence, voting models predict that democracies tend to redistribute from the rich to the poor, and this effect will be stronger with higher income inequality as the middle-class has more incentives to form coalitions with the poor (see #CITATION_TAG; Persson and Tabellini, 1994).",,['This paper studies the relationship between political conflict and economic growth in a simple model of endogenous growth with distributive conflicts.']
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Four political systems and a qualitative index of political rights account for differences in political institutions. Pluralistic systems are associated with higher agricultural protection levels, although in a nonlinear fashion. All studies but one, exploit the cross-country variation in the data and find mixed and often weak evidence on the effect of democracy on agricultural protection (see #CITATION_TAG; Swinnen et al. 2000; Olper, 2001) 4.","The analysis incorporates the effects of development, of constraints on tax collection feasibility, and of comparative advantages and terms of trade.","['This paper analyzes the influence of political systems and rights in patterns of agricultural protection across commodities, countries, and over time.']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . The political economy framework for the comparative analysis of marketing channel dyads proposed by Stern and Reve (1980) focused mainly on relationships between channel members. Moreover, as the dependent variable displays a strong positive autocorrelation, we follow the most conservative method of estimating standard errors also by clustering at the country level, allowing arbitrary country-specific serial correlation (see #CITATION_TAG).",,"['This article extends the political economy framework by indicating how environmental factors (i.e., factors external to a dyad) might influence and affect the structure and processes of the dyad.']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Over the past 20 years, an insurance-inflected discourse has migrated from the purely financial side of the health system into the heart of traditional medicine - the doctor-patient relationship. Over the same period, the body of law that structures most private group health insurance - ERISA - has effectively delegated control of risk pooling and resource allocation to the employers that sponsor group plans. The discourse of managing risk bonds these two components of health law and the health care system: patient care and access to coverage. From a normative perspective, the greatest problem with risk-centered governance arises from a democracy deficit. Because almost all health insurance risk pools are based in workplaces, there is potential to draw on the social networks created by work as a mechanism for building new, localized publics engaged with health policy. Treating insurance risk pools as potential mechanisms of governance, rather than merely as actuarial units, would force the publicizing (at least within the workplace) of myriad political decisions: who gets included and excluded in the pooling process, how allocation decisions are made, and whether there are systems of accountability and checks and balances sufficient to produce a risk allocation system that is equitable, as well as efficient and flexible. For example, #CITATION_TAG, using panel data, find that health policy interventions are superior in democracies.","Drawing on a history of ERISA that has not been explored in legal scholarship, I demonstrate how the private welfare state of workplace-based health insurance has evolved into the creation of what amounts to corporate sovereignty in controlling access to health coverage.","['I argue in this article that the concept of risk-centered governance is the best theoretical paradigm for understanding health law and the health care system.', 'Rather than focus on doctrinal strands, I argue that scholars should analyze the law of health care as a set of governance practices organized around managing and allocating financial, as well as clinical, risk.', 'The article builds on the egalitarian potential of social insurance as a technology of governance, and argues for filling a gap that exists not only in the current system, but also in all proposals for reform']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Leadership turnover is managed by a selectorate - a group of individuals on whom the leader depends to hold onto power. This requires that the selectorate's hold on power is not too dependent on a specific leader being in office. A puzzling result of our study is the asymmetric effect of regime change on the level of protection: why should a regime change be relevant only for a transition to democracy, and not vice versa? One possible explanation of such asymmetric effect of transitions to democracy and autocracy could be based on theories explaining (lack of) leadership turnovers and economic performance under autocracies (see #CITATION_TAG; Acemoglu et al. 2004).",The paper develops a simple theoretical model of accountability in the absence of regularized elections. Good policy is institutionalized when the selectorate removes poorly performing leaders from office. We use these case studies to identify the selectorate in specific instances of successful autocracy.,"['One of the key goals of political economy is to understand how institutional arrangements shape policy outcomes.', 'This paper studies a comparatively neglected aspect of this - the forces that shape heterogeneous performance of autocracies.', 'The paper looks empirically at spells of autocracy to establish cases where it has been successful according to various objective criteria.']"
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Drawing from economic and cognitive theories, researchers have argued that firms within an industry tend to cluster together, following similar strategies. Their positioning in strategic groups, in turn, is argued to influence firm actions and firm performance. As such we follow the approach discussed by Smith and Todd (2005) and Abadie (2005) and applied by #CITATION_TAG and Persson and Tabellini (2008).","Instead, strategic groups represent a range of viable strategic positions firms may stake out and use as reference points.",['We extend this research to examine performance implications of competitive positioning not just among but also within groups.']
"This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Changes within networks receive less research attention, although considerable research exists on explaining business network structures in different research traditions. So far, the relevant literature discusses network pictures mainly as a theoretical concept. Furthermore, the direction of causation is hard to establish (see #CITATION_TAG; Gundlach and Paldam, 2009).","The study is exploratory or iterative in the sense that revisions occur to the research question, method, theory, and context as an integral part of the research process. The study develops a concept of network change as well as an operationalization for comparing perceptions of change, where the study introduces a template model of dottograms to systematically analyze differences in perceptions. The study then applies the model to analyze findings from a case study of Norwegian/Japanese seafood distribution, and the chapter provides a rich description of a complex system facing considerable pressure to change. In-depth personal interviews and cognitive mapping techniques are the main research tools applied, in addition to tracer studies and personal observation. The dottogram method represents a valuable contribution to case study research as it enables systematic within-case and across-case analyses.","['The study here examines how business actors adapt to changes in networks by analyzing their perceptions or their network pictures.', 'This study analyzes changes in networks in terms of the industrial network approach.', 'This approach sees networks as connected relationships between actors, where interdependent companies interact based on their sensemaking of their relevant network environment.', ""Another major contribution of the study is the analysis of the role that network pictures play in actors' efforts to change their network position."", 'The study develops seven propositions in an attempt to describe the role of network pictures in network change.']"
"The DEM profile of flaring plasmas generally exhibits a double peak distribution in temperature, with a cold component around log T [?] However, the M7.7 flare on 19 July 2012 poses a very intriguing violation of this paradigm: the temperature decreases with altitude from the tip of the cusp toward the top of the arcade; the hottest region is slightly above the X-ray loop-top source that is For the past twenty years, drawing on the Industrial Network Approach, Industrial Marketing and Purchasing Group researchers have been trying to get a better understanding of organisational networks related issues. Researchers frequently highlight that whatever the researched phenomena, it is important to consider actors' subjective views of the world. The concept of Network Pictures as introduced in the IMP (Industrial Marketing and Purchasing) body of literature by Ford et al. (2002b), refers to those subjective views and despite its recognised importance no in-depth research had been conducted so far on the concept which has thus remained blurred. Ford et al. Here the events of interest are roughly categorized as standard or non-standard flares, depending on whether they were similar to the prototypical Tsuneta flare (#CITATION_TAG).",The concept's theoretical foundations are uncovered by reviewing some principles from Sense-Making Theory. The method consisted of operationalising the construct of Network Pictures and then testing it in two different network contexts to see if it was usable and useful for carrying out research in organisational networks.,"['(2002b) brought in this concept to emphasise that the network is in fact a varying thing depending on what people see.', ""The question is whether this can be translated into a research device, so that researchers may see in a structured and analytical way what an actor's picture is."", ""The relevance of actors' views to obtain a clearer understanding of organisational networks is highlighted when the relation that is believed to exist between those views and action in organisational networks is addressed."", 'With the aim of developing Network Pictures as research tool a two-stage method is put forward and carried out.']"
"This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. Location analysis is concerned with locating one or more service facilities while fulfilling some constraints such as the demand of the customers and minimizing the total cost. Despite the cost of transporting goods or services, there is a fixed cost associated with opening a given facility such as the cost of the land, taxes or trunking (or hauling) cost to supply product, services and labour. This cost may vary from one area to another. Simulated Annealing is one of the meta-heuristic methods derived from the annealing process of a solid. #CITATION_TAG assume that fixed costs are zone-dependent, where zones are non-overlapping convex polygons.","Several parameters in SA will be tested such as initial starting points, initial temperature and cooling schedules. The problems of locating 2 to 15 facilities are solved by using C++.",['The aim of this study is to put forward a Simulated Annealing (SA) procedure for solving the uncapacitated continuous location-allocation problem in the presence of a zone-dependent fixed cost.']
This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. #CITATION_TAG and Salhi (2007) deal with the Euclidean CMSWP by proposing a perturbation-based heuristic,This procedure is based  on an effective use of borderline customers. Several implementations are considered and the two most appropriate are then computationally enhanced by using a reduced neighbourhood when solving the transportation problem.,['This paper proposes a perturbation-based heuristic for the capacitated multisource Weber problem.']
"This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. Reviews the research work and conceptual development of the International Marketing and Purchasing (IMP) group into the nature of buyer-seller relationships which has evolved during the past 20 years. The themes of interaction, relationships and networks encapsulate the major research thrusts of this group and underlie much of the contemporary academic research in Europe. For each cluster of customers, the location of its facility is found using the Weiszfeld algorithm (#CITATION_TAG).","Addresses these themes, which represent the major phases of challenging conceptual and empirical research with which the IMP group has been concerned since its inception in 1976.",['Aims to show the development process of the IMP research and to integrate some of its various themes and findings.']
"This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. In this paper we consider the location of stops along the edges of an already existing public transportation network. This can be the introduction of bus stops along some given bus routes, or of railway stations along the tracks in a railway network. Unfortunately, this problem is NP-hard, even if only the Euclidean distance is used. The Alternating Transportation-Location (ATL) method of #CITATION_TAG takes a set of M open facilities as input.","In this paper, we give a reduction to a finite candidate set leading to a discrete set covering problem. Moreover, we identify network structures in which the coefficient matrix of the resulting set covering problem is totally unimodular, and use this result to derive efficient solution approaches. Various extensions of the problem are also discussed","['The goal is to cover all given demand points with a minimal amount of additional traveling time, where covering may be defined with respect to an arbitrary norm (or even a gauge).']"
"This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. Solution techniques for location-allocation problems usually are not a part of microcomputer-based geoprocessing systems because of the large volumes of data to process and store and the complexity of algorithms. In Type II and Type III, the fixed costs were generated from discrete uniform distributions in the range of [2, #CITATION_TAG] for 50 fixed points, [50, 400] for 287 fixed points, [1000, 10000] for 654 fixed points, and [10000, 100000] for 1060 fixed points, see Luis (2008) for more details.","The strategies used, preprocessing interpoint distance data as both candidate and demand strings, and use of them to update an allocation table, allow the solution of large problems (3000 nodes) in a microcomputer-based, interactive decisionmaking environment. Moreover, these strategies yield solution times which increase approximately linearly with problem size.","['In this paper, it is shown that processing costs for the most accurate, heuristic, location-allocation algorithm can be drastically reduced by exploiting the spatial structure of location-allocation problems.']"
"This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. We do not look at the MSWP without fixed costs, but refer the reader to #CITATION_TAG.",The new solution methods are  discussed for both the well known multisource Weber problem and its counterpart the capacitated case.,"['In this survey, we examine an important class of facility location problems known as the multisource Weber  problem (also referred to as the continuous location-allocation problem).']"
"Securitization makes mortgage-related risks internationally tradeable and thus contributes considerably to the international diversification of macroeconomic risk: in the years 2003-2008, the increase in international cross-holdings of securitized mortgage debt has lowered industrialized countries' conditional consumption volatility (relative to the United States) by about 10-15 percentage points. Domestic credit leads to better international risk sharing only if debt is securitized and traded internationally. Conversely, the risk-sharing benefits from securitization seem to evaporate if credit dries up -as it did in the recent financial crisis. La titrisation fait que les risques attaches aux hypotheques peuventetre transiges internationalement. Voila qui contribue de maniere significativea la diversification internationale des risques macroeconomiques : dans les annees 2003-2008, l'accroissement dans la dette hypothecaire titrisee detenue dans d'autres pays a reduit la volatilite de la consommation relative (par rapport auxEtats-Unis) des pays industrialises de 10-15 points de pourcentage. On examine le role du credit domestique dans l'explication de ce resultat. On montre que le credit domestique entraine un meilleur partage international du risque seulement An earlier version of this paper was circulated under the title 'Home bias: asset prices, securitization of mortgage debt and international risk sharing.' We have benefited from remarks by seminar participants at the ZEW-Bundesbank Workshop on Housing and Asset Markets, the The securitization of mortgage-related debt has played a major role in the emergence and proliferation of the current financial crisis (see #CITATION_TAG for a detailed account).","Starting with the trends leading up to the crisis, I explain how these events unfolded and how four different amplification mechanisms magnified losses in the mortgage market into large dislocations and turmoil in financial markets.",['This paper summarizes and explains the main events of the liquidity and credit crunch in 2007-08.']
To assess potential public health impacts of changes to indoor air quality and temperature due to energy efficiency retrofits in English dwellings to meet 2030 carbon reduction targets. The only guidance that exists relates to the replacement of existing window trickle vents #CITATION_TAG,It examines the longitudinal structure of industry strategic groups and identifies the strategic patterns followed by these strategic groups over time. the mobility rates of firms between strategic groups are also assessed. dramatic and concerted change) satisfactorily models the processes of strategic group change.,['This article focuses upon dynamic aspects of strategic groups in the context of the US insurance industry from 1970-84.']
To assess potential public health impacts of changes to indoor air quality and temperature due to energy efficiency retrofits in English dwellings to meet 2030 carbon reduction targets. #CITATION_TAG Occupant ventilation practices have also been shown to be counter-productive to creating a healthy indoor environment.,"A total of 198 cases (with at least two of three symptoms: wheezing, rhinitis, eczema) and 202 healthy controls, living in 390 homes, were examined by physicians. Ventilation rates were measured by a passive tracer gas method, and inspections were carried out in the homes. Families with allergic children should be given the advice to have good ventilation in the home. In investigations, of associations between environmental factors and allergies, the air change rate in homes has to be considered.",['The aim of the study was to test the hypothesis that a low-ventilation rate in homes is associated with an increased prevalence of asthma and allergic symptoms among children.']
"To assess potential public health impacts of changes to indoor air quality and temperature due to energy efficiency retrofits in English dwellings to meet 2030 carbon reduction targets. We model the effect in 2030 of policies that aim to reduce total carbon dioxide (CO(2)) emissions by 50% by 2050 globally compared with the effect of emissions in 1990. Changes in modes of production of electricity to reduce CO(2) emissions would, in all regions, reduce PM(2.5) and deaths caused by it, with the greatest effect in India and the smallest in the EU. Health benefits greatly offset costs of greenhouse-gas mitigation, especially in India where pollution is high and costs of mitigation are low. #CITATION_TAG This would further tip the balance towards installing mitigating ventilation systems so as to dilute 'stale' indoor air.","We use three models: the POLES model, which identifies the distribution of production modes that give the desired CO(2) reductions and associated costs; the GAINS model, which estimates fine particulate matter with aerodynamic diameter 2.5 microm or less (PM(2.5)) concentrations; and a model to estimate the effect of PM(2.5) on mortality on the basis of the WHO's Comparative Risk Assessment methods.","['In this report, the third in this Series on health and climate change, we assess the changes in particle air pollution emissions and consequent effects on health that are likely to result from greenhouse-gas mitigation measures in the electricity generation sector in the European Union (EU), China, and India.']"
"To assess potential public health impacts of changes to indoor air quality and temperature due to energy efficiency retrofits in English dwellings to meet 2030 carbon reduction targets. Existing approaches to segmentation, in particular business segmentation, are often conceptualized and applied in a limiting way, providing a narrow interpretation of the surrounding business network. This could be attributed to a rather myopic view of the multiple complexities and indirect links inherent in networks of business exchange relationships. As such, the managerial challenge becomes one of creating a business network segmentation from the perspective of a focal company within this network, taking a far wider interpretation of the concept of segmentation. 2 Housing is responsible for one-quarter of total UK CO 2 emissions #CITATION_TAG and 52% of this is from space heating.","Using the concept of network pictures, we outline the different dimensions that are important within a business network segmentation, and exemplify their use through a case study of an entrepreneurial company.","[""The task for companies developing innovative segmentation approaches is to simultaneously enhance the company's understanding of downstream as well as upstream preferences and resources, while going beyond immediate interaction partners to include relevant indirect business partners."", 'The challenge is therefore not to identify attractive customer segments, but attractive network segments.']"
"To assess potential public health impacts of changes to indoor air quality and temperature due to energy efficiency retrofits in English dwellings to meet 2030 carbon reduction targets. A survey of dwellings in the Warm Front home energy efficiency scheme was carried out in five urban areas of England. #CITATION_TAG The SIT is a measure of the thermal condition of the dwelling ranked against all other dwellings, and is a function of the dwelling's energy and ventilation performance.",Warm Front energy efficiency improvements lead to substantial improvements of both living room and bedroom temperatures which are likely to have benefits in terms of thermal comfort and well-being.,['The objective of this study is to quantify the extent to which variation in heating season indoor temperatures is explained by dwelling and household characteristics and increased by energy efficiency improvements in low income households.']
"The growth in computer games and wireless networks has catalyzed the production of a new generation of hand-held game consoles that support multi-player gaming over IEEE 802.11 networks. Understanding the traffic characteristics of network games running on these new hand-helds is important for building traffic models and adequately planning wireless network infrastructures to meet future demand. This paper examines the traffic characteristics of IEEE 802.11 network games on the Nintendo DS and the Sony PSP. In addition, the games and hand-held platforms differ in their ability to handle degraded wireless network conditions and in the amount of broadcast traffic sent. Wireless Local Area Networks (WLANs) are now common on academic and corporate campuses. As ``Wi-Fi\u27\u27 technology becomes ubiquitous, it is increasingly important to understand trends in the usage of these networks. Most calls last less than a minute. Furthermore, growth in WLAN deployment at universities [#CITATION_TAG] and even in entire towns [5] increases the likelihood that concurrent wireless hosts compete on the same wireless channels.","We employ several measurement techniques, including syslogs, telephone records, SNMP polling and tcpdump packet sniffing. We compare this trace to a trace taken after the network\u27s initial deployment two years ago. We define a new metric for mobility, the ``session diameter.\u27\u27 We use this metric to show that embedded devices have different mobility characteristics than laptops, and travel further and roam to more access points.","['This paper analyzes an extensive network trace from a mature 802.11 WLAN, including more than 550 access points and 7000 users over seventeen weeks.', 'This is the largest WLAN study to date, and the first to look at a large, mature WLAN and consider geographic mobility.']"
"The growth in computer games and wireless networks has catalyzed the production of a new generation of hand-held game consoles that support multi-player gaming over IEEE 802.11 networks. Understanding the traffic characteristics of network games running on these new hand-helds is important for building traffic models and adequately planning wireless network infrastructures to meet future demand. This paper examines the traffic characteristics of IEEE 802.11 network games on the Nintendo DS and the Sony PSP. In addition, the games and hand-held platforms differ in their ability to handle degraded wireless network conditions and in the amount of broadcast traffic sent. The traffic generated by one host on a WLAN can have dramatic impact on the performance of other hosts on the WLAN [1, #CITATION_TAG].",,['This paper investigates whether the industrial relations climate in Indian States has affected the pattern of manufacturing growth in the period 1958-92.']
"The growth in computer games and wireless networks has catalyzed the production of a new generation of hand-held game consoles that support multi-player gaming over IEEE 802.11 networks. Understanding the traffic characteristics of network games running on these new hand-helds is important for building traffic models and adequately planning wireless network infrastructures to meet future demand. This paper examines the traffic characteristics of IEEE 802.11 network games on the Nintendo DS and the Sony PSP. In addition, the games and hand-held platforms differ in their ability to handle degraded wireless network conditions and in the amount of broadcast traffic sent. There have been numerous studies of traffic models for popular PC games [2, 3, 6, 10, 12] and even game consoles [#CITATION_TAG].",It also enables other researchers to use a realistic Halo 2 traffic model in network simulations.,"['This paper analyses the traffic characteristics of, and proposes a traffic model for, the Xbox game Halo 2.', 'Our goal is to help players and network providers to estimate the amount of traffic caused by the game and the impact on access links or provider networks.', 'We focus on the following characteristics: bandwidth, packet rate and distribution of packet inter-arrival times and packet lengths.']"
"The growth in computer games and wireless networks has catalyzed the production of a new generation of hand-held game consoles that support multi-player gaming over IEEE 802.11 networks. Understanding the traffic characteristics of network games running on these new hand-helds is important for building traffic models and adequately planning wireless network infrastructures to meet future demand. This paper examines the traffic characteristics of IEEE 802.11 network games on the Nintendo DS and the Sony PSP. In addition, the games and hand-held platforms differ in their ability to handle degraded wireless network conditions and in the amount of broadcast traffic sent. Employment laws in India and Zimbabwe require employers to obtain permission from the government to retrench or lay off workers. However, in both countries a substantial decline in the demand for employees (other things equal) followed the new legislation. In Zimbabwe it is difficult to be precise about a causal connection between the drop in the demand for labor (allowing for concurrent increased wages) and the new legislation because enactment occurred simultaneously with Independence; however, the current economic climate induced high levels of investment in capital but not investments in long-term commitments to employees. Upon achieving independence in 1980, the government of Zimbabwe passed a new Employment Act, requiring employers to obtain permission from the Ministry of Labor to fire or lay off workers. Comparable regulations were imposed in India by the Industrial Disputes (Amendment) Act of 1976, requiring that written permission be obtained, normally from the relevant state government, either to close a plant or to retrench workers. Any addition to economic security in the lives of workers is clearly a laudable goal in its own right. But the question addressed in this article is whether these particular job security regulations have had undesirable side effects, which may even have thwarted the original goals of the legislation. The traffic generated by one host on a WLAN can have dramatic impact on the performance of other hosts on the WLAN [#CITATION_TAG, 8].",,['The immediate goal of these items of legislation was to protect the livelihood of workers and to maintain jobs.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Key account management (KAM) is a rapidly growing area of interest in business-  to-business marketing. However, unnoticed by marketing, a quiet revolution has  taken place in supply chain management (SCM), where the traditional emphasis on  least-cost transactions has given way to a focus on long-term relationships with  a few key suppliers. Management decision-making in this context is affected by managers' interpretations of the available information at a particular point in time (#CITATION_TAG) also referred to as sensemaking (Weick, 1979 (Weick,, 1995.",A detailed case study of a long-term relationship  between a business-to-business services provider and a key customer in the  construction industry suggests there is a definable overlap.,['This article uses a cross-disciplinary approach to explore whether  these developments from the field of SCM provide insights into key business-to-  business relationships.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. In the ""stagnationist"" macroeconomics, the long-term problem of growth and distribution has been discussed in a short-period Kaleckian framework which abstracts from labor market-output market interaction to determine real wages. However, even in this framework, one can challenge the stagnationist conclusion that there is no trade-off between wage and profit and between distribution and growth. Evidently, in a general framework, there is no unambiguous relationship between distribution and growth. Thus, stability is a prerequisite for change (Lundgren, 1992) and an inherent feature of a network (#CITATION_TAG).",So distributive justice should be taken as a separate objective on its own merit.,"['One way of doing this is to bring in the role of foreign price competition.', 'Another way is to allow the possibility of saving out of wage income in a closed ""stagnationist"" framework presented here.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Few supported employment programmes have been specifically designed for people with autism, especially those who are more able. This study examines the outcome of a supported employment service (NAS Prospects) for adults with autism or Asperger syndrome (IQ 60+) over an 8 year period. Individuals supported by Prospects show a rise in salaries, contribute more tax and claim fewer benefits. Although the programme continues to incur a financial deficit, this has decreased. Moreover, there are many non-financial benefits, which are difficult to quantify. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003; Porter, 1985) or delineated by a shared understanding of different companies (#CITATION_TAG; Reger & Palmer, 1996).",,['The importance of specialist employment support of this kind is discussed.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. BACKGROUND Sickness absence costs the UK economy around PS20 billion per year. Senior personnel changes in organisations; shifts in organisational structures; changes in strategy; as well as acquisitions, mergers and bankruptcies can all cause such change (Halinen et al., 1999; #CITATION_TAG).","METHODS A Markov model was developed to assess the cost-effectiveness of three interventions: a workplace intervention; a physical activity and education intervention and a physical activity, education and workplace visit intervention.","['This study aims to assess the cost-effectiveness of interventions to return employees with musculoskeletal disorders to work, one of the major causes of long-term sickness absence, using a mathematical model.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. It raises questions about the optimal nature and organization of employment support for this service user group. Following the argument that managers act on the subjective interpretations of their environment (Borders, Johnston, & Rigdon, 2001; Ford et al., 2003), which can be related to managerial research by Weick (1979 Weick (, 1995 (see also Möller, 2010) as well as sociopsychological theories of cognitive cycles (#CITATION_TAG (Neisser,, 1976, network change must be seen in relation to how actors perceive their network.",Methods: Service use and frequency were measured at baseline and 12 months. Comparisons paid particular attention to the differences between people entering work and those who remained unemployed. Costs were analysed from a government perspective (excluding earnings) and a societal perspective (excluding welfare benefits and taxes).,"['Aims: To explore the impact of moving into employment on service use, earnings, benefits and tax allowances claimed.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. There have been few trials in other parts of the world. Implementation of IPS can be challenging in the UK context where IPS is not structurally integrated with mental health services, and economic disincentives may lead to lower levels of motivation in individuals with severe mental illness and psychiatric professionals. While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983; #CITATION_TAG; Stern & Reve, 1980).",Method Individuals with severe mental illness in South London were randomised to IPS or local traditional vocational services (treatment as usual) (ISRCTN96677673).,['Aims To investigate the effectiveness and cost-effectiveness of IPS in the UK.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. The rapid growth of supported employment programs in the last five years has been accompanied by the strong belief that through such programs many people with severe disabilities would be more satisfied with their lives and receive higher earnings than in sheltered workshops, work activity centers, and adult day care. Following the argument that managers act on the subjective interpretations of their environment (#CITATION_TAG; Ford et al., 2003), which can be related to managerial research by Weick (1979 Weick (, 1995 (see also Möller, 2010) as well as sociopsychological theories of cognitive cycles (Neisser, 1967 (Neisser,, 1976, network change must be seen in relation to how actors perceive their network.",,['This paper presents data collected in New York State that test these propositions and compare the benefits with the costs of operating the supported employment program.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. The paper reviews what is known about outcome in adult life for more able individuals within the autistic spectrum. An initial study of five exporters and seven importers was undertaken in 2006 (Appendix A lists the companies involved) as part of a larger study (#CITATION_TAG).","The stability of IQ and other measures over time, and variables related to outcome, are also investigated.","['The review focuses predominantly on long-term follow-up research and covers outcome in terms of cognitive, linguistic, academic and adaptive functioning; educational and employment history; independence and social relationships; and behavioural and psychiatric problems.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Business networks have also been characterised in strategic marketing as value-creating systems (#CITATION_TAG) where companies co-operate in order to develop value for customers and simultaneously compete to appropriate value (Möller & Svahn, 2006; Normann & Ramirez, 1993), and marketing alliances where firms cooperate based on formalised and collaborative agreements (Das, Sen, & Sengupta, 1998; Gulati, Nohria, & Zaheer, 2000).","METHODS Based on their predominant work activity over the study period, participants were classified into two groups: supported employment and unemployed.","['BACKGROUND The purpose of this study was to examine the effects of a supported employment programme on measures of executive functions for 44 adults with autism, assessed at the beginning and the end of the programme period.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. In financially constrained health systems across the world, increasing emphasis is being placed on the ability to demonstrate that health care interventions are not only effective, but also cost-effective. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003; #CITATION_TAG) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001; Reger & Palmer, 1996).","Particular emphasis is placed on the importance of the appropriate representation of uncertainty in the evaluative process and the implication this uncertainty has for decision making and the need for future research. This highly practical guide takes the reader through the key principles and approaches of modelling techniques. It begins with the basics of constructing different forms of the model, the population of the model with input parameter estimates, analysis of the results, and progression to the holistic view of models as a valuable tool for informing future research exercises. ABOUT THE SERIES: Economic evaluation of health interventions is a growing specialist field, and this series of practical handbooks will tackle, in-depth, topics superficially addressed in more general health economics books. Each volume will include illustrative material, case histories and worked examples to encourage the reader to apply the methods discussed, with supporting material provided online.","['This book deals with decision modelling techniques that can be used to estimate the value for money of various interventions including medical devices, surgical procedures, diagnostic technologies, and pharmaceuticals.', 'This book will help analysts understand the contribution of decision-analytic modelling to the evaluation of health care programmes.', 'This series is aimed at health economists in academia, the pharmaceutical industry and the health sector, those on advanced health economics courses, and health researchers in associated fields.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. For the past twenty years, drawing on the Industrial Network Approach, Industrial Marketing and Purchasing Group researchers have been trying to get a better understanding of organisational networks related issues. Researchers frequently highlight that whatever the researched phenomena, it is important to consider actors' subjective views of the world. The concept of Network Pictures as introduced in the IMP (Industrial Marketing and Purchasing) body of literature by Ford et al. (2002b), refers to those subjective views and despite its recognised importance no in-depth research had been conducted so far on the concept which has thus remained blurred. Ford et al. Conversely, Ford et al. (2003) and #CITATION_TAG suggest that network pictures, although based on individual managers' sensemaking, can be integrated by researchers to form a broad picture.",The concept's theoretical foundations are uncovered by reviewing some principles from Sense-Making Theory. The method consisted of operationalising the construct of Network Pictures and then testing it in two different network contexts to see if it was usable and useful for carrying out research in organisational networks.,"['(2002b) brought in this concept to emphasise that the network is in fact a varying thing depending on what people see.', ""The question is whether this can be translated into a research device, so that researchers may see in a structured and analytical way what an actor's picture is."", ""The relevance of actors' views to obtain a clearer understanding of organisational networks is highlighted when the relation that is believed to exist between those views and action in organisational networks is addressed."", 'With the aim of developing Network Pictures as research tool a two-stage method is put forward and carried out.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Recent research shows increasing interest in the concept of network pictures (Henneberg, Mouzas, & Naudé, 2009; Henneberg, Naudé, & Mouzas, 2010; Leek & Mason, 2010; #CITATION_TAG).","Using qualitative methodologies including in-depth interviews and network mapping, the study reveals practitioners' network size and variety of contacts, and their role in client acquisition and retention.","['The focus of the study is public relations practitioners operating in seven consultancies across the UK, in Manchester, London, Yorkshire and Cheshire.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. A disability is a natural part of human experience that should not inhibit one's ability to experience independence, equal opportunity and economic self-sufficiency. It is not known how caregivers of adults with a specific disability, an autism spectrum disorder, would frame these opportunities for their children living in the USA. For instance, resource dependencies, high switching costs and risk-reducing strategies favour stability (#CITATION_TAG).",,"['This survey explored the needs of 143 families supporting an adult with autism and the opportunities afforded them in socialization, employment and residential living.']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. High levels of unemployment among persons with mental illness are a significant social disability. Evidence outside North America is more limited. Systematic review of studies of the effectiveness of IPS conducted principally in the United Kingdom. As such, the concept of network pictures is influenced by, and related to, the research themes of cognitive strategic groups (Osborne et al., 2001; Porac, Thomas, & Baden-Fuller, 1989) cognitive scripts and cognitive mapping (Fiol & Huff, 1992; Johnson, Daniels, & Asch, 1998), and managerial cognition/sensemaking in organisations (Colville & Pye, 2010; Daft & Weick, 1984; #CITATION_TAG).",Methods.,['To examine the evidence for the effectiveness of the IPS model of supported employment within the United Kingdom.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Previous studies have determined that information technology dominates numerous e-government projects; information and communications technology has been used mainly as a tool for enhancing the efficiency and service delivery of governments. Electronic government(e-government) should achieve public innovation goals, such as redesigning information relationships among stakeholders, enhancing citizen participation in the policymaking process, and reinforcing policy enforcement to create public value. Dubois and Gadde (2002) for instance discuss a number of limitations: first, case studies are seen to provide little basis for scientific generalization (Weick, 1979; #CITATION_TAG).","In this study, service design tools were combined with a qualitative research method, and observation and individual interviews of participants were conducted to record their perceptions of the tax service process.","['Therefore, this study focused on a crucial e-government service, the Taiwan taxation service, to determine whether, in the current era in which people depend highly on network tools to send and receive information, online services are suitable for taxpayers and how to improve the service process.', 'The taxation authority should integrate all online taxation services to achieve the expected public service (one-stop e-government window).', 'This research facilitates relevant government agencies to provide effective e-government services, identify problems, and modify service delivery processes']"
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. This article conceptualizes community cultural wealth as a critical race theory (CRT) challenge to traditional interpretations of cultural capital. Various forms of capital nurtured through cultural wealth include aspirational, navigational, social, linguistic, familial and resistant capital. These forms of capital draw on the knowledges Students of Color bring with them from their homes and communities into the classroom. Recent research has stressed the importance of network structures in understanding business exchanges (#CITATION_TAG; Möller & Rajala, 2007).","CRT shifts the research lens away from a deficit view of Communities of Color as places full of cultural poverty disadvantages, and instead focuses on and learns from the array of cultural knowledge, skills, abilities and contacts possessed by socially marginalized groups that often go unrecognized and unacknowledged.",['This CRT approach to education involves a commitment to develop schools that acknowledge the multiple strengths of Communities of Color in order to serve a larger purpose of struggle toward social and racial justice.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Metaphoric frames are prominently featured in public discourse. As such, network pictures are akin to metaphors: they are rich sensemaking devices in their own right (#CITATION_TAG).","They highlight certain aspects of the target issues they are used to describe, thereby encouraging specific patterns of inference. Building off prior work, we contrasted two metaphors for crime: virus and beast. In a pilot study, we identified specific causes, examples, and solutions to crime that were congruent with each frame (one but not the other; e.g., people thought ""drug use"" better exemplified a crime virus, whereas ""murder"" better exemplified a crime beast). Participants (n = 469) read or listened to a short metaphorically-framed crime report, completed a filler task, and were prompted for the information they had seen/heard.",['Our goal was to test whether they would influence memory as well.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Empirical studies from management science focus on network types and organizational fit. Novel planning algorithms and innovative coordination schemes are developed mostly in the field of operations research in order to propose new software features. Operations and production management realize cost-benefit analysis of IT software implementations. The success of software solutions for network coordination depends strongly on the fit of three dimensions: network configuration, coordination scheme and software functionality. Change may be seen as an evolutionary process, where revolution is possible but unusual (#CITATION_TAG), similar to a continuous process where stable periods are broken by radical changes (Halinen et al., 1999).",The classification encompasses criteria related to research methodology and content.,['This paper groups recent supply chain management research focused on organizational design and its software support.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Core curriculum and multicultural education are two major approaches advocated in the current school reform movement. This article argues that neither of these approaches adequately addresses the problem of those minority groups who have not traditionally done well in the public school. Core curriculum advocates falsely assume that as a result of instituting a core curriculum, demanding higher standards, and patching up supposed individual deficiencies, all students will perform as expected. Multicultural education advocates inadequately design their program to focus on cultural differences in content and form. Minorities whose cultural frames of reference are oppositional to the cultural frame of reference of American mainstream culture have greater difficulty crossing cultural boundaries at school to learn. Core curriculum and multicultural advocates have yet to understand and take this into account. Second, case studies are often rich descriptions of events without clear analytical framing; they at best only partially support quasi-deductive theories, and they suggest some notion of statistical generalization where multiple case studies are used (#CITATION_TAG).",,['This article contends that the crucial issue in cultural diversity and learning is the relationship between the minority cultures and the American mainstream culture.']
"The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Existing approaches to segmentation, in particular business segmentation, are often conceptualized and applied in a limiting way, providing a narrow interpretation of the surrounding business network. This could be attributed to a rather myopic view of the multiple complexities and indirect links inherent in networks of business exchange relationships. As such, the managerial challenge becomes one of creating a business network segmentation from the perspective of a focal company within this network, taking a far wider interpretation of the concept of segmentation. Recent research shows increasing interest in the concept of network pictures (Henneberg, Mouzas, & Naudé, 2009; #CITATION_TAG; Leek & Mason, 2010; Tonge, 2010).","Using the concept of network pictures, we outline the different dimensions that are important within a business network segmentation, and exemplify their use through a case study of an entrepreneurial company.","[""The task for companies developing innovative segmentation approaches is to simultaneously enhance the company's understanding of downstream as well as upstream preferences and resources, while going beyond immediate interaction partners to include relevant indirect business partners."", 'The challenge is therefore not to identify attractive customer segments, but attractive network segments.']"
"While it finds no empirical basis for this orthodox standpoint it observes that long-term unemployment dampens aggregate production which in turn aggravates unemployment problem. This paper analysed the OECD data on employment protection for OECD countries over the time span 1990-2008 on the basis of alternative dynamic panel data models and panel causality tests and examines the validity of the neo-liberal argument that strictness of employment protection hurts labour through increased long-term and youth unemployment rates. In the context of India, an influential study was conducted by #CITATION_TAG.",,['This paper investigates whether the industrial relations climate in Indian States has affected the pattern of manufacturing growth in the period 1958-92.']
"While it finds no empirical basis for this orthodox standpoint it observes that long-term unemployment dampens aggregate production which in turn aggravates unemployment problem. This paper analysed the OECD data on employment protection for OECD countries over the time span 1990-2008 on the basis of alternative dynamic panel data models and panel causality tests and examines the validity of the neo-liberal argument that strictness of employment protection hurts labour through increased long-term and youth unemployment rates. In recent years, comparative economics experienced a revival, with a new focus on comparing capitalist economies. In the late 1990s La Porta and his collaborators (La Porta et al., 1997, 1998, 1999, 2000 2006, 2008 #CITATION_TAG; Shleifer, 2002, 2003; Beck et al., 2003a Beck et al.,, 2003b Botero et al., 2004) set in motion a series of systematic analysis of the relationships between legal and economic variables.","The authors argue that, to understand capitalist institutions, one needs to understand the basic tradeoff between the costs of disorder and those of dictatorship. They then apply this logic to study the structure of efficient institutions, the consequences of colonial transplantation, and the politics of institutional choice.Labor Policies,Decentralization,National Governance,Environmental Economics&Policies,Economic Theory&Research,National Governance,Environmental Economics&Policies,Economic Theory&Research,Governance Indicators,Banks&Banking Reform",['The theme of the new research is that institutions exert a profound influence on economic development.']
"While it finds no empirical basis for this orthodox standpoint it observes that long-term unemployment dampens aggregate production which in turn aggravates unemployment problem. This paper analysed the OECD data on employment protection for OECD countries over the time span 1990-2008 on the basis of alternative dynamic panel data models and panel causality tests and examines the validity of the neo-liberal argument that strictness of employment protection hurts labour through increased long-term and youth unemployment rates. Any issue that arises, or can be recast as a matter of contracting, is usefully examined in terms of transaction costs. Transaction cost economics maintains that governance of contractual relations is mainly achieved through institutions of private ordering instead of legal centralism. The economics of organization in presented in terms of transaction costs, showing that hierarchy also serves efficiency and permits a variety of predictions about the organization of work. In the late 1990s La Porta and his collaborators (La Porta et al., 1997, 1998, 1999, 2000 2006, 2008 Djankov et al., 2003; Shleifer, 2002, 2003; #CITATION_TAG Beck et al.,, 2003b Botero et al., 2004) set in motion a series of systematic analysis of the relationships between legal and economic variables.","It sets out the basic principles of transaction cost economics, applies the basic arguments to economic institutions, and develops public policy implications. The book first summarizes the transaction cost economics approach to the study of economic organization. It develops the underlying behavioral assumptions and the types of transactions; alternative approaches to the world of contracts are presented. Assuming that firms are best regarded as a governance structure, a comparative institutional approach to the governance of contractual relations is set out. The evidence, theory, and policy of vertical integration are discussed, on the basis that the decision to integrate is paradigmatic to transaction cost analysis. The incentives and bureaucratic limits of internal organization are presented, including the dilemma of why a large firm can't do everything a collection of small firms can do. Efficient labor organization is explored; on the assumption that an authority relation prevails between workers and managers, what governance structure supports will be made in response to various types of job attributes are discussed, and implications for union organization are developed. Considering antitrust ramifications of transaction cost economics, the book summarizes transaction cost issues that arise in the context of contracting, merger, and strategic behavior, and challenges earlier antitrust preoccupation with monopoly.","['This study is based on the belief that economic organization is shaped by transaction cost economizing decisions.', 'This approach is based on behavioral assumptions of bounded rationalism and opportunism, which reflect actual human nature.', 'These assumptions underlie the problem of economic organization: to create contract and governance structures that economize on bounded rationality while safeguarding transactions against the hazards of opportunism.']"
"While it finds no empirical basis for this orthodox standpoint it observes that long-term unemployment dampens aggregate production which in turn aggravates unemployment problem. This paper analysed the OECD data on employment protection for OECD countries over the time span 1990-2008 on the basis of alternative dynamic panel data models and panel causality tests and examines the validity of the neo-liberal argument that strictness of employment protection hurts labour through increased long-term and youth unemployment rates. Franchisee-franchisor relationships in the Norwegian distribution system of a multinational oil refiner provide the context for analysis. These models, however, over-emphasised the forces of 'realisation 'and overlooked the 'profit squeeze' force (also pinpointed by Karl Marx) -better income distribution and higher real wages reduce profitability and dampen investment and growth (see Bhaduri and Margin, 1990 #CITATION_TAG, 1993.","The theoretical model frames opportunism as a determinant of transaction costs and implicates cooperation and formalization as control structures that alleviate opportunism. The model also examines whether the proposed theoretical relationships are enduring. A test of the model using multisample data across two time periods indicates that opportunistic behavior consistently increases transaction costs. Furthermore, cooperative interaction curbs bargaining costs, and formalization reduces opportunism.",['This study focuses on organizational efforts to constrain ex post transaction costs in interorganizational exchange.']
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. Moreover, follow-up results are suggestive of longterm beneficial effects with significant job retention 7-8 years after the initiation of the supported employment programme (#CITATION_TAG).","The first approach is that of Differentiated Fit. The second approach is that of Shared Values. We further maintain that differentiated fit and shared values, while being alternatives, are not mutually exclusive ways of effectively managing headquarters subsidiary relations.",['This paper elaborates and provides empirical support for two different approaches to managing the nexus of headquarters subsidiary relations in a multinational corporation (MNC).']
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. Sickness absence costs the UK economy around PS20 billion per year. In order to estimate QALYs for adults with autism being in either the 'employed' or the 'unemployed' health state, we utilised relevant data reported in #CITATION_TAG, who conducted an economic analysis to support NICE public health guidance on managing long-term sickness absence and incapacity for work (NICE, 2009).",,"['This study aims to assess the cost-effectiveness of interventions to return employees with musculoskeletal disorders to work, one of the major causes of long-term sickness absence, using a mathematical model.A Markov model was developed to assess the cost-effectiveness of three interventions: a workplace intervention; a physical activity and education intervention and a physical activity, education and workplace visit intervention.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. It raises questions about the optimal nature and organization of employment support for this service user group The impact of supported employment on health and social care service usage by adults with autism is not known; nevertheless, #CITATION_TAG reported changes in costs to mental health, primary and secondary care, local authority and voluntary day care services incurred by people with mental health problems (mainly schizophrenia, bipolar disorder, anxiety or depression) associated with gaining employment after registration with supported employment schemes.",Methods: Service use and frequency were measured at baseline and 12 months. Comparisons paid particular attention to the differences between people entering work and those who remained unemployed. Costs were analysed from a government perspective (excluding earnings) and a societal perspective (excluding welfare benefits and taxes).,"['Aims: To explore the impact of moving into employment on service use, earnings, benefits and tax allowances claimed.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. It considers the transaction to be the ultimate unit of microeconomic analysis, and defines hierarchical transactions as ones for which a single administrative entity spans both sides of the transaction, some form of subordination prevails and, typically, consolidated ownership obtains. Peer groups can be understood as an internal organizational response to the frictions of intermediate product markets, while conglomerate organization can be seen as a response to failures in the capital market. In both contexts, the same human factors, such as bounded rationality and opportunism, occur. In the United Kingdom, the cost-effectiveness of supported employment versus standard care (local traditional vocational services) for people with severe mental illness has been evaluated in a randomised controlled trial (RCT; #CITATION_TAG; follow-up data in Heslin et al., 2011).","Discusses the advantages of the transactional approach by examining three issues: price discrimination, insurance, and vertical integration. Develops the concept of the organizational failure framework, and demonstrates why it is always the combination of human with environmental factors, not either taken by itself, that causes transactional problems. The study also describes each of the transactional relations of interest, and presents the advantages of internal organization with respect to the transactional condition. The same transactional factor which impede autonomous contracting between individuals also impede market exchange between technologically separable work groups. Examines the reasons for and properties of the employment relation, which is commonly associated with voluntary subordination. The study then examines more complex structures -- the movement from simple hierarchies to the vertical integration of firms, then multidivisional structures, conglomerates, monopolies and oligopolies. Discusses the market structure in relation to technical and organizational innovation. The analysis also examines the relation of organizational innovation to technological innovation.","['This study analyzes organization of economic activity within and between markets and hierarchies.', 'The study proposes a systems approach to the innovation process.', 'Its purpose is to permit the realization of the distinctive advantages of both small and large firms which apply at different stages of the innovation process.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. Agency theory is an important, yet controversial, theory. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (Beyer and Kilsby, 1996; #CITATION_TAG Bond et al.,, 2008 Crowther et al., 2001; Graetz, 2010; Griffin et al., 1996; Heffernan and Pilkington, 2011; McCaughrin et al., 1993; Noble et al., 1991; Rhodes et al., 1987; Stevens and Martin, 1999).",,"['This paper reviews agency theory, its contributions to organization theory, and the extant empirical work and develops testable propositions.', 'The principal recommendation is to incorporate an agency perspective in studies of the many problems having a cooperative structure.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. In the federative MNC, the headquarters and the subsidiaries are involved in a perpetual bargaining process. Thus a crucial issue is what power bases are there that the different actors in the federative MNC can use to influence strategic decisions. Clinical effectiveness dataA limitation of the economic analysis was the narrow clinical evidence base: the source trial of effectiveness data (Mawhood and Howlin, 1999) had a small sample size (N = 50); moreover, its quasi-experimental design may have introduced bias in the analysis due to confounding factors  ( #CITATION_TAG et al., 2000)",A causal model is developed and tested with data from 97 subsidiaries in 20 MNC divisions.,"['In this paper, we argue that a fruitful approach to the analysis of inter-organisational power in multinational corporations (MNCs) is to model the organisation as a federation.', ""In this paper, we focus on the power bases associated with the local business network in which the different subsidiaries are embedded, and on the headquarters' knowledge concerning these networks.""]"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (#CITATION_TAG; Bond et al., 1997 Bond et al.,, 2008 Crowther et al., 2001; Graetz, 2010; Griffin et al., 1996; Heffernan and Pilkington, 2011; McCaughrin et al., 1993; Noble et al., 1991; Rhodes et al., 1987; Stevens and Martin, 1999).","First, the eclectic approach to foreign entry mode, proposed by Hill, Hwang and Kim [1990] and Kim and Hwang [1992], is used to examine transaction-specific, organizational capability and strategic factors that influence channel choices in foreign markets. Second, the study empirically examines the performance consequences of channel integration.",['This study enhances our understanding of channel integration in foreign markets on two fronts.']
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. The analysis considered two measures of outcome: the total number of weeks in employment and the quality-adjusted life year (QALY; #CITATION_TAG).",Much of the support for the QALY is based on its simplicity as a tool for resolving complex choices.,"['This paper seeks to highlight some of the critical issues concerning the use of the Quality Adjusted Life Years (QALYs) to measure the outcome of health care choices, in decisions related to both individual patient care and social resource allocation.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. Nevertheless, there is evidence that supported employment also has a positive effect on employment rates in adults with autism with mild or moderate intellectual disability (#CITATION_TAG).","The first focuses on the internal transaction costs associated with the governance and organization of the activities within the MNE, and here we highlight the costs of information acquisition and transmission, the costs of coordination, and the costs of aligning the interests of different stakeholders within the MNE. The second addresses the implications of different assumptions about the risk propensity of the MNE.","['The objectives of this paper are to outline the contribution of internalization theory to our understanding of the governance of the MNE, and to highlight aspects of the theory that we believe have received insufficient attention in the literature.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. The research has shown that people with Asperger's syndrome have had difficulty in finding employment in most cases. The vast majority of people with Asperger's syndrome live in a partnership and almost...Tematem teto prace je ""Spolecenske a pracovni uplatneni dospelych osob s Aspergerovym syndromem"". Na zaver teoreticke casti je nastineno podporovane zamestnavani osob s Aspergerovym syndromem, chranena pracovni mista a pracovni rehabilitace. Adults with autism are more likely to switch jobs frequently, have difficulty adjusting to new job settings and earn lower wages compared with typically developing peers (Howlin, 2000; #CITATION_TAG; Jennes-Coussens et al., 2006; Müller et al., 2003).","In the theoretical part of the work, a reader is acquainted with the term Asperger syndrome, with the development of expert terminology, symptoms, diagnosis and prevalence. Then a reader is acquainted with the social position of adults with this syndrome, specifically with a person with Asperger syndrome in the role of a friend, a partner and a parent. The theoretical part describes training and positive work characteristics of these people, but also difficulties that these people may have in the work process. The practical part of this work contains an interpretation of the quantitative research, which was conducted by means of an online questionnaire survey.","['The topic of this work is ""Social inclusion and employment of adults with Asperger syndrome"".', 'The thesis aims to present the specifics of social inclusion and employment of adults with Asperger syndrome.', 'Subsequently, the work deals with the employment of adults with Asperger syndrome.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. Adults with autism are more likely to switch jobs frequently, have difficulty adjusting to new job settings and earn lower wages compared with typically developing peers (#CITATION_TAG; Hurlbutt and Chalmers, 2004; Jennes-Coussens et al., 2006; Müller et al., 2003).","We develop and test hypotheses derived from literature on MNC knowledge flows integrated with the perspective of knowledge-creating, self-interested MNC subsidiaries. The hypotheses are developed using a simultaneous equation model applied to a unique dataset encompassing a German MNC, HeidelbergCement. Enablers and impediments of knowledge outflows are assessed in order to explain why subsidiaries share their knowledge with other MNC units.",['This empirical paper explores knowledge outflow from MNC subsidiaries and its impact on the MNC performance.']
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. There is also evidence from non-UK studies on adults with autism for a positive impact of supported employment programmes on employment levels and job retention (Hillier et al., 2007; Keel et al., 1997), on autistic behaviours (#CITATION_TAG), quality of life (Garcia-Villamisar et al., 2002) and executive function (Garcia-Villamisar and Hughes, 2007).","The average length of time of the community employment was 30 months.Based on their predominant work activity over the study period, participants were classified into two groups: supported employment and unemployed.","['The purpose of this study was to examine the effects of a supported employment programme on measures of executive functions for 44 adults with autism, assessed at the beginning and the end of the programme period.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. The recent marketing literature reflects a growing interest in relationship management issues. In particular, several recent studies have drawn on transaction cost and agency theory to examine how interfirm relationships are organized. Although previous research has shown that different mechanisms can be used, the tendency has been to examine individual mechanisms in isolation. To take into account the uncertainty characterising the model input parameters, a probabilistic analysis was undertaken, in which input parameters were assigned probabilistic distributions rather than being expressed as point estimates (#CITATION_TAG).",The authors develop hypotheses about interdependences between different control mechanisms. They also identify some of the contextual factors that influence their use. The framework is tested empirically by examining how chemical manufacturers organize their supplier relationships.,"[""The general premise is that explicit control mechanisms must be deployed in a relationship to manage a partner's potential opportunism.""]"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. More than half of employment intermediated by this resource belongs to standard companies. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (Beyer and Kilsby, 1996; Bond et al., 1997 Bond et al.,, 2008 #CITATION_TAG; Graetz, 2010; Griffin et al., 1996; Heffernan and Pilkington, 2011; McCaughrin et al., 1993; Noble et al., 1991; Rhodes et al., 1987; Stevens and Martin, 1999).",,"['La metodologia empleada en la investigacion del recurso (documental, cualitativa y cuantitativa) demuestra que estos resultados son posibles gracias a la conjuncion de varios elementos: financiacion sostenible, metodo de trabajo, fuerte liderazgo politico, trabajo con el tejido empresarial (presentando a las empresas, perfiles profesionales basados en un certero analisis de las competencias), trabajo en red, plazo de intervencion marcado por la persona y la composicion de los equipos multiprofesionales, entre otros factores.This article aims to identify the variables that explain why the Vocational Rehabilitation Centres (CRL) for people with severe and enduring mental illness of the Madrid get labor insertion rates close to 50 %, with a group that presents rates unemployment above 80%.', 'This work is able to present the professional profiles companies based on a sound analysis of the skills and talents of its people served.']"
"Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. High levels of unemployment among persons with mental illness are a significant social disability. Evidence outside North America is more limited. Systematic review of studies of the effectiveness of IPS conducted principally in the United Kingdom. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (Beyer and Kilsby, 1996; Bond et al., 1997 Bond et al.,, 2008 Crowther et al., 2001; Graetz, 2010; Griffin et al., 1996; #CITATION_TAG; McCaughrin et al., 1993; Noble et al., 1991; Rhodes et al., 1987; Stevens and Martin, 1999).",Methods.,['To examine the evidence for the effectiveness of the IPS model of supported employment within the United Kingdom.']
"It is argued that ISE practices reinforced participants preexisting sense that museums and science centers were ""not for us."" This paper explores how people from low-income, minority ethnic groups perceive and experience exclusion from informal science education (ISE) institutions, such as museums and science centers. Drawing on qualitative data from four focus groups, interviews, four accompanied visits to ISE institutions, and field notes, this paper presents an analysis of exclusion from science learning opportunities during visits alongside participants' attitudes, expectations, and conclusions about participation in ISE. This article conceptualizes community cultural wealth as a critical race theory (CRT) challenge to traditional interpretations of cultural capital. Various forms of capital nurtured through cultural wealth include aspirational, navigational, social, linguistic, familial and resistant capital. These forms of capital draw on the knowledges Students of Color bring with them from their homes and communities into the classroom. As researchers who have investigated the relationships between power and learning in multicultural settings have argued, it is important to note the relationships between perceived deficits and power (Aikenhead, 2002; Ogbu, 1992; #CITATION_TAG).","CRT shifts the research lens away from a deficit view of Communities of Color as places full of cultural poverty disadvantages, and instead focuses on and learns from the array of cultural knowledge, skills, abilities and contacts possessed by socially marginalized groups that often go unrecognized and unacknowledged.",['This CRT approach to education involves a commitment to develop schools that acknowledge the multiple strengths of Communities of Color in order to serve a larger purpose of struggle toward social and racial justice.']
"It is argued that ISE practices reinforced participants preexisting sense that museums and science centers were ""not for us."" This paper explores how people from low-income, minority ethnic groups perceive and experience exclusion from informal science education (ISE) institutions, such as museums and science centers. Drawing on qualitative data from four focus groups, interviews, four accompanied visits to ISE institutions, and field notes, this paper presents an analysis of exclusion from science learning opportunities during visits alongside participants' attitudes, expectations, and conclusions about participation in ISE. During the latter part of the Porfirio Diaz administration (1876-1910), the middle class grew as the city became a commercial and administrative center. Sociologists both criticized and praised the middle class and its role in the country's future. Members of the middle class distinguished themselves from the Porfirian elite and lower classes through bodily behaviors learned from urban conduct manuals and short stories. The Mexican Revolution (1910-1920) was a devastating blow to the middle class, which rallied around issues of housing, employment, and transportation. In the neighborhood of Santa Maria la Ribera, residents petitioned for urban services and infrastructure improvements. Continuing a long history of civic engagement, the city's middle class publicly organized in response to the anti-clerical policies of the Plutarco Calles administration (1924-1928). Economic and political difficulties hindered the efforts of post-revolutionary municipal and federal leaders to win state loyalty from Mexico City's public employees. At the same time, new mass media, fashions, and popular culture of the 1920s and 1930s challenged existing class distinctions and gender norms. Educational opportunities opened up wider prospects for the middle class, or those seeking middle-class status. Technical schools and the National Polytechnic School offered one set of possibilities. The Lazaro Cardenas administration (1936-1940) aimed to unite the middle class and the working class. As the state bureaucracy grew in the 1930s, Cardenas brought public employees into a close relationship with the National Revolutionary Party (PNR), which later became the Party of the Mexican Revolution (PRM). By the end of the Cardenas era, many sectors of the middle class felt politically marginalized. In contrast, middle-class public employees became beneficiaries of the country's new corporate state Here Thomas draws on his sense of the differences between ""high"" and ""low"" cultural practices, tastes, and class in terms of the forms of knowledge and behaviors that he believes would be more or less valued within an ISE institution (#CITATION_TAG; Bourdieu, 1984).",,"[""This dissertation looks at Mexico City's middle class from 1890 to 1940.""]"
"It is argued that ISE practices reinforced participants preexisting sense that museums and science centers were ""not for us."" This paper explores how people from low-income, minority ethnic groups perceive and experience exclusion from informal science education (ISE) institutions, such as museums and science centers. Drawing on qualitative data from four focus groups, interviews, four accompanied visits to ISE institutions, and field notes, this paper presents an analysis of exclusion from science learning opportunities during visits alongside participants' attitudes, expectations, and conclusions about participation in ISE. In examining the concepts of authoritative language and symbolic power we often look to one legitimate  language placed at the top of a hierarchy that serves to marginalize all others, and in so doing,  marginalizes its speakers. We look at subversive moments of resistance to challenge this structure,  but more often than not, it would seem, this hierarchical structure of language is the one that prevails. The situational context of the dual language system  of Malta provides just such juxtaposition, within which to study authoritative language, symbolic  power, and strategies of condescension, as it is a place where the normal rules of language hierarchy  do not apply. While strategies of condescension  in the use of English abound, there are still ways to negotiate moments of power and meet in the  middle.peer-reviewe Language practices can be understood as forms of capital that delineate the information, culture, and people who are more valued in a particular field (#CITATION_TAG).",,"[""It is important, therefore, to examine language spaces that serve as a counterpoint or a place  where the hierarchy isn't so black and white.""]"
"This is a repository copy of Topography discretization techniques for Godunov-type shallow water numerical models: a comparative study. Hyperbolic balance laws have steady state solutions in which the flux gradients are nonzero but are exactly balanced by the source terms. The Hydr-Rec approach has been successfully applied-as a topography discretization technique -to various high-order Godunov-type models such as WENO-FV and DG methods (#CITATION_TAG Shu 2006, Noelle et al. 2007), and further enhanced -as a wetting and drying condition -with both FV and DG second-order Godunov-type models (Liang 2010, Kesserwani and Liang 2012).","In our earlier work [31-33], we designed high order well-balanced schemes to a class of hyperbolic systems with separable source terms. We make the observation that the traditional RKDG methods are capable of maintaining certain steady states exactly, if a small modification on either the initial condition or the flux is provided. The computational cost to obtain such a well balanced RKDG method is basically the same as the traditional RKDG method. The same idea can be applied to the finite volume WENO schemes. We will first describe the algorithms and prove the well balanced property for the shallow water equations, and then show that the result can be generalized to a class of other balance laws. We perform extensive one and two dimensional simulations to verify the properties of these schemes such as the exact preservation of the balance laws for certain steady state solutions, the non-oscillatory property for general solutions with discontinuities, and the genuine hig","['In this paper, we present a different approach to the same purpose: designing high order well-balanced finite volume weighted essentially non-oscillatory (WENO) schemes and Runge-Kutta discontinuous Galerkin (RKDG) finite element methods.']"
"This is a repository copy of Topography discretization techniques for Godunov-type shallow water numerical models: a comparative study. Background: Research on root cause analysis (RCA), a pivotal component of many patient safety improvement programmes, is limited. In its simplistic view, a Godunov-type scheme provides a first-order accurate model that is conceptually based (#CITATION_TAG).","Hypothesis: Participants in RCAs would: (1) differ in demographic profile from non-participants, (2) encounter problems conducting RCAs as a result of insufficient system support, (3) encounter more problems if they had conducted fewer RCAs and (4) have positive attitudes regarding RCA and safety. Design, setting and participants: Anonymous questionnaire survey of 252 health professionals, drawn from a larger sample, who attended 2-day SIP courses across New South Wales, Australia.",['Objective: To study a cohort of health professionals who conducted RCAs after completing the NSW Safety Improvement Program (SIP).']
"This is a repository copy of Topography discretization techniques for Godunov-type shallow water numerical models: a comparative study. International comparisons of nationally sponsored healthcare staff training programmes  In the UK, the National Health Service (NHS) treats over one million people every day, but international estimates of serious and largely preventable error are around the 10% mark, at least for general hospital care.1,2 The Chief Medical Officers' report, ""An Organisation with a Memory"",3 found that there was a lack of systems for reporting and analysing incidents, and a culture of blame that suppressed learning that is not conducive to developing and implementing safety solutions. The Department of Health's response was to publish ""Building a safer NHS for patients"",4 which set the policy context for a new body, the National Safety Agency (NPSA). The House of Commons Committee of Public Accounts' report2 recommended, in relation to the use of data from the National Reporting and Learning System, that ""Learning lessons is most likely to come from the information on contributory factors and currently only a percentage of reports to the National Patient Safety Agency contain this information..."". In order for healthcare organisations to ... Such a numerical model has been referred to be ""well-balanced"" or to satisfy the ""C-property"" (Bermúdez and Vázquez-Cendón 1994, Greenberg and LeRoux 1996, #CITATION_TAG.",,"['A central objective of the NPSA was to develop a mandatory risk reporting system, which would enable the agency to analyse and integrate these and other sources of safety information to learn lessons and develop and disseminate solutions.', 'A critical report by the House of Commons Committee of Public Accounts2 precipitated a change in focus at the NPSA, to include greater concentration on the development and dissemination of local safety solutions in NHS trusts.']"
"This is a repository copy of Topography discretization techniques for Godunov-type shallow water numerical models: a comparative study. It may be necessary to reorient the expectations of the power of RCA, or accept that RCA produces communication about clinical processes that would otherwise not have taken place, and whose effects may not be registering for some time to come. In recent years, they have received applied improvements and have been incorporated into water industry standard software (#CITATION_TAG), and used to support flood risk management (Néelz and Pender 2010).",The interview data were discourse analysed and arranged into over-arching themes.,"['PURPOSE Increased public awareness of clinical failure and rising levels of litigation are spurring health policy makers in industrialized countries to mandate that clinicians report and investigate clinical errors and near misses.', 'This paper seeks to understand the value of root cause analysis (RCA) recommendations for practice improvement purposes.', 'The paper presents an analysis of interviews with nine senior health managers who were asked about their views on RCA as practice improvement method.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? OBJECTIVES Patient safety has received increased attention in recent years, but mostly with a focus on the epidemiology of errors and adverse events, rather than on practices that reduce such events. The practices focused primarily on hospitalized patients, but some involved nursing home or ambulatory patients. For most practices, the project team required that the primary outcome consist of a clinical endpoint (i.e., some measure of morbidity or mortality) or a surrogate outcome with a clear connection to patient morbidity or mortality. Many patient safety practices drawn primarily from nonmedical fields (e.g., use of simulators, bar coding, computerized physician order entry, crew resource management) deserve additional research to elucidate their value in the health care environment. Such practices target a diverse array of safety problems. Within the empirical setting of this study, these costs occur inexorably out of intra-organizational coordination, but the same categories are also valid for inter-organizational coordination (#CITATION_TAG).","SEARCH STRATEGY AND SELECTION CRITERIA Patient safety practices were defined as those that reduce the risk of adverse events related to exposure to medical care across a range of diagnoses or conditions. Potential patient safety practices were identified based on preliminary surveys of the literature and expert consultation. Protocols specified the inclusion criteria for studies and the structure for evaluation of the evidence regarding each practice. Pertinent studies were identified using various bibliographic databases (e.g., MEDLINE, PsycINFO, ABI/INFORM, INSPEC), targeted searches of the Internet, and communication with relevant experts. This criterion was relaxed for some practices drawn from the non-health care literature. The evidence supporting each practice was summarized using a prospectively determined format. The project team then used a predefined consensus technique to rank the practices according to the strength of evidence presented in practice summaries. A separate ranking was developed for research priorities. Appropriate use of prophylaxis to prevent venous thromboembolism in patients at risk; Use of perioperative beta-blockers in appropriate patients to prevent perioperative morbidity and mortality; Use of maximum sterile barriers while placing central intravenous catheters to prevent infections; Appropriate use of antibiotic prophylaxis in surgical patients to prevent postoperative infections; Asking that patients recall and restate what they have been told during the informed consent process; Continuous aspiration of subglottic secretions (CASS) to prevent ventilator-associated pneumonia; Use of pressure relieving bedding materials to prevent pressure ulcers; Use of real-time ultrasound guidance during central line insertion to prevent complications; Patient self-management for warfarin (Coumadin) to achieve appropriate outpatient anticoagulation and prevent complications; Appropriate provision of nutrition, with a particular emphasis on early enteral nutrition in critically ill and surgical patients; and Use of antibiotic-impregnated central venous catheters to prevent catheter-related infections.",['This project aimed to collect and critically review the existing evidence on practices relevant to improving patient safety.']
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Drawing on existing literature, it is argued that this policy agenda represents a new frontier in medical/managerial relations, introducing a disciplinary expertise within the health service that provides managers with the knowledge and legitimacy to survey and scrutinise medical performance, made real through procedures for incident reporting and root-cause analysis. The following procedures were used to evaluate the scales of the dependent variables; (i) unrotated principal component analysis (PCA) with subsequent (ii) pro-max (oblique) rotated PCA were conducted, 4 An oblique rotation was used at this stage because it allows correlated factors instead of an assumption of independence among the factors as is maintained in an orthogonal rotation (#CITATION_TAG).","The extent of regulatory change is investigated, drawing on an ethnographic case study of one hospital. This leads to new and rearticulated forms of self-surveillance, self-management or 'governmentality', ultimately negating the need for external groups to explicitly manage or regulate professional practice.","[""This paper explores how current 'patient safety' reforms offer to change the regulation of medicine."", ""I describe this as 'adaptive regulation' to account for how doctors seek to maintain their regulatory monopoly and limit managerial encroachment.""]"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Work by known experts in the field of accident investigation and analysis. Websites for accident investigation reports. While a few studies looked solely at death as an outcome, most used a variety of outcomes including near misses. Most techniques used interviewing and primary document review to investigate incidents. However the extent and sophistication of the various attempts varied widely. Only a third of papers referred to an established model of accident causation. In most studies examined there was little or no information on the training of investigators, how the data was extracted or any information on quality assurance for data collection and analysis. There was some variation in the level of expertise and training required but to undertake the investigation to an acceptable depth all required some expertise. In most papers there was little or no discussion of implementation of any changes as a result of the investigations. A quarter of publications gave some description of the implementation of changes, though few addressed evaluation of changes. Examining implementation of recommendations is a key issue. To compensate for the differences, both formal and informal initiatives will often be introduced by the companies (#CITATION_TAG; Rabbiosi, 2011).","REVIEW METHODS Twelve techniques from other high-risk industries were reviewed in detail using criteria developed for the purpose. Rigorous searching and screening identified 138 papers for formal appraisal and a further 114 were designated as providing potentially useful background information. A formal appraisal instrument was designed, piloted and modified until acceptable reliability was achieved. From the 138 papers, six techniques were identified as representing clearly definable approaches to incident investigation and analysis. All relevant papers were reviewed for each of the six techniques: Australian Incident Monitoring System, the Critical Incident Technique, Significant Event Auditing, Root Cause Analysis, Organisational Accident Causation Model and Comparison with Standards approach. All techniques included papers that identified clinical issues and some attempt to assess underlying errors, causes and contributory factors. Studies should examine depth of investigation and analysis, adequacy and feasibility of recommendations and cost effectiveness.","['OBJECTIVES To carry out a review of published and unpublished work on the analysis on methods of accident investigation in high-risk industries, and of critical incidents in healthcare.', 'To develop and pilot guidelines for the analysis of critical incidents in healthcare for the hospital sector, mental health and primary care.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Agency theory is an important, yet controversial, theory. One day Deng Xiaoping decided to take his grandson to visit Mao. Chairman Mao, &quot; the awe-struck child replied. &quot; (&quot;Capitalism, &quot; 1984, p. 62) Agency theory has been used by scholars in accounting (e.g., Demski &amp; Feitham, 1978), eco-nomics (e.g., Spence &amp; Zeckhauser, 1971), fi-nance (e.g., Fama, 1980), marketing (e.g., Basu All forms of organizations are subject to risks of opportunism (Williamson, 1975), but opportunism will not disappear with common ownership (#CITATION_TAG; Schotter and Beamish, 2011; Taplin, 2006).",,"['This paper reviews agency theory, its contributions to organization theory, and the extant empirical work and develops testable propositions.', 'The principal recommendation is to in-corporate an agency perspective in studies of the many problems having a cooperative structure.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Patient safety has been an under-recognised and under-researched concept until recently. It is now high on the healthcare quality agenda in many countries of the world including the UK. The recognition that human error is inevitable in a highly complex and technical field like medicine is a first step in promoting greater awareness of the importance of systems failure in the causation of accidents. Plane crashes are not usually caused by pilot error per se but by an amalgam of technical, environmental, organisational, social and communication factors which predispose to human error or worsen its consequences. In healthcare, the systematic investigation of error in the administration of medication will often reveal similarly complex causation. The NHS is putting in place a comprehensive programme to learn more effectively from adverse events and near misses. In such cases it could be better to invest in bonding and make sure that the subsidiary shares the same company culture and adheres to MNC rules of conduct (#CITATION_TAG).",,"['This aims to reduce the burden of the estimated 850,000 adverse events which occur in hospitals each year as well as targeting high risk areas such as medication error.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Because of intensified competition from low wage economies, such firms have been forced to restructure production processes to heighten both their productive efficiency and attain greater flexibility at the plant level. Much of this change has involved the introduction of high performance work practices (HPWP), a central focus of much recent scholarship on post-Fordism. We describe the failure to implement HPWP as some firms seek efficiency gains from work restructuring rather than broader effectiveness goals that would have deepened employee participation. All forms of organizations are subject to risks of opportunism (Williamson, 1975), but opportunism will not disappear with common ownership (Eisenhardt, 1989; Schotter and Beamish, 2011; #CITATION_TAG).",,"['This study examines the organizational changes and varied response amongst managers to those changes in seven subsidiaries of multinational apparel firms.', 'Drawing from several qualitative strategies, this paper focuses on the role of managers as agents of strategy implementation and discusses how they negotiate, accept or resist such changes.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Recent research suggests that both the scholarly and the managerial perspectives on intra-organizational conflict in multinational corporations (MNC) between headquarters (HQ) and their foreign subsidiaries have changed. Today, conflict is not necessarily regarded as dysfunctional or the result of inefficient global integration. Instead, conflict is now considered a normal consequence of organizing and managing across national borders. All forms of organizations are subject to risks of opportunism (Williamson, 1975), but opportunism will not disappear with common ownership (Eisenhardt, 1989; #CITATION_TAG; Taplin, 2006).","We specifically focus on subsidiary conflict negotiation tactics, the effects of organizational and individual managerial power, and the characteristics and roles of MNC managers that act as boundary spanners during intra-organizational conflict processes. A qualitative, iterative, multiphase research approach was used to develop new theory pertaining to the phenomenon.","['This research advances the literature on HQ-subsidiary relationships by adding new insights to the Headquarters-subsidiary conflict discussion, especially in the so far under-researched case of headquarter initiative rejection by foreign subsidiaries.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Despite recurring concerns about common method variance (CMV) in survey research, the information systems (IS) community remains largely uncertain of the extent of such potential biases. Since most of the variables in the study come from a questionnaire, variables in the study may be affected by shared and spurious covariance (Buckley et al., 1990), which can be problematic since measurement artifacts and the phenomenon under investigation can be difficult to separate (#CITATION_TAG).","First, we describe the available approaches for assessing CMV and conduct an empirical study to compare them. Accordingly, the marker-variable technique was employed to infer the effect of CMV on correlations from previously published studies. Our reanalysis reveals that contrary to the concerns of some skeptics, CMV-adjusted structural relationships not only remain largely significant but also are not statistically differentiable from uncorrected estimates.","['To address this uncertainty, this paper attempts to systematically examine the impact of CMV on the inferences drawn from survey research in the IS area.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Most of the article consists of a description of these parallels. A concept that unites outer speech and gesture is the hypothesis of inner speech. MNC headquarters start trust processes with subsidiaries by behaving in a manner that subsidiaries see as competent, fair and transparent (#CITATION_TAG).","The argument is based on the very close temporal, semantic, pragmatic, pathological, and developmental parallels between speech and referential and discourse-oriented gestures.",['In this article I argue that gestures and speech are parts of the same psychological structure and share a computational stage.']
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Human communication is grounded in fundamentally cooperative, even shared, intentions. In this original and provocative account of the evolutionary origins of human communication, Michael Tomasello connects the fundamentally cooperative structure of human communication (initially discovered by Paul Grice) to the especially cooperative structure of human (as opposed to other primate) social interaction. Tomasello argues that human cooperative communication rests on a psychological infrastructure of shared intentionality (joint attention, common ground), evolved originally for collaboration and culture more generally. The basic motives of the infrastructure are helping and sharing: humans communicate to request help, inform others of things helpfully, and share attitudes as a way of bonding within the cultural group. Requesting help in the immediate you-and-me and here-and-now, for example, required very little grammar, but informing and sharing required increasingly complex grammatical devices. Conventional communication, first gestural and then vocal, evolved only after humans already possessed these natural gestures and their shared intentionality infrastructure along with skills of cultural learning for creating and passing along jointly understood communicative conventions. Trust begets trust, but for the process to activate it needs to be initiated (#CITATION_TAG).",These cooperative motives each created different functional pressures for conventionalizing grammatical constructions.,"[""Drawing on empirical research into gestural and vocal communication by great apes and human infants (much of it conducted by his own research team), Tomasello argues further that humans' cooperative communication emerged first in the natural gestures of pointing and pantomiming."", 'Challenging the Chomskian view that linguistic knowledge is innate, Tomasello proposes instead that the most fundamental aspects of uniquely human communication are biological adaptations for cooperative social interaction in general and that the purely linguistic dimensions of human communication are cultural conventions and constructions created by and passed along within particular cultural groups.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? In situations with high cultural differences, recruiting local managers imply that companies incur high selection, training and control costs, and sending a ""trustworthy"" expatriate manager to the subsidiary could reduce such costs (#CITATION_TAG; Boyacigiller and Adler, 1991; Nohria and Ghoshal, 1994; Yan et al., 2002).","Specifically, we look at the monitoring, bonding, maladaptation, and bargaining costs of conducting activities in specific subsidiaries in a foreign country. We hypothesize that the transaction costs of using expatriates are lower than those generated by local employees, especially in the higher managerial echelons of foreign subsidiaries, but also that costs can be reduced as individuals become more experienced. We also conjecture that ex post transaction costs are influenced by cultural differences between the host and the home countries, and by characteristics of the companies and their subsidiaries. The framework is empirically corroborated by survey data on a sample of 145 Norwegian MNCs.Human resources Staffing decisions Multinational firms Transaction costs","['This paper analyzes staffing decisions in foreign subsidiaries from the perspective of transaction cost theory.', 'We focus on the ex post transaction costs of the employment relation.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? ABSTRACT: Literature on technology and innovation management has identified the research and development (R&amp;D) and marketing (RDM) interface as a critical organizational complexity that when managed well can affect company success in innovation. The culture pertaining to R&amp;D personnel or technologists, meaning engineers and ""hard"" scientists, can be referred to as the T-culture. The culture pertaining to marketing or social ""soft"" scientists (meaning business, sociology, psychology, etc.) can be referred to as the S-culture. Consequently, by implementing more formalized procedures such as rules and routines, clearer role responsibilities, and a better identification of complementary tasks and responsibilities between the MNC and the subsidiary, opportunities for opportunism are reduced (Dahlstrom and Nygaard, 1999; #CITATION_TAG).","Cross-functional teams and collaboration mechanisms, such as boundary spanners, have been identified as solutions to the problems in communication between the two groups. However, these methods have not been as effective as desired and questions still exist as to how boundary spanners are created for success at this crucial interface.","['This chapter takes a different theoretical perspective: that of creating professional biculturalism in order to effectively manage the RDM interface and thereby enhance creativity and productivity in the innovation process.', 'This chapter specifies propositions regarding the importance of professional biculturalism in bridging the disconnect between the T and S cultures.']"
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? As SPL rises above or below comfortable SPL, speech breathing requires more energy. For example, the subsidiary could be a center of excellence within the MNC organization and have more expertise than even headquarters (#CITATION_TAG).","The energy required to alter SPL was also studied and compared to energy expenditures during a quiet breathing condition. Twenty-four adults (12 women, 12 men) were studied while reading a standard passage at low, comfortable, and high SPLs for 7 minutes with quiet breathing periods between each task to achieve respiratory steady state and serve as a control to which the reading tasks were compared. The last 2 minutes of exhaled air for all speaking and quiet breathing tasks were collected using a Hans Rudolph mouth breathing face mask. A Sensor Medics Vmax 29 series diagnostic instrument system measured all ventilatory responses and energy expenditures. Volume and timing alterations in ventilation were characterized by measuring tidal volume (V[T]), inspiratory time (T[I]), inspiratory flow rate (V[T]/T[I]), and expiratory time (T[E]). Average ventilation, energy expenditure, and adequacy of ventilation were measured using minute ventilation (V[E]), oxygen consumption (VO2), carbon dioxide production (VCO2), and partial pressure of end-tidal carbon dioxide (end-tidal PET[CO2]).",['This study was completed to determine how ventilatory responses change by means of speech reading at three different sound pressure levels (SPL) as compared to quiet breathing prior to each task.']
"According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Proponents of the model known as the ""human revolution"" claim that modern human behaviors arose suddenly, and nearly simultaneously, throughout the Old World ca. This view of events stems from a profound Eurocentric bias and a failure to appreciate the depth and breadth of the African archaeological record. In fact, many of the components of the ""human revolution"" claimed to appear at 40-50 ka are found in the African Middle Stone Age tens of thousands of years earlier. These features include blade and microlithic technology, bone tools, increased geographic range, specialized hunting, the use of aquatic resources, long distance trade, systematic processing and use of pigment, and art and decoration. These items do not occur suddenly together as predicted by the ""human revolution"" model, but at sites that are widely separated in space and time. The African Middle and early Late Pleistocene hominid fossil record is fairly continuous and in it can be recognized a number of probably distinct species that provide plausible ancestors for H. sapiens. The appearance of Middle Stone Age technology and the first signs of modern behavior coincide with the appearance of fossils that have been attributed to H. helmei, suggesting the behavior of H. helmei is distinct from that of earlier hominid species and quite similar to that of modern people. The final measurement model shows excellent fit on all fit statistics: χ 2 = 53.00 (p >.28; RMSEA =.026; AGFI =.91; CFI =.99 (#CITATION_TAG; Jöreskog and Sörbom, 1981; Tanaka and Huba, 1985).","Because the earliest modern human fossils, Homo sapiens sensu stricto, are found in Africa and the adjacent region of the Levant at >100 ka, the ""human revolution"" model creates a time lag between the appearance of anatomical modernity and perceived behavioral modernity, and creates the impression that the earliest modern Africans were behaviorally primitive.","['This fundamental behavioral shift is purported to signal a cognitive advance, a possible reorganization of the brain, and the origin of language.']"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. In the pursuit of enhanced patient safety, new forms of organisational learning have been introduced within healthcare services across the developed world. It offers an alternative to the current orthodoxy of policy and raises questions about the capacity of such systems to shape the production of knowledge to the determinant of service improvement and to act as a mechanism of organisational power. By the same token, the emotional elements that transpired through the individual statements and in the meetings were gradually filtered out when the narrative elements were substituted by a more causal type of account in the final report (see also #CITATION_TAG).","The approach taken in this paper departs significantly from methods found within mainstream patient safety research. Specifically, it considers how narratives - the stories produced by staff in a large teaching hospital in the UK - about patient safety events are developed within the interactions of clinical practice, reflecting a dynamic mix of emotion and shared notions of responsibility. It then shows how these are re-produced as incident reports which transform knowledge through check-boxes and pre-defined categorisations leading to de-contextualised 'narrow narratives'. The paper then examines how these accounts are further re-produced through risk management activities as they become de-authored and re-constructed to reflect managerial assumptions about learning.","['This paper examines how these systems contribute to the creation of knowledge about patient safety.', 'Rather than attempting to define clinical incidents through taxonomies or classifications, it considers how knowledge is socially constructed in clinical practice and through the processes of risk management.']"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. While RCA is formally endorsed by policy makers in USA, UK, Australia, and Denmark (Øvretveit, 2005) and is in the process of being adopted by other countries, we have only a partial understanding of the challenges of using this approach, despite research suggesting it is not without problems (#CITATION_TAG; Iedema, Jorm, Long et al., 2006; Wallace, 2006; Wu, Lipshutz, & Pronovost, 2008).","First, RCA team members find themselves in the unusual position of having to derive organizational-managerial generalizations from the specifics of in situ activity. Second, they are constrained by the expectation inscribed into RCA that their recommendations result in 'systems improvements' assumed to flow forth from an extension of formal rules and spread of procedures.","['This paper presents evidence from a root cause analysis (RCA) team meeting that was recently conducted in a Sydney Metropolitan Teaching Hospital to investigate an iatrogenic morphine overdose.', 'We argue that this perspective misrecognizes the importance of RCA as a means to engender solutions that leave the procedural detail of clinical processes unspecified, and produce cross-hospital discussions about the organizational dimensions of care.']"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. It may be necessary to reorient the expectations of the power of RCA, or accept that RCA produces communication about clinical processes that would otherwise not have taken place, and whose effects may not be registering for some time to come. The need for a well-crafted, presentable, and correctly formulated document that could be used ""for judging the quality of the investigation process"" (NPSA 2008), dominated the RCA process (see also #CITATION_TAG).",The interview data were discourse analysed and arranged into over-arching themes.,"['Purpose Increased public awareness of clinical failure and rising levels of litigation are spurring health policy makers in industrialized countries to mandate that clinicians report and investigate clinical errors and near misses.', 'This paper seeks to understand the value of root cause analysis (RCA) recommendations for practice improvement purposes.', 'The paper presents an analysis of interviews with nine senior health managers who were asked about their views on RCA as practice improvement method.']"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Many formal error identification techniques currently exist which have been developed in nonaviation contexts but none have been validated for use to this end. This paper describes a new human error identification technique (HET - human error template) designed specifically as a diagnostic tool for the identification of design-induced error on the flight deck. A common way to investigate clinical incidents is through Root Cause Analysis (RCA), a methodology combining elements from engineering, psychology, and the 'human factors' tradition (#CITATION_TAG; Vincent, Taylor-Adams, & Stanhope, 1998).","HET is benchmarked against three existing techniques (SHERPA - systematic human error reduction and prediction approach; human error HAZOP - hazard and operability study; and HEIST - human error In systems tool). HET outperforms all three existing techniques in a validation study comparing predicted errors to actual errors reported during an approach and landing task in a modern, highly automated commercial aircraft.",['Human factors certification criteria are being developed for large civil aircraft with the objective of reducing the incidence of designinduced error on the flight deck.']
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. A century and a half after the publication of ""Origin of Species"", evolutionary thinking has expanded beyond the field of biology to include virtually all human-related subjects - anthropology, archeology, psychology, economics, religion, morality, politics, culture, and art. Now a distinguished scholar offers the first comprehensive account of the evolutionary origins of art and storytelling. Brian Boyd explains why we tell stories, how our minds are shaped to understand them, and what difference an evolutionary understanding of human nature makes to stories we love. Art is a specifically human adaptation, Boyd argues. It offers tangible advantages for human survival, and it derives from play, itself an adaptation widespread among more intelligent animals. More particularly, our fondness for storytelling has sharpened social cognition, encouraged cooperation, and fostered creativity. What triggers our emotional engagement with these works? What patterns facilitate our responses? The need to hold an audience's attention, Boyd underscores, is the fundamental problem facing all storytellers. Enduring artists arrive at solutions that appeal to cognitive universals: an insight out of step with contemporary criticism, which obscures both the individual and universal. RCA is the umbrella term describing methodologies and tools for the retrospective and structured investigation of adverse incidents, near misses and sentinel events (#CITATION_TAG).","After considering art as adaptation, Boyd examines Homer's ""Odyssey"" and Dr. Seuss' ""Horton Hears a Who!""","['Published for the bicentenary of Darwin\'s birth and the 150th anniversary of the publication of ""Origin of Species"", Boyd\'s study embraces a Darwinian view of human nature and art, and offers a credo for a new humanism.']"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Studies from across the world have shown that clinical mistakes are a major threat to the safety of patient care (World Health Organisation 2004). For the National Health Service (NHS) of England and Wales it is estimated that one in ten hospital patients experience some form of error, and each year these cost the service over PS2billion in remedial care (Department of Health 2000). Unsurprisingly, 'patient safety' is now a major international health policy priority, questioning the efficacy of existing regulatory practices and proposing a new ethos of learning. Within England and Wales, the National Patient Safety Agency (NPSA) has been created to lead policy development and champion service-wide learning, whilst throughout the NHS the National Reporting and Learning System (NRLS) has been introduced to enable this learning (NPSA 2003). We are also aware that RCA has been interpreted as an emerging form of self-surveillance (#CITATION_TAG) and potentially extending the principle of concertive control among healthcare practitioners (Iedema, Jorm, Long et al., 2006).",,"['This paper investigates the extent to which, in seeking to better manage the threats to patient safety, this policy agenda represents a transition in medical regulation']"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Work by known experts in the field of accident investigation and analysis. Websites for accident investigation reports. While a few studies looked solely at death as an outcome, most used a variety of outcomes including near misses. Most techniques used interviewing and primary document review to investigate incidents. However the extent and sophistication of the various attempts varied widely. Only a third of papers referred to an established model of accident causation. In most studies examined there was little or no information on the training of investigators, how the data was extracted or any information on quality assurance for data collection and analysis. There was some variation in the level of expertise and training required but to undertake the investigation to an acceptable depth all required some expertise. In most papers there was little or no discussion of implementation of any changes as a result of the investigations. A quarter of publications gave some description of the implementation of changes, though few addressed evaluation of changes. Examining implementation of recommendations is a key issue Although variations exist (Bagian et al., 2002; #CITATION_TAG) there remains an enduring commitment to this stepped, orderly, and disciplined approach.","Twelve techniques from other high-risk industries were reviewed in detail using criteria developed for the purpose. Rigorous searching and screening identified 138 papers for formal appraisal and a further 114 were designated as providing potentially useful background information. A formal appraisal instrument was designed, piloted and modified until acceptable reliability was achieved. From the 138 papers, six techniques were identified as representing clearly definable approaches to incident investigation and analysis. All relevant papers were reviewed for each of the six techniques: Australian Incident Monitoring System, the Critical Incident Technique, Significant Event Auditing, Root Cause Analysis, Organisational Accident Causation Model and Comparison with Standards approach. All healthcare techniques had the potential of being applied in any specialty or discipline related to healthcare. All techniques included papers that identified clinical issues and some attempt to assess underlying errors, causes and contributory factors. Studies should examine depth of investigation and analysis, adequacy and feasibility of recommendations and cost effectiveness.","['To carry out a review of published and unpublished work on the analysis on methods of accident investigation in high-risk industries, and of critical incidents in healthcare.', 'To develop and pilot guidelines for the analysis of critical incidents in healthcare for the hospital sector, mental health and primary care.']"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. This grounded the mirror system hypothesis of Rizzolatti and Arbib (1998) which offers the mirror system for grasping as a key neural ""missing link"" between the abilities of our nonhuman ancestors of 20 million years ago and modern human language, with manual gestures rather than a system for vocal communication providing the initial seed for this evolutionary process. The present article, however, goes ""beyond the mirror"" to offer hypotheses on evolutionary changes within and outside the mirror systems which may have occurred to equip Homo sapiens with a language-ready brain. Crucial to the early stages of this progression is the mirror system for grasping and its extension to permit imitation. Imitation is seen as evolving via a so-called simple system such as that found in chimpanzees (which allows imitation of complex ""object-oriented"" sequences but only as the result of extensive practice) to a so-called complex system found in humans (which allows rapid imitation even of complex sequences, under appropriate conditions) which supports pantomime. It is argued that these stages involve biological evolution of both brain and body. By contrast, it is argued that the progression from protosign and protospeech to languages with full-blown syntax and compositional semantics was a historical phenomenon in the development of Homo sapiens, involving few if any further biological changes. For example, #CITATION_TAG warn that closure and consensus are often enemies of the capacity of organisations to learn from incidents that require them ""to confront the possibility that the story being told is simultaneously a tale of disorder in which the reality of danger masquerades as safety and a tale of order in which the reality masquerades as danger"" (p. 456).","The starting point is the observation that both premotor area F5 in monkeys and Broca's area in humans contain a ""mirror system"" active for both execution and observation of manual actions, and that F5 and Broca's area are homologous brain regions. This is hypothesized to have provided the substrate for the development of protosign, a combinatorially open repertoire of manual gestures, which then provides the scaffolding for the emergence of protospeech (which thus owes little to nonhuman vocalizations), with protosign and protospeech then developing in an expanding spiral.","['The article analyzes the neural and functional grounding of language skills as well as their emergence in hominid evolution, hypothesizing stages leading from abilities known to exist in monkeys and apes and presumed to exist in our hominid ancestors right through to modern spoken and signed languages.']"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Alluvial rivers can show unpredictable channel changes and humans living along the river corridor repeatedly have to cope with the alterations of their physical environment. This was specifically the case in the mid-sixteenth century, when the Viennese were confronted with one major problem: the Danube River successively abandoned its main arm that ran close to the city and shifted further north. This was at the time when Vienna became the permanent residence of the Holy Roman Empire (except from 1583 to 1611 when it was moved to Prague), due to the Habsburgs holding the crown. Vienna was also the capital of the so-called Danube Monarchy, which came into being in the early sixteenth century. The city assumed increasing significance, being home to and hosting important authorities and persons. In particular after the first siege by the Ottoman army in 1529, the resource need for a complex fortification system was extremely high. As noted by #CITATION_TAG, the increasing number of NHS investigations can be explained in the context of ""a return to big government"" (p.380).",,"[""This paper aims to highlight: (1) the morphological Danube dynamics together with floods and extreme weather situations in the sixteenth century; (2) the main actors in the transformation of the Danube; (3) changes of the river's course from 1550 to 1600/1650 and the consequences for bridging the river and the infrastructure as a whole; (4) the massive engineering measures that were undertaken in order to secure Vienna's requirements in the sixteenth century; (5) the question of floodplain settlements, and (6) the contested use of resources on the Viennese Danube floodplain.""]"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. In 1998 the Veterans Health Administration (VHA) created the National Center for Patient Safety (NCPS) to lead the effort to reduce adverse events and close calls systemwide. NCPS has always aimed to develop a program that would be applicable both within the VA and beyond.NCPS's full patient safety program was tested and implemented throughout the VA system from November 1999 to August 2000. It was developed from the Total Quality Management movement where it was conceived primarily as an ""organizational learning device"" (#CITATION_TAG).","Core concepts included a non-punitive approach to patient safety activities that emphasizes systems-based learning, the active seeking out of close calls, which are viewed as opportunities for learning and investigation, and the use of interdisciplinary teams to investigate close calls and adverse events through a root cause analysis (RCA) process. Program components included an RCA system for use by caregivers at the front line, a system for the aggregate review of RCA results, information systems software, alerts and advisories, and cognitive acids.","[""NCPS's aim is to foster a culture of safety in the Department of Veterans Affairs (VA) by developing and providing patient safety programs and delivering standardized tools, methods, and initiatives to the 163 VA facilities.To create a system-oriented approach to patient safety, NCPS looked for models in fields such as aviation, nuclear power, human factors, and safety engineering.""]"
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Faculty/student relationships have a significant impact on student retention and success (Tinto, 1975, 1993). However, little is known about how faculty perceive community college transfer students and how they make meaning of their interactions with these students. Overall, we conducted 120 days of observations across the two sites, carried out 102 ethnographic interviews (#CITATION_TAG) and 34 semi-structured interviews.","This qualitative, descriptive, ethnographic interview study describes faculty/student interaction and the ways in which community college transfer students are perceived by the faculty at one small, private, nonprofit, masters-level university. The research questions used for this study were designed to investigate the faculty's perception of community college students, the students' academic experiences and the faculty/student relationship. The researcher gathered data during 12 in-depth interviews. An ethnographic design captured the distinct cultural influences of the study site and helped to answer to the study's overarching research question: how do faculty perceive community college transfer students and the institution from which the student transferred? A thematic analysis of the interview data revealed four themes. The first, Student/Faculty Relationship: A Two-Way Street of Hesitation and Reluctance, describes how both community college transfer students, and the faculty that teach them, are hesitant and reluctant to work with one another. The second, A Balancing Act: Aligning Faculty and Student Expectations, relates to the fact that what a faculty member expects of a student, and what students perceive to be expected of them, should align for the student to be successful. The third, A Second Class Institution: The Community College as a Stepping Stone describes how faculty perceive attending a community college as a stepping stone towards more prestigious goal of attending a four year institution. Finally, the fourth theme, Isolation: A Community College Transfer Experience, details the ways community college transfer students are perceived as intentionally, and unintentionally, given a separate university experience, and the ways in which this Isolation disadvantages these students.",['The site was selected because of the limited research available on community college transfer students at small private colleges.']
"The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. : This paper addresses an aspect of organizational learning that has not been extensively developed - the impact of emotion on organizational learning. The study of emotion in organizations is seen as an important part of the development of organizational learning. There is a link between the emotional and the political within organizations. In particular, the way RCA has been translated into practice ignores the idea that learning often stems from the inquiry process itself, not the report; that the inquiry process should not be divorced from the practice of clinical work; that emotions are part and parcel of the organisational learning process (#CITATION_TAG); and that focussing on feasible solutions (a closure orientation) and emphasising consensus reduces the capacity to learn through creativity and divergent thinking (cf. Engeström, 2001).","Awareness of the impact of emotion on organizational learning can be developed through an investigation of two areas. First, organizational learning is more than a product of organizational responses to individual learning. The paper contains a discussion of these themes using brief case examples to illustrate and develop the issues.","['The paper argues that attention to the emotional dynamics of organizing,  and to the links between emotion and organizational politics, will increase the possibilities for understanding organizational learning.']"
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. People can consciously re-experience past events and pre-experience possible future events. Brain imaging shows considerable overlap in brain activation between the two, with slightly more frontal-lobe activity in imagining the future (e.g., #CITATION_TAG).","Participants were cued with a noun for 20s and instructed to construct a past or future event within a specified time period (week, year, 5-20 years). Once participants had the event in mind, they made a button press and for the remainder of the 20s elaborated on the event. Future events recruited regions involved in prospective thinking and generation processes, specifically right frontopolar cortex and left ventrolateral prefrontal cortex, respectively. In contrast to the construction phase, elaboration was characterized by remarkable overlap in regions comprising the autobiographical memory retrieval network, attributable to the common processes engaged during elaboration, including self-referential processing, contextual and episodic imagery.",['This fMRI study examined the neural regions mediating the construction and elaboration of past and future events.']
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the leakage features. According to #CITATION_TAG, evolution proceeds in small increments rather than in a single ""unimaginable"" leap.",,"['In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased.']"
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. Do humans and nonhumans share the ability to form abstract concepts? Until the 1960s, many researchers questioned whether avian subjects could form categorical constructs, much less more abstract formulations, including concepts such as same-different or exact understanding of number. Although ethologists argued that nonhumans, including birds, had to have some understanding of divisions such as prey versus predator, mate versus nonmate, food versus nonfood, or basic relational concepts such as more versus less, simply in order to survive, no claims were made that these abilities reflected cognitive processes, and little formal data from psychology laboratories could initially support such claims. Researchers like Anthony Wright, however, succeeded in obtaining such data and inspired many others to pursue these topics, with the eventual result that several avian species are now considered ""feathered primates"" in terms of cognitive processes. According to #CITATION_TAG, moreover, he learned them more in a human-like than an ape-like fashion.",,"['Here I review research on numerical concepts in the Gray parrot (Psittacus erithacus), demonstrating that at least one subject, Alex, understood number symbols as abstract representations of real-world collections, in ways comparing favorably to those of apes and young human children.']"
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. #CITATION_TAG notes, for example, that infants point to interesting objects in their environments, not to request them, but to share the experience with those around them.",Grice (1982) and Bar-On and Green (2010) each provide \u27continuity stories\u27 which attempt to explain how a human-like language could emerge from the primitive communication practices of non-human animals. I then introduce the recent evolutionary literature on non-cooperative communication in order to construct a continuity story which better satisfies the proposed desiderata while retaining the positive aspects of the proposals of Grice and Bar-On and Green.,['I offer desiderata for a proper account of linguistic continuity in order to argue that these previous accounts fall short in important ways.']
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. The idea that memory is composed of distinct systems has a long history but became a topic of experimental inquiry only after the middle of the 20th century. Beginning about 1980, evidence from normal subjects, amnesic patients, and experimental animals converged on the view that a fundamental distinction could be drawn between a kind of memory that is accessible to conscious recollection and another kind that is not. Subsequent work shifted thinking beyond dichotomies to a view, grounded in biology, that memory is composed of multiple separate systems supported, for example, by the hippocampus and related structures, the amygdala, the neostriatum, and the cerebellum. Declarative memory, in turn, can be divided into episodic memory, which is personal memory for past episodes, and semantic memory, which is basic knowledge about the world (#CITATION_TAG).",,['This article traces the development of these ideas and provides a current perspective on how these brain systems operate to support behavior.']
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. As SPL rises above or below comfortable SPL, speech breathing requires more energy. Manual language is effortful, requiring considerable expenditure of energy, while the physiological costs of speech are so low as to be nearly unmeasurable (#CITATION_TAG).","The energy required to alter SPL was also studied and compared to energy expenditures during a quiet breathing condition. Twenty-four adults (12 women, 12 men) were studied while reading a standard passage at low, comfortable, and high SPLs for 7 minutes with quiet breathing periods between each task to achieve respiratory steady state and serve as a control to which the reading tasks were compared. The last 2 minutes of exhaled air for all speaking and quiet breathing tasks were collected using a Hans Rudolph mouth breathing face mask. A Sensor Medics Vmax 29 series diagnostic instrument system measured all ventilatory responses and energy expenditures. Volume and timing alterations in ventilation were characterized by measuring tidal volume (V[T]), inspiratory time (T[I]), inspiratory flow rate (V[T]/T[I]), and expiratory time (T[E]). Average ventilation, energy expenditure, and adequacy of ventilation were measured using minute ventilation (V[E]), oxygen consumption (VO2), carbon dioxide production (VCO2), and partial pressure of end-tidal carbon dioxide (end-tidal PET[CO2]).",['This study was completed to determine how ventilatory responses change by means of speech reading at three different sound pressure levels (SPL) as compared to quiet breathing prior to each task.']
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. Proponents of the model known as the ""human revolution"" claim that modern human behaviors arose suddenly, and nearly simultaneously, throughout the Old World ca. This view of events stems from a profound Eurocentric bias and a failure to appreciate the depth and breadth of the African archaeological record. In fact, many of the components of the ""human revolution"" claimed to appear at 40-50 ka are found in the African Middle Stone Age tens of thousands of years earlier. These features include blade and microlithic technology, bone tools, increased geographic range, specialized hunting, the use of aquatic resources, long distance trade, systematic processing and use of pigment, and art and decoration. These items do not occur suddenly together as predicted by the ""human revolution"" model, but at sites that are widely separated in space and time. The African Middle and early Late Pleistocene hominid fossil record is fairly continuous and in it can be recognized a number of probably distinct species that provide plausible ancestors for H. sapiens. The appearance of Middle Stone Age technology and the first signs of modern behavior coincide with the appearance of fossils that have been attributed to H. helmei, suggesting the behavior of H. helmei is distinct from that of earlier hominid species and quite similar to that of modern people. If on anatomical and behavioral grounds H. helmei is sunk into H. sapiens, the origin of our species is linked with the appearance of Middle Stone Age technology at 250-300 ka.Copyright 2000 Academic Press. #CITATION_TAG write of the ""revolution that wasn't,"" suggesting a more gradual rise in technological sophistication from the Middle Stone Age around 250,000-300,000 years ago, and Shea (2011) similarly argues that human technology over the past 200,000 years is characterized by a variability that persists today, rather than by the abrupt appearance of ""modern behavior."" Given that our species is estimated to have emerged some 200,000 years ago, it seems unlikely that there was a dramatic rewiring of the brain within the past 100,000 years.","Because the earliest modern human fossils, Homo sapiens sensu stricto, are found in Africa and the adjacent region of the Levant at >100 ka, the ""human revolution"" model creates a time lag between the appearance of anatomical modernity and perceived behavioral modernity, and creates the impression that the earliest modern Africans were behaviorally primitive.","['This fundamental behavioral shift is purported to signal a cognitive advance, a possible reorganization of the brain, and the origin of language.']"
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. A fundamental, universal property of human language is that its phonology is combinatorial. We present a novel model to investigate the hypothesis that combinatorial phonology results from optimising signal systems for perceptual distinctiveness. Some have argued against the gestural theory on the grounds that it must have required an unlikely transition from a visuo-manual format to an auditory-vocal one (e.g., Burling, 2005; #CITATION_TAG).","That is, one can identify a set of basic, distinct units (phonemes, syllables) that can be productively combined in many different ways. Our model differs from previous models in two important respects. First, signals are modelled as trajectories through acoustic space. Hence, both holistic and combinatorial signals have a temporal structure. Second, we use the methodology from evolutionary game theory. Crucially, we show a path of ever increasing fitness from holistic to combinatorial signals, where every innovation represents an advantage even if no-one else in a population has yet obtained it.","['In this paper, we review a number of theories and models that have been developed to explain the evolutionary transition from holistic to combinatorial signal systems, but find that in all problematic linguistic assumptions are made, or crucial components of evolutionary explanations are omitted.']"
"A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. A series of recent empirical observations demonstrate structured activity patterns that exist during passive task states. Rather paradoxically, this network is revealed by reverse subtraction; that is, the activation during involvement in some designated task is subtracted from that under passive conditions in which subjects were given no explicit instructions, and were free to let their minds wander (#CITATION_TAG).","One observation is that a network of regions, referred to as the default network, shows preferentially greater activity during passive task states as compared to a wide range of active tasks. The second observation is that distributed regions spontaneously increase and decrease their activity together within functional-anatomic networks, even under anesthesia. Maps of spontaneous network correlations also provide tools for functional localization and study of comparative anatomy between primate species.","['For all of these reasons, we advocate the systematic exploration of rest activity.']"
"They also help to localise certain riverine and urban landmarks (such as river arms or bridges) relevant for the history of Vienna. Alluvial rivers can show unpredictable channel changes and humans living along the river corridor repeatedly have to cope with the alterations of their physical environment. This was specifically the case in the mid-sixteenth century, when the Viennese were confronted with one major problem: the Danube River successively abandoned its main arm that ran close to the city and shifted further north. This was at the time when Vienna became the permanent residence of the Holy Roman Empire (except from 1583 to 1611 when it was moved to Prague), due to the Habsburgs holding the crown. Vienna was also the capital of the so-called Danube Monarchy, which came into being in the early sixteenth century. The city assumed increasing significance, being home to and hosting important authorities and persons. In particular after the first siege by the Ottoman army in 1529, the resource need for a complex fortification system was extremely high. Such was the case at the inflow of the Donaukanal near Nußdorf (#CITATION_TAG, in this issue) and at the banks of the Donaukanal close to the city walls (Thiel 1904).",,"[""This paper aims to highlight: (1) the morphological Danube dynamics together with floods and extreme weather situations in the sixteenth century; (2) the main actors in the trans- formation of the Danube; (3) changes of the river's course from 1550 to 1600/1650 and the consequences for bridging the river and the infrastructure as a whole; (4) the massive engineering measures that were undertaken in order to secure Vienna's requirements in the sixteenth century; (5) the question of floodplain settlements, and (6) the contested use of resources on the Viennese Danube floodplain.""]"
