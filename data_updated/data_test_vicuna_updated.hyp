We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. Speech output from speech-generating devices (SGD) and SGD software, such as talking word processors, has changed the landscape of options for aided communication. Combined with interatomic interactions, an entirely new class of superfluid or strongly correlated systems becomes accessible with ultracold atoms [34, 53, 63, #CITATION_TAG, 65].Learner-oriented roles of speech output are summarized in terms of graphic symbol learning, communicative functions and social regulation, learner preference, challenging behaviors, natural speech production, comprehension, and literacy. Roles for the learner - partner dyad include changes to interaction patterns. Methodological issues are discussed and practical implications are drawn where appropriate.
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The electron plasma frequency $\omega_{pe}$ and electron gyrofrequency $\Omega_e$ are two parameters that allow us to describe the properties of a plasma and to constrain the physical phenomena at play, for instance, whether a maser instability develops. The importance of the maser instability in coronal active regions depends on the complexity and topology of the magnetic field configurations.Comment: 10 pages, 7 figures, 1 appendix (with 1 figure This problem emerged dramatically when the analysis of the same large loop structure observed with Yohkoh/SXT on the solar limb led to three Living Reviews in Solar Physics http://www.livingreviews.org/lrsp-2014-4 different results depending mostly on the different ways to treat the background (Priest et al., 2000; #CITATION_TAG; Reale, 2002a).We perform an in-depth analysis of the $\omega_{pe}$/$\Omega_e$ ratio for simple theoretical and complex solar magnetic field configurations. Using the combination of force-free models for the magnetic field and hydrostatic models for the plasma properties, we determine the ratio of the plasma frequency to the gyrofrequency for electrons. For the sake of comparison, we compute the ratio for bipolar magnetic fields containing a twisted flux bundle, and for four different observed active regions. We also study how $\omega_{pe}$/$\Omega_e$ is affected by the potential and non-linear force-free field models. We demonstrate that the ratio of the plasma frequency to the gyrofrequency for electrons can be estimated by this novel method combining magnetic field extrapolation techniques and hydrodynamic models.
        Leading Edge Essay Distilling Pathophysiology from Complex Disease Genetics Aravinda Chakravarti,1,* Andrew G. Clark,2 and Vamsi K. Mootha3 1Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 2Cornell University, Ithaca, NY 14850, USA 3Massachusetts General Hospital, Boston, MA 02114, USA *Correspondence: aravinda@jhmi.edu http://dx.doi.org/10.1016/j.cell.2013.09.001 Technologies for genome-wide sequence interrogation have dramatically improved our ability to identify loci associated with complex human disease. However, a chasm remains between correlations and causality that stems, in part, from a limiting theoretical framework derived fromMendelian genetics and an incomplete understanding of disease physiology. It does not take much perspicacity to see that what really makes this difference is not the tall hat and the umbrella, but the wealth and nourishment of which they are evidence, and that a gold watch or membership of a club in Pall Mall might be proved in the same way to have the like sovereign virtues.. George Bernard Shaw, The Doctor's Dilemma (Preface), 1909 Distinguishing correlation from causality is the essence of experimental science. Nowhere is the need for this distinction greater today than in complex disease genetics, where proof that specific genes have causal effects on human disease phenotypes remains an enormous burden and challenge. This is particularly so in this age of routine -omic surveys, which can produce more false-positive than true-positive findings (Kohane et al., 2006). Moreover, genomic mapping and sequencing approaches that are invaluable for producing a list of unbiased candidates are, by themselves, insufficient for implicating specific gene(s) in a disease or biological process. We admit at the outset that the answers are not straightforward, and that there are serious technical and intellectual impediments to demonstrating causality for the common complex disorders of man where multiple interacting genes are involved. Nevertheless, the casual conflation of ''mapped locus'' to ''proven gene'' is a constant source of confusion and obfuscation in biology and medicine that requires remedy. Consider that two types of genomic surveys, one horizontal and the other vertical, are now routine for attempting to understand human biology and disease. In contrast, in vertical or deep surveys, we examine the effects of the genome as the DNA information gets processed, and its encoded functions get executed through its transcriptome, proteome, and effectors such as the metabolome. In turn, this implies that proving a gene's specific role in a biological process, either in wild-type or mutant form, may not be straightforward because its role may only be evident when examined in relation to its eptember 26, 2013 a2013 Elsevier Inc. 21 Box 1. biochemical partners, and in particular contexts of diet, pathogen exposure, etc. This is a particular problem in genetic studies of any outbred nonexperimental organism, such as the human, and studies of human disease, where investigations are observational not experimental. It is the strong belief of contemporary human geneticists that uncovering the genetic underpinnings of any disease, however complex, is the surest unbiased route to understanding its pathophysiology and, thus, enabling its future rational therapies (Brooke et al., 2008). Consequently, for this view to prevail, we should require experimental evidence, be it in cells, tissues, experimental models, or the rare patient, for the role of a specific gene in a disease process. We know that even in a simple model organism, budding yeast, synthetic lethality-- where death or some other phenotype occurs only through the conspiracy of mutations at two different genes--is widely prevalent (Costanzo et al., 2010). Interactions of greater complexity and involving more than two genes are also known in yeast (Hartman et al., 2001) and must be true for humans as well. A human genome will typically harbor 20 genes that are fully inactivated, without 22 Cell 155, September 26, 2013 a2013 Else any overt disease phenotype, presumably due to the buffering by other genes (MacArthur et al., 2012). Acknowledging this complexity, there are two general ways forward. The question then is how ''complex'' are complex traits and diseases? The New Genetics: Understanding the Function of Variation With the rediscovery of Mendel's rules of transmission more than 100 years ago, there was a vicious debate on the relative importance of single-gene versus multifactorial inheritance (Provine, 1971). Geneticists quickly, and successfully, focused on deciphering the specific mechanisms of gene inheritance and understanding the physiology of the gene in lieu of answering why some phenotypes had complex etiology and transmission. Nevertheless, the rare examples of deciphering the genetic basis of complex phenotypes, such as for truncate (wing) in Drosophila (Altenburg and Muller, 1920), clearly emphasized that traits were more than the additive properties of multiple genes. Today, it is quite clear that Mendelian inheritance of traits, including diseases, is the exception not the rule. Nevertheless, the entire language of genetics is in terms of individual genes for individual phenotypes, with one function, rather than the ensemble and emergent properties of genomes. This absence of a specific genetics language for the proper description of the multigenic architecture of traits (the ensemble) remains as an impediment to our understanding of the nature and degree of genetic complexity of the phenotype. The case of amyotrophic lateral sclerosis (ALS), a devastating, progressive motor neuron disease, illustrates this point (Ludolph et al., 2012). Despite the lack of evidence, we largely describe ALS as being ''heterogeneous'' and comprised of single-gene mutations that can individually lead to disease. In 1993, mutations in superoxide dismutase 1 (SOD1) were identified in an autosomal-dominant form of the disease; subsequently, the disorder has become synonymous with aberrant clearance of free radicals as its central pathology. What is often not appreciated, however, is that fewer than 10% of all cases of ALS are familial and even fewer follow an apparent Mendelian pattern. Even within this subset of cases, more than 20 distinct genes, spanning other pathways including RNA homeostasis, have been identified, and SOD1 represents a minority of cases. The molecular etiology for the majority of the sporadic forms of the disease remains unclear, and the scientific problem in understanding ALS is more than simply identification of additional genes. Are these the key rate-limiting steps to ALS or simply one of several required in concert? Is the aberrant clearance of free radicals the fundamental defect or one of many such pathologies or a common downstream consequence? Given the diversity and number of deleterious, even loss-of-function, genetic variants in all of our genomes (Abecasis et al., 2012; MacArthur et al., 2012) and, in the absence of stronger evidence bearing on these questions, it is fair to assume that ALS patients harbor multiple mutations with a plurality of molecular defects and that free radical metabolism is only one of a set of canonical pathophysiologies that define the disease. No doubt, this plurality is the case for cancer (Vogelstein et al., 2013), Crohn's disease (Jostins et al., 2012), and even rare developmental disorders such as Hirschsprung disease (McCallion et al., 2003). Molecular biology, genetics' twin, on the other hand, appears to have been far more successful in deciphering and describing not only its individual components (e.g., DNA, RNA, protein) but also their mutual relationships (e.g., DNAprotein interaction) and ensembles (e.g., transcriptional complex), although this is also far from complete (Watson et al., 2007). The consequences of the primary and interaction effects are often well understood, even though not completely described, at both the molecular and cellular levels (Alberts et al., 2007). Although the use of genetic tools and genetic perspectives are fundamental to this progress, these advances have not as yet led to a major revision of our understanding of trait or disease variation. The major reason for this discrepancy is that, with few exceptions (Raj et al., 2010), molecular and cell biology has focused on the impact of deleting or overexpressing genes and not grappled with the consequences of allelic variation. Classical Mendelian genetics has been a boon to uncovering biology from yeast to humans whenever a mutation with a simple inheritance pattern can be isolated. This approach has been revolutionary in the unicellular yeast, particularly because genetics (and gene manipulation), biochemistry, and cell biology were melded to understand function at a variety of levels. This kind of multilevel approach has been less straightforward, but still largely successful, for a metazoan such asDrosophilawheremore genes andmultiple specialized cells often rescue the effects of a mutation or enhance its minor effect. Success in this endeavor will require a synthesis of many biological disciplines that includes the role of genetic variation as intrinsic to the biological process, not an aspect to be ignored. Consequently, melding variation-based genetic and molecular biological thinking is of critical importance for both fields and is central to our understanding of mechanisms of trait variation, including interindividual variation in disease risk. If most disease, in most humans, is the consequence of the effects of variation at many genes, then knowledge of their functional relationships, rather than merely their identities, is central to understanding the phenotype. This is clearly a problem of ''Systems Biology'' but one that incorporates genetic variation directly. The ability to integrate the realities of such widespread genetic variation, which are ultimately at the causal root of disease mechanisms, with systems biology approaches to understand functional contingencies is central to the challenge of deciphering complex human disease. Genetic Dissection of Complex Phenotypes Genetic transmission rules imply that, even in an intractable species such as us, one can map genomic segments that must contain a disease or trait gene. Such mapping requires identification of the segregation of common sites of variation across the genome, now easy to identify through sequencing, and recognition of a genomic segment identical-by-descent in affected individuals, both within and between families. This task has become easier and more powerful as sequencing technology has improved to provide a nearly complete catalog of variants above 1% frequency in the population; further improvements to sample rarer variants are ongoing (Abecasis et al., 2012). Consequently, genetic mapping, once the province of rare Mendelian disorders, Cell 155, S is now applicable to any human trait or disease. For most complex traits examined, many such loci have been mapped, but the vast majority of the specific genes remain unidentified. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (Moon et al., 2004), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. In fact, most of the mapped loci underlying complex traits remain unresolved at the gene or mechanistic level. Despite the beginning clues to human disease pathophysiology that complex disease mapping is providing, and the slow identification of individual genes, it appears highly unlikely that we can understand traits and diseases this way. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (Yang et al., 2011). For Mendelian disorders, gene identification within a locus is made possible by each mutation being necessary and sufficient for the phenotype, being functionally deleterious and rare, and having an inheritance pattern consistent with the phenotype. It's the mutation that eventually reveals the biology and explains the phenotype. Any component locus for a complex disease has no such restriction, as the causal variants are neither necessary nor sufficient, nor coding (in fact, they are frequently noncoding and regulatory) nor rare (Emison et al., 2010; Jostins et al., 2012). Currently, the major attempts to overcome this impediment involve reliance on single severe mutations at the very same component genes and eptember 26, 2013 a2013 Elsevier Inc. 23 Genetic association studies in humans can synergize with prior knowledge and systems-level quantitative analysis to generate predictions of what pathways and modules are disrupted, where (anatomically), and when (developmentally) to yield a specificmorphological or biochemical phenotype. Consequently, these strategies themselves depend on the hidden biology we seek and are applicable only to the most common human diseases. It appears to us that ignorance of biology has become rate limiting for understanding disease pathophysiology, except perhaps for the Mendelian disorders. There are two ways to get out of this vicious cycle (Figure 1). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (Ilsley et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the vier Inc. complex inheritance problem (Yosef et al., 2013). One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013). Proving Causality: Molecular Koch's Postulates The evidence that a specific gene is involved in a particular human disease has historically been nonstatistical and based on our experience with identifying mutations in Mendelian diseases. Unfortunately, as already mentioned, all of these rules break down in complex phenotypes where neither cosegregation nor exclusivity to affecteds nor obviously deleterious alleles are likely; moreover, many mutations are suspected to be noncoding and in a diversity of regulatory RNA molecules. Consequently, statistical evidence of enrichment has been the mainstay, but this has two negative consequences: first, scanning across the genome or multiple loci covering tens to hundreds of megabases requires very large sample sizes and very strict levels of significance to guard against themany expected falsepositive findings; second, genetic effects that are small or genes with only a few causal alleles are notoriously difficult to detect, although they may be very important to understanding pathogenesis. This difficulty translates into a low power of detection, as common disease alleles cannot be distinguished from bystander associated alleles, whereas rare alleles are observed too infrequently to provide statistical significance. Consequently, although many genes are ''named'' as being responsible in a complex disease or disease process, proof of their involvement is either absent or circumstantial and not direct. In the late 19th century when bacteria were first shown to cause human disease, they were indiscriminately implicated in all manner of disease with little proof (Brown and Goldstein, 1992). One particularly embarrassing example was alcaptonuria, which Sir Archibald Garrod subsequently showed was inherited and which was his first ''inborn error of metabolism.'' We are likely to repeat this ''witch-hunt'' unless we are careful to note that mapping a locus is not equivalent to identifying the gene, and that identifying a gene and its mutations at a locus depends on numerous untested assumptions (mutational type, mutational frequency in cases and controls, coding or regulatory, cell autonomy). Inmicrobiology, Robert Koch set out three postulates that had to be satisfied to connect a specific bacterium (among the multitudes encountered, not unlike current genome analysis) to a disease: the agent had to be isolated from an affected subject, the agent had to produce disease when transmitted to an animal, and the agent had to be recoverable from an animal's lesion (Falkow, 1988). Simply because we cannot follow Koch to the letter in human patients does not absolve us from the responsibility of demonstrating a rigorous level of proof. This is particularly true if we are to pursue therapeutic targets for these diseases. It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-a-vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families Cell 155, S shows how environmental factors beyond diet can be examined even for congenital disorders (Sparrow et al., 2012). Despite these successes, pursuit of Koch's postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates. Ultimately, a lack of understanding of fundamental physiology is the biggest impediment to our understanding of genetically complex human disease. A unique aspect of genetics research seldom appreciated is that genetic effects are chronic biological exposures and as such can pinpoint the earliest stages of disease not readily studied otherwise. In reality, we still do not fully understand the pathogenesis stemming from some of the earliest identified human disease genes. With better understanding of disease mechanism, it seems likely that many disorders that we think of as ''genetic'' may have ameliorative diet, exercise, or other benign environmental ''treatments.'' Given the potential scientific and medical payoffs of disease gene discovery (Chakravarti, 2001), we argue in this Essay of the need for a rigorous examination of the assumptions under which we connect genes to phenotypes. Below we discuss the nature of the ''proof'' that we desire in order to make fundamental discoveries in human pathophysiology. Success in this difficult task requires us to solve a logical conundrum: how can we understand the genes underlying a phenotype if some of these component factors, in isolation, do not have recognizable phenotypes on their own? Both of these goals are approachable, particularly with recent advances in genome-editing technologies that allow the creation of multiple mutations within a single experimental organism (Wang et al., 2013). The second approach is to focus research on why the disease is complex in the first place. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. The chief criteria have been to demonstrate cosegregation with the phenotype in families, exclusivity of the mutation to affected individuals (rare alleles absent in controls), and the nature of themutation (a plausibly deleterious allele at a conserved site within a protein). We need to move beyond lists of plausible genes, to provide rigorous proof for their role in disease. But this goal is unlikely to be achieved in the absence of a superior understanding of the biology of hierarchical function within genomes, how variation alters these functions, and how these altered functions lead to human disease. What chiefly distinguishes cerebral cortex from other parts of the central nervous system is the great diversity of its cell types and interconnexions. It would be astonishing if such a structure did not profoundly modify the response patterns of fibres coming into it. In the cat's visual cortex, the receptive field arrangements of single cells suggest that there is indeed a degree of complexity far exceeding anything yet seen at lower levels in the visual system. In a previous paper we described receptive fields of single cortical cells, observing responses to spots of light shone on one or both retinas (Hubel & Wiesel, 1959). In the past, the technique of recording evoked slow waves has been used with great success in studies of functional anatomy. Daniel & Whitteiidge (1959) have recently extended this work in the primate. As the case for Marfan syndrome demonstrates, the identification of fibrillin 1 mutations was insufficient to identify therapies without the concomitant understanding of the pathophysiology (#CITATION_TAG).In the present work this method is used to examine receptive fields of a more complex type (Part I) and to make additional observations on binocular interaction (Part II). It was employed by Talbot & Marshall (1941) and by Thompson, Woolsey & Talbot (1950) for mapping out the visual cortex in the rabbit, cat, and monkey. Yet the method of evoked potentials is valuable mainly for detecting behaviour common to large populations of neighbouring cells; it cannot differentiate functionally between areas of cortex smaller than about 1 mm2. To overcome this difficulty a method has in recent years been developed for studying cells separately or in small groups during long micro-electrode penetrations through nervous tissue. Responses are correlated with cell location by reconstructing the electrode tracks from histological material. These techniques have been applied to
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Evidence is, however, mixed concerning the role of personality traits in predicting such success. Factors affecting academic performance have been the focus of research for many years (#CITATION_TAG; Lent, Brown, & Hacket, 1994; Moran & Crowley, 1979).Less than one fifth of Final Grade variance was explained by all the individual difference variables in combination.
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. E-Learning is embedded in learning and, without an understanding of what learning encompasses, it can be difficult for academics to develop into good teachers. It is suggested that, although this may appear to be a simple aim, it is not necessarily understood or applied by university academics in their teaching. Academics may have a 'philosophy of teaching', but in many cases even this may not be consciously held or successfully implemented. Openness, conscientiousness, and intrinsic motivation are correlated with a deep learning approach, while neuroticism, agreeableness, and extrinsic motivation are associated with a shallow learning approach (Busato et al., 1999; Duff et al., 2004; #CITATION_TAG).One inference is that university teachers need to develop a theory of learning and teaching.
        Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Constantinople is a city whose origin can be traced back to the establishment of Greek cities and colonies in early antiquity. Eventually it became the capital of the East Roman Empire, and since then its major role in the region has not diminished, whether under the rule of Byzantine emperors or Ottoman sultans. For more than 2000 years the city and its inhabitants have endured numerous changes and crises. Plague, war and economic regression have at times reduced its population to only a fraction of the previous size. The city has been subject to numerous sieges, the longest lasting eight years! Conquered only once prior to the major transformation in 1453, the city flourished again after each crisis and today it is still an important centre in this part of the world, on the border between the Mediterranean and the Black Sea. How could Constantinople maintain its leading position for such a long time, after suffering so many crises? The authors explore how the inhabitants of the ancient city of Constantinople managed to maintain a resilient food supply system. Constantinople differs in many ways from our modern cities, which are dependent on resources from a global hinterland that are transported using fossil fuels, and thus it can serve as an educational example for our time. At its first peak during the 6th century it was dependent on a complex grain transport system with ships travelling all the way to North Africa. This system collapsed in conjunction with the Arab expansion in the 7th century, and the collapse became a major part of a long recession that profoundly affected the city. That the city nonetheless survived cannot be explained by any single factor. A particularly interesting aspect, related to today's global transport system, is the urban agriculture system within and just outside the city walls. In our society, where the supply of food is considered as something obvious, one can question whether we lack memory as well as preparations for similar crises despite the fact that the food supply crisis of the Second World War is only 65 years behind us.Urban mind The most diffi cult blockade on the food supply lines, at the end of the fourteenth century AD, lasted an astonishing 8 years, but it did not succeed in starving out the urban population (#CITATION_TAG).They rather delimited an area with dispersed "sub-communities" and numerous acres of, for example, orchards and vineyards. This system was continuous and was maintained by the inhabitants' living memory as well as by important institutions.
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Widespread evidence for outward propagation of Alfvén waves is reported from ground optical polarimetric observations (#CITATION_TAG), and non-thermal broadening has been shown to correlate with swaying motions detected in the corona from SDO/AIA data (speed of ∼ 20 km s -1 and periods of few minutes) (McIntosh et al., 2011).Treatment of DC with intact IgG or Fab of mAb DF272 enhanced their T cell stimulatory capacity. By using a retrovirus-based cDNA expression library generated from DC, we cloned and sequenced the mAb DF272-defined cell surface receptor and could demonstrate that it is identical with B7-H1 (programmed death-1 ligand), a recently identified new member of the B7 family of costimulatory molecules.
        The fungicides used to control diseases in cereal production can have adverse effects on non-target fungi, with possible consequences for plant health and productivity. The fungal community on wheat leaves consisted mainly of basidiomycete yeasts, saprotrophic ascomycetes and plant pathogens. This study examined fungicide effects on fungal communities on winter wheat leaves in two areas of Sweden. Seed is among the most key input for improving crop production and productivity. Increasing the quality of seeds can increase the yield potential of the crop by significant folds. In recent years seed has become an international commodity used to exchange germplasm around the world. Seed is, however, also an efficient means of introducing plant pathogens into a new area as well as providing a means of their survival from one cropping season to another. Seed borne mycroflora are significant destroyers of food stuffs and grains during storage rendering them unfit for human consumption by retarding their nutritive value and often by producing mycotoxins. Seed-borne pathogens have been involved in seed rots during germination and seedling mortality leading to poor crop stand reduction in plant growth and productivity of crops. The seed-borne pathogens associated with seeds externally or internally may cause seed abortion, seed rot, seed necrosis, reduction or elimination of germination capacity, as well as seedling damage resulting in development of disease at later stages of plant growth by systemic or local infection. Infected seeds play considerable role in the establishment of economically important plant diseases in the field resulting in heavy reduction of crop yields. The ascomycete A. pullulans is one of the most common inhabitants of the phyllosphere of many crops [48] and is also present in many other habitats [#CITATION_TAG].It also discusses the detection mechanism and implies some management strategies that are implemented to reduce the loss due to seed borne fungi.
        The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Alluvial rivers can show unpredictable channel changes and humans living along the river corridor repeatedly have to cope with the alterations of their physical environment. This was specifically the case in the mid-sixteenth century, when the Viennese were confronted with one major problem: the Danube River successively abandoned its main arm that ran close to the city and shifted further north. This was at the time when Vienna became the permanent residence of the Holy Roman Empire (except from 1583 to 1611 when it was moved to Prague), due to the Habsburgs holding the crown. Vienna was also the capital of the so-called Danube Monarchy, which came into being in the early sixteenth century. The city assumed increasing significance, being home to and hosting important authorities and persons. In particular after the first siege by the Ottoman army in 1529, the resource need for a complex fortification system was extremely high. As noted by #CITATION_TAG, the increasing number of NHS investigations can be explained in the context of "a return to big government" (p.380).nan
        The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Working within parallel languages such as x10, which embodies the partitioned global address space (PGAS) model, some work has been done in loop parallelisation [#CITATION_TAG], and, while these are not included in the main release, there have been some experiments in parallelisation in the Fortran refactoring tool Photran [25].We examine a novel refactoring, extract concurrent, that introduces additional concurrency within a loop by arranging for some user-selected code in the loop body to run in parallel with other iterations of the loop.
        Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. Identification of key factors associated with the risk of developing cardiovascular disease and quantification of this risk using multivariable prediction algorithms are among the major advances made in preventive cardiology and cardiovascular epidemiology in the 20th century. The ongoing discovery of new risk markers by scientists presents opportunities and challenges for statisticians and clinicians to evaluate these biomarkers and to develop new risk formulations that incorporate them. Some researchers have advanced that the improvement in the area under the receiver-operating-characteristic curve (AUC) should be the main criterion, whereas others argue that better measures of performance of prediction models are needed. Although the AUC has been criticised on various methodological grounds (see, for example, [15, #CITATION_TAG]) this interpretation suggests that it also has a core theoretical weakness, at least when viewed from some perspectives.In this paper, we address this question by introducing two new measures, one based on integrated sensitivity and specificity and the other on reclassification tables. We discuss the properties of these new measures and contrast them with the AUC. We also develop simple asymptotic tests of significance. We illustrate the use of these measures with an example from the Framingham Heart Study. We propose that scientists consider these types of measures in addition to the AUC when assessing the performance of newer biomarkers.
        The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. A Climate Change Damage Function (CCDF) is a reduced form relationship linking macroeconomic aggregates (e.g., potential GDP) to climate indicators (e.g., average temperature levels). This function is used in a variety of studies about climate change impacts and policy analysis. However, despite the fact that this function is key in determining results in many integrated assessment models, it is not typically calibrated in a consistent and rigorous way. The developments in archaeology are part of broader trends in anthropology and psychology more generally (see e.g., #CITATION_TAG, Cronk et al. 2000, Dunbar & Barrett 2007, Durham 1991, Smith & Winterhalder 1992b, Sperber 1996, but the types of data dealt with by archaeologists and the diachronic questions they generally address have led to an emphasis on some theoretical perspectives rather than others and to the development of specifically archaeological methodologies for obtaining information relevant to testing evolutionary hypotheses.nan
        Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Particular foci of work in this vein are explorations of the place, role and impact of public bioethics in policy and biomedicine [32, #CITATION_TAG, 61, 62], and examinations of the mutual reinforcement-and perhaps co-production-of social and epistemic innovation in regards to controversial and/or promissory technoscience [22, 23].Second, it will argue that an "ethical" model has emerged alongside and partially displaced a "technical" model of expertise in scientific governance. The article will introduce the notion of "proper talk," a set of techniques for facilitating ethical debate, characterized by the active elicitation of public engagement and the inclusion of emotions and subjectivity.
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Context.This paper addresses the impulsive heating of very diffuse coronal loops, such as can occur in a nanoflare-heated corona with low filling factor. The high temperatures could never be directly measured in the corona due to the small emission measure and the most promising signature of such heating is blue-shifted plasma from the loop footpoints. These equations can be solved numerically and several specific codes have been used extensively to investigate the physics of coronal loops and of X-ray flares (e.g., Nagai, 1980; Peres et al., 1982; Doschek et al., 1982; Nagai and Emslie, 1984; Fisher et al., 1985a,a,a; MacNeice, 1986; Gan et al., 1991; Hansteen, 1993; Betta et al., 1997; Antiochos et al., 1999; Ofman and Wang, 2002; Müller et al., 2003; #CITATION_TAG; Sigalotti and Mendoza-Briceño, 2003; Bradshaw and Cargill, 2006).Methods.We derive an analytical model in order to gain some simple physical insights into the system and use a one dimensional hydrodynamic model that treats the electrons and ions as a coupled fluid to simulate nanoflare heating with time-scales of 30 s. Our analytical model also provide a means of verifying our numerical results.
        This is a repository copy of Topography discretization techniques for Godunov-type shallow water numerical models: a comparative study. It may be necessary to reorient the expectations of the power of RCA, or accept that RCA produces communication about clinical processes that would otherwise not have taken place, and whose effects may not be registering for some time to come. In recent years, they have received applied improvements and have been incorporated into water industry standard software (#CITATION_TAG), and used to support flood risk management (Néelz and Pender 2010).The interview data were discourse analysed and arranged into over-arching themes.
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Following the 2002 enactment of the Belgian law on euthanasia, which requires the consultation of an independent second physician before proceeding with euthanasia, the Life End Information Forum (LEIF) was founded which provides specifically trained physicians who can act as mandatory consultants in euthanasia requests. They tended to more often discuss the request with the attending physician (100% vs 95%) and with the family (76% vs 69%), and also more frequently helped the attending physician with performing euthanasia (44% vs 24%). Irrespective of whether euthanasia is a legal practice within a country, similar services may prove useful to also improve quality of consultations in various other difficult end-of-life decision-making situations. The value of specific training for the quality of second physician consultation was recently documented (#CITATION_TAG).This study assesses quality of consultations in Flanders and Brussels and compares these between LEIF and non-LEIF consultants.A questionnaire was sent in 2009 to a random sample of 3,006 physicians in Belgium from specialties likely involved in the care of dying patients. Several questions about the last euthanasia request of one of their patients were asked.
        Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. It is often suggested in the mass media and popular academic literature that scientists promote a secular and reductionist understanding of the implications of the life sciences for the concept of being human. Is adhering to this view considered to be one of the components of the notion of being a good scientist? This has long been a concern of STS scholars, who have shown extensively how scientists have views on the impact of their research on wider society but nevertheless seek to demarcate these from their professional work [#CITATION_TAG, 33, 49, 54].When discussing this question the interviewees distinguished between their 'personal' and 'professional' views.
        It argues that the analysis of the US Interagency Working Group on Social Cost of Carbon did not go far enough into the tail of low-probability, high-impact scenarios, and, via its approach to discounting, it mis-estimated climate risk, possibly hugely. This note considers the treatment of risk and uncertainty in the recently established 'social cost of carbon' (SCC) for analysis of federal regulations in the United States. Climate change is a serious and urgent issue. The Earth has already warmed by 0.7degC since around 1900 and is committed to further warming over coming decades simply due to past emissions. On current trends, average global temperatures could rise by 2-3degC within the next fifty years or so, with several degrees more in the pipeline by the end of the century if emissions continue to grow. affect the essential components of lives and livelihoods of people around the world -- water supply, food production, human health, availability of land, and ecosystems. It looks in particular at how these impacts intensify with increasing amounts of warming. The latest science suggests that the Earth's average temperature will rise by even more than 5 or 6degC if feedbacks amplify the warming effect of greenhouse gases through the release of carbon dioxide from soils or methane from permafrost. In general, impact studies have focused predominantly on changes in average conditions and rarely examine the consequences of increased variability and more extreme weather. In addition, almost all impact studies have only considered global temperature rises up to 4 or 5degC and therefore do not take account of threshold effects that could be triggered by temperatures higher than 5 or 6degC Doing so permits greater confidence that the target will be met, while the existence of a long-run quantity target in the first place can be supported by reasoning about the efficiency of price and quantity instruments under uncertainty (#CITATION_TAG).Throughout the chapter, changes in global mean temperature are expressed relative to pre-industrial levels (1750-1850). The chapter builds up a comprehensive picture of impacts by incorporating two effects that are not usually included in existing studies (extreme events and threshold effects at higher temperatures).
        It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. There has been significant recent interest in the commercial utilisation of algae based on their valuable chemical constituents many of which exhibit multiple bioactivities with applications in the food, cosmetic, agri- and horticultural sectors and in human health. Compounds of particular commercial interest include pigments, lipids and fatty acids, proteins, polysaccharides and phenolics which all display considerable diversity between and within taxa. The chemical composition of natural algal populations is further influenced by spatial and temporal changes in environmental parameters including light, temperature, nutrients and salinity, as well as biotic interactions. As reported bioactivities are closely linked to specific compounds it is important to understand, and be able to quantify, existing chemical diversity and variability. The high biodiversity of the latter and how they can serve the biotechnological field has been recently pointed out (#CITATION_TAG; Barra et al., 2014).nan
        We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Different types of psychotic symptoms may exist, some being normal variants and some having implications for mental health and functioning. Intermittent, infrequent psychotic experiences were common, but frequent experiences were not. Magical Thinking was only weakly associated with these variables. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas may represent expressions of underlying vulnerability to psychotic disorder, but Magical Thinking may be a normal personality variant. An alternative approach to solving the reverse inference problem has been developed by Poldrack and colleagues (e.g., #CITATION_TAG; Yarkoni, Poldrack, Nichols, Van Essen, & Wager, 2011).Method: Eight hundred and seventy-five Year 10 students from 34 schools participated in a cross-sectional survey that measured psychotic-like experiences using the Community Assessment of Psychic Experiences; depression using the Centre for Epidemiologic Studies Depression Scale; and psychosocial functioning using the Revised Multidimensional Assessment of Functioning Scale. Factor analysis was conducted to identify any subtypes of psychotic experiences. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas were strongly associated with distress, depression and poor functioning.
        The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. Recent research shows increasing interest in the concept of network pictures (Henneberg, Mouzas, & Naudé, 2009; Henneberg, Naudé, & Mouzas, 2010; Leek & Mason, 2010; #CITATION_TAG).Using qualitative methodologies including in-depth interviews and network mapping, the study reveals practitioners' network size and variety of contacts, and their role in client acquisition and retention.
        Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. The cerebellar granular layer has been suggested to perform a complex spatiotemporal reconfiguration of incoming mossy fiber signals. This characteristic connectivity has recently been investigated in great detail and been correlated with specific functional properties of these neurons. Important advances have also been made in terms of determining the membrane and synaptic properties of the neuron, and clarifying the mechanisms of activation by input bursts. In response to mossy fibers (mf) inputs, cerebellar (GrCs) respond with stereotyped patterns displaying a limited number of spikes (typically two or less [19, 20]) which are confined in a restricted time window, by the intervention of Golgi cells inhibition [21, #CITATION_TAG].Central to this role is the inhibitory action exerted by Golgi cells over granule cells: Golgi cells inhibit granule cells through both feedforward and feedback inhibitory loops and generate a broad lateral inhibition that extends beyond the afferent synaptic field. These include theta-frequency pacemaking, network entrainment into coherent oscillations and phase resetting.
        Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. The increased adoption of AMR techniques in the past decade is driven in part by the public availability of AMR codes and frameworks. Others have also provided evidence that autistic-like behavior can be prevented with mTOR treatment in mouse models of TSC (Tsai et al, 2012; Talos et al, 2012; #CITATION_TAG).Two basic techniques are in use to extend the dynamic range of Eulerian grid simulations in multi-dimensions: cell refinement, and patch refinement, otherwise known as block-structured adaptive mesh refinement (SAMR). I provide a partial list of resources for those interested in learning more about AMR simulations.
        The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Since becoming editor of AMR, I have tried to find a simple way to communicate the neces-sary ingredients of a theoretical contribution. There are several excellent treatises on the sub-ject, but they typically involve terms and con-cepts that are difficult to incorporate into every-day communications with authors and review-ers. My experience has been that available frameworks are as likely to obfuscate, as they are to clarify, meaning. Besides exposure to the works of Kaplan, Dubin, and others varies widely across the Academy. Any attempt to build theory needs to answer fundamental questions (Sushil, 2012; #CITATION_TAG), related to "what", "how" and "why" (Whetten, 1989).nan
        Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. alpha-Methylacyl-CoA racemase (AMACR) is a mitochondrial and peroxisomal enzyme involved in the metabolism of branched-chain fatty acid and bile acid intermediates. Recently, AMACR has been demonstrated to be over-expressed in localized and metastatic prostate cancer, suggesting that it may be an important tumor marker. AMACR protein over-expression was found in a number of cancers, including colorectal, prostate, ovarian, breast, bladder, lung, and renal cell carcinomas, lymphoma, and melanoma. Reverse transcriptase-polymerase chain reaction for AMACR using laser capture microdissected prostate tissue confirmed gene over-expression at the mRNA level. The use of the biomarker P504S, although not prostate specific [#CITATION_TAG], has facilitated the differentiation between normal, dysplastic, and malignant tissues in prostate biopsy samples.A survey of online Expressed Sequence Tags (ESTs) and Serial Analysis of Gene Expression (SAGE) databases revealed that AMACR was over-expressed in multiple cancers. Based on prior work, AMACR protein expression was divided into two categories: negative (negative to weak staining intensity) and positive (moderate to strong staining intensity).
        A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Recent research in multivoxel pattern-based fMRI analysis has led to considerable success at decoding within individual subjects. However, the goal of being able to decode across subjects is still challenging: It has remained unclear what population-level regularities of neural representation there might be. On the contrary, to decode across subjects, it is beneficial to abstract away from subject-specific patterns of neural activity and, instead, to operate on the similarity relations between those patterns: Our new approach performs decoding purely within similarity space. Beginnings are starting to be made in reproducing data across as well as within subjects (Accamma and Suma, 2012; #CITATION_TAG).nan
        Currently, cryptography is in wide use as it is being exploited in various domains from data confidentiality to data integrity and message authentication. Basically, cryptography shuffles data so that they become unreadable by unauthorized parties. However, clearly visible encrypted messages, no matter how unbreakable, will arouse suspicions. Fundamentally, steganography conceals secret data into innocent-looking mediums called carriers which can then travel from the sender to the receiver safe and unnoticed. This paper proposes a novel steganography scheme for hiding digital data into uncompressed image files using a randomized algorithm and a context-free grammar. Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of `model adaptation'. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the `transfer' can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). A context-free grammar or CFG is a mathematical system for modeling the structure of languages such as natural languages like English, French and Arabic, or computer programming languages like C++, Java, and C# [#CITATION_TAG].nan
        We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Such theories yield different classes of explanation, depending on the extent to which they emphasize adaptation to bounds, and adaptation to some ecology that differs from the immediate local environment. One move in this direction is the "computational rationality" approach (#CITATION_TAG; Lewis, Howes, & Singh, 2014), which applies Russell and Subramanian's (1995) notion of bounded optimality for artificial intelligence agents to the analysis of human behavior.The framework is based on the idea that behaviors are generated by cognitive mechanisms that are adapted to the structure of not only the environment but also the mind and brain itself. We call the framework computational rationality to emphasize the incorporation of computational mechanism into the definition of rational action. Theories are specified as optimal program problems, defined by an adaptation environment, a bounded machine, and a utility function. We illustrate this variation with examples from three domains: visual attention in a linguistic task, manual response ordering, and reasoning. We explore the relation of this framework to existing "levels" approaches to explanation, and to other optimality-based modeling approaches.Copyright (c) 2014 Cognitive Science Society, Inc.
        Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. A future sustainable electricity supply will be characterised by distributed electricity generation structure and will be based on the integration and use of renewable energy sources. Since many renewable energy sources like wind energy and solar energy are intermittent or vary in intensity throughout the day, the balancing problem between energy supply and energy consumption will increase. With an increased penetration of renewable energy sources into the grids it could become necessary to integrate more energy storage simultaneously with further renewable energy integration. PAGE09 uses a simple economic module (Hope et al., 1993; #CITATION_TAG; Hope, 2006; Hope, 2008; Hope, 2011) and expands it to consider climate issues and the linkages between the economic and the climate systems through some stylized equations within the climate module.nan
        The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. #CITATION_TAG have investigated first approaches towards large-scale requirements elicitation using social networks.A description of what they intend to measure is given together with how data are elicited and the advantages and limitation of the indicators. The glossary is divided into two parts for journal publication but the intention is that it should be used as one piece. The second part highlights a life course approach and will be published in the next issue of the journal.
        The 2007-9 period saw an unprecedented crisis emerge in global financial markets with the collapse of several large western financial institutions, and the nearest moment of systemic crisis yet witnessed in the globalised financial system. The crisis has thus provoked a significant questioning of market theories, and in particular understandings of market within orthodox neoclassical economics. Within the social sciences, a significant element of this response has built on a growing heterodox socioeconomic literature which is heavily critical of hegemonic conceptions of the market within economics. However, whilst a small body of work in economic geography has begun to engage with this literature, geographical thinking has not directly sought to conceptualise the nature and significance of market spatiality. Utilising a cultural economy approach, this paper therefore argues that economic geographical theories need to foreground the concept of market rather than treat markets as a 'component' of wider processes. Drawing on the growing heterodox socioeconomic literature on markets, it thus proposes a practice-oriented 'socio-spatial approach' for framing conceptions of market spatiality, arguing that such a spatial epistemology opens up a range of theoretical possibilities for further contesting hegemonic neoclassical theories of the market beyond current socioeconomic critiques. It seeks to illustrate the utility of such a framework through a case study analysis of the limitations inherent in existing policy practices surrounding the early phase of the recent global financial crisis. Four political systems (dictatorship, one-party, dominant party, democracy) and an index of political rights account for differences in political institutions. Pluralistic systems are associated with higher agricultural protection levels, although in a nonlinear fashion. The financial crisis that gripped the global financial system in the latter half of 2007 has prompted had significant and far-reaching re-evaluation of orthodox market theories and their associated neoliberal policy prescriptions (Cooper, 2008; #CITATION_TAG; Norfield, 2010), and this has persisted -if not deepened -with the sovereign debt crisis in the EU since 2010 (Lane, 2010; Mody and Sandri,2012).nan
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. This test -also referred to as the VASP index -specifically assesses the activity of the P2Y12 receptor [31] (the target of clopidogrel antiplatelet action), and is widely used for monitoring the responsiveness to clopidogrel [32, #CITATION_TAG].For pharmacokinetic analysis, blood was drawn at 0, 20, 40, 60, 90, 120, 180, 240 and 360 min after clopidogrel loading and peak plasma concentrations (C(max)) of the AMC were quantified with liquid chromatography-tandem mass spectrometry (LC-MS/MS). Platelet function testing was performed at baseline and 360 min after the clopidogrel loading.The VASP-assay, the VerifyNow P2Y12-assay and 20 micromol L(-1) adenosine diphosphate (ADP)-induced light transmittance aggregometry (LTA) showed strong correlations with C(max) of the AMC (VASP: R(2) = 0.56, P < 0.001; VerifyNow platelet reactivity units (PRU): R(2) = 0.48, P < 0.001; VerifyNow %inhibition: R(2) = 0.59, P < 0.001; 20 micromol L(-1) ADP-induced LTA: R(2) = 0.47, P < 0.001).
        Research published in this series may include views on policy, but the institute itself takes no institutional policy positions. The Institute for the Study of Labor (IZA) in Bonn is a local and virtual international research center and a place of communication between science, politics and business. IZA is an independent nonprofit organization supported by Deutsche Post Foundation. IZA Discussion Papers often represent preliminary work and are circulated to encourage discussion. Citation of such a paper should account for its provisional character. A revised version may be available directly from the author. The IZA research network is committed to the IZA Guiding Principles of Research Integrity. IZA engages in (i) original and internationally competitive research in all fields of labor economics, (ii) development of policy concepts, and (iii) dissemination of research results and concepts to the interested public. Is the way that people make risky choices, or tradeoffs over time, related to cognitive ability? This interpretation suggests that the positive correlation between risk aversion and IQ emphasized, among others, by #CITATION_TAG, could be an artifact of the format of the price list, as already argued by Andersson et al. (2013).We conduct choice experiments measuring risk aversion, and impatience over an annual time horizon, for a randomly drawn sample of roughly 1,000 German adults. Subjects also take part in two different tests of cognitive ability, which correspond to sub-modules of one of the most widely used IQ tests. Interviews are conducted in subjects' own homes. We perform a series of additional robustness checks, which help rule out other possible confounds.
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Five hundred and sixty-five first-year students completed a self-report questionnaire on three different occasions. Important factors include learning style (e.g., #CITATION_TAG; Chamorro-Premuzic & Furnham, 2008; Diseth, 2011; Sins et al., 2008) and self-regulation (e.g., Nasiriyan et al., 2011; Ning & Downing, 2010).nan
        The editorial reviews a number of insights by prominent scholars including Gerd Gigerenzer's treatise that "Scientists' tools are not neutral." The present study builds on the existing literature that underscores the value of fuzzy-set qualitative comparative analysis (fsQCA) (e.g., Fiss, 2011; #CITATION_TAG; Woodside & Zhang, 2013) and shows that the proposed methodological tool offers much in terms of understanding causal relationships, by virtue of providing information that is unique in comparison with the information that conventional correlational methods provide.The editorial includes an example of testing an MRA model for fit and predictive validity. The same data used for the MRA is used to conduct a fuzzy-set qualitative comparative analysis (fsQCA).
        First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. Increased regulatory competition has sharpened the comparative awareness of advantages or disadvantages of different national models of political economy, economic organization, governance and regulation. Although institutional change is slow and subject to functional complementarities as well as social and cultural entrenchment, at least some features of successful modern market economies have been in the process of converging over the last decades. Furthermore, at least to some extent, public enforcement is being reduced in favor of private enforcement by way of disclosure, enhanced liability, and correspondent litigation for damages. Corporatist approaches to governance are giving way to market approaches, and outsider and market-oriented corporate governance models seem to be replacing insider-based regimes. This transition is far from smooth and poses a daunting challenge to regulators and academics trying to redefine the fundamental governance and regulatory setting. They are confronted with the task of making or keeping the national regulatory structure attractive to investors in the face of competitive pressures from other jurisdictions to adopt state-of-the-art solutions. At the same time, however, they must establish a coherent institutional framework that accommodates the efficient, modern rules with the existing and hard-to-change institutional setting. 62 For a detailed, critical discussion of "market discipline", see #CITATION_TAG.As bureaucratic ex-ante control is replaced by judicial ex-post control, administrative discretion is replaced by the rule of law as guidelines for the economy. As a reflection of the transnationality of the issues addressed, the world's three leading economies and their legal systems are included on an equal basis: the EU, the U.S., and Japan across each of the subtopics of corporations, bureaucracy and regulation, markets, and intermediaries.
        Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. 2] [23] Evidence from four well-designed trials demonstrated that arthroscopic interventions 10 24 and meniscectomy [#CITATION_TAG] [26] [27] were no better or provided no additional effect, than the comparator (ie, sham surgery, physical therapy or a combination of physical and medical therapy) to relieve pain and improve function in the middle-aged patients with knee OA or early signs of knee OA.nan
        It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Lionfish are representative venomous fish, having venomous glandular tissues in dorsal, pelvic and anal spines. Some properties and primary structures of proteinaceous toxins from the venoms of three species of lionfish, Pterois antennata, Pterois lunulata and Pterois volitans, have so far been clarified. Nevertheless, the lionfish hyaluronidases as well as the stonefish hyaluronidases almost maintain structural features (active site, glyco_hydro_56 domain and cysteine location) observed in other hyaluronidases. Natural quick action characterizing these biocatalysts is a very interesting feature for biocatalytic manipulation of these carbohydrate-related molecules (Madokoro et al., 2011; #CITATION_TAG) also in view of bioactivity expressed by the polymer and HA oligosaccharides (Ariyoshi et al., 2012).The hyaluronidases of P. antennata and P. volitans were shown to be optimally active at pH 6.6, 37degC and 0.1 M NaCl and specifically active against hyaluronan. The primary structures (483 amino acid residues) of the lionfish hyaluronidases were elucidated by a cDNA cloning strategy using degenerate primers designed from the reported amino acid sequences of the stonefish hyaluronidases.
        Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Cancer progression is driven by a small number of genetic alterations accumulating in a neoplasm. These few driver alterations reside in a cancer genome alongside tens of thousands of other mutations that are widely believed to have no role in cancer and termed passengers. Many passengers, however, fall within protein coding genes and other functional elements and can possibly have deleterious effects on cancer cells. Surprisingly, despite selection against them, passengers accumulate and largely evade selection during progression. Although individually weak, the collective burden of passengers alters the course of progression leading to several phenomena observed in oncology that cannot be explained by a traditional driver-centric view. The second set of models, called "McF_4" and "McF_6", are based on McFarland et al's work [#CITATION_TAG] and lead to logistic-like behavior, as death rate depends on total population size.Our approach combines evolutionary simulations of cancer progression with the analysis of cancer sequencing data. In our simulations, individual cells stochastically divide, acquire advantageous driver and deleterious passenger mutations, or die. We tested predictions of the model using cancer genomic data.
        This paper presents a participatory methodology to design cards on social issues with the purpose to democratise knowledge among co-designers on the learning content of educational games. Although there is a consensus on the instructional potential of Serious Games (SGs), there is still a lack of methodologies and tools not only for design but also to support analysis and assessment. Most of the participatory models to design educational games are founded on educational theories and game design (see for example: Amory, 2007; #CITATION_TAG).The LM-GM model includes a set of pre-defined game mechanics and pedagogical elements that we have abstracted from literature on game studies and learning theories. Designers and analysts can exploit these mechanics to draw the LM-GM map for a game, so as to identify and highlight its main pedagogical and entertainment features, and their interrelations.
        Part 1 signposts classic work from cultural/media studies, marketing and sociology, which has been centrally concerned with meanings of popular culture designed for children and young people (e.g. via critiques of the gendered content of iconic popular cultural phenomena). Halfway through the paper is a 'commercial break'. With reference to a specific popular cultural artefact (the Toys 'Ia' Us Christmas toy catalogue), I argue that both meanings and matterings are crucial for geographers engaging with children and young people's popular cultures. This paper calls for more direct, careful, sustained research on geographies of children, young people and popular culture. I present three sets of empirical and conceptual resources for researchers developing work in this area. I argue that these conceptualisations can extend and unsettle classic work on popular culture, by questioning how popular cultural texts, objects and phenomena matter. Here, I present some personal ref lections on working at the intersection between the ideas discussed in Parts 1 and 2. Perhaps most inf luentially, a great deal of research within Anglo-American cultural/media studies, marketing and sociology has considered the way in which children and young people have increasingly been targeted as a market segment within contemporary consumer capitalism (#CITATION_TAG, Steinberg and Kincheloe 1997, Gunter and Furnham 1998, Langer 2002, Marshall 2010.We develop a novel distant supervised model that integrates the results from open information extraction techniques to perform relation extraction task from biomedical literature. Unlike state-of-the-art models for relation extraction in biomedical domain which are mainly based on supervised methods, our approach does not require manually-labeled instances. In addition, our model incorporates a grouping strategy to take into consideration the coordinating structure among entities co-occurred in one sentence.
        Large river valleys have long been seen as important factors to shape the mobility, communication, and exchange of Pleistocene hunter-gatherers. However, rivers have been debated as either natural entities people adapt and react to or as cultural and meaningful entities people experience and interpret in different ways. Both ecological and cultural factors are crucial to explaining these patterns. Whereas the Earlier Upper Paleolithic record displays a general tendency toward conceiving rivers as mobility guidelines, the spatial consolidation process after the colonization of the European mainland is paralleled by a trend of conceptualizing river regimes as frontiers, separating archaeological entities, regional groups, or local networks. The Late Upper Paleolithic Magdalenian, however, is characterized again by a role of rivers as mobility and communication vectors. Here, we attempt to integrate both perspectives. Tracing changing patterns in the role of certain river regimes through time thus contributes to our growing During the recent ten years or so there has been a distinct shift in social sciences towards studying things as real objects in themselves instead of treating them simply as correlative (Meillassoux 2008) of human social order or, ultimately, thought. It is also Ian Hodder's view that things have often not received the attention they deserve in archaeology. In his definition of what a thing is, Hodder follows a somewhat Heideggerian line of thinking. For Hodder objects become things when they enter the human realm. Things are for humans while objects always remain partly withdrawn from relations, as GrahamHarman (2005) would argue. At the beginning of his book Hodder claims that most recent thing-oriented approaches in archaeology have concentrated on what things can do for people, while the objective should have been to study the things themselves. Hodder's entanglement theory is based on three 'axioms': 1) humans depend on things; 2) things depend on other things; 3) things depend on humans. The relationship between humans and things is of a dialectic kind, humans and things constantly moving closer and further away from each other. This movement is not simply movement between two entities, but always includes a third entity the presence of which means that things are always moving towards something and, at the same time, further away from something. Things and persons become identified in this process, but it is also what constitutes ownership, an important notion in social archaeology. Moving closer to a thing identifies me with it while moving away from a thing identifies me as an individual thing. The listing of things, something Ian Bogost (2012, p. 38) has labelled 'Latour litanies', is an effective way to remind us of how things are surrounded by other things and how intimately things depend on other things even in such a simple operation as fire making, not to mention the up to 20,000 parts needed for a modern car to function, both Hodder's examples of entanglement. Things are connected in various ways. In fact Hodder uses the term 'thing' as synonymous with 'drawing together'. Things not only draw together people, but other things as well. In explaining the ways things are connected, Hodder explores the term 'network'. In Hodder's view 'network' implies too much dependence (as in 'it depends'), and he replaces network with entanglement. Others have done the same. Tim Ingold (2008), for example, has replaced network with 'meshwork', a term similar to Timothy Morton's (2010) 'mesh' (implying infinite connections and infinitesimal differences). Ian Bogost (2012) has proposed the term 'mess'. The main reason Hodder seems to abandon networks is that things are not equal in their affordances. Some things are more central than others. While stating that things depend on humans may seem at first like tautology (after all humans are things), things' dependence on humans is not at all a trivial notion. As noted above, some things are more central than others. As humans have dispersed over the whole globe, they have become, on a global scale, what Levi Bryant (2012), one of the prominent object-oriented philosophers, would call a rogue object, an object that emerges out of nowhere and changes everything. Human dispersal on earth surely has been such an event. Each spatial feature has therefore to be understood as an entanglement of natural properties and sociocultural dimensions (e.g., Gamble 1993; Tilley 1994; Rockman 2003; Meskell and Preucel 2004; Edgeworth 2011; #CITATION_TAG).Hodder provides an extensive array of examples of how things are connected to other things.Althoughhe never refers to any authors of the so-called objectoriented philosophies, his approach clearly shows some of the same characteristics. Unlike network, mess resists neat compartmentalization and order. Network for Hodder does not convey the 'stickiness' of dependence between things and humans.
        Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. The diagnosis of autonomic neuropathy frequently depends on results of tests which elicit reflex changes in heart rate. Few well-documented normal ranges are available for these tests. In view of the decline in heart rate variation with increasing age, normal ranges for tests of autonomic function must be related to the age of the subject. physical activity levels (Britton et al., 2007; Soares-Miranda et al., 2014), and age (#CITATION_TAG).A computerised method of measurement of R-R interval variation was used to study heart rate responses in 310 healthy subjects aged 18-85 years. Normal ranges (90% and 95% confidence limits) for subjects aged 20-75 years were calculated for heart rate difference (max-min) and ratio (max/min) and standard deviation (SD).
        Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Most studies on the link between the built environment and modal choice characterize and model this relationship by objectively measureable characteristics such as density and diversity. Recently, within the debate on residential self-selection, attention has also been paid to the importance of subjective influences such as the individual's perception of the built environment and his/her residential attitudes and preferences, resulting in models that take account of both the objective and subjective characteristics of the built environment. However, self-selection might occur on other points than residential location as well. Accordingly, what people at your workplace think and do (the subjective norm, corporate culture) influences your travel behaviour (Bonham and Koth, 2010; Heinen et al., 2011; McDonald, 2007; #CITATION_TAG).To this end, a modal choice model for leisure trips is developed using data on personal lifestyles and attitudes, collected via an Internet survey, and estimated using a path model consisting of a set of simultaneously estimated equations between observed variables. Moreover, we compared the results of a model with and without these subjective influences.
        Time Series Forecasting (TSF) uses past patterns of an event in order to predict its future values and is a key tool to support decision making. In the last decades, Computational Intelligence (CI) techniques, such as Artificial Neural Networks (ANN) and more recently Support Vector Machines (SVM), have been proposed for TSF. In this work, we propose a novel Evolutionary SVM (ESVM) approach for TSF based on the Estimation Distribution Algorithm to search for the best number of inputs and SVM hyperparameters. Feature-based time series representations have attracted substantial attention in a wide range of time series analysis methods. Recently, the use of time series features for forecast model averaging has been an emerging research focus in the forecasting community. Nonetheless, most of the existing approaches depend on the manual choice of an appropriate set of features. Exploiting machine learning methods to extract features from time series automatically becomes crucial in state-of-the-art time series analysis. are useful to aid tactical decisions, such as planning production resources or evaluating alternative economic strategies [#CITATION_TAG].We first transform time series into recurrence plots, from which local features can be extracted using computer vision algorithms. The extracted features are used for forecast model averaging.
        Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. Perhaps paradoxically, however, there is still little understanding of the circumstances under which certain methods will perform better than others. and is reported in [#CITATION_TAG], in a meta-analysis of classification studies, as being used in 'the vast majority' of comparative studies of classification rules (p19).Numerous methods have been designed in the past twenty years or so, but by different communities who had different approaches and interests. Comparative studies have thus been carried out to assess the relative merits of the methods.
        Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. 3 Many observed problems with incentive systems can be attributed to imbalanced multitask incentives (#CITATION_TAG).We applied unsupervised learning since the data sets did not have sentiment annotations. Note that unsupervised learning is a more realistic scenario than supervised learning which requires an access to a training set of sentiment-annotated data. We used SentiWordNet to establish a gold sentiment standard for the data sets and evaluate performance of Word2Vec and Doc2Vec methods.
        Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. Tensor models can be interpreted as theory of dynamical fuzzy spaces. It is found that the momentum distribution of the low-lying low-momentum spectra is in agreement with that of the metric tensor modulo the general coordinate transformation in the general relativity at least in the dimensions studied numerically, i.e. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19] [#CITATION_TAG] [21] [22] [23] [24] [25] [26].nan
        This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. However, many existing safety cases, in their attempt to manage potentially complex arguments, are poorly structured, presented and understood. This creates problems in developing and maintaining safety cases, and in capturing successful safety arguments for use on future projects. Drawing on safety-case argumentation, we create a diagrammatic summary of the structured argument of fitness for purpose, using a visual notation closely based on the standard safety-critical argumentation notation, goal structuring notation (GSN) [#CITATION_TAG, 29].A safety case should present a clear, comprehensive and defensible argument that a system is acceptably safe to operate within a particular context. This approach is based upon a graphical technique -- the Goal Structuring Notation (GSN) -- and has three strands. Firstly, a method for the use of GSN is defined together with an approach to supporting incremental safety case development. Thirdly, the concept of `Safety Case Patterns&apos; is defined as a means of supporting and promoting the reuse of successful safety arguments between safety cases. Examples of the approach are provided throughout.
        Phonological errors were scarce in both groups. Many business customers today consolidate their supply bases and implement preferred supplier programs. Consequently, vendors increasingly face the alternative of either gaining a key supplier status with their customers or being pushed into the role of a backup supplier. As product and price become less important differentiators, suppliers of routinely purchased products search for new ways to differentiate themselves in a buyer-seller relationship. At present, all models assume cascading of information (i.e. partial information from one level can be accessed by the next) from conceptual to lexico-semantic representations, as this allows multiple candidates to be activated for a particular target (Goldrick, 2006; Levelt et al., 1999); interactivity between other levels is still a topic of much debate (#CITATION_TAG; Rapp & Goldrick, 2000; Roelofs, 2004; Vigliocco & Hartsuiker, 2002).The authors identify service support and personal interaction as core differentiators, followed by a supplier's know-how and its ability to improve a customer's time to market.
        Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Although the associations between oral biologic variables such as malocclusion and oral-health-related quality of life (OHRQOL) have been explored, little research has been done to address the influence of psychological characteristics on perceived OHRQOL. It has also been found that psychological factors might explain more about the impact of dental disorders upon individuals than their clinical symptoms [39] [40] [41] [#CITATION_TAG] [43].The child perception questionnaire (CPQ11-14) and the PWB subscale of the child health questionnaire were administered at baseline and follow-up. Occlusal changes were assessed by using the dental aesthetic index. A waiting-list comparison group was used to account for age-related effects.Although the treatment subjects had significantly better OHRQOL scores at follow-up, the results were significantly modified by each subject's PWB status (P <0.01).
        A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. Proponents of the model known as the "human revolution" claim that modern human behaviors arose suddenly, and nearly simultaneously, throughout the Old World ca. This view of events stems from a profound Eurocentric bias and a failure to appreciate the depth and breadth of the African archaeological record. In fact, many of the components of the "human revolution" claimed to appear at 40-50 ka are found in the African Middle Stone Age tens of thousands of years earlier. These features include blade and microlithic technology, bone tools, increased geographic range, specialized hunting, the use of aquatic resources, long distance trade, systematic processing and use of pigment, and art and decoration. These items do not occur suddenly together as predicted by the "human revolution" model, but at sites that are widely separated in space and time. The African Middle and early Late Pleistocene hominid fossil record is fairly continuous and in it can be recognized a number of probably distinct species that provide plausible ancestors for H. sapiens. The appearance of Middle Stone Age technology and the first signs of modern behavior coincide with the appearance of fossils that have been attributed to H. helmei, suggesting the behavior of H. helmei is distinct from that of earlier hominid species and quite similar to that of modern people. If on anatomical and behavioral grounds H. helmei is sunk into H. sapiens, the origin of our species is linked with the appearance of Middle Stone Age technology at 250-300 ka.Copyright 2000 Academic Press. #CITATION_TAG write of the "revolution that wasn't," suggesting a more gradual rise in technological sophistication from the Middle Stone Age around 250,000-300,000 years ago, and Shea (2011) similarly argues that human technology over the past 200,000 years is characterized by a variability that persists today, rather than by the abrupt appearance of "modern behavior." Given that our species is estimated to have emerged some 200,000 years ago, it seems unlikely that there was a dramatic rewiring of the brain within the past 100,000 years.Because the earliest modern human fossils, Homo sapiens sensu stricto, are found in Africa and the adjacent region of the Levant at >100 ka, the "human revolution" model creates a time lag between the appearance of anatomical modernity and perceived behavioral modernity, and creates the impression that the earliest modern Africans were behaviorally primitive.
        The data stems from studies of searchers interaction with an XML information retrieval system. In this paper we present findings from a comparative study of data collected from client and server side loggings. The purpose is to see what factors of effort can be captured from the two logging methods. Throughout the eastern United States, plant species distributions and community patterns have developed in re- sponse to heterogeneous environmental conditions and a wide range of historical factors, including complex histories of natural and anthropogenic disturbance. Despite increased rec- ognition of the importance of disturbance in determining forest composition and structure, few studies have assessed the relative influence of current environment and historical factors on moder vegetation, in part because detailed knowl- edge of prior disturbance is often lacking. Similar to the forested uplands throughout the north- eastern United States, the site is physiographically heteroge- neous and has a long and complex history of natural and anthropogenic disturbance. Few species vary in accordance with ionic gradients, damage from the 1938 hurricane, or a 1957 fire. Such stages may be identified for instance in information seeking mediation, as in [#CITATION_TAG] where stages are identified as sets of cognitive and operational elements and transitions between stages are identified through vocabulary changes in dialogue.Soil analyses and historical sources document four catego- ries of historical land use on areas that are all forested today: cultivated fields, improved pastures/mowings, unimproved pastures, and continuously forested woodlots. Ordination and logistic regressions indicate that although species have re- sponded individualistically to a wide range of environmental and disturbance factors, many species are influenced by three factors: soil drainage, land use history, and C:N ratios.
        The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. This grounded the mirror system hypothesis of Rizzolatti and Arbib (1998) which offers the mirror system for grasping as a key neural "missing link" between the abilities of our nonhuman ancestors of 20 million years ago and modern human language, with manual gestures rather than a system for vocal communication providing the initial seed for this evolutionary process. The present article, however, goes "beyond the mirror" to offer hypotheses on evolutionary changes within and outside the mirror systems which may have occurred to equip Homo sapiens with a language-ready brain. Crucial to the early stages of this progression is the mirror system for grasping and its extension to permit imitation. Imitation is seen as evolving via a so-called simple system such as that found in chimpanzees (which allows imitation of complex "object-oriented" sequences but only as the result of extensive practice) to a so-called complex system found in humans (which allows rapid imitation even of complex sequences, under appropriate conditions) which supports pantomime. It is argued that these stages involve biological evolution of both brain and body. By contrast, it is argued that the progression from protosign and protospeech to languages with full-blown syntax and compositional semantics was a historical phenomenon in the development of Homo sapiens, involving few if any further biological changes. For example, #CITATION_TAG warn that closure and consensus are often enemies of the capacity of organisations to learn from incidents that require them "to confront the possibility that the story being told is simultaneously a tale of disorder in which the reality of danger masquerades as safety and a tale of order in which the reality masquerades as danger" (p. 456).The starting point is the observation that both premotor area F5 in monkeys and Broca's area in humans contain a "mirror system" active for both execution and observation of manual actions, and that F5 and Broca's area are homologous brain regions. This is hypothesized to have provided the substrate for the development of protosign, a combinatorially open repertoire of manual gestures, which then provides the scaffolding for the emergence of protospeech (which thus owes little to nonhuman vocalizations), with protosign and protospeech then developing in an expanding spiral.
        Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Modelling strategies for predicting the potential impacts of climate change on the natural distribution of species have often focused on the characterization of a species' bioclimate envelope. A number of recent critiques have questioned the validity of this approach by pointing to the many factors other than climate that play an important part in determining species distributions and the dynamics of distribution changes. Such factors include biotic interactions, evolutionary change and dispersal ability. However, it is stressed that the spatial scale at which these models are applied is of fundamental importance, and that model results should not be interpreted without due consideration of the limitations involved. #CITATION_TAG suggested a hierarchical approach to modelling environment-biota relationships whereby bioclimatic envelope models should form the first step, identifying the broad outlines of species' distributions.A hierarchical modelling framework is proposed through which some of these limitations can be addressed within a broader, scale-dependent context
        Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category "toys to pick up before bed" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the "just previous" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, "boring" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term "categorization." Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as "how do prototype representations compare between 6 and 8 months of age?" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. A large body of evidence supports the importance of focused attention for encoding and task performance. Yet young children with immature regulation of focused attention are often placed in elementary-school classrooms containing many displays that are not relevant to ongoing instruction. We know that environment matters because there are significant effects of household chaos (Petrill et al., 2004), excessive classroom decorations (#CITATION_TAG) and environmental noise (for a review see, Klatte et al., 2013) on children's cognition.We placed kindergarten children in a laboratory classroom for six introductory science lessons, and we experimentally manipulated the visual environment in the classroom.
        Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Apart from individuals with clinical psychosis, community surveys have shown that many otherwise well individuals endorse items designed to identify psychosis. Finally, socioeconomic variables have been suggested as possible modulatory variables on the expression of PLE (#CITATION_TAG; Scott, Chant, Andrews & McGrath, 2006).nan
        THE importance of the choice situation is reflected in the considerable amount of theory and research on conflict. Conflict theory has generally dealt, however, with the phenomena that lead up to the choice. What happens after the choice has received little attention. The present paper is concerned with some of the consequences of making a choice. Previous consideration of the consequences of choice have been limited to relatively unspecified hypotheses (1, 3) or to qualitative analysis (4). However, a recent theory by Festinger (2) makes possible several explicit predictions. When "dissonance" exists, the person will attempt to eliminate or reduce it. Although space limitations preclude further discussion of the theory, it may be said that several derivations are possible concerning the consequences of making a choice. Choosing between two alternatives creates dissonance and a consequent pressure to reduce it. The magnitude of the dissonance and the 1 This paper is based on a thesis offered in partial fulfillment of the requirements for the Ph.D. degree at the University of Minnesota. They have been studied both analytically and numerically in Ceniceros and Tian (2002), Forest and Lee (1986), Grenier (1998), #CITATION_TAG, Kamvissis (1996), Kamvissis et al. (2003), Miller and Kamvissis (1998), Tovbis et al. (2004 Tovbis et al. (, 2006.According to this analysis of the choice situation, all cognitive elements (items of information) that favor the chosen alternative are "consonant," and all cognitive elements that favor the unchosen alternative are "dissonant" with the choice behavior. The dissonance is reduced by making the chosen alternative more desirable and the unchosen alternative less desirable after the choice than they were before it. Exposing a person to new relevant cognitive elements, at least some of which are consonant, facilitates the reduction of dissonance.
        INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. The differences model, which argues that males and females are vastly different psychologically, dominates the popular media. Gender differences can vary substantially in magnitude at different ages and depend on the context in which measurement occurs. Overinflated claims of gender differences carry substantial costs in areas such as the workplace and relationships. These results, as well as the results herein, corroborate the gender similarities hypothesis: "most psychological gender differences are in the close-to-zero (d ≤ 0.10) or small (0.11 < d < 0.35) range, a few are in the moderate range (0.36 < d < 0.65), and very few are large (d < 0.66-1.00) or very large (d < 1.00)" (#CITATION_TAG).nan
        Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. Economists have been paying increasing attention to the study of situations in which consumers face a discrete rather than a continuous set of choices. Such models are potentially very important in evaluating the impact of government programs upon consumer welfare. But very little has been said in general regarding the tools of applied welfare economics indiscrete choice situations. Consumer surplus in the Logit model has been derived by #CITATION_TAG as (up to a constant)This paper shows how the conventional methods of applied welfare economics can be modified to handle such cases.
        A factor u of a word w is a cover of w if every position in w lies within some occurrence of u in w. A word w covered by u thus generalizes the idea of a repetition, that is, a word composed of exact concatenations of u. We consider the challenges of lack of data, incomplete knowledge and modelling in the context of a rapidly changing knowledge base. There is a mismatch in scale between these cellular models and tissue structures that are affected by tumours, and bridging this gap requires substantial computational resource. We present concurrent programming as a technology to link scales without losing important details through model simplification. Recall that the locus of a factor v of w, given by its start and end position in w, can be found in O(log log |v|) time [#CITATION_TAG].Computer simulation can be used to inform in vivo and in vitro experimentation, enabling rapid, low-cost hypothesis generation and directing experimental design in order to test those hypotheses. Here, we outline a framework that supports developing simulations as scientific instruments, and we select cancer systems biology as an exemplar domain, with a particular focus on cellular signalling models. Our framework comprises a process to clearly separate scientific and engineering concerns in model and simulation development, and an argumentation approach to documenting models for rigorous way of recording assumptions and knowledge gaps.
        Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. Four years previously, he was diagnosed with logopenic PPA (#CITATION_TAG).Criteria for the 3 variants of PPA--nonfluent/agrammatic, semantic, and logopenic--were developed by an international group of PPA investigators who convened on 3 occasions to operationalize earlier published clinical descriptions for PPA subtypes. Patients are first diagnosed with PPA and are then divided into clinical variants based on specific speech and language features characteristic of each subtype. Classification can then be further specified as "imaging-supported" if the expected pattern of atrophy is found and "with definite pathology" if pathologic or genetic data are available. The working recommendations are presented in lists of features, and suggested assessment tasks are also provided.
        Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Previous research has suggested that psychosis is better described as a continuum rather than a dichotomous entity. In general non-clinical populations, women have been found to have higher incidence of positive psychotic symptoms (Maric, Krabbendam, Vollebergh, de Graaf & Van Os, 2003), and specifically auditory hallucinations (#CITATION_TAG; Tien, 1991).Multinomial logistic regression models were used to interpret the nature of the latent classes, or groups, by estimating the associations with demographic factors, clinical variables, and experiences of traumatic events.The best fitting latent class model was a four-class solution: a psychosis class, a hallucinatory class, an intermediate class, and a normative class.
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. The European Association for Palliative Care Task (EAPC) Force on the Development of Palliative Care in Europe was created in 2003 and the results of its work are now being reported in full, both here and in several other publications. Different models of service delivery have been developed and implemented throughout the countries of Europe. For example, in addition to the UK, the countries of Germany, Austria, Poland and Italy have a well-developed and extensive network of hospices. The model for mobile teams or hospital support teams has been adopted in a number of countries, most notably in France. Day Centres are a development that is characteristic of the UK with hundreds of these services currently in operation. The number of beds per million inhabitants ranges between 45--75 beds in the most advanced European countries, to only a few beds in others. The countries with the highest development of palliative care in their respective subregions as measured in terms of ratio of services per one million inhabitants are: Western Europe -- UK (15); Central and Eastern Europe -- Poland (9); Commonwealth of Independent States -- Armenia (8). To date, worldwide, this is the only professional PC organisation to have done so (Federatie Palliatieve Zorg Vlaanderen 2003; #CITATION_TAG).Four studies, each with different working methods, made up the study protocol: a literature review, a review of all the existing palliative care directories in Europe, a qualitative `Eurobarometer' survey and a quantitative `Facts Questionnaire' survey.
        It argues that the analysis of the US Interagency Working Group on Social Cost of Carbon did not go far enough into the tail of low-probability, high-impact scenarios, and, via its approach to discounting, it mis-estimated climate risk, possibly hugely. This note considers the treatment of risk and uncertainty in the recently established 'social cost of carbon' (SCC) for analysis of federal regulations in the United States. In response to the increasing impact of regulation, several governments have introduced economic analysis as a way of trying to improve regulatory policy. We find that there is growing interest in the use of economic tools, such as benefit-cost analysis; however, the quality of analysis in the U.S. and European Union frequently fails to meet widely accepted guidelines. Perhaps unsurprisingly, the reality of carrying out benefit-cost analysis of federal regulations falls short of best practice, and it does not appear to be having a significant impact on many regulatory decisions, except higher-profile cases (#CITATION_TAG).nan
        In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Abstract The Modularity of Mind proposes an alternative to the "New Look" or "interaetionist" view of cognitive architecture that has dominated several decades of cognitive science. Whereas interactionism stresses the continuity of perceptual and cognitive processes, modularity theory argues for their distinctness. It is argued, in particular, that the apparent plausibility of New Look theorizing derives from the failure to distinguish between the (correct) claim that perceptual processes are inferential and the (dubious) claim that they are unencapsidated, that is, that they are arbitrarily sensitive to the organism's beliefs and desires. In fact, according to modularity theory, perceptual processes are computationally isolated from much of the background knowledge to which cognitive processes have access. The postulation of autonomous, domain-specific psychological mechanisms underlying perceptual integration connects modularity theory with the tradition of faculty psychology, in particular, with the work of Franz Joseph Call. It also references #CITATION_TAG 'Modularity of Mind'.nan
        This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service "free at the point of access", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland "outcomes frameworks" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. Background: National curriculum assessment (NCA) in England has been in place for nearly 20 years. It has its origins in a political desire to regulate education, holding schools accountable. However, its form and nature also reflect educational and curriculum concerns and technical assessment issues. Sources of evidence: The sources quoted are in the public domain, but in addition to academic articles, include political biographies and published official papers. This change reflects the political purposes of the system for accountability, and the pressure associated with this has led to growing criticism of the effects on children and their education. In the 1990s the UK Department of Education became rabid about routine educational testing, eventually pushing through dramatic reforms in the teeth of professional opposition (#CITATION_TAG).nan
        The growth in computer games and wireless networks has catalyzed the production of a new generation of hand-held game consoles that support multi-player gaming over IEEE 802.11 networks. Understanding the traffic characteristics of network games running on these new hand-helds is important for building traffic models and adequately planning wireless network infrastructures to meet future demand. This paper examines the traffic characteristics of IEEE 802.11 network games on the Nintendo DS and the Sony PSP. In addition, the games and hand-held platforms differ in their ability to handle degraded wireless network conditions and in the amount of broadcast traffic sent. Employment laws in India and Zimbabwe require employers to obtain permission from the government to retrench or lay off workers. However, in both countries a substantial decline in the demand for employees (other things equal) followed the new legislation. In Zimbabwe it is difficult to be precise about a causal connection between the drop in the demand for labor (allowing for concurrent increased wages) and the new legislation because enactment occurred simultaneously with Independence; however, the current economic climate induced high levels of investment in capital but not investments in long-term commitments to employees. Upon achieving independence in 1980, the government of Zimbabwe passed a new Employment Act, requiring employers to obtain permission from the Ministry of Labor to fire or lay off workers. Comparable regulations were imposed in India by the Industrial Disputes (Amendment) Act of 1976, requiring that written permission be obtained, normally from the relevant state government, either to close a plant or to retrench workers. Any addition to economic security in the lives of workers is clearly a laudable goal in its own right. But the question addressed in this article is whether these particular job security regulations have had undesirable side effects, which may even have thwarted the original goals of the legislation. The traffic generated by one host on a WLAN can have dramatic impact on the performance of other hosts on the WLAN [#CITATION_TAG, 8].nan
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Such achievement goals fall into two categories: performance goals, where an individual is looking for favourable feedback, and learning goals, where an individual desires to increase competency (#CITATION_TAG; Dweck, 1986; Dweck & Leggett, 1988; Eccles & Wigfield, 2002; Eppler & Harju, 1997).nan
        Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. Long-term potentiation (LTP) is a synaptic change supposed to provide the cellular basis for learning and memory in brain neuronal circuits. Although specific LTP expression mechanisms could be critical to determine the dynamics of repetitive neurotransmission, this important issue remained largely unexplored. A mathematical model of mossy fiber-granule cell neurotransmission showed that increasing release probability efficiently modulated the first-spike delay. Independent regulation of spike burst initiation and frequency during LTP may provide mechanisms for temporal recoding and gain control of afferent signals at the input stage of cerebellar cortex Whole-cell recordings from GrCs were obtained with patch-clamp technique [20, 29, #CITATION_TAG] by using an Axopatch 200B amplifier (Molecular Devices, Union City, CA, USA) (-3dB; cut-off frequency = 2 kHz).In agreement with a presynaptic expression caused by increased release probability, similar changes were observed by raising extracellular [Ca(2+)]. Glutamate spillover, by causing tonic NMDA and AMPA receptor activation, accelerated excitatory postsynaptic potential (EPSP) temporal summation and maintained a sustained spike discharge.
        Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Biotic interactions and their dynamics influence species' relationships to climate, and this also has important implications for predicting future distributions of species. It is already well accepted that biotic interactions shape species' spatial distributions at local spatial extents, but the role of these interactions beyond local extents (e.g. 10 km(2)  to global extents) are usually dismissed as unimportant. Simplified ecosystems where there are relatively few interacting species and sometimes a wealth of existing ecosystem monitoring data (e.g. arctic, alpine or island habitats) offer settings where the development of modelling tools that account for biotic interactions may be less difficult than elsewhere. Climate influences species distributions directly through species' physiological tolerances or indirectly through its effect on available habitats, food resources and biotic interactions such as the presence of competitors (Ara ujo & Peterson, 2012, #CITATION_TAG.In this review we consolidate evidence for how biotic interactions shape species distributions beyond local extents and review methods for integrating biotic interactions into species distribution modelling tools. A range of species distribution modelling tools is available to quantify species environmental relationships and predict species occurrence, such as: (i) integrating pairwise dependencies, (ii) using integrative predictors, and (iii) hybridising species distribution models (SDMs) with dynamic models. These methods have typically only been applied to interacting pairs of species at a single time, require a priori ecological knowledge about which species interact, and due to data paucity must assume that biotic interactions are constant in space and time. To better inform the future development of these models across spatial scales, we call for accelerated collection of spatially and temporally explicit species data. Ideally, these data should be sampled to reflect variation in the underlying environment across large spatial extents, and at fine spatial resolution.
        Leading Edge Essay Distilling Pathophysiology from Complex Disease Genetics Aravinda Chakravarti,1,* Andrew G. Clark,2 and Vamsi K. Mootha3 1Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 2Cornell University, Ithaca, NY 14850, USA 3Massachusetts General Hospital, Boston, MA 02114, USA *Correspondence: aravinda@jhmi.edu http://dx.doi.org/10.1016/j.cell.2013.09.001 Technologies for genome-wide sequence interrogation have dramatically improved our ability to identify loci associated with complex human disease. However, a chasm remains between correlations and causality that stems, in part, from a limiting theoretical framework derived fromMendelian genetics and an incomplete understanding of disease physiology. It does not take much perspicacity to see that what really makes this difference is not the tall hat and the umbrella, but the wealth and nourishment of which they are evidence, and that a gold watch or membership of a club in Pall Mall might be proved in the same way to have the like sovereign virtues.. George Bernard Shaw, The Doctor's Dilemma (Preface), 1909 Distinguishing correlation from causality is the essence of experimental science. Nowhere is the need for this distinction greater today than in complex disease genetics, where proof that specific genes have causal effects on human disease phenotypes remains an enormous burden and challenge. This is particularly so in this age of routine -omic surveys, which can produce more false-positive than true-positive findings (Kohane et al., 2006). Moreover, genomic mapping and sequencing approaches that are invaluable for producing a list of unbiased candidates are, by themselves, insufficient for implicating specific gene(s) in a disease or biological process. We admit at the outset that the answers are not straightforward, and that there are serious technical and intellectual impediments to demonstrating causality for the common complex disorders of man where multiple interacting genes are involved. Nevertheless, the casual conflation of ''mapped locus'' to ''proven gene'' is a constant source of confusion and obfuscation in biology and medicine that requires remedy. Consider that two types of genomic surveys, one horizontal and the other vertical, are now routine for attempting to understand human biology and disease. In contrast, in vertical or deep surveys, we examine the effects of the genome as the DNA information gets processed, and its encoded functions get executed through its transcriptome, proteome, and effectors such as the metabolome. In turn, this implies that proving a gene's specific role in a biological process, either in wild-type or mutant form, may not be straightforward because its role may only be evident when examined in relation to its eptember 26, 2013 a2013 Elsevier Inc. 21 Box 1. biochemical partners, and in particular contexts of diet, pathogen exposure, etc. This is a particular problem in genetic studies of any outbred nonexperimental organism, such as the human, and studies of human disease, where investigations are observational not experimental. It is the strong belief of contemporary human geneticists that uncovering the genetic underpinnings of any disease, however complex, is the surest unbiased route to understanding its pathophysiology and, thus, enabling its future rational therapies (Brooke et al., 2008). Consequently, for this view to prevail, we should require experimental evidence, be it in cells, tissues, experimental models, or the rare patient, for the role of a specific gene in a disease process. We know that even in a simple model organism, budding yeast, synthetic lethality-- where death or some other phenotype occurs only through the conspiracy of mutations at two different genes--is widely prevalent (Costanzo et al., 2010). Interactions of greater complexity and involving more than two genes are also known in yeast (Hartman et al., 2001) and must be true for humans as well. A human genome will typically harbor 20 genes that are fully inactivated, without 22 Cell 155, September 26, 2013 a2013 Else any overt disease phenotype, presumably due to the buffering by other genes (MacArthur et al., 2012). Acknowledging this complexity, there are two general ways forward. The question then is how ''complex'' are complex traits and diseases? The New Genetics: Understanding the Function of Variation With the rediscovery of Mendel's rules of transmission more than 100 years ago, there was a vicious debate on the relative importance of single-gene versus multifactorial inheritance (Provine, 1971). Geneticists quickly, and successfully, focused on deciphering the specific mechanisms of gene inheritance and understanding the physiology of the gene in lieu of answering why some phenotypes had complex etiology and transmission. Nevertheless, the rare examples of deciphering the genetic basis of complex phenotypes, such as for truncate (wing) in Drosophila (Altenburg and Muller, 1920), clearly emphasized that traits were more than the additive properties of multiple genes. Today, it is quite clear that Mendelian inheritance of traits, including diseases, is the exception not the rule. Nevertheless, the entire language of genetics is in terms of individual genes for individual phenotypes, with one function, rather than the ensemble and emergent properties of genomes. This absence of a specific genetics language for the proper description of the multigenic architecture of traits (the ensemble) remains as an impediment to our understanding of the nature and degree of genetic complexity of the phenotype. The case of amyotrophic lateral sclerosis (ALS), a devastating, progressive motor neuron disease, illustrates this point (Ludolph et al., 2012). Despite the lack of evidence, we largely describe ALS as being ''heterogeneous'' and comprised of single-gene mutations that can individually lead to disease. In 1993, mutations in superoxide dismutase 1 (SOD1) were identified in an autosomal-dominant form of the disease; subsequently, the disorder has become synonymous with aberrant clearance of free radicals as its central pathology. What is often not appreciated, however, is that fewer than 10% of all cases of ALS are familial and even fewer follow an apparent Mendelian pattern. Even within this subset of cases, more than 20 distinct genes, spanning other pathways including RNA homeostasis, have been identified, and SOD1 represents a minority of cases. The molecular etiology for the majority of the sporadic forms of the disease remains unclear, and the scientific problem in understanding ALS is more than simply identification of additional genes. Are these the key rate-limiting steps to ALS or simply one of several required in concert? Is the aberrant clearance of free radicals the fundamental defect or one of many such pathologies or a common downstream consequence? Given the diversity and number of deleterious, even loss-of-function, genetic variants in all of our genomes (Abecasis et al., 2012; MacArthur et al., 2012) and, in the absence of stronger evidence bearing on these questions, it is fair to assume that ALS patients harbor multiple mutations with a plurality of molecular defects and that free radical metabolism is only one of a set of canonical pathophysiologies that define the disease. No doubt, this plurality is the case for cancer (Vogelstein et al., 2013), Crohn's disease (Jostins et al., 2012), and even rare developmental disorders such as Hirschsprung disease (McCallion et al., 2003). Molecular biology, genetics' twin, on the other hand, appears to have been far more successful in deciphering and describing not only its individual components (e.g., DNA, RNA, protein) but also their mutual relationships (e.g., DNAprotein interaction) and ensembles (e.g., transcriptional complex), although this is also far from complete (Watson et al., 2007). The consequences of the primary and interaction effects are often well understood, even though not completely described, at both the molecular and cellular levels (Alberts et al., 2007). Although the use of genetic tools and genetic perspectives are fundamental to this progress, these advances have not as yet led to a major revision of our understanding of trait or disease variation. The major reason for this discrepancy is that, with few exceptions (Raj et al., 2010), molecular and cell biology has focused on the impact of deleting or overexpressing genes and not grappled with the consequences of allelic variation. Classical Mendelian genetics has been a boon to uncovering biology from yeast to humans whenever a mutation with a simple inheritance pattern can be isolated. This approach has been revolutionary in the unicellular yeast, particularly because genetics (and gene manipulation), biochemistry, and cell biology were melded to understand function at a variety of levels. This kind of multilevel approach has been less straightforward, but still largely successful, for a metazoan such asDrosophilawheremore genes andmultiple specialized cells often rescue the effects of a mutation or enhance its minor effect. Success in this endeavor will require a synthesis of many biological disciplines that includes the role of genetic variation as intrinsic to the biological process, not an aspect to be ignored. Consequently, melding variation-based genetic and molecular biological thinking is of critical importance for both fields and is central to our understanding of mechanisms of trait variation, including interindividual variation in disease risk. If most disease, in most humans, is the consequence of the effects of variation at many genes, then knowledge of their functional relationships, rather than merely their identities, is central to understanding the phenotype. This is clearly a problem of ''Systems Biology'' but one that incorporates genetic variation directly. The ability to integrate the realities of such widespread genetic variation, which are ultimately at the causal root of disease mechanisms, with systems biology approaches to understand functional contingencies is central to the challenge of deciphering complex human disease. Genetic Dissection of Complex Phenotypes Genetic transmission rules imply that, even in an intractable species such as us, one can map genomic segments that must contain a disease or trait gene. Such mapping requires identification of the segregation of common sites of variation across the genome, now easy to identify through sequencing, and recognition of a genomic segment identical-by-descent in affected individuals, both within and between families. This task has become easier and more powerful as sequencing technology has improved to provide a nearly complete catalog of variants above 1% frequency in the population; further improvements to sample rarer variants are ongoing (Abecasis et al., 2012). Consequently, genetic mapping, once the province of rare Mendelian disorders, Cell 155, S is now applicable to any human trait or disease. For most complex traits examined, many such loci have been mapped, but the vast majority of the specific genes remain unidentified. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (Moon et al., 2004), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. In fact, most of the mapped loci underlying complex traits remain unresolved at the gene or mechanistic level. Despite the beginning clues to human disease pathophysiology that complex disease mapping is providing, and the slow identification of individual genes, it appears highly unlikely that we can understand traits and diseases this way. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (Yang et al., 2011). For Mendelian disorders, gene identification within a locus is made possible by each mutation being necessary and sufficient for the phenotype, being functionally deleterious and rare, and having an inheritance pattern consistent with the phenotype. It's the mutation that eventually reveals the biology and explains the phenotype. Any component locus for a complex disease has no such restriction, as the causal variants are neither necessary nor sufficient, nor coding (in fact, they are frequently noncoding and regulatory) nor rare (Emison et al., 2010; Jostins et al., 2012). Currently, the major attempts to overcome this impediment involve reliance on single severe mutations at the very same component genes and eptember 26, 2013 a2013 Elsevier Inc. 23 Genetic association studies in humans can synergize with prior knowledge and systems-level quantitative analysis to generate predictions of what pathways and modules are disrupted, where (anatomically), and when (developmentally) to yield a specificmorphological or biochemical phenotype. Consequently, these strategies themselves depend on the hidden biology we seek and are applicable only to the most common human diseases. It appears to us that ignorance of biology has become rate limiting for understanding disease pathophysiology, except perhaps for the Mendelian disorders. There are two ways to get out of this vicious cycle (Figure 1). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (Ilsley et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the vier Inc. complex inheritance problem (Yosef et al., 2013). One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013). Proving Causality: Molecular Koch's Postulates The evidence that a specific gene is involved in a particular human disease has historically been nonstatistical and based on our experience with identifying mutations in Mendelian diseases. Unfortunately, as already mentioned, all of these rules break down in complex phenotypes where neither cosegregation nor exclusivity to affecteds nor obviously deleterious alleles are likely; moreover, many mutations are suspected to be noncoding and in a diversity of regulatory RNA molecules. Consequently, statistical evidence of enrichment has been the mainstay, but this has two negative consequences: first, scanning across the genome or multiple loci covering tens to hundreds of megabases requires very large sample sizes and very strict levels of significance to guard against themany expected falsepositive findings; second, genetic effects that are small or genes with only a few causal alleles are notoriously difficult to detect, although they may be very important to understanding pathogenesis. This difficulty translates into a low power of detection, as common disease alleles cannot be distinguished from bystander associated alleles, whereas rare alleles are observed too infrequently to provide statistical significance. Consequently, although many genes are ''named'' as being responsible in a complex disease or disease process, proof of their involvement is either absent or circumstantial and not direct. In the late 19th century when bacteria were first shown to cause human disease, they were indiscriminately implicated in all manner of disease with little proof (Brown and Goldstein, 1992). One particularly embarrassing example was alcaptonuria, which Sir Archibald Garrod subsequently showed was inherited and which was his first ''inborn error of metabolism.'' We are likely to repeat this ''witch-hunt'' unless we are careful to note that mapping a locus is not equivalent to identifying the gene, and that identifying a gene and its mutations at a locus depends on numerous untested assumptions (mutational type, mutational frequency in cases and controls, coding or regulatory, cell autonomy). Inmicrobiology, Robert Koch set out three postulates that had to be satisfied to connect a specific bacterium (among the multitudes encountered, not unlike current genome analysis) to a disease: the agent had to be isolated from an affected subject, the agent had to produce disease when transmitted to an animal, and the agent had to be recoverable from an animal's lesion (Falkow, 1988). Simply because we cannot follow Koch to the letter in human patients does not absolve us from the responsibility of demonstrating a rigorous level of proof. This is particularly true if we are to pursue therapeutic targets for these diseases. It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-a-vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families Cell 155, S shows how environmental factors beyond diet can be examined even for congenital disorders (Sparrow et al., 2012). Despite these successes, pursuit of Koch's postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates. Ultimately, a lack of understanding of fundamental physiology is the biggest impediment to our understanding of genetically complex human disease. A unique aspect of genetics research seldom appreciated is that genetic effects are chronic biological exposures and as such can pinpoint the earliest stages of disease not readily studied otherwise. In reality, we still do not fully understand the pathogenesis stemming from some of the earliest identified human disease genes. With better understanding of disease mechanism, it seems likely that many disorders that we think of as ''genetic'' may have ameliorative diet, exercise, or other benign environmental ''treatments.'' Given the potential scientific and medical payoffs of disease gene discovery (Chakravarti, 2001), we argue in this Essay of the need for a rigorous examination of the assumptions under which we connect genes to phenotypes. Below we discuss the nature of the ''proof'' that we desire in order to make fundamental discoveries in human pathophysiology. Success in this difficult task requires us to solve a logical conundrum: how can we understand the genes underlying a phenotype if some of these component factors, in isolation, do not have recognizable phenotypes on their own? Both of these goals are approachable, particularly with recent advances in genome-editing technologies that allow the creation of multiple mutations within a single experimental organism (Wang et al., 2013). The second approach is to focus research on why the disease is complex in the first place. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. The chief criteria have been to demonstrate cosegregation with the phenotype in families, exclusivity of the mutation to affected individuals (rare alleles absent in controls), and the nature of themutation (a plausibly deleterious allele at a conserved site within a protein). We need to move beyond lists of plausible genes, to provide rigorous proof for their role in disease. But this goal is unlikely to be achieved in the absence of a superior understanding of the biology of hierarchical function within genomes, how variation alters these functions, and how these altered functions lead to human disease. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (#CITATION_TAG).We estimate and partition genetic variation for height, body mass index (BMI), von Willebrand factor and QT interval (QTi) using 586,898 SNPs genotyped on 11,586 unrelated individuals. We show that the variance explained by each chromosome is proportional to its length, and that SNPs in or near genes explain more variation than SNPs between genes.
        Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Understanding value stability and change is essential for understanding values of both individuals and cultures.Yet theoretical thinking and empirical evidence on this topic have been scarce. Hooghe (2003, p. 93) implies a similar idea when arguing that association membership is unlikely to 'introduce qualitatively new values, but enforces already existing values' (see also #CITATION_TAG; Katz & Lazersfeld, 1955).In this article, the authors suggest a model outlining processes of individual value change. They identify five facilitators of value change (priming, adaptation, identification, consistency maintenance, and direct persuasion) and consider the moderating role of culture in each. In addition, the authors discuss the roles of culture, personal values, and traits as general moderators of value change.
        In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. The diffusion of innovation has been defined traditionally as the process by which an innovation is communicated through a social system to its members. The diffusion process is one of the most widely studied of all social processes with research in all of the social sciences education geography and business. Such widespread interest results in frequent new findings but also has the disadvantage that researchers interested in the subject must keep up with and know the language of several disciplines. The process of diffusion has been linked to characteristics of the innovation itself, the social system (community of potential adopters), channels of communication, and time (Rogers, 1995; #CITATION_TAG).The 4 parts of the diffusion process are: 1) the innovation 2) channels of communication 3) time and 4) the social system. Chapter 1 introduces basic concepts such as the S-shaped curve. Chapter 2 presents a fundamental deterministic diffusion model that allows for the review and integration of several widely cited models: the external-influence model the internal-influence model and the mixed-influence model. Chapter 3 discusses flexible diffusion models which are pattern-sensitive and can accommodate a variety of diffusion patterns. Chapter 4 includes selected advanced diffusion models such as dynamic models multi-innovation models space and time models and models that incorporate change agents. Chapter 5 discusses the applications of diffusion models in different contexts and disciplines.
        Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Simple, deterministic relationships between environmental stress and social change are inadequate. Extreme drought, for instance, triggered both social collapse and ingenious management of water through irrigation. Human responses to change, in turn, feed into climate and ecological systems, producing a complex web of multidirectional connections in time and space. Humans cannot predict the future. However, since the beginning of the industrial revolution, and especially after the start of the "great acceleration" following the end of WWII, there has been rapid economic expansion coupled with rapid urban growth-all driven by rapid expansion of fossil fuel use, especially oil (#CITATION_TAG).nan
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. INTRODUCTION The manner in which students are admitted for third level education and in particular the points system have become matters of increasing public controversy in recent years. While the subject is understandably emotive, a sad feature of many of the arguments presented to date has been the very limited factual support. A student who passes this examination has certainly been fully accepted by the university in accordance with its own criteria for assessment and any subsequent failure to graduate is indicative of faults in this assessment rather than any pre-university assessments. Such analyses cannot, of course, provide definitive answers to many important questions concerning entry to third level, as many of these are essentially political in character. SOME RELATED WORK 2.1 The literature on the predictive value of school leaving, scholastic aptitude and related tests for subsequent university performance is immense. Virtually all of this work has been undertaken outside Ireland and the only detailed research published in the Irish context has been Nevin (1974). Factors affecting academic performance have been the focus of research for many years (Farsides & Woodfield, 2003; Lent, Brown, & Hacket, 1994; #CITATION_TAG).The restriction to first year has also the important advantage of allowing the analysis of very recent university and school examinations to be presented. Our review of such work will be confined to what we consider most relevant and useful in the Irish context.
        Fifty per cent of responders had needed to consider MRE under GA. Aims To survey the perceived indications for magnetic resonance imaging of the small bowel (MRE) by experts, when MR enteroclysis (MREc) or MR enterography (MREg) may be chosen, and to determine how the approach to MRE is modified when general anaesthesia (GA) is required. Proximal SB distension is frequently less optimal in MRFT than in MRE. We, however, focused more on the choice between MREc and MREg as this has been more controversial and more studied, albeit only in patients with Crohn's disease [45, #CITATION_TAG].METHODS Data were collected from all patients undergoing small bowel (SB) magnetic resonance imaging (MRI) examination over a 32-mo period. Patients either underwent a magnetic resonance (MR) follow-through (MRFT) or a MR enteroclysis (MRE) in the supine position. The quality of proximal and distal SB distension as well as the presence of motion artefact and image quality were assessed by 2 radiologists. MRE is, therefore, the preferred MR examination method of the SB.
        Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. More specifically, we utilize representation learning methods (#CITATION_TAG), where words or phrases are embedded into the same vector space.The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.
        Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. The Stern Review on the Economics of Climate Change concluded that there can be "no doubt" the economic risks of business-as-usual (BAU) climate change are "very severe" [Stern, 2006. The Economics of Climate Change. Subsequently, a number of critiques have appeared, arguing that discounting is the principal explanation for this discrepancy. Together, the issues of risk and uncertainty on the one hand, and 'dangerous' climate change on the other, raise very strongly questions about the limits of a welfare-economic approach, where the loss of natural capital might be irreversible and impossible to compensate. There will always be an imperative to carry out integrated assessment modelling, bringing together scientific 'fact' and value judgement systematically. Ironically, the Stern Review is one of those voices. They also deal with intergenerational fairness, income regional distribution and, some of them, at least to a certain extent, risk and uncertainty management (#CITATION_TAG).A fixation with cost-benefit analysis misses the point that arguments for stabilisation should, and are, built on broader foundations
        It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Chitin is one the most abundant polymers in nature and interacts with both carbon and nitrogen cycles. Processes controlling chitin degradation are summarized in reviews published some 20 years ago, but the recent use of culture-independent molecular methods has led to a revised understanding of the ecology and biochemistry of this process and the organisms involved. The case of chitinases is illustrative (#CITATION_TAG).Principal environmental drivers of chitin degradation are identified which are likely to influence both community composition of chitin degrading bacteria and measured chitin hydrolysis activities.
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Dual antiplatelet therapy with aspirin and clopidogrel has been shown to reduce subsequent cardiac events in patients with acute coronary syndrome or coronary artery stenting. Clopidogrel, a thienopyridine, is a prodrug that is transformed in vivo to an active metabolite by the cytochrome P450 (CYP) enzyme system. The genes encoding CYP enzymes are polymorphic. Recent data demonstrated patients carrying a genetic variant of CYP enzymes (e.g. Furthermore, concomitant gastrointestinal ulcer prophylaxis with a proton pump inhibitor (PPI) is commonly prescribed to patients because of the increased risk of bleeding with dual antiplatelet therapy. Patients with loss of function polymorphism in the CYP2C19 gene are less responsive to clopidogrel [#CITATION_TAG, 6], although the importance of this phenomenon remains controversial [7] [8] [9] [10] and may be limited to the risk of stent thrombosis [11].PPIs are extensively metabolized by the cytochrome P450 system and have been associated with decreased antiplatelet activity of clopidogrel.
        Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Despite this, concerns remain that the invasion process is too complex for accurate predictions to be made. To identify potentially invasive species, risk assessment protocols based on species traits associated with invasiveness have been developed (#CITATION_TAG).nan
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. These arguments have been described extensively in Kimsma and Van Leeuwen (Asking to die. Inside the Dutch debate about euthanasia, Kluwer Academic Publishers, Dordrecht, 1998). This experience is confirmed by a Dutch qualitative study of the psychological and philosophical aspects of euthanasia requests (#CITATION_TAG).After some general introductory descriptions, by way of formulating a frame of reference, I shall describe the effects of this practice on patients, physicians and families, followed by a more philosophical reflection on the significance of these effects for the assessment of the authenticity of a request and the nature of unbearable suffering, two key concepts in the procedure towards euthanasia or physician-assisted suicide.
        Notwithstanding the significance to organizations of external reactions to bad behavior, the corporate social responsibility literature tends to focus on the meaning of and expectations for responsible behavior, rather than on the meaning of irresponsible behavior. We draw on attribution theory to describe how attributions of irresponsibility stem from the observer's subjective assessments of effect undesirabili... While increasingly more companies undertake CSR initiatives in an attempt to contribute to society or pursue their strategic goals, examples of corporate social irresponsibility abound (e.g., Carson, 2003; #CITATION_TAG; Murphy & Schlegelmilch, 2013).nan
        Drawing on the classic model of balanced affect, the Francis Burnout Inventory (FBI) conceptulises good work-related psychological health among clergy in terms of negative affect being balanced by positive affect. The high scorer on the psychoticism scale is characterised by Eysenck and Eysenck (1976), in their study of psychoticism as a dimension of personality, as being 'cold, impersonal, hostile, lacking in sympathy, unfriendly, untrustful, odd, unemotional, unhelpful, lacking in insight, strange, with paranoid ideas that people were against him.' Lie scales were originally introduced into personality inventories to detect the tendency of some respondents to 'fake good' and so to distort the resultant personality scores (#CITATION_TAG).The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.
        Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Infertility practice and reproductive technologies are generally seen as 'controversial' areas of scientific inquiry that raise many complex ethical issues. For example, Hooeyer [25] has shown how moral qualms around trade in human body parts are managed through systems of 'compensation' which ascribe value to biomaterials without the formation of 'markets', and Frith et al. [#CITATION_TAG] have underscored the routine engagement with ethical issues that constitutes clinical practice within the infertility clinic.We use the concept of ethical boundary-work to develop a theory of 'settled' and 'controversial' morality to illuminate how infertility clinicians drew boundaries between different conceptions of the role ethics played in their practice. We argue that by creating a space of 'no-ethics' in their practice--part of a settled morality that does not require articulation--the informants re-appropriate an area of their practice from 'outside' influences and control.
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. (A Study of the Effects of Multiple Doses of Dexlansoprazole, Lansoprazole, Omeprazole or Esomeprazole on the Pharmacokinetics and Pharmacodynamics of Clopidogrel in Healthy Participants; NCT00942175 However, it was recently demonstrated that generation of clopidogrel active metabolite and inhibition of platelet function are reduced less by the co-administration of dexlansoprazole or lansoprazole with clopidogrel than by the co-administration of esomeprazole or omeprazole [#CITATION_TAG].However, PPIs may inhibit CYP2C19, potentially reducing the effectiveness of clopidogrel.MethodsA randomized, open-label, 2-period, crossover study of healthy subjects (n = 160, age 18 to 55 years, homozygous for CYP2C19 extensive metabolizer genotype, confined, standardized diet) was conducted.
        Deep brain stimulation (DBS) is a standard therapy for several movement disorders, and the list of further indications that are investigated is growing rapidly. BACKGROUND Deep brain stimulation (DBS) is a recent treatment modality. Few studies have examined referral practices for DBS. PATIENTS Reviewed were 197 medical records of patients referred for DBS between December 1, 2005, and November 30, 2009. Referrals by movement disorder specialists vs other sources differed significantly in their percentages of good candidates (66.7% vs 40.4%, P = .002) and possible future candidates (14.7% vs 32.7%, P = .02) but not poor candidates (18.7% vs 25.0%, P = .60). Adequate expertise is necessary, as movement disorder specialists are more likely to identify good candidates for DBS (#CITATION_TAG).Yearly percentages were computed. Referral sources were categorized as movement disorder specialists vs non-movement disorder physicians and self-referred.
        Securitization makes mortgage-related risks internationally tradeable and thus contributes considerably to the international diversification of macroeconomic risk: in the years 2003-2008, the increase in international cross-holdings of securitized mortgage debt has lowered industrialized countries' conditional consumption volatility (relative to the United States) by about 10-15 percentage points. Domestic credit leads to better international risk sharing only if debt is securitized and traded internationally. Conversely, the risk-sharing benefits from securitization seem to evaporate if credit dries up -as it did in the recent financial crisis. La titrisation fait que les risques attaches aux hypotheques peuventetre transiges internationalement. Voila qui contribue de maniere significativea la diversification internationale des risques macroeconomiques : dans les annees 2003-2008, l'accroissement dans la dette hypothecaire titrisee detenue dans d'autres pays a reduit la volatilite de la consommation relative (par rapport auxEtats-Unis) des pays industrialises de 10-15 points de pourcentage. On examine le role du credit domestique dans l'explication de ce resultat. On montre que le credit domestique entraine un meilleur partage international du risque seulement An earlier version of this paper was circulated under the title 'Home bias: asset prices, securitization of mortgage debt and international risk sharing.' We have benefited from remarks by seminar participants at the ZEW-Bundesbank Workshop on Housing and Asset Markets, the The securitization of mortgage-related debt has played a major role in the emergence and proliferation of the current financial crisis (see #CITATION_TAG for a detailed account).Starting with the trends leading up to the crisis, I explain how these events unfolded and how four different amplification mechanisms magnified losses in the mortgage market into large dislocations and turmoil in financial markets.
        It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Marine microalgae have been used for a long time as food for humans, such as Arthrospira (formerly, Spirulina), and for animals in aquaculture. The biomass of these microalgae and the compounds they produce have been shown to possess several biological applications with numerous health benefits. Scientific knowledge present in literature for each of these sources seems to differ, from the well-known seaweed (Rinaudo, 2007) to relatively new sources of polysaccharides such as extremophiles and microalgae (#CITATION_TAG).It goes through the most studied activities of sulphated polysaccharides (sPS) or their derivatives, but also highlights lesser known applications as hypolipidaemic or hypoglycaemic, or as biolubricant agents and drag-reducers.
        This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. The implications of alternative near-term emissions targets for long-term climate goals are investigated by a concurrent study (#CITATION_TAG).nan
        Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. This creates a price differential between services that are identical for the consumer and generates network externalities despite network interconnection. #CITATION_TAG find that if firms set linear prices but can discriminate between on-and off-net calls, then above cost termination charges induce 3 In the initial stages of wireless telecommunication the most important regulatory issue was the fixedto-mobile (FTM) termination rate, i.e. the price to be paid by the incumbent land-line operator for calls terminating on a mobile network.Our companion article developed a clear conceptual framework of negotiated or regulated interconnection agreements between rival operators and studied competition between interconnected networks, under the assumption of nondiscriminatory pricing.
        Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. A nerve cell receives multiple inputs from upstream neurons by way of its synapses. Neuron processing functions are thus influenced by changes in the biophysical properties of the synapse, such as long-term potentiation (LTP) or depression (LTD). One major obstacle is the high dimensionality of the neuronal input-output space, which makes it unfeasible to perform a thorough computational analysis of a neuron with multiple synaptic inputs. Granule cells have a small dendritic tree (on average, they receive only four mossy fiber afferents), which greatly bounds the input combinatorial space, reducing the complexity of information-theoretic calculations. These selective mechanisms may have important consequences on the encoding of cerebellar mossy fiber inputs and the plasticity and computation at the next circuit stage, including the parallel fiberPurkinje cell synapses. Compensation systems have several roles beyond creating incentives (e.g., #CITATION_TAG; Ittner & Larcker 2002).Numerical simulations and LTP experiments quantified how changes in neurotransmitter release probability (p) modulated information transmission of a cerebellar granule cell.
        In their formulation, the causal conjuncture is a sequence of conditions or events. The main aim of the technique is to identify all necessary and sufficient conditions that lead to a specific outcome condition (#CITATION_TAG).This comment clarifies and corrects aspects of their analysis and present methods for assessing temporality that are more amenable to truth table analysis and the use of existing software, fsQCA. The methods presented utilize codings that indicate event order in addition to codings that indicate whether specific events occurred. They also demonstrate how to use ``don't care'' codings to bypass consideration of event sequences when they are not relevant (e.g., as when only a single event occurs).
        Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Our findings are reported in two parts: (1) how much and what kind of qualitative research has explored unassisted cessation (Tables 3 and 4); (#CITATION_TAG) what are the views and experiences of smokers who quit unassisted? (Table 5).Design/methodology/approach: The quantitative approach in this study was carried out by collecting survey data using a questionnaire instrument directly applied to 400 respondents in some cities in Indonesia. The analysis is conducted with SPSS, Wrap-PLS and Structural Equation Models (SEM).
        The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. Tanzi and Davoodi (1997), and #CITATION_TAG argue that corruption shifts public expenditures from growth-promoting to lowproductivity projects.It provides a synthetic review of recent studies that analyze this phenomenon empirically.
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. The importance of accurate estimation of student's future performance is essential in order to provide the student with adequate assistance in the learning process. If this assumption is invalid, conditional probabilities between attributes can be modelled as a Bayesian Network (#CITATION_TAG).We presented empirical experiments on the prediction of performance with a data set of high school students containing 8 attributes.
        The 2007-9 period saw an unprecedented crisis emerge in global financial markets with the collapse of several large western financial institutions, and the nearest moment of systemic crisis yet witnessed in the globalised financial system. The crisis has thus provoked a significant questioning of market theories, and in particular understandings of market within orthodox neoclassical economics. Within the social sciences, a significant element of this response has built on a growing heterodox socioeconomic literature which is heavily critical of hegemonic conceptions of the market within economics. However, whilst a small body of work in economic geography has begun to engage with this literature, geographical thinking has not directly sought to conceptualise the nature and significance of market spatiality. Utilising a cultural economy approach, this paper therefore argues that economic geographical theories need to foreground the concept of market rather than treat markets as a 'component' of wider processes. Drawing on the growing heterodox socioeconomic literature on markets, it thus proposes a practice-oriented 'socio-spatial approach' for framing conceptions of market spatiality, arguing that such a spatial epistemology opens up a range of theoretical possibilities for further contesting hegemonic neoclassical theories of the market beyond current socioeconomic critiques. It seeks to illustrate the utility of such a framework through a case study analysis of the limitations inherent in existing policy practices surrounding the early phase of the recent global financial crisis. This paper presents a theory of competition among pressure groups for political influence. Political equilibrium depends on the efficiency of each group in producing pressure, the effect of additional pressure on their influence, the number of persons in different groups, and the deadweight cost of taxes and subsidies. An increase in deadweight costs discourages pressure by subsidized groups and encourages pressure by taxpayers. The financial crisis that gripped the global financial system in the latter half of 2007 has prompted had significant and far-reaching re-evaluation of orthodox market theories and their associated neoliberal policy prescriptions (#CITATION_TAG; Kay, 2009; Norfield, 2010), and this has persisted -if not deepened -with the sovereign debt crisis in the EU since 2010 (Lane, 2010; Mody and Sandri,2012).nan
        Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Ninety-five percent of ex-smokers have quit smoking without professional assistance. Research investigating self-initiated smoking cessation has increased to the point where a sizable data base on this phenomenon is now available. First, we were aware of a small but not unsubstantial body of quantitative evidence on smokers who quit unassisted; [#CITATION_TAG] [49] [50] [51] [52] and second, in the course of our literature search we had identified a considerable number of qualitative studies on smoking cessation.nan
        During the stationary growth phase, the stoichiometric relationship depended on the limiting nutrient, but with generally increasing C:N:P ratio. Understanding these dynamics will be important for improving models of aquatic primary production and biogeochemical cycles in a warming climate. Most oceanic biogeochemical models include dynamic C:Chl a ratios with photoacclimation parameterization [1, [7] [8], and it is important to understand interaction effects of several environmental parameters for improved parameterization [9] [#CITATION_TAG].Photosynthetic rates, growth rates, cell carbon, cell protein, and chlorophyll a content of two diatom and two dinoflagellate species were measured. The microalgae were chosen to have one small and one large species from each phylogenetic group; the two size categories differed from each other by 1.5 orders of magnitude in terms of cell carbon or cell protein. The cultures for the experiments were grown under continuous light at an irradiance high enough for the light-saturation of growth for all four species. The four species were found to have similar maximum photosynthetic rates per unit chlorophyll a. The higher growth rates of the diatoms were shown to be related to their higher photosynthetic rates per unit carbon.
        Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. The demand for aggregation in evaluating managerial performance arises because reporting all the basic transactions and other nonfinancial information about performance is costly and impracticable (see Ashton [1982], Casey [1978], and Holmstrom and Milgrom [1987]). A buyer generally tries to avoid dependence on a particular supplier (#CITATION_TAG) but companies today tend to trade in some of their independence against cost savings by having fewer, heavily bound, high value suppliers (Swift & Coe, 1994).We identify necessary and sufficient conditions on the joint density function of the signals under which linear aggregation, a simple and commonly employed way to construct a performance evaluation measure, is optimal. This characterization suggests that the linear form of aggregation is optimal for a large class of situations. We interpret these weights in terms of statistical characteristics (sensitivity and precision) of the joint distribution of the signals.
        In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Wheeler's argument draws on analytic philosophy, continental philosophy, and empirical work to "reconstruct" the philosophical foundations of cognitive science in a time of a fundamental shift away from a generically Cartesian approach. Indeed, they are often seen to be philosophically flawed (#CITATION_TAG).Wheeler begins with an interpretation of Descartes. He defines Cartesian psychology as a conceptual framework of explanatory principles and shows how each of these principles is part of the deep assumptions of orthodox cognitive science (both classical and connectionist). Wheeler then turns to Heidegger's radically non-Cartesian account of everyday cognition, which, he argues, can be used to articulate the philosophical foundations of a genuinely non-Cartesian cognitive science.
        Lipids comprise the bulk of the dry mass of the brain. In addition to providing structural integrity to membranes, insulation to cells and acting as a source of energy, lipids can be rapidly converted to mediators of inflammation or to signaling molecules that control molecular and cellular events in the brain. The advent of soft ionization procedures such as electrospray ionization (ESI) and atmospheric pressure chemical ionization (APCI) have made it possible for compositional studies of the diverse lipid structures that are present in brain. These include phospholipids, ceramides, sphingomyelin, cerebrosides, cholesterol and their oxidized derivatives. These proteins may be potential therapeutic targets since they transport lipids required for neuronal growth or convert lipids into molecules that control brain physiology. In this review, we examine the structure of the major lipid classes in the brain, describe methods used for their characterization, and evaluate their role in neurological diseases. The potential utility of characterizing lipid markers in the brain, with specific emphasis on disease mechanisms, will be discussed. Combining lipidomics and proteomics will enhance existing knowledge of disease pathology and increase the likelihood of discovering specific markers and biochemical mechanisms of brain diseases. Abstract: Phospholipase A2 (PLA2) is the name for the class of lipolytic enzymes that hydrolyze the acyl group from the sn-2 position of glycerophospholipids, generating free fatty acids and lysophospholipids. The products of the PLA2-catalyzed reaction can potentially act as second messengers themselves, or be further metabolized to eicosanoids, platelet-activating factor, and lysophosphatidic acid. The presence of PLA2 in the central nervous system, accompanied by the relatively large quantity of potential substrate, poses an interesting dilemma as to the role PLA2 has during both physiologic and pathologic states. Several different PLA2 enzymes exist in brain, some of which have been partially characterized. However, under pathological situations, increased PLA2 activity may result in the loss of essential membrane glycerophospholipids, resulting in altered membrane permeability, ion homeostasis, increased free fatty acid release, and the accumulation of lipid peroxides. These processes, along with loss of ATP, may be responsible for the loss of membrane phospholipid and subsequent neuronal injury found in ischemia, spinal cord injury, and other neurodegenerative diseases. The incorporation of PUFAs into phospholipid subclasses is highly choreographed such that most PUFAs are initially incorporated into 1,2-diacyl phospholipids subclasses before they are remodeled into the ether-linked phospholipids classes by coenzyme A (CoA)-dependent or CoA-independent transacylase activities [#CITATION_TAG, 84, 126, 165, 167].They are classified into two subtypes, CA2+-dependent and Ca2+-independent, based on their catalytic dependence on Ca2+.
        Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. For companies, the ability to create superior value to customers by forging stronger relationships with them is not only considered a perquisite for survival, but also as a way to increase profitability. Because of the sensitiveness of the information, the collected customer data is not published. However, due to the limitations of the research methodology, the argument that increased customer loyalty will lead to greater customer value cannot be researched to the full extent. Suppliers, on the other hand, try to stabilize their customer base because customer retention is less costly than customer acquisition (#CITATION_TAG).The assessment was based on the analysis of the statistical relationship between customer loyalty and customer value within the Finnish market of the case company. This study was implemented as a quantitative research by using RFM-analysis to evaluate Finnish customers based on two variables; value and loyalty.
        The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The Semantic Web presents the vision of a distributed, dynamically growing knowledge base founded on formal logic. Common users, however, seem to have problems even with the simplest Boolean expression. As queries from web search engines show, the great majority of users simply do not use Boolean expressions. Ginseng relies on a simple question grammar which gets dynamically extended by the structure of an ontology to guide users in formulating queries in a language seemingly akin to English. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [3] [#CITATION_TAG] 6, 9, 13, 14, 16].We address this problem by presenting Ginseng, a quasi natural language guided query interface to the Semantic Web. Based on the grammar Ginseng then translates the queries into a Semantic Web query language (RDQL), which allows their execution.
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. However, the most popular of these, the Barrett-Crane model, does not have the good boundary state space and there are indications that it fails to yield good low-energy n-point functions. Some patients express wishes of abbreviation of suffering at the end of their life, but when the time comes, they do not want to be informed of the imminence of their death (#CITATION_TAG (Bernheim, 2001 Pardon et al. 2012a).We present an alternative dynamics that can be derived as a quantization of a Regge discretization of Euclidean general relativity, where second class constraints are imposed weakly. Its state space matches the SO(3) loop gravity one and it yields an SO(4)-covariant vertex amplitude for Euclidean loop gravity.
        This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Over the past 20 years, an insurance-inflected discourse has migrated from the purely financial side of the health system into the heart of traditional medicine - the doctor-patient relationship. Over the same period, the body of law that structures most private group health insurance - ERISA - has effectively delegated control of risk pooling and resource allocation to the employers that sponsor group plans. The discourse of managing risk bonds these two components of health law and the health care system: patient care and access to coverage. From a normative perspective, the greatest problem with risk-centered governance arises from a democracy deficit. Because almost all health insurance risk pools are based in workplaces, there is potential to draw on the social networks created by work as a mechanism for building new, localized publics engaged with health policy. Treating insurance risk pools as potential mechanisms of governance, rather than merely as actuarial units, would force the publicizing (at least within the workplace) of myriad political decisions: who gets included and excluded in the pooling process, how allocation decisions are made, and whether there are systems of accountability and checks and balances sufficient to produce a risk allocation system that is equitable, as well as efficient and flexible. For example, #CITATION_TAG, using panel data, find that health policy interventions are superior in democracies.Drawing on a history of ERISA that has not been explored in legal scholarship, I demonstrate how the private welfare state of workplace-based health insurance has evolved into the creation of what amounts to corporate sovereignty in controlling access to health coverage.
        According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Patient safety has been an under-recognised and under-researched concept until recently. It is now high on the healthcare quality agenda in many countries of the world including the UK. The recognition that human error is inevitable in a highly complex and technical field like medicine is a first step in promoting greater awareness of the importance of systems failure in the causation of accidents. Plane crashes are not usually caused by pilot error per se but by an amalgam of technical, environmental, organisational, social and communication factors which predispose to human error or worsen its consequences. In healthcare, the systematic investigation of error in the administration of medication will often reveal similarly complex causation. The NHS is putting in place a comprehensive programme to learn more effectively from adverse events and near misses. In such cases it could be better to invest in bonding and make sure that the subsidiary shares the same company culture and adheres to MNC rules of conduct (#CITATION_TAG).nan
        Multistable perception is the spontaneous alternation between two or more perceptual states that occurs when sensory information is ambiguous. Multistable phenomena permit dissociation of neural activity related to conscious perception from that related to sensory stimulation, and therefore have been used extensively to study the neural correlates of consciousness. The finding that activity in a given brain region (e.g. frontoparietal cortex Kleinschmidt et al., 1998; Lumer and Rees, 1999; #CITATION_TAG; Weilnhammer et al., 2013) correlates with perception does not reveal whether that region drives the alternations, or reflects a consequence of processes occurring elsewhere.nan
        To understand price changes one must determine the relative impact of supply and demand shifts on price. Commodity price booms are best explained by macroeconomic rather than market-specific factors. The demand for grains and oilseeds as biofuel feedstocks was the main cause of the price rise but macroeconomic and financial factors explain its extent. The futures market may be an important monetary transmission mechanism, but it is commodity investors, not speculators, who, by investing in commodities as an asset class, may have generalized prices rises across markets.Food prices, commodity prices, money, futures markets #CITATION_TAG emphasizes the impact of common factors on the general level of agricultural food prices.nan
        Ternary algebras, constructed from ternary commutators, or as we call them, ternutators, defined as the alternating sum of products of three operators, have been shown to satisfy cubic identities as necessary conditions for their existence. These generalizations are algebraic structures in which the two entries Lie bracket has been replaced by a bracket with n entries. Three-Lie algebras have surfaced recently in multi-brane theory in the context of the Bagger-Lambert-Gustavsson model. Its appearance in Physics was due to the pioneering work of Nambu [4], and more recently, the work of Bagger and Lambert [5] renewed interest in ternary algebras in the Theoretical Physics community (see also the review article [#CITATION_TAG]).Each type of n-ary bracket satisfies a specific characteristic identity which plays the r\^ole of the Jacobi identity for Lie algebras. Particular attention will be paid to generalized Lie algebras, which are defined by even multibrackets obtained by antisymmetrizing the associative products of its n components and that satisfy the generalized Jacobi identity (GJI), and to Filippov (or n-Lie) algebras, which are defined by fully antisymmetric n-brackets that satisfy the Filippov identity (FI). Because of this, Filippov algebras will be discussed at length, including the cohomology complexes that govern their central extensions and their deformations (Whitehead's lemma extends to all semisimple n-Lie algebras). When the skewsymmetry of the n-Lie algebra is relaxed, one is led the n-Leibniz algebras. The standard Poisson structure may also be extended to the n-ary case. We shall review here the even generalized Poisson structures, whose GJI reproduces the pattern of the generalized Lie algebras, and the Nambu-Poisson structures, which satisfy the FI and determine Filippov algebras.
        There is evidence that breastfeeding reduces the risk of type 2 diabetes. To evaluate the effect of breastfeeding on long-term (breast carcinoma, ovarian carcinoma, osteoporosis and type 2 diabetes mellitus) and short-term (lactational amenorrhoea, postpartumdepression, postpartum weight change) maternal health outcomes. The interplay of factors that affect post-partum loss or retention of weight gained during pregnancy is not fully understood. Weight change patterns varied significantly among sites. In Brazil, India, Norway and USA, mothers on average lost weight during the first year followed by stabilization in the second year. Lactation intensity and duration explained little of the variation in weight change patterns. In most sites, obese mothers tended to lose less weight than normal-weight mothers. Culturally defined mother-care practices probably play a role in weight change patterns among lactating women. We updated the systematic review by Neville et al. (33) by including 5 additional studies (Table 5) (191) (192) (#CITATION_TAG) (194) (195).Mothers of 1743 breastfed children enrolled in the MGRS had weights measured at days 7, 14, 28 and 42 post-partum, monthly from 2 to 12 months and bimonthly thereafter until 24 months post-partum. Height, maternal age, parity and employment status were recorded and breastfeeding was monitored throughout the follow-up.
        Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Irrigation farmers in the lower reaches of the Vaal and Riet Rivers are experiencing substantial yield reductions in certain crops and more profitable crops have been withdrawn from production, hypothesised, as a result of generally poor but especially fluctuating water quality. Leaching is justified financially and there is a strong motivation for a change in the current water pricing system. Organizations have been urged to view IT adoption decision making as a social phenomenon which needs systems approaches to reveal competing interests among stakeholders [#CITATION_TAG].In this paper secondary data is used in a linear programming model to test this hypothesis by calculating the potential loss in farm level optimal returns. Linear crop-water quality production functions (Ayers & Westcot, 1983; adapted from Maas & Hoffmann, 1977) are used to calculate net returns for the eight most common crops grown.
        Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. [28] This was followed in the late 1980s and 1990s by three in-depth sociological studies (from the US and Sweden) investigating unassisted cessation as a phenomenon in its own right, [29, #CITATION_TAG, 32] and one US sociological study in which unassisted cessation data were reported but this was not the primary focus of the study.A sample of 58 subjects who quit their addiction without any treatment was interviewed. The subjects were interviewed about their life background, addiction history, pre-resolution events (life events and internal factors), strategies used which helped subjects to reach recovery and other information related to the addiction. The Rosenbaum test was administered to test current self-regulation.
        Biological theories of sexual orientation, typically presented in human sexuality classes, are considered by many social psychologists to cause reductions in students' sexual prejudice. Yet when biological theories were not presented to 36 psychology students in a 10-week seminar on lesbian, gay, bisexual and transgender (LGBT) psychology, both sexual prejudice and two forms of essentialist thinking reduced significantly. Enrolled students reported increased exposure to issues of homosexuality since entering college, and many had sexual minority friends. #CITATION_TAG assessed students' prejudice and their interest in 26 topics at the beginning and end of a course titled 'The Psychology of Homosexuality'.We investigated who enrolled in a class about sexual diversity and what they most wanted to learn. Students left the class with significantly decreased homophobia.
        Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Health inequities are one of the central problems in public health ethics; a feminist approach leads us to examine not only the connections between gender, disadvantage, and health, but also the distribution of power in the processes of public health, from policy making through to programme delivery. bioethics that it is 'too close' to science recall some internal critiques, including those from feminist bioethicists who have sought especially deep critical engagement with biomedical institutions and practices [44, #CITATION_TAG, 60] and bioethical scholarship itself [42, 57, 58].The complexity of public health demands investigation using multiple perspectives and an attention to detail that is capable of identifying the health issues that are important to women, and investigating ways to address these issues.
        This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Eye movement data can provide an in-depth view of human reasoning and the decision-making process, and modern information retrieval (IR) research can benefit from the analysis of this type of data. In her study of regular and irregular morpho-syntax in English, #CITATION_TAG concluded that models which propose differences in processing regular and irregular morpho-syntax fail to account for the whole spectrum of regular morphology.To address this objective, a multimethod research design was employed that involved observation of participants' eye movements, talk-aloud protocols, and postsearch interviews. We present a novel stepwise methodological framework for the analysis of relevance judgments and eye movements on the Web and show new patterns of relevance criteria use during predictive relevance judgment.
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Although advance directives may seem useful instruments in decision-making regarding incompetent patients, their validity in cases of dementia has been a much debated subject and little is known about their effectiveness in practice. Insight into the experiences and wishes of people with dementia regarding advance directives is totally lacking in empirical research.Ethics and actual practice are two "different worlds" when it comes to approaching advance directives in cases of dementia. It is clear, however, that the use of advance directives in practice remains problematic, above all in cases of advance euthanasia directives, but to a lesser extent also when non-treatment directives are involved. is hotly debated (#CITATION_TAG).nan
        The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. For instance, recent findings dispute the idea that people are rational decision-makers (#CITATION_TAG).Methods: The data derive from the surveys of the Helsinki health study, collected in 2000, 2001, and 2002 from 40-60 year old employees working for the City of Helsinki (n = 8970, response rate 67%). The study measured occupation based social class and Karasek's demand-control model. Age adjusted prevalence percentages and fitted logistic regression models were calculated. The relation between social class and both health outcomes considerably attenuated when job control was controlled for, but was reinforced when controlling for job demands. Controlling for both job control and job demands attenuated the relation between social class and self rated health and limiting longstanding illness among women, however, was reinforced among men.
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. In the target population, clopidogrel is usually prescribed with aspirin, and it has been suggested that inhibition of antiplatelet effect may result from an interaction of PPIs with aspirin absorption [#CITATION_TAG, 45], independent of the interaction with clopidogrel [46, 47].Platelet activation was assessed by soluble serum P-selectin.
        The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Scholars (e.g. Mandal and Deshmukh, 1994; #CITATION_TAG; Ali and Govindan, 2011; Sushil, 2012) have outlined two limitations of ISM, that is, it usually involves a small sample size which may not be enough for statistical reasons, and manager bias may influence the final ISM model.In the paper, 11 enablers of Six Sigma are identified from literature survey and experts' opinion and then these are validated by questionnaire survey in India.
        INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Do epistemic intuitions tell us anything about knowledge? Stich has argued that we respond to cases according to our contingent cultural programming, and not in a manner that tends to reveal anything significant about knowledge itself. I've argued that a cross-culturally universal capacity for mindreading produces the intuitive sense that the subject of a case has or lacks knowledge. I argue that existing work on cross-cultural variation in mindreading favors my position over Stich's Perhaps this is why philosophers will argue, explicitly or implicitly, that premises can be considered true or false in virtue of their intuitive appeal-viz., the premise just seems to be true or false (Audi 2004, Bealer 1998, Huemer 2005, #CITATION_TAG.nan
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Previous solar observations have shown that coronal loops near 1 MK are difficult to reconcile with simple heating models. The electron densities in these loops, however, are too high to be consistent with thermodynamic equilibrium. Models proposed to explain these properties generally rely on the existence of smaller scale filaments within the loop that are in various stages of heating and cooling. In particular, as already mentioned in Section 3.2.2, steady hydrodynamic loop modeling (i.e., assuming equilibrium condition and, therefore, dropping the time-dependent terms in Eqs. ( 4), ( 5), and ( 6)), showed that flows may not be able to explain the evidence of isothermal loops (Patsourakos et al., 2004), as instead proposed by #CITATION_TAG.These loops have lifetimes that are long relative to a radiative cooling time, suggesting quasi-steady heating. Such a framework implies that there should be a distribution of temperatures within a coronal loop. EIS is capable of observing active regions over a wide range of temperatures (Fe VIII-Fe XVII) at relatively high spatial resolution (1'' -->). We also derive volumetric filling factors in these loops of approximately 10%.
        The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . In modeling turbulent reactive flows based on the transport equation for the joint probability density function (jpdf) of velocity and composition, the change in fluid composition due to convection and reaction is treated exactly, while molecular mixing has to be modeled. In this model the change in particle composition is determined by particle interactions along the edges of a Euclidean minimum spanning tree (EMST) constructed in composition space. One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [#CITATION_TAG].The model is applied to the diffusion flame test model problem proposed by Norris and Pope [Combust.
        Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. 1 and 21, on a global scale urban land expansion will be much more rapid than urban population growth-in some places resulting in large, complex, urbanizing regions comprised of aggregations of interconnected cities and interspersed rural landscapes with multiple impacts, dependence and feedbacks (#CITATION_TAG; Seitzinger et al. 2012).We illustrate how three key themes that are currently addressed separately in the urban sustainability and land change literatures can lead to incorrect conclusions and misleading results when they are not examined jointly: the traditional system of land classification that is based on discrete categories and reinforces the false idea of a rural-urban dichotomy; the spatial quantification of land change that is based on place-based relationships, ignoring the connections between distant places, especially between urban functions and rural land uses; and the implicit assumptions about path dependency and sequential land changes that underlie current conceptualizations of land transitions. We then examine several environmental "grand challenges" and discuss how urban land teleconnections could help research communities frame scientific inquiries.
        In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. (This might be in the manner envisaged by #CITATION_TAG.)To fulfil this aim, standard cognitive neuropsychological tests and experimental paradigms have been applied in an intensive series of measurements, consisting five sessions. Each of the cognitive functionings in focus (complex, verbalised naive theory of mind, executive functions and working memory) were measured repeatedly within and across these sessions, with various measure tools (2-3 times per specific tool). A test group of high functioning young adults with autism spectrum disorder (N=20) and a neurotypical control group (N=20) were matched along age, sex and IQ. A group of developmental controls (neurotypical adolescents, N=10) was also involved, in order to control if patterns of stability in the autism group mirror delayed maturation of these cognitive skills, or a qualitatively atypical pattern.
        This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. #CITATION_TAG and Salhi (2007) deal with the Euclidean CMSWP by proposing a perturbation-based heuristicThis procedure is based  on an effective use of borderline customers. Several implementations are considered and the two most appropriate are then computationally enhanced by using a reduced neighbourhood when solving the transportation problem.
        Neural connections, providing the substrate for functional networks, exist whether or not they are functionally active at any given moment. However, it is not known to what extent brain regions are continuously interacting when the brain is "at rest." Neuroimaging has revealed that almost all functional networks that support aspects of task related processing have a comparable resting state network (#CITATION_TAG), and the integrity of these networks varies across individuals in a manner that is predictive of complex forms of cognition such as meta cognitive accuracy (Baird et al., 2013), spontaneous thought (Gorgolewski et al., 2014), reading comprehension (Smallwood et al., 2013) and executive control (Reineberg et al., 2015).Independently, we extract the major covarying networks in the resting brain, as imaged with functional magnetic resonance imaging in 36 subjects at rest. The sets of major brain networks, and their decompositions into subnetworks, show close correspondence between the independent analyses of resting and activation brain dynamics.
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Evidence-centered design (ECD) is a comprehensive framework for describing the conceptual, computational and inferential elements of educational assessment. At first blush, ECD and educational data mining (EDM) might seem in conflict: structuring situations to evoke particular kinds of evidence, versus discovering meaningful patterns in available data. Learning is a latent variable, typically measured as academic performance in assessment work and examinations (#CITATION_TAG).We first introduce ECD and relate its elements to the broad range of digital inputs relevant to modern assessment. We then discuss the relation between EDM and psychometric activities in educational assessment.
        In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. However, a striking similarity is the resistance of municipalities to the coordination approaches. Dutch municipal governments are relatively autonomous vis-à-vis central government with respect to management issues, including the design, implementation and maintenance of electronic services ( #CITATION_TAG).Data were collected through content analyses and interviews.
        This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Leadership turnover is managed by a selectorate - a group of individuals on whom the leader depends to hold onto power. This requires that the selectorate's hold on power is not too dependent on a specific leader being in office. A puzzling result of our study is the asymmetric effect of regime change on the level of protection: why should a regime change be relevant only for a transition to democracy, and not vice versa? One possible explanation of such asymmetric effect of transitions to democracy and autocracy could be based on theories explaining (lack of) leadership turnovers and economic performance under autocracies (see #CITATION_TAG; Acemoglu et al. 2004).The paper develops a simple theoretical model of accountability in the absence of regularized elections. Good policy is institutionalized when the selectorate removes poorly performing leaders from office. We use these case studies to identify the selectorate in specific instances of successful autocracy.
        This is seen by the growing number of terminologies used to define subprojects concerning particular classes of bioactive carbohydrates. Sulfated fucans (SFs) and sulfated galactans (SGs) are relatively new classes of sulfated polysaccharides (SPs) that occur mostly in marine organisms, and exhibit a broad range of medicinal effects. Their structures are taxonomically dependent, and their therapeutic actions include benefits in inflammation, coagulation, thrombosis, angiogenesis, cancer, oxidation, and infections. Some red algae, marine angiosperm and invertebrates express SPs of unique structures composed of regular repeating oligomeric units of well-defined sulfation patterns. Seeing that, fucanomics and galactanomics may comprise distinguished glycomics subprojects. The evaluation of this vertex amplitude is discussed in several papers [8, 9, 16, #CITATION_TAG], where variations on the face and edge amplitudes have also been considered.nan
        Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. These generalizations are algebraic structures in which the two entries Lie bracket has been replaced by a bracket with n entries. Three-Lie algebras have surfaced recently in multi-brane theory in the context of the Bagger-Lambert-Gustavsson model. In the sequel, it is found that 3-ary algebras [#CITATION_TAG] [28] [29] describe the symmetries of the tensor models.Each type of n-ary bracket satisfies a specific characteristic identity which plays the r\^ole of the Jacobi identity for Lie algebras. Particular attention will be paid to generalized Lie algebras, which are defined by even multibrackets obtained by antisymmetrizing the associative products of its n components and that satisfy the generalized Jacobi identity (GJI), and to Filippov (or n-Lie) algebras, which are defined by fully antisymmetric n-brackets that satisfy the Filippov identity (FI). Because of this, Filippov algebras will be discussed at length, including the cohomology complexes that govern their central extensions and their deformations (Whitehead's lemma extends to all semisimple n-Lie algebras). When the skewsymmetry of the n-Lie algebra is relaxed, one is led the n-Leibniz algebras. The standard Poisson structure may also be extended to the n-ary case. We shall review here the even generalized Poisson structures, whose GJI reproduces the pattern of the generalized Lie algebras, and the Nambu-Poisson structures, which satisfy the FI and determine Filippov algebras.
        lable at ScienceDirect Contents lists avai Journal of Clinical Gerontology & Geriatrics journal homepage: www.e- jcgg.com Letter to the Editor Evidence on the role of prebiotics, probiotics, and synbiotics in gut health and disease prevention in the elderly To the Editor In their review article, Patel et al1 elegantly described the process of ageing of the small bowel under normal conditions. In order to properly introduce the topic, it is reasonable to add information about the aging stomach and colon, as evidence shows age-related declines in these structures. However, four general benefits have been described: (1) suppression of growth or epithelial binding/invasion by pathogenic bacteria, (2) improvement of intestinal barrier function, (3) modulation of the immune system, and (4) modulation of pain perception.1,3,4 The majority of bacteria in the colon are anaerobes that can ferment carbohydrates that escape digestion, to form short chain fatty acids (SCFAs) and anions that have a distinct role in promoting gut health (e.g., acetate, propionate, and butyrate).3 Studies in the elderly described a shift in the composition of intestinal microbiota, with a lower number of beneficial organisms such as bifidobacteria and lactobacilli1 and an increase in Enterobacteriaceae and certain Proteobacteria.3 Compared to younger adult controls, aged persons seem to have a lower number of Firmicutes and more abundant Bacteroidetes.3 Butyrate is the major http://dx.doi.org/10.1016/j.jcgg.2014.01.001 2210-8335/Copyright 2014, Asia Pacific League of Clinical Gerontology & Geriatrics. Conflicts of interest The authors have no conflicts of interest relevant to this article : Antibiotic-associated diarrhoea (AAD) occurs most frequently in older (>=65 years) inpatients exposed to broad-spectrum antibiotics. When caused by Clostridium difficile, AAD can result in life-threatening illness. Although underlying disease mechanisms are not well understood, microbial preparations have been assessed in the prevention of AAD. However, studies have been mostly small single-centre trials with varying quality, providing insufficient data to reliably assess effectiveness. #CITATION_TAG nother recent large trial evaluated the evidence of the preventive effect of probiotics on upper respiratory tract infections (URTI) in an aged sample, and was unable to find differences between groups.: We did a multicentre, randomised, double-blind, placebo-controlled, pragmatic, efficacy trial of inpatients aged 65 years and older and exposed to one or more oral or parenteral antibiotics. A computer-generated randomisation scheme was used to allocate participants (in a 1:1 ratio) to receive either a multistrain preparation of lactobacilli and bifidobacteria, with a total of 6 x 10(10) organisms, one per day for 21 days, or an identical placebo. Patients, study staff, and specimen and data analysts were masked to assignment. AAD (including CDD) occurred in 159 (10*8%) participants in the microbial preparation group and 153 (10*4%) participants in the placebo group (relative risk [RR] 1*04; 95% CI 0*84-1*28; p=0*71). CDD was an uncommon cause of AAD and occurred in 12 (0*8%) participants in the microbial preparation group and 17 (1*2%) participants in the placebo group (RR 0*71; 95% CI 0*34-1*47; p=0*35).
        A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the leakage features. According to #CITATION_TAG, evolution proceeds in small increments rather than in a single "unimaginable" leap.nan
        OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of "clusters" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). Plant sensitive factor attachment protein receptors (SNAREs) encoded by genes of the same sub-family are generally considered as redundant in promoting vesicle-associated membrane fusion events. Nonetheless, the application of innovative experimental approaches highlighted that members of the same gene sub-family often have different functional specificities. Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (#CITATION_TAG).When transiently overexpressed, the SYP51 and the SYP52 distributed between the TGN and the tonoplast.
        to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' "cogito"), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. Face to face conversation necessarily involves a great deal of bodily movement beyond that required for speaking. Eye movements (Richardson et al., 2007), postural sway (Shockley et al., 2009), and even blinking (#CITATION_TAG) have all been found to become subtly intertwined in conversation, leading to a dynamic entanglement of the participants.Gaze and blinking in dyadic conversation are examined, along with their relation to speech turn. Eight pairs provide 15 minutes of conversation each, including five participants who partake in two dyads each. Many aspects of systematic variation are found to be relatively invariant within the individual, but individuals display large qualitative differences, one from the other.
        Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Second, the relation between ethnic and religious diversity, on the one hand, and social capital, civic engagement and trust, on the other hand, has attracted significant scholarly discussion in recent years (e.g., #CITATION_TAG, 2002 Coffé & Geys, 2006; Delhey & Newton, 2005; Gijsberts, van der Meer, & Dagevos, forthcoming; Hallberg & Lund, 2005; Putnam, 2007).nan
        In previous work the authors considered the asymmetric simple exclusion process on the integer lattice in the case of step initial condition, particles beginning at the positive integers. There it was shown that the probability distribution for the position of an individual particle is given by an integral whose integrand involves a Fredholm determinant. In one an apparently new distribution function arises and in another the distribution function F 2 arises. Socioeconomic inequalities in premature mortality in Britain increased over the second half of the 20th century, particularly from the early 1970s onwards.1 The magnitude of mortality differentials reflects the trend in income inequality, which has also undergone a dramatic increase over the past quarter century.1 The present British government have emphasised their commitment to reducing health inequalities. For example the Minister of Health, Alan Milburn, has stated that "Our ambition is to do something that no government--Tory or Labour--has ever done. The mortality data are the Office for National Statistics digital records of all deaths in England and Wales, and equivalent records from the General Register Office (Scotland). In the second result an apparently new distribution function arises and in the third the distribution function F 2 of random matrix theory [#CITATION_TAG] arises.Not only to improve the health of the nation, but also to improve the health of the worst off at a faster rate".2 A set of targets for the reduction of health inequalities has been presented. The full postcode of the usual residence of the deceased was used to assign each death to the parliamentary constituency in which the deceased usually lived.
        Setting Tertiary referral centre for oncology patients in Utrecht, the Netherlands. Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008; #CITATION_TAG) still tend to focus on father's occupation or the 'dominant' parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents' characteristics (Beller, 2009; Lampard, 2007a; Marks, 2009; Schoon, 2008).Participants 189 bereaved family members and close friends of terminally ill cancer patients who died by euthanasia and 316 bereaved family members and close friends of comparable cancer patients who died a natural death between 1992 and 1999.
        Biobanking, the large-scale, systematic collection of data and tissue for open-ended research purposes, is on the rise, particularly in clinical research. However, the positioning of biobanking infrastructures and transfer of tissue and data between research and care is not an innocuous go-between. Instead, it involves changes in both domains and raises issues about how distinctions between research and care are drawn and policed. Based on an analysis of the emergence and development of clinical biobanking in the Netherlands, this article explores how processes of bio-objectification associated with biobanking arise, redefining the ways in which distinctions between research and clinical care are governed. The development of genomics has dramatically expanded the scope of genetic research, and collections of genetic biosamples have proliferated in countries with active genomics research programs. In this essay, we consider a particular kind of collection, national biobanks. National biobanks are often presented by advocates as an economic "resource" that will be used by both basic researchers and academic biologists, as well as by pharmaceutical diagnostic and clinical genomics companies. Although national biobanks have been the subject of intense interest in recent social science literature, most prior work on this topic focuses either on bioethical issues related to biobanks, such as the question of informed consent, or on the possibilities for scientific citizenship that they make possible. At the same time, the labour performed by most donors is also minimized and made invisible by integrating it into routine aspects of care (Mitchell and Waldby 2010; #CITATION_TAG).We emphasize, by contrast, the economic aspect of biobanks, focusing specifically on the way in which national biobanks create biovalue.
        Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. Although less data exist for nonaffective psychosis, available evidence suggests that median age-of-onset is in the range late teens through early 20s. Roughly half of all lifetime mental disorders in most studies start by the mid-teens and three quarters by the mid-20s. Later onsets are mostly secondary conditions. Severe disorders are typically preceded by less severe disorders that are seldom brought to clinical attention. SUMMARY: First onset of mental disorders usually occur in childhood or adolescence, although treatment typically does not occur until a number of years later. Anxiety disorders other than phobiaswhich include generalised anxiety disorder and panic disorder, depression, alcohol and substance misuse disorders, and schizophreniabegin most commonly between the late teens and early adulthood (#CITATION_TAG).nan
        Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. However, as it is very difficult and expensive to obtain annotated material for languages different from English, we only consider unsupervised approaches, where no annotated training set is necessary. (#CITATION_TAG) have introduced a system for unsupervised extraction of entities and relations between these entities from clinical texts written in Italian, which utilized a thesaurus for extraction of entities and clustering methods for relation extraction.We therefore propose a complete system that is structured in two steps. In the first one domain entities are extracted from the clinical records by means of a metathesaurus and standard natural language processing tools. The second step attempts to discover relations between the entity pairs extracted from the whole set of clinical records. For this last step we investigate the performance of unsupervised methods such as clustering in the space of entity pairs, represented by an ad hoc feature vector. The resulting clusters are then automatically labelled by using the most significant features.
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Public and healthcare professionals differ in their attitudes towards euthanasia and physician-assisted suicide (PAS), the legal status of which is currently in the spotlight in the UK. The unexpected direction of association between religiosity and attitudes may reflect a broader cultural shift in attitudes since earlier research in this area. Perhaps surprisingly, in a survey of British students there was a significant positive correlation between religious belief and a positive attitude towards euthanasia, a finding suggesting a cultural shift to #CITATION_TAG.One hundred and fifty-one undergraduate students (early-stage nursing training, late-stage nursing training and non-nursing controls) were approached on a UK university campus and asked to complete a self-report questionnaire. Participants were of mixed gender and were on average 25.5 years old.
        Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. The cerebellar granular layer has been suggested to perform a complex spatiotemporal reconfiguration of incoming mossy fiber signals. This characteristic connectivity has recently been investigated in great detail and been correlated with specific functional properties of these neurons. Important advances have also been made in terms of determining the membrane and synaptic properties of the neuron, and clarifying the mechanisms of activation by input bursts. #CITATION_TAG find that selection improves when a firm uses performance measures that are less distorted and have less uncontrollable risk.Central to this role is the inhibitory action exerted by Golgi cells over granule cells: Golgi cells inhibit granule cells through both feedforward and feedback inhibitory loops and generate a broad lateral inhibition that extends beyond the afferent synaptic field. These include theta-frequency pacemaking, network entrainment into coherent oscillations and phase resetting.
        Increasing life span and lack of medication for prevention or treatment of progressive dementias will significantly increase the number of individuals with advanced dementia worldwide. Providing optimal care for them will stretch health care resources and will require evaluation of different treatment strategies. This paper is presenting measures that may be used in this patient population. Only very few residents who understood others and were not depressed were abusive. Behavioral interventions preventing escalation of resistiveness to care into combative behavior and the treatment of depression can be expected to decrease or prevent abusive behavior of most nursing home residents with dementi Recent work developed also from sharing newly collected [3] and existing data [#CITATION_TAG] -the Dutch author analyzing the U.S. data and vice versa.DESIGN: Analysis of Minimum Data Set (MDS) of the Resident Assessment Instrument (RAI) information. SETTING: We used MDS-RAI data from 8 Dutch nursing homes and 10 residential homes that volunteered to collect data for care planning. We included the data of residents within a 12-month time window for each facility separately, resulting in a range from April 4, 2007, to December 1, 2008. PARTICIPANTS: We selected 929 residents older than 65 with Alzheimer's disease or other dementia who were dependent in decision making and not comatose. MEASUREMENTS: Cognitive Performance Scale, MDS Depression Scale and several individual items from the MDS-RAI (ability to understand others, verbally and physically abusive behavioral symptoms, resist care, diagnosis of Alzheimer's disease and of dementia other than Alzheimer's disease, diagnosis of depression, presence of delusions, hallucinations, pain frequency and constipation, and number of days receiving medications).
        Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Audibility, the perceptual aspect of AVH, may result from a disinhibition of the auditory cortex in response to self-generated speech. In isolation, this aspect leads to audible thoughts: Gedankenlautwerden. This failure may be related to the fact that cerebral activity associated with AVH is predominantly present in the speech production area of the right hemisphere. Since normal inner speech is derived from the left speech area, an aberrant source may lead to confusion about the origin of the language fragments. When alienation is not accompanied by audibility, it will result in the experience of thought insertion. Further, #CITATION_TAG suggest that the audibility component, independent of content and appraisal, should be considered separately as an important perceptual component of AVHs and reflects the neural pathophysiology underlying AVH.The second component is alienation, which is the failure to recognize the content of AVH as self-generated. The 2 hypothesized components are illustrated using case vignettes.Copyright 2010 S. Karger AG, Basel.
        Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. In the current knowledge society, adoption of information technology (IT) innovation initiatives has become a necessity for the success of most organizations. The decision to adopt information technology solutions however must be made on welldefined user requirements, and not on mere high-expectations. Even after the deployment of the information system, the shape of organizational operations was still transforming. Naturally any IT adoption ought to be done for the benefit of the organization [#CITATION_TAG].nan
        Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Secondary pneumococcal pneumonia is a serious complication during and shortly after influenza infection. This increased susceptibility to secondary bacterial pneumonia is at least in part caused by excessive IL-10 production and reduced neutrophil function in the lungs. Subsequent studies have suggested that it is the cytokines released by lymphocytes, such as IFNγ and IL-10 that mediate the suppressive effects of virus [1, [#CITATION_TAG] [23] [24].C57BL/6 mice were intranasally inoculated with 10 median tissue culture infective doses of influenza A (A/PR/8/34) or PBS (control) on day 0.
        requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Speech output from speech-generating devices (SGD) and SGD software, such as talking word processors, has changed the landscape of options for aided communication. Schlosser [#CITATION_TAG] described how practitioners faced a difficult task when matching appropriate systems to individuals with disabilities.Learner-oriented roles of speech output are summarized in terms of graphic symbol learning, communicative functions and social regulation, learner preference, challenging behaviors, natural speech production, comprehension, and literacy. Roles for the learner - partner dyad include changes to interaction patterns. Methodological issues are discussed and practical implications are drawn where appropriate.
        The mind flows in a "stream of consciousness," which often neglects immediate sensory input in favor of focusing on intrinsic, self-generated thoughts or images. Although considerable research has documented the disruptive influences of task-unrelated thought for perceptual processing and task performance, the brain dynamics associated with these phenomena are not well understood. We often lose ourselves in our thoughts, decoupling experience from the here and now (#CITATION_TAG).Using an experience sampling paradigm coupled with continuous high-density electroencephalography, we observed that task-unrelated thought was associated with a reduction of the P1 ERP, replicating prior observations that mind-wandering is accompanied by a reduction of the brain-evoked response to sensory input.
        A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. The idea that memory is composed of distinct systems has a long history but became a topic of experimental inquiry only after the middle of the 20th century. Beginning about 1980, evidence from normal subjects, amnesic patients, and experimental animals converged on the view that a fundamental distinction could be drawn between a kind of memory that is accessible to conscious recollection and another kind that is not. Subsequent work shifted thinking beyond dichotomies to a view, grounded in biology, that memory is composed of multiple separate systems supported, for example, by the hippocampus and related structures, the amygdala, the neostriatum, and the cerebellum. Declarative memory, in turn, can be divided into episodic memory, which is personal memory for past episodes, and semantic memory, which is basic knowledge about the world (#CITATION_TAG).nan
        We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. BACKGROUND AND OBJECTIVES The expansion of evidence-based practice across sectors has lead to an increasing variety of review types. However, the diversity of terminology used means that the full potential of these review types may be lost amongst a confusion of indistinct and misapplied terms. A limited number of review types are currently utilized within the health information domain. Notwithstanding such limitations, this typology provides a valuable reference point for those commissioning, conducting, supporting or interpreting reviews, both within health information and the wider health care domain. of allowed states in the combined lattice plus external trap, in striking contrast with the uniform case [#CITATION_TAG].METHODS Following scoping searches, an examination was made of the vocabulary associated with the literature of review and synthesis (literary warrant). A simple analytical framework -- Search, AppraisaL, Synthesis and Analysis (SALSA) -- was used to examine the main review types. A description of the key characteristics is given, together with perceived strengths and weaknesses.
        Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Introduction There is a growing need for largescale data and biobanks for biomedical research. The Radboud Biobank was conceived from the standards that were laid down in the String of Pearls Initiative (PSI), a unique partnership between the eight University Medical Centers (UMCs) in the Netherlands, that contributes to innovation in health care by facilitating biomedical research. The establishment of the RadboudBiobank creates an efficient and high quality facility for scientific research and medical innovation. Ratchet effects have received surprisingly little empirical study beyond Roy's (1952) famous description (#CITATION_TAG).These procedures are generic and established with a view on standardization, quality and efficiency, transcending the interests of single departments. Furthermore, (quality) standards are set in the field of ICT, legal and ethical aspects, communication and distribution.
        Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. Naming ability on the PALPA54 subtest (#CITATION_TAG) indicated residual lexical capacity with scores of 59/60 for spoken (with no penalty for phonemic paraphasias as long as the target was recognizable) and 59/60 for written naming.Intended both as a clinical instrument and research tool, PALPA is a set of resource materials enabling the user to select language tasks that can be tailored to the investigation of an individual patient's impaired and intact abilities. The materials consist of sixty rigorously controlled tests of components of language structure such as orthography and phonology, word and picture semantics and morphology and syntax. The tests make use of simple procedures such as lexical decision, repetition and picture naming and have been designed to assess spoken and written input and output modalities. Each test is also accompanied by detailed instructions of how and why it was constructed, how to use it, and by presenter's forms and marking sheets.
        Reports of firms ' behaviors with regard to corporate social responsibility (CSR) are often contrary to their stated standards of social responsibility. Corporate wrongdoing attracts the attention of the media and watchdog organizations, triggering questions about why companies engage in CSR and how they contribute to social well-being (Bielak et al., 2007; #CITATION_TAG).nan
        5G technology is using millimeter-wave band to improve the wireless communication system. However, narrow transmitter and receiver beams have caused the beam coverage area to be limited. Due to propagation limitations of mm wave band, beam forming technology with multi-beam based communication system, has been focused to overcome the problem. The difference in the measured and simulated results is mainly caused by the shift in the resonant frequencies [#CITATION_TAG].The design is based on merging two slots into a diversity Y-shaped slot antenna using an inverted configuration. In order to achieve good isolation between the two slots, a slot stub is added at the merging corner part forming a Y-shaped slot antenna. The effects of different angles of the Y-junction in the diversity antenna on the mutual coupling are analyzed through HFSS simulations.
        Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Nicotine replacement therapies (NRTs) have been demonstrated to be effective in clinical trials but may have lower efficacy when purchased over-the-counter (OTC). [#CITATION_TAG] Further complicating the relationship, some regard commitment as a component of motivation, [57] operationalizing motivation as, for example, 'determination to quit' [58] or 'commitment to quit'. [59] The greater research interest in reasons for quitting or pros and cons of quitting (i.e., motivation) as opposed to commitment may be because motivation is simpler to measure, for example by asking people to rate or rank reasons, costs or benefits.nan
        This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Changes within networks receive less research attention, although considerable research exists on explaining business network structures in different research traditions. So far, the relevant literature discusses network pictures mainly as a theoretical concept. Furthermore, the direction of causation is hard to establish (see #CITATION_TAG; Gundlach and Paldam, 2009).The study is exploratory or iterative in the sense that revisions occur to the research question, method, theory, and context as an integral part of the research process. The study develops a concept of network change as well as an operationalization for comparing perceptions of change, where the study introduces a template model of dottograms to systematically analyze differences in perceptions. The study then applies the model to analyze findings from a case study of Norwegian/Japanese seafood distribution, and the chapter provides a rich description of a complex system facing considerable pressure to change. In-depth personal interviews and cognitive mapping techniques are the main research tools applied, in addition to tracer studies and personal observation. The dottogram method represents a valuable contribution to case study research as it enables systematic within-case and across-case analyses.
        Phonological errors were scarce in both groups. The interdependence between the two, mediated by the quality of the relationship, has direct implications for the earning of rents through collaborations. These relationship-specific expenditures can be of an internally generated nature, endogenous to the alliance form itself, and need not exceed alternative forms, while the associated benefits have the capacity to potentially exceed the alternatives. If semantic and syntactic sources of information compete during the selection of lexical items, a reduction in the strength/reliability of semantic information might increase reliance on syntactic information (#CITATION_TAG).Based on a perspective of value, we explain how a more inclusive and integrative perspective, one which combines elements from transaction costs and resource-based theory, provides more robust insight into collaboration formation, management, and instability. In doing so, we differentiate rent-yielding firm-specific assets at the core of the resource-based view from the transaction specific assets at the core of transaction cost theory. In the search for value, we explain why the transaction costs incurred in the exchange of resources are not independent of the nature of resources to be transacted and, similarly, why the returns realized from these resources are not independent of the relationship- and transaction-specific expenditures incurred in effectively combining them and maintaining the combination. The paper contributes in three key related ways: (a) the explicit recognition of the relationship as a value-bearing asset embedded in a larger and endogenous institutional context, namely a system of resource relationships-both intraorganizational and inter-organizational-among partner firms and the collaboration, (b) the recognition of the evolving relationship between production and exchange which, at the level of the collaboration, is directly dependent on the nature, evolution, and dynamics of the relationship among the parties to the transaction, and (c) the provision of a nontrust explanation for why firms might knowingly forego opportunities to take advantage of their partners. Drawing from this, the paper occasions (a) a shift in focus from the form to the process of governance, which has direct implications for value creation and realization and (b) a shift in the primary identity of transaction-specific and relationship-specific expenditures from cost to investment in future value.
        Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Next-generation DNA sequencing technologies are enabling genome-wide measurements of somatic mutations in large numbers of cancer patients. A common approach to identify driver mutations is to find genes that are mutated at significant frequency in a large cohort of cancer genomes. However, the current understanding of the somatic mutational process of cancer [3,5,6] places two additional constraints on the expected patterns of somatic mutations in a cancer pathway. There are numerous examples of sets of mutually exclusive mutations [5,6]. Thus, it probably pays off to try to use other approaches that incorporate information about non-silent mutation rates, pathway information together with combinatorial properties of drivers in pathwayws, or functional consequences of mutations to differentiate drivers from passengers [73] [74] [#CITATION_TAG] [76] [77] [78].Thus, each cancer patient may exhibit a different combination of mutations that are sufficient to perturb the necessary pathways. First, an important cancer pathway should be perturbed in a large number of patients. Second, since driver mutations are relatively rare and typically a single driver mutation is sufficient to perturb a pathway, a reasonable assumption is that most patients have a single driver mutation in a pathway. Thus, the genes in a driver pathway exhibit a pattern of mutually exclusive driver mutations, where driver mutations are observed in exactly one gene in the pathway in each patient.
        NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the "Western diet" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The "Selfish Brain" is a term coined by Robert L. DuPont in the title of his book where he wrote: "With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: "The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the "push" principle borrowed from the economic "push-pull" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: "According to the 'push' principle, the environment pushes excess amounts of energy into the organism" (Hill and Peters, 1998). According to DuPond, "What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both "pull" and "push" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this "push" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called "unique" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the "puSh" component of energy metaBolISm The environment in Western-type societies can be characterized as "pushing" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental "Western Diet" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: "antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental "push." In a recent article entitled "Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy," J. Zimmerman wrote: "In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. "Control" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. The alarming increase in the incidence of obesity and obesity-associated disorders makes the etiology of obesity a widely studied topic today. As opposed to 'homeostatic feeding', where food intake is restricted to satisfy one's biological needs, the term 'non-homeostatic' feeding refers to eating for pleasure or the trend to over-consume (palatable) food. Overconsumption is considered a crucial factor in the development of obesity. At a molecular level, insulin and leptin resistance are hallmarks of obesity. The inter-relationship between neuronal populations in the arcuate nucleus and other areas regulating energy homeostasis (lateral hypothalamus, paraventricular nucleus, ventromedial hypothalamus etc.) Their traits notwithstanding, these behaviors are highly evolutionary significant: "Although at first glance, hijacking of the homeostatic regulatory mechanisms by its hedonic counterpart may seem conflicting, it should be borne in mind that during evolution, humans have lived in an environment where food availability was restricted and uncertain (e.g., hunter-gatherers) and the biological system has been 'hard-wired' to maximize energy stores" (#CITATION_TAG).nan
        The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. There have been few trials in other parts of the world. Implementation of IPS can be challenging in the UK context where IPS is not structurally integrated with mental health services, and economic disincentives may lead to lower levels of motivation in individuals with severe mental illness and psychiatric professionals. While this research tradition looks at structures of competition between related companies, the channel management or supply chain literature treats each individual business relationship as a separate entity, arguing that companies have to respond appropriately to changes in their business environment (Achrol, Reve, & Stern, 1983; #CITATION_TAG; Stern & Reve, 1980).Method Individuals with severe mental illness in South London were randomised to IPS or local traditional vocational services (treatment as usual) (ISRCTN96677673).
        The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . The dynamics of fully developed hydrodynamic turbulence still is a basically unsolved theoretical problem, due to the strong-coupling long-range nonlinearities in the Navier-Stokes equations. Averaging the response spectrum over all possible orientational configurations and sweep velocities results in a novel self-consistency integral for the 4D energy spectrum function. These two features are manifestations of a fundamental cascade property, the so-called sweeping of the small scales by the large scales [#CITATION_TAG].After taking a (2+1)D spatiotemporal spectral transform of the fluctuating vorticity fields, care is taken of large-scale sweeping which arises as a collective zero mode from the nonlinear flow terms. The "unswept" small-scale nonlinearities are then shown to be asymptotically locally isotropic (i.e., for wave numbers k-[?]) by internal consistency, which allows to close the nonlinear hierarchy. The Navier-Stokes equations (without external forcing) are integrated to give the spectral response of the fluctuating small-scale velocity fields on the presence of a locally isotropic blob of turbulence while it is being swept around over an arbitrary steady state mean velocity profile, using viscous boundary conditions at y=0. The distribution of turbulence sweep velocities is modeled by means of Levy-type densities, having an algebraic tail with power p>1. The generic case (which includes Von Karman's logarithmic mean velocity profile) is found to correspond to 1<p<3. Asymptotic analysis of the self-consistency integral leads to a differential equation which fixes the scaling exponent l of the unswept frequency D and admits a nonempty, integrable and positive definite Airy-type frequency spectrum E(i)(k,D/k(l))~k(m) with so-called "normal" Kolmogorov scaling, that is, m=-7/3 and l=2/3. Anomalous scaling is possible for one special mean profile.
        This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Influencing more environmentally friendly and sustainable behaviour is a current focus of many projects, ranging from government social marketing campaigns, education and tax structures to designers' work on interactive products, services and environments. These approaches make different assumptions about 'what people are like': how users will respond to behavioural interventions, and why, and in the process reveal some of the assumptions that designers and other stakeholders, such as clients commissioning a project, make about human nature. While much focus has been given to the release of software tools that aid researchers in developing and analysing computational models [#CITATION_TAG][9][10][11][12][13], the same attention has not been given to providing researchers with a means of showing that their developed tool can adequately support the investigation of a specific biological research question: that the tool is fit for purpose.There is a wide variety of techniques and methods used, intended to work via different sets of cognitive and environmental principles. The models are characterised using systems terminology and the application of each model to design for sustainable behaviour is examined via a series of examples.
        Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [#CITATION_TAG, 8].METHODS Physical activity was assessed for up to 21 consecutive days using the Computer Science Applications (CSA) accelerometer. Random effects models were employed to estimate variance components for subject, day of the week, and residual error from which the number of days of assessment required to achieve 80% reliability were estimated. Inter-individual variation, or differences between subjects, was proportionally the largest source of variance (55-60% of total) in accelerometer counts and time spent in moderate to vigorous activity.
        Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Recommender systems are used to provide filtered information from a large amount of elements. They provide personalized recommendations on products or services to users. Colombo-Mendoza et al [#CITATION_TAG] propose RecomMetz, a context-aware mobile recommender system based on Semantic Web technologies.Recommender systems can be developed using different techniques and algorithms where the selection of these techniques depends on the area in which they will be applied. The system proposed is called RecomMetz, and it is a context-aware mobile recommender system based on Semantic Web technologies. In addition, location, crowd and time were considered as three different kinds of contextual information in RecomMetz. In a nutshell, RecomMetz has unique features: (1) the items to be recommended have a composite structure (movie theater + movie + showtime), (2) the integration of the time and crowd factors into a context-aware model, (3) the implementation of an ontology-based context modeling approach and (4) the development of a multi-platform native mobile user interface intended to leverage the hardware capabilities (sensors) of mobile devices.
        To assess potential public health impacts of changes to indoor air quality and temperature due to energy efficiency retrofits in English dwellings to meet 2030 carbon reduction targets. #CITATION_TAG Occupant ventilation practices have also been shown to be counter-productive to creating a healthy indoor environment.A total of 198 cases (with at least two of three symptoms: wheezing, rhinitis, eczema) and 202 healthy controls, living in 390 homes, were examined by physicians. Ventilation rates were measured by a passive tracer gas method, and inspections were carried out in the homes. Families with allergic children should be given the advice to have good ventilation in the home. In investigations, of associations between environmental factors and allergies, the air change rate in homes has to be considered.
        According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? ABSTRACT: Literature on technology and innovation management has identified the research and development (R&amp;D) and marketing (RDM) interface as a critical organizational complexity that when managed well can affect company success in innovation. The culture pertaining to R&amp;D personnel or technologists, meaning engineers and "hard" scientists, can be referred to as the T-culture. The culture pertaining to marketing or social "soft" scientists (meaning business, sociology, psychology, etc.) can be referred to as the S-culture. Consequently, by implementing more formalized procedures such as rules and routines, clearer role responsibilities, and a better identification of complementary tasks and responsibilities between the MNC and the subsidiary, opportunities for opportunism are reduced (Dahlstrom and Nygaard, 1999; #CITATION_TAG).Cross-functional teams and collaboration mechanisms, such as boundary spanners, have been identified as solutions to the problems in communication between the two groups. However, these methods have not been as effective as desired and questions still exist as to how boundary spanners are created for success at this crucial interface.
        The chronological development of universities ranges from the state at which universities are considered to be knowledge accumulators followed by knowledge factories and finally the knowledge hubs. The various national systems of innovations are aligned with the knowledge hubs and it involves a substantial amount of research activities. The newly established Mbeya University of Science and Technology is recognised as a knowledge hub in some particular niches. However, there are a limited number of research activities conducted at the university and this study is an attempt to identify the reasons that limit research activities. This issue of the Malta Medical Journal contains a historical perspective on medical publications in Malta over the years and it is a tribute to the medical community that over the last one hundred and seventy years, dedicated members of that profession have published articles of relevance to the practice of medicine in the Maltese Islands. On the other hand, in academic arena as discussed in several studies (#CITATION_TAG; Jones, 1997; Ali, 2012) the phrase "Publish or perish" has been used to researchers and academicians to describe the need to publish their research works.nan
        Solar radiation and ambient temperature have acted as selective physical forces among populations and thereby guided species distributions in the globe. Circadian clocks are universal and evolve when subjected to selection, and their properties contribute to variations in fitness within specific environments. Because of their position in the hierarchy and repressive actions, cryptochromes are the key components of the feedback loops on which circadian clocks are built. Plasma levels of corticosterone exhibit both circadian and ultradian rhythms. The circadian component of these rhythms is regulated by the suprachiasmatic nucleus (SCN). Furthermore, there is an ultradian rhythm of e.g., free corticosterone in the blood (Qian et al., 2012; #CITATION_TAG) that translates into synchronized rhythms of free glucocorticoid hormone in peripheral (the subcutaneous tissue) and central (the hippocampus) tissues (Qian et al., 2012).Two approaches were used to dissociate the hypothalamic-pituitary-adrenal (HPA) axis from normal circadian input in rats: (i) exposure to a constant light (LL) environment and (ii) electrolytic lesioning of the SCN. Blood was sampled using an automated sampling system.
        Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. For example, #CITATION_TAG model flexible operation of heat pumps combined with various types of thermal storage.The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks.
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The double filter ratio temperature analysis technique is applied to two sets of TRACE images having a 20 minute time gap resulting in temperatures between 1.0 and 1.3 MK. The first coronal loop structures were identified properly after a rocket launch in 1968, which provided for the first time an image of an X-ray flare (#CITATION_TAG), with a resolution of a few arcsec.The CDS sparse raster line intensities are used to create emission measure (EM) loci plots determining temperature values over an overlapping 40 minute observing period.
        Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Abstract Neuroscientific research into mental health commands generous funding, suggesting neuroscience is understood by a variety of actors and institutions as having significant potential to enhance the therapeutic practices of psychiatrists. The article discusses the respondents ' ambivalent expectations regarding the therapeutic promise of brain research, and shows how these are structured by understandings of the ontology of personality disorder. In sum, the necessity of large material and symbolic investments in neuroscience should, perhaps, be reflected upon more critically, and analytic encounters with this discipline must keep in mind it's at times surprising commitment to the realms of the social and the psychological Within the clinic, neurological explanations for opaque conditions can sometimes have traction as a framework through which to deal with the uncertainties associated with them [#CITATION_TAG].nan
        Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. The hydrogen energy arena has seen a proliferation of futures studies: scenarios, forecasts, roadmaps and visions that describe what a hydrogen future might look like, and how it might be achieved. In 2006, the current authors published a review of 40 such studies that examined the state of the art of futures studies in the hydrogen field, and interrogated the literature for insight into the drivers, barriers, and possible transitions for hydrogen. What does the literature tell us about the evolution of the hydrogen innovation system and prospects for its growth? In order to identify key uncertainties and possibilities for hydrogen energy in the UK, we reviewed the literature [#CITATION_TAG], ran a stakeholder workshop [2], and conducted stakeholder interviews and participant observation at hydrogen stakeholder events.; What are methodological improvements since 2006, and what knowledge gaps remain?
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. We expect better resolution switching from narrow-band instruments with few channels (Weber et al., 2005) to spectrometers (#CITATION_TAG; Landi et al., 2012b), but it is difficult to achieve a temperature resolution better than Δ log ≈ 0.05 (Landi et al., 2012b).We also test the effects of 4) atomic data uncertainties on the results, and 5) the number of ions whose lines are available for the DEM reconstruction. Also, the DEM curves obtained using lines calculated with an isothermal plasma and with a Gaussian distribution with FWHM of log T = 0.05 are very similar. The availability of small sets of lines also does not worsen the performance of the MCMC technique, provided these lines are formed in a wide temperature range.
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Brady-Amoon and Fuertes (2011) attribute their insignificant correlation (r=0.16, n=271) to the fact that study participants included a more diverse group of students from a variety of ethnic backgrounds, thereby supporting the findings of #CITATION_TAG that the interaction between prior academic ability and GPA differs for students from different ethnic groups.SAT/ACT scores and HSGPA were collected and used in various ways by participating institutions in the admissions process while situational judgment measures and biodata were collected for research purposes only during the first few weeks of the participating students' freshman year.
        Throughout Earth's history, life has increased greatly in abundance, complexity, and diversity. At the same time, it has substantially altered the Earth's environment, evolving some of its variables to states further and further away from thermodynamic equilibrium. For instance, concentrations in atmospheric oxygen have increased throughout Earth's history, resulting in an increased chemical disequilibrium in the atmosphere as well as an increased redox gradient between the atmosphere and the Earth's reducing crust. This is applied to the processes of planet Earth to characterize the generation and transfer of free energy and its dissipation, from radiative gradients to temperature and chemical potential gradients that result in chemical, kinetic, and potential free energy and associated dynamics of the climate system and geochemical cycles. Here, I present this hierarchical thermodynamic theory of the Earth system. This perspective allows us to view life as being the means to transform many aspects of planet Earth to states even further away from thermodynamic equilibrium than is possible by purely abiotic means. In this perspective pockets of low-entropy life emerge from the overall trend of the Earth system to increase the entropy of the universe at the fastest possible rate. Abstract: The East Sea is undergoing physical changes caused by global warming: deepening oxygen minimum layer and increasing temperature. However, any correspondence to biogeochemical change was not noticed yet. However, note that the East Sea is still high concentration in oxygen compared with the condition denitrified. The present-day average value for this influx is about 342 W m −2 [#CITATION_TAG], which by using the surface area of the Earth, 511 • 10 12 m 2, yields a global total sum of about 175 000 TW (1 TW = 10 12 W).Here, we present indirect evidences of denitrification in response to a global warming and estimate the amount of denitrification by the linear inverse model. Intense N/P ratio and oxygen minimum zones are founded between 900m and 2200m in the Ulleung Basin. Two stations are strongly expected to occur denitrification, where is located within the Ulleung Basin, show a series of phenomena at specific depths that nitrate profile is reversed, N/P ratio is less than 12.4, and nitrite shows a peak.
        Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Tuberous sclerosis complex (TSC) is a multiorgan genetic disease caused by mutations in the TSC1 or TSC2 genes. TSC has been recognized for many years as an important cause of severe neurological disease with patients suffering from epilepsy, developmental delay, autism, and psychiatric problems. It is estimated that approximately 70 to 90% of all TSC patients have seizures at some point during their life (Holmes et al, 2007; #CITATION_TAG; Sahin, 2012).In addition, I will discuss the development of new animal models, translational data, and recent clinical trials using mammalian target of rapamycin complex 1 inhibitors such as rapamycin.The past few years have seen spectacular advances that have energized TSC-related research and challenged existing symptomatic treatments.
        In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. Thus, a key problem for theory and research is to specify the conditions under which behavior is more likely to resemble one end of this continuum or the other. In short, what is needed are theories of when rationality is likely to be more or less bounded. Advancements in the discipline of organisational sociology in recent decades, such as the emergence of 'new institutionalism ' (DiMaggio and Powell, 1983; #CITATION_TAG), have highlighted the significance of the professional and/or legal rules, cognitive structures, norms and the prevailing values in which innovation takes place.The former is premised on the assumption that individuals are constantly engaged in calculations of the costs and benefits of different action choices, and that behavior reflects such utility-maximizing calculations. In the latter model, by contrast, \u27oversocialized\u27 individuals are assumed to accept and follow social norms unquestioningly, without any real reflection or behavioral resistance based on their own particular, personal interests. We suggest that these two general models should be treated not as oppositional but rather as representing two ends of a continuum of decision-making processes and behaviors.
        The effectiveness of alcohol brief intervention (ABI) has been established by a succession of meta-analyses but, because the effects of ABI are small, null findings from randomized controlled trials are often reported and can sometimes lead to skepticism regarding the benefits of ABI in routine practice. This article first explains why null findings are likely to occur under null hypothesis significance testing (NHST) due to the phenomenon known as "the dance of the p-values." From the standpoint of scientific progress, the chief problem about null findings under the conventional NHST approach is that it is not possible to distinguish "evidence of absence" from "absence of evidence." Reactivity to assessment has attracted recent attention in the brief alcohol intervention literature. In a review of such trials, it was calculated that control group participants reduce their drinking by approximately 20% (#CITATION_TAG, 36).Primary studies were identified from existing reviews published in English language, peer-reviewed journals between 1995 and 2005. Change in alcohol consumption and selected study-level characteristics for each primary study were extracted. Consumption change data were pooled in random effects models and meta-regression was used to explore predictors of change. Extreme heterogeneity was identified and the extent of observed reduction in consumption over time was greater in studies undertaken in Anglophone countries, with single gender study participants, and without special targeting by age. Heterogeneity was reduced but was still substantial in a sub-set of 15 general population studies undertaken in English language countries.
        Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. At current status of negotiation five forest-related activities have been listed to be implemented as mitigation actions by developing countries, namely: reducing emissions from deforestation (which implies a land-use change) and reducing emissions from forest degradation, conservation of forest carbon stocks, sustainable management of forest, Enhancement of forest carbon stocks (all relating to carbon stock changes and GHG emissions within managed forest land use). The UNFCCC negotiations and related country submissions on REDD+ have advocated that methodologies and tools become available for estimating emissions and removals from deforestation and forest land management with an acceptable level of certainty. Amazonia contains half of all remaining tropical moist forest (#CITATION_TAG).nan
        Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Abstract Smallholder livelihoods in the Peruvian Altiplanoare frequently threatened by weather extremes, includingdroughts, frosts and heavy rainfall. Its highlyvariable, semi-arid climate is closely linked to weatherextremes, such as droughts, frosts and heavy rainfall, whichfrequently challenge people's livelihoods. In 2007, severaldistricts in the Peruvian Altiplano declared a state of emer-gencycausedbyfrosts,hailandtransientdroughtsinthemidstof the agricultural season (INDECI 2009). Despite regionaldevelopment programmes to improve the smallholder sys-tems, poverty and undernourishment remain significant(FONCODES 2006). Produc-tive resources are heterogeneously distributed and liveli-hood options differ among the smallholders which requiresthat the respective vulnerability-creating mechanisms betackled appropriately. One further possibility is suggested by the fact that both CD80 and CD86 can induce signalling by the inhibitory CTLA-4 receptor on T cells as well as activating the CD28 pathway [#CITATION_TAG], therefore an inhibitory milieu may be propagated via the infected macrophage.Given the persistence ofsignificant undernourishment despite regional developmentefforts,weproposeaclusterapproachtoevaluatesmallholders'vulnerability to weather extremes with regard to food security.We applied this approach to 268 smallholder households usinginformation from two existing regional assessments and fromour own household survey. The vulnerability patternswere then ranked according to the different amounts of pur-chase. A second validation aspect accounted for independentlyreported mechanisms explaining smallholders' sensitivity andadaptive capacity.
        The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. The structure or composition of all vegetation types in the region have been shaped by past land-use, fire, or other disturbances, and vegetation patterns will con-tinue to change through time. Conservation efforts aimed at maintaining early succes-sional vegetation types may require intensive management comparable in intensity to the historical disturbances that allowed for their widespread development The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981; Rubertone, 1985; #CITATION_TAG; Eberhardt et al., 2003; Forman et al., 2008).Methods Historical changes in land-use and land-cover across the study region were determined from historical maps and documentary sources. Modern vegetation and soils were sampled and land-use and fire history determined for 352 stratified-random study plots. Ordination and classification were used to assess vegetation variation, and G-tests of independence and Kruskal-Wallis tests were used to evaluate relationships among individual species distributions, past land-use, surficial landforms and edaphic condi-tions.
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Patients receiving dual antiplatelet treatment with aspirin and clopidogrel are commonly treated with proton pump inhibitors (PPIs). Attenuating effects on platelet response to clopidogrel have been reported solely for the PPI omeprazole. Whether this occurs with all PPIs or is even of significant amplitude with omeprazole remains a matter of debate [9, [24] [25] [26] [27] [28] [#CITATION_TAG].PPIs differ in their metabolisation properties as well as their potential for drug-drug interactions. In a cross-sectional observational study, consecutive patients under clopidogrel maintenance treatment (n = 1,000) scheduled for a control coronary angiography were enrolled. Adenosine diphosphate (ADP)-induced platelet aggregation (in AU*min) was measured with multiple electrode platelet aggregometry (MEA). Attenuating effects of concomitant PPI treatment on platelet response to clopidogrel were restricted to the use of omeprazole. Specifically designed and randomized clinical studies are needed to define the impact of concomitant PPI treatment on adverse events after percutaneous coronary intervention.
        Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Extant literature and suppliers interviewed for this study view a solution as a customized and integrated combination of goods and services for meeting a customer's business needs. In contrast, customers view a solution as a set of customer-supplier relational processes comprising (1) customer requirements definition, (2) customization and integration of goods and/or services and (3) their deployment, and (4) postdeployment customer support, all of which are aimed at meeting customers ' business needs. The relational process view can help suppliers deliver more effective solutions at profitable prices. Customer variables include adaptiveness to supplier offerings and political an This description also applies for suppliers: they generally try to leverage an existing relationship by cross-selling or offering new services (Davies et al., 2007), providing capital, information, and dedicated staff or adapting their production and logistics to customer demands (#CITATION_TAG).Supplier variables include contingent hierarchy, documentation emphasis, incentive externality, customer interactor stability, and process articulation.
        This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Autism spectrum disorders (ASDs) are a group of clinically and genetically heterogeneous neurodevelopmental disorders characterized by impaired social interactions, repetitive behaviors and restricted interests (Baird et al., 2006; Zoghbi and Bear, 2012). The genetic defects in ASDs may interfere with synaptic protein synthesis. Synaptic dysfunction caused by aberrant protein synthesis is a key pathogenic mechanism for ASDs (Kelleher and Bear, 2008; Richter and Klann, 2009; Ebert and Greenberg, 2013). The mammalian target of the rapamycin (mTOR) pathway plays central roles in synaptic protein synthesis (Hay and Sonenberg, 2004; Hoeffer and Klann, 2010; Hershey et al., 2012). Recently, Gkogkas and colleagues published exciting data on the role of downstream mTOR pathway in autism (Gkogkas et al., 2013) (Figure  (Figure11). However, the authors used a theoretical linguistic approach (prosodic nonconcatenative morphology developed by #CITATION_TAG) rather than models of processing to account for their data.nan
        Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [12] [13] [14] [15] [16] [17], with some exceptions (!1 day [18];!6-7 days [#CITATION_TAG, 20]).MetS was defined according to the National Cholesterol Education Program Adult Treatment Panel III guidelines. Logistic regressions examined the associations between the subcomponents of physical activity and sedentary behavior and the odds of having MetS or individual risk factors.MetS was observed in 10.2% of men and 5.2% of women. Breaks in sedentary time were inversely associated with abdominal obesity (OR = 0.71, 95% confidence interval [CI] = 0.55-0.91) and hypertriglyceridemia (OR = 0.79, 95% CI 0.63-0.99).
        The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Many formal error identification techniques currently exist which have been developed in nonaviation contexts but none have been validated for use to this end. This paper describes a new human error identification technique (HET - human error template) designed specifically as a diagnostic tool for the identification of design-induced error on the flight deck. A common way to investigate clinical incidents is through Root Cause Analysis (RCA), a methodology combining elements from engineering, psychology, and the 'human factors' tradition (#CITATION_TAG; Vincent, Taylor-Adams, & Stanhope, 1998).HET is benchmarked against three existing techniques (SHERPA - systematic human error reduction and prediction approach; human error HAZOP - hazard and operability study; and HEIST - human error In systems tool). HET outperforms all three existing techniques in a validation study comparing predicted errors to actual errors reported during an approach and landing task in a modern, highly automated commercial aircraft.
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. El objetivo del presente estudio fue evaluar las propiedades psicometricas del cuestionario de procesos de estudio revisado - 2 factores (CPE-R-2F) en estudiantes de ciencias de la salud en Cartagena, Colombia. Para determinar el numero de factores que explicaban el constructo se condujo analisis de factores (exploratorio). El analisis de factores confirmatorio determino la validez de constructo y el alfa de Cronbach la consistencia interna del instrumento. Learning style (deep or shallow) and selfregulated learning strategies are also relevant, and have been shown to mediate between other factors (such as factors of personality and factors of motivation) and academic performance (#CITATION_TAG; Entwhistle, 2005; Swanberg & Martinsen, 2010).The number of factors that explained the construct was determined using exploratory factor analysis. R-SPQ-2F is a scale with acceptable internal consistency and two-factor structure with questionable construct validity.
        According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Drawing on existing literature, it is argued that this policy agenda represents a new frontier in medical/managerial relations, introducing a disciplinary expertise within the health service that provides managers with the knowledge and legitimacy to survey and scrutinise medical performance, made real through procedures for incident reporting and root-cause analysis. The following procedures were used to evaluate the scales of the dependent variables; (i) unrotated principal component analysis (PCA) with subsequent (ii) pro-max (oblique) rotated PCA were conducted, 4 An oblique rotation was used at this stage because it allows correlated factors instead of an assumption of independence among the factors as is maintained in an orthogonal rotation (#CITATION_TAG).The extent of regulatory change is investigated, drawing on an ethnographic case study of one hospital. This leads to new and rearticulated forms of self-surveillance, self-management or 'governmentality', ultimately negating the need for external groups to explicitly manage or regulate professional practice.
        OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of "clusters" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). The SNARE protein of Arabidopsis, SYP121, contributes to vesicle traffic and also controls the gating of K+ channels for K+ uptake by binding to the KC1 channel subunit. The SNARE (for soluble N-ethylmaleimide-sensitive factor protein attachment protein receptor) protein SYP121 (=SYR1/PEN1) of Arabidopsis thaliana facilitates vesicle traffic, delivering ion channels and other cargo to the plasma membrane, and contributing to plant cell expansion and defense. More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (#CITATION_TAG).Here, we report isolating a minimal sequence motif of SYP121 prerequisite for its interaction with KC1. We made use of yeast mating-based split-ubiquitin and in vivo bimolecular fluorescence complementation assays for protein-protein interaction and of expression and electrophysiological analysis.
        Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Selection of accelerometer therefore remains primarily an issue of practicality, technical support, and comparability with other studies. Studies employing multiple accelerometers to estimate energy expenditure report only marginal improvements in explanatory power. Although the issue of epoch length has not been studied in adults, the use of count cut points based on 1-min time intervals maybe inappropriate in children and may result in underestimation of physical activity. Among children and adolescents, the number of monitoring days required ranges from 4 to 9 d, making it difficult to draw a definitive conclusion for this population. Face-to-face distribution and collection of accelerometers is probably the best option in field-based research, but delivery and return by express carrier or registered mail is a viable option.Accelerometer-based activity assessments requires careful planning and the use of appropriate strategies to increase compliance. In adults,!3-5 days of monitoring are normally considered appropriate, which is in accordance with recommendations given [#CITATION_TAG].Accelerometers are best placed on hip or the lower back. Among adults, 3-5 d of monitoring is required to reliably estimate habitual physical activity.
        Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. : social phobia has been under-recognised and under-treated in many countries. Little is known about its economic impact. The defining questions for social phobia have not been studied much before. The number of identified subjects is small and thus raises the possibility of type II errors. They also incur considerable healthcare costs, especially relating to the use of primary care services, experience high levels of productivity losses and receive higher social benefits compared with people in the general population [#CITATION_TAG] [7] [8].Methods: secondary analysis of 1993-1994 Psychiatric Morbidity Survey data compared 63 people with social phobia and 8501 people without psychiatric morbidity. Limitations: analyses were performed post hoc on data collected for other purposes.
        A factor u of a word w is a cover of w if every position in w lies within some occurrence of u in w. A word w covered by u thus generalizes the idea of a repetition, that is, a word composed of exact concatenations of u. Secondary lymphoid organs develop during embryogenesis or in the first few weeks after birth according to a highly coordinated series of interactions between newly emerging hematopoietic cells and immature mesenchymal or stromal cells. Lymphotoxin signaling also maintains the expression of adhesion molecules and chemokines that govern the ultimate structure and function of secondary lymphoid organs. Li & Smyth [#CITATION_TAG] provided a linear-time algorithm for computing the maximal cover array of a word, and showed that, analogous to the border array [8], it actually determines the structure of all the covers of every prefix of the word.These interactions are orchestrated by homeostatic chemokines, cytokines, and growth factors that attract hematopoietic cells to sites of future lymphoid organ development and promote their survival and differentiation. In turn, lymphotoxin-expressing hematopoietic cells trigger the differentiation of stromal and endothelial cells that make up the scaffolding of secondary lymphoid organs.
        Large river valleys have long been seen as important factors to shape the mobility, communication, and exchange of Pleistocene hunter-gatherers. However, rivers have been debated as either natural entities people adapt and react to or as cultural and meaningful entities people experience and interpret in different ways. Both ecological and cultural factors are crucial to explaining these patterns. Whereas the Earlier Upper Paleolithic record displays a general tendency toward conceiving rivers as mobility guidelines, the spatial consolidation process after the colonization of the European mainland is paralleled by a trend of conceptualizing river regimes as frontiers, separating archaeological entities, regional groups, or local networks. The Late Upper Paleolithic Magdalenian, however, is characterized again by a role of rivers as mobility and communication vectors. Here, we attempt to integrate both perspectives. Tracing changing patterns in the role of certain river regimes through time thus contributes to our growing A majority of laymen, politicians and scholars consciously or subconsciously understand settled living as the highest rung on the evolutionary ladder. Accounts of people surviving and even thriving in peripheral areas are often instrumental to construct and maintain the dichotomy between 'the desert and the sown.' It is sometimes stated that mobile peoples obtain their material culture from neighboring settled populations, rather than produce their own, and that they do not leave recognizable archaeological traces apart from 'ephemeral campsites.' From the 24 chapters in this volume, however, it is clear that there is indeed an 'archaeology of mobility. Such an archaeology of mobility encompasses much more than tracing ephemeral campsites. Much like any other group, mobile people produce, appear to use and discard a distinct material culture which includes functional objects, art and architecture. There have been edited books on the archaeology of nomadism in various regions, and there have been individual archaeological and anthropological monographs, but nothing with the kind of coverage provided in this volume. It presents many new ideas and thoughtful approaches, especially in the Central Asian region Archaeologists, consequently, have ever since attempted to tackle the Bspatiality^of past human social units from a whole range of different angles (e.g., Shott 1986; Kelly 1992; Close 2000; Brantingham 2006; #CITATION_TAG; Grove 2009 Grove, 2010 Turq et al. 2013; Cameron 2013; Van Dommelen 2014).'By applying specific and well-defined methods, it is eminently possible to come to a better understanding of mobile people in archaeological contexts.
        Interestingly, this framework also coincides with recent findings obtained by #CITATION_TAG.nan
        Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. In particular, most of syntax (long thought to be there) is not located in Broca's area and its vicinity (operculum, insula, and subjacent white matter). This cerebral region, implicated in Broca's aphasia, does have a role in syntactic processing, but a highly specific one: It is the neural home to receptive mechanisms involved in the computation of the relation between transformationally moved phrasal constituents and their extraction sites (in line with the Trace-Deletion Hypothesis). By contrast, basic combinatorial capacities necessary for language processing - for example, structure-building operations, lexical insertion - are not supported by the neural tissue of this cerebral region, nor is lexical or combinatorial semantics. The dense body of empirical evidence supporting this restrictive view comes mainly from several angles on lesion studies of syntax in agrammatic Broca's aphasia. Syntactic abilities are nonetheless distinct from other cognitive skills and are represented entirely and exclusively in the left cerebral hemisphere. Although more widespread in the left hemisphere than previously thought, they are clearly distinct from other human combinatorial and intellectual abilities. Combinatorial aspects of the language faculty reside in the human left cerebral hemisphere, but only the transformational component (or algorithms that implement it in use) is located in and around Broca's area. English passives are "harder" with regard to a number of variables (Caplan & Waters, 1999; Drai & Grodzinsky, 2006; Druks, 2002; #CITATION_TAG; Mauner, Fromkin, & Cornell, 1993) as they contain more words, more functional morphemes, have a non-canonical word order and, in some theories, involve a transformation from canonical order (or "movement" of constituents).Five empirical arguments are presented: experiments in sentence comprehension, cross-linguistic considerations (where aphasia findings from several language types are pooled and scrutinized comparatively), grammaticality and plausibility judgments, real-time processing of complex sentences, and rehabilitation.
        Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is common, debilitating and associated with high societal costs. Intervention costs of both treatments are offset by net societal cost reductions in a short time. Published economic analyses have explored the cost-effectiveness of a very limited range of interventions for social anxiety disorder and concluded that escitalopram [53], group CBT [54] [55] [#CITATION_TAG] and computer-based self-help [55] [56] [57] are cost-effective options.We conducted a 4-year follow-up study of participants who had received ICBT or CBGT for SAD within the context of a randomized controlled non-inferiority trial. The cost-effectiveness analyses were conducted taking a societal perspective.
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Some of the most successful pathogens of human, such as Mycobacterium tuberculosis (Mtb), HIV, and Leishmania donovani not only establish chronic infections but also remain a grave global threat. These pathogens have developed innovative strategies to evade immune responses such as antigenic shift and drift, interference with antigen processing/presentation, subversion of phagocytosis, induction of immune regulatory pathways, and manipulation of the costimulatory molecules. Costimulatory molecules expressed on the surface of various cells play a decisive role in the initiation and sustenance of immunity. Exploitation of the "code of conduct" of costimulation pathways provides evolutionary incentive to the pathogens and thereby abates the functioning of the immune system. Impairment by pathogens in the signaling events delivered by costimulatory molecules may be responsible for defective T-cell responses; consequently organisms grow unhindered in the host cells. In another study, hot monolithic loops visible with the Yohkoh/SXT were instead resolved as stranded cooler structures with TRACE at later times (#CITATION_TAG), although the large time delay (1 to 3 hours) is hardly compatible with the cooling time from SXT to TRACE sensitivity.nan
        A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. When we speak of consciousness we are referring to the sum total of events in awareness. The term by no means exhausts the realm of things psychological, but it does encompass all of an individual's direct experience. When we speak of the flow of consciousness we are referring to the changes that take place in consciousness over time. The events of consciousness are, of course, extremely complex and varied. They do not contain the imagery of current perceptual activity but they contain imaginai qualities that one can describe in terms of forms, colors, sounds, words, smells, tastes, temperatures, and the like. There are dream-like segments in waking states -in one study 25% of waking thought samples were rated by participants as having at least a trace of dream-like qualities Cox, 1987-1988), which agrees approximately with other results (Foulkes and Fleisher, 1975; #CITATION_TAG Klinger, -1979) -as well as there being waking-like cognitive content in dreams.They embrace images in every sensory modality and in every degree of vividness, realism, and believability, including inner dialogue, hallucinations, reveries, and dreamlike sequences; and they also embrace qualities that are at the same time less figured and more pervasive than these--the affects.
        The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. Summaries of research concerning people with various long-term conditions show that they have much in common as they face the challenges of trying to live as well as possible within the context of physical, mental, or social discomfort and limitation [5] - [#CITATION_TAG].DESIGN The model was derived from a metasynthesis of qualitative research about the reported experiences of adults with a chronic illness. The 292 primary research studies included a variety of interpretive research methods and were conducted by researchers from numerous countries and disciplines. METHODS Metastudy, a metasynthesis method developed by the author in collaboration with six other researchers consisted of three analytic components (meta-data-analysis, metamethod, and metatheory), followed by a synthesis component in which new knowledge about the phenomenon was generated from the findings. The Shifting Perspectives Model indicated that living with chronic illness was an ongoing and continually shifting process in which an illness-in-the-foreground or wellness-in-the-foreground perspective has specific functions in the person's world.
        Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. Intriguingly, the degree of coupling may be higher when HRV is increased and at lower breathing frequencies (#CITATION_TAG; Tzeng et al., 2003), suggesting that unhealthy populations or experiments that are designed to reduce HRV may be more prone to decoupling of cardiorespiratory oscillations.The beat-to-beat RR interval time series of 98 anaesthetized, spontaneously breathing subjects were represented graphically as (1) raw RR interval time series, (2) RR consecutive difference time series and (3) a phase portrait of the RR consecutive difference time series. We then examined the relationships between the presence of cardioventilatory coupling in these epochs and the plot appearance and entropy measures derived from these plots.
        Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. This has been found in non-clinical populations (mainly schoolchildren) [65][66][67]#CITATION_TAG[69][70][71][72][73][74] as well as young people referred for orthodontic treatmentThese were related to the child's emotional and social well-being.
        Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category "toys to pick up before bed" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the "just previous" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, "boring" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term "categorization." Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as "how do prototype representations compare between 6 and 8 months of age?" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. We argue that what infants learn about naming nonsolid substances is contextually bound - most nonsolids that toddlers are familiar with are foods and thus, typically experienced when sitting in a highchair. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (#CITATION_TAG).We examine developmental interactions between context, exploration, and word learning. We asked whether 16-month-old children's naming of nonsolids would improve if they were tested in that typical context. Furthermore, context-based differences in exploration drove differences in the properties attended to in real-time.
        The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. OBJECTIVE Negative affect precedes binge eating and purging in bulimia nervosa (BN), but little is known about factors that precipitate negative affect in relation to these behaviors. The final maintenance factor of mood intolerance (i.e. inability to appropriately cope with adverse affective states followed by dysfunctional impulsive behaviours) [13] is believed to directly affect and maintain binge eating [12, 17, [#CITATION_TAG] [22] [23] [24].METHOD A total of 133 women with current BN recorded their mood, eating behavior, and the occurrence of stressful events every day for 2 weeks. Multilevel structural equation mediation models evaluated the relations among Time 1 stress measures (i.e., interpersonal stressors, work/environment stressors, general daily hassles, and stress appraisal), Time 2 negative affect, and Time 2 binge eating and purging, controlling for Time 1 negative affect.
        Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Every user has a distinct background and a specific goal when searching for information on the Web. Effective personalization of information access involves two important challenges: accurately identifying the user context and organizing the information in such a way that matches the particular context. They usually generate user models that describe user interests according to a set of features [#CITATION_TAG].A spreading activation algorithm is used to maintain the interest scores based on the user's ongoing behavior.
        Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Advanced communication and transport technologies allow food sequestration from the farthest reaches of the planet, but have markedly increasing urban dependence on global food systems over the past 50 years. Simultaneously, such advances have eroded collective memory of food production, while suitable spaces for urban gardening have been lost. Urban gardening and urban social movements can build local ecological and social response capacity against major collapses in urban food supplies. Urban governance for resilience should be historically informed about major food crises and allow for redundant food production solutions as a response to uncertain futures.SUPER, ''Sustainable Urban Planning for Ecosystem Services and Resilience However, clear delineations between urban and rural areas and use of urban green spaces for purely recreational purposes did not emerge until the nineteenth and twentieth centuries, and were reinforced by the development of a globalized economy, the fossil fuel energy regime, and technological innovations such as the steam engine and the railway (McNeill 2000; Barthel and Isendahl 2012; #CITATION_TAG).Hence, they should be incorporated as central elements of sustainable urban development.
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Demographic data, such as age and gender, have been cited as significant (#CITATION_TAG), as are data gathered from learner activity on online learning systems (Bayer et al., 2012; López et al., 2012).Participants (N= 153, 105 = male & 48= female) completed creativity test. Cumulative grade point average (CGPA) was used to select the participants. A multiple regression analysis revealed creativity, age and gender explained 0.143 of the variance in academic achievement. Multiple regression analysis showed interaction effects between creativity, age and gender as low predictors of academic achievement.
        In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. The role of local governments in attracting roots tourists is one of most important factors analyzed in the studies of diaspora tourism. Governments of several countries have actively sought to promote varied forms of roots tourism in order to attract members of their respective diasporas. In contrast, African American roots tourism in Brazil is marked by the almost complete inaction of the government, at both the state and federal levels. This type of tourism was initiated and continues to develop largely as the result of tourist demand, and with very little participation on the part of the state. Our hope is that, by focusing on 'agency' alongside 'structure' (Orlikowksi and Barley, 2001), more light will be shed on the process of technological and organisational change (#CITATION_TAG) such that, eventually, the diffusion of egovernment will be better understood.The chapter also analyzes whether the left-leaning Workers' Party, then in charge of the state government, challenged the longstanding discourse of baianidade (Bahianness) that has predominantly represented blackness (in tourism and other realms) through domesticated and stereotypical images.
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Whether this occurs with all PPIs or is even of significant amplitude with omeprazole remains a matter of debate [9, [#CITATION_TAG] [25] [26] [27] [28] [29].Comparisons were made between proton pump inhibitor use and non-use.
        Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Different types of psychotic symptoms may exist, some being normal variants and some having implications for mental health and functioning. Intermittent, infrequent psychotic experiences were common, but frequent experiences were not. Magical Thinking was only weakly associated with these variables. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas may represent expressions of underlying vulnerability to psychotic disorder, but Magical Thinking may be a normal personality variant Other PLE dimensions have been labeled as persecutory ideas, bizarre experiences and magical thinking (#CITATION_TAG), or delusions, paranoia, grandiosity and paranormal beliefs (Wigman, Van Winkel, Raaijmakers et al., 2011a).Eight hundred and seventy-five Year 10 students from 34 schools participated in a cross-sectional survey that measured psychotic-like experiences using the Community Assessment of Psychic Experiences; depression using the Centre for Epidemiologic Studies Depression Scale; and psychosocial functioning using the Revised Multidimensional Assessment of Functioning Scale. Factor analysis was conducted to identify any subtypes of psychotic experiences. Four subtypes of psychotic-like experiences were identified: Bizarre Experiences, Perceptual Abnormalities, Persecutory Ideas, and Magical Thinking. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas were strongly associated with distress, depression and poor functioning.
        Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [12] [13] [14] [15] [16] [17], with some exceptions (!1 day [18];!6-7 days [19, #CITATION_TAG]).The participants were divided into those with or without MetS according to the Japanese criteria for MetS. A triaxial accelerometer was used to measure light-intensity lifestyle activity [1.6-2.9 metabolic equivalents (METs)] and sedentary time (<=1.5 METs). Logistic regression was used to predict MetS from the levels of light-intensity lifestyle activity and sedentary time with age, sex, smoking, calorie intake, accelerometer wear time, and MVPA as covariates.The odds ratios (OR) for MetS in the highest and middle tertiles of light-intensity lifestyle activity were 0.44 [95% confidence interval (CI): 0.24 to 0.81] and 0.51 (95% CI: 0.29 to 0.89) relative to the lowest tertile, after adjustment for age, sex, smoking, calorie intake, accelerometer wear time and MVPA (Ptrend = 0.012).
        Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. There is a large and growing number of alien species in ecosystems all over the world. However, action against invasives is often hindered by a lack of relevant ecological information such as the expected distribution and impact of the invader. In Europe, radio-tracking (Clergeau & Vergnes, 2011; Strubbe & Matthysen, 2011) and habitat selection studies (#CITATION_TAG; Newson et al., 2010) indicate that parakeets prefer to forage in city parks and gardens, where bird feeders and ornamental vegetation present parakeets with abundant food.We determined the abundance of parakeets and native hole-nesters in 44 study sites using point counts. We examined the relationship between parakeet numbers and a set of habitat and landscape variables and to assess the effect of competition, we studied the relationships between the number of parakeets and the number of native hole-nesters.
        In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. It is argued that the account of Savage-Rumbaugh's ape language research in Savage-Rumbaugh, Shanker and Taylor (1998. Apes, Language and the Human Mind. Oxford University Press, Oxford) is profitably read in the terms of the theoretical perspective developed in Clark (1997. The authors, though, make heavy going of a critique of what they take to be standard approaches to understanding language and cognition in animals, and fail to offer a worthwhile theoretical position from which to make sense of their own data. Research in artificial intelligence demonstrated convincingly that symbolic reasoning machines cannot replicate the power and fluidity of human cognition (#CITATION_TAG; Beer, 2000; Wheeler, 2005).This model of 'distributed' cognition helps makes sense of the lexigram activity of Savage-Rumbaugh's subjects, and points to a re-evaluation of the language behaviour of humans
        In poor countries, labor productivity in agriculture is considerably lower than in the rest of the economy. Third, labor productivity in agriculture is severely mis-measured in the US. Its unique feature is that it gives rise to an endogenously variable number of sectors. For example, Adamopoulous and Restuccia (2014) and Donovan (2014) point to the scale or risk of farming; Restuccia et al. (2008) and #CITATION_TAG to barriers of moving workers or intermediate goods between agriculture and non-agriculture; and Lagakos and Waugh (2013) to selection of the workers in the two sectors.Each new sector is created by a pervasive innovation, which creates a new market and into and out of which there are entry and exit of firms. In the construction of our model we found inspiration in a number of growth models, both endogenous and evolutionary as well as on empirical work on structural change. Within our model the ability to create new sectors at the right times is the crucial determinant of the growth potential of an economic system.
        This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. The EMF 22 international scenarios engaged ten of the world's leading integrated assessment (IA) models to focus on the combined implications of three factors integral to international climate negotiations: (1) the long-term climate-related target, expressed in this study in terms of the CO2-equivalent (CO2-e) concentration associated with the GHGs  regulated under the Kyoto Protocol, (2) whether or not this target can be temporarily exceeded prior to 2100 ("overshoot") allowing for greater near-term flexibility, and  (3) the nature of international participation in emissions mitigation. Key model comparison studies for instance include the previous EMF climate-change-oriented studies like the EMF19 study on carbon constraints and advanced energy technologies (Weyant 2004), the EMF21 study on non-CO 2 Kyoto gas mitigation (Weyant et al. 2006), and the EMF22 study on climate control scenarios including phased participation (#CITATION_TAG).The EMF 22 international scenarios are based on combinations of these dimensions, embodied in ten specific climate-action cases that all modeling groups in the study attempted to represent
        The crucial question in the public debate of extreme events is increasingly whether and to what extent the event has been caused by anthropogenic warming. This study thus highlights the challenges of probabilistic event attribution of complex weather events and identifies Part of the EQUIP special issue of Climatic Change This article is part of a Special Issue on "Managing Uncertainty in Predictions of Climate and Its Impacts" edited by Andrew Challinor and Chris Ferro. The 2010 summer heat wave in western Russia was extraordinary, with the region experiencing the warmest July since at least 1880 and numerous locations setting all-time maximum temperature records. Studies into the European heat wave of 2003 (Stott et al. 2004, the England and Wales floods of 2000 (Pall et al. 2011), and the Russian heat wave of 2010 (#CITATION_TAG; Rahmstorf and Coumou 2011; Otto et al. 2012) have sought to determine to what extent the risks of these events occurring have changed because of anthropogenic greenhouse gas emissions, many of them using the emerging method of probabilistic event attribution (PEA).Model simulations and observational data are used to determine the impact of observed sea surface temperatures (SSTs), sea ice conditions and greenhouse gas concentrations.
        Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is a prevalent, disabling disorder. There was substantial variation across medication classes in the number of dropouts due to adverse events, with an average number needed to harm of 14.4. However, evidence for the efficacy of b-blockers in treating performance anxiety was lacking. This review is an updated version of a Cochrane Review in The Cochrane Library, Issue 4, 2004. Several studies have assessed the clinical effectiveness of psychological and pharmacological treatments for social anxiety disorder [10] [#CITATION_TAG] [12] [13].A systematic review and meta-analysis was conducted of all published and unpublished placebo-controlled randomized controlled trials (RCTs) undertaken between 1966 and 2007.
        The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. Méon and Sekkat (2005), Pellegrini and Gerlagh (2004), #CITATION_TAG, and Mauro (1995) find that it leads to lower investment.It starts from an endogenous growth model and extends it to account for the detrimental effects of corruption on the potentially productive components of government spending, namely military and investment spending. Second, allowing for the cyclical economic fluctuations in specific countries leaves the estimated elasticities close to those of the full sample. Third, there are significant conditioning variables that need to be taken into account, namely the form of  government, political instability and natural resource endowment.
        In previous work the authors considered the asymmetric simple exclusion process on the integer lattice in the case of step initial condition, particles beginning at the positive integers. There it was shown that the probability distribution for the position of an individual particle is given by an integral whose integrand involves a Fredholm determinant. In one an apparently new distribution function arises and in another the distribution function F 2 arises. CONTEXT Previous studies may have underestimated the contribution of health behaviors to social inequalities in mortality because health behaviors were assessed only at the baseline of the study. DESIGN, SETTING, AND PARTICIPANTS Established in 1985, the British Whitehall II longitudinal cohort study includes 10 308 civil servants, aged 35 to 55 years, living in London, England. In previous work [#CITATION_TAG] the authors considered the asymmetric simple exclusion process (ASEP) on the integer lattice Z in the case of step initial condition, particles beginning at the positive integers Z +.Analyses are based on 9590 men and women followed up for mortality until April 30, 2009. Socioeconomic position was derived from civil service employment grade (high, intermediate, and low) at baseline.
        Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. Annually, 12% of Canadians from 15 to 64 years suffer from a mental disorder or substance dependence. Few studies have examined the prevalence of mental disorders among Canadian workers. About one-third of society's depression-related productivity losses can be attributed to work disruptions. The impact of mental illness on the workplace has been examined in terms of its effect on presenteeism, absenteeism and disability days. The presence of any of these has been used to indicate decreased productivity, the largest burden arising from presenteeism. In total, Canada annually loses about $4.5 billion from this decreased productivity. Mental illness is also associated with short-term and long-term disability, which in turn is often related to insurance coverage. Mental illness related disability claims have doubled and mental illness accounts for 30% of disability claims, at a cost of $15 to $33 billion annually. The needs of the working population and employers must be addressed. We must be aware of patterns of mental disorder among occupational groups and industry sectors. Effective policies and programs must be based on solid evidence. For example, one study reported that workers with MDD missed an average of 32 days of work in a 12-month assessment period [5], while another found that about 30% of work disability claims in Canada were attributed to mental illness, predominant depression, and other mood disorders [#CITATION_TAG].Major trends in the literature are also commented on, and significant gaps in knowledge are identified.
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Undamped, or even growing waves were observed by SDO/AIA (#CITATION_TAG; Nisticò et al., 2013).Special efforts are made to disentangle the effects of family size and birth order, since these effects have often been confounded in the past.
        Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. The most common were allergies, arthritis/joint pain or stiffness, and back or neck disorders. However, the greater proportion of the total economic burden of MDD lies in reduced productivity, or presenteeism, in which the depressed individual remains in the work setting but with productivity suffering both in quality and quantity [7, #CITATION_TAG].Methods: Using the Stanford Presenteeism Scale, information was collected from workers at five locations on work impairment and absenteeism based on self-reported "primary" chronic health conditions. Survey data were merged with employee demographics, medical and pharmaceutical claims, smoking status, biometric health risk factors, payroll records, and job type.
        This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Four political systems and a qualitative index of political rights account for differences in political institutions. Pluralistic systems are associated with higher agricultural protection levels, although in a nonlinear fashion. All studies but one, exploit the cross-country variation in the data and find mixed and often weak evidence on the effect of democracy on agricultural protection (see #CITATION_TAG; Swinnen et al. 2000; Olper, 2001) 4.The analysis incorporates the effects of development, of constraints on tax collection feasibility, and of comparative advantages and terms of trade.
        Fifty per cent of responders had needed to consider MRE under GA. Aims To survey the perceived indications for magnetic resonance imaging of the small bowel (MRE) by experts, when MR enteroclysis (MREc) or MR enterography (MREg) may be chosen, and to determine how the approach to MRE is modified when general anaesthesia (GA) is required. In brief patients prefer MREg [46], while radiologists prefer MREc images [46] [47] [48] [49] [#CITATION_TAG].Forty patients with suspected Crohn's disease (CD) were examined with both MRI methods. MRI per OS was performed with a 6% mannitol solution and MRE with nasojejunal intubation and a polyethylenglycol solution. MRI protocol consisted of balanced fast field echo (B-FFE), T2 and T1 sequences with and without gadolinium. Two experienced radiologists individually evaluated bowel distension and pathological findings including wall thickness (BWT), contrast enhancement (BWE), ulcer (BWU), stenosis (BWS) and edema (EDM). However, CD was diagnosed with high diagnostic accuracy (sensitivity, specificity, positive and negative predictive values: MRI per OS 88%, 89%, 89%, 89%; MRE 88%, 84%, 82%, 89%) and inter-observer agreement (MRI per OS k = 0.95; MRE k = 1). However, both methods diagnosed CD with a high diagnostic accuracy and reproducibility.
        The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Finally, as testing the significance of the mediation or indirect effects using bootstrap procedure has been recommended [#CITATION_TAG], Mplus [49] was specified to (a) create 5,000 bootstrap samples from the data set by random sampling with replacement and (b) generate indirect effects and bias-corrected confidence intervals (95 % CIs) around the indirect effects when analysing the (final) structural models (Fig. 2).Separate sections describe examples of moderating and mediating variables and the simplest statistical model for investigating each variable. The strengths and limitations of incorporating mediating and moderating variables in a research study are discussed as well as approaches to routinely including these variables in outcome research. The routine inclusion of mediating and moderating variables holds the promise of increasing the amount of information from outcome studies by generating practical information about interventions as well as testing theory.
        Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. Agency theory is an important, yet controversial, theory. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (Beyer and Kilsby, 1996; #CITATION_TAG Bond et al.,, 2008 Crowther et al., 2001; Graetz, 2010; Griffin et al., 1996; Heffernan and Pilkington, 2011; McCaughrin et al., 1993; Noble et al., 1991; Rhodes et al., 1987; Stevens and Martin, 1999).nan
        Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. In transport scheme appraisal, savings in travel time typically represent a substantial proportion of the benefits of a scheme--benefits used to justify its often enormous financial costs. Travel demand analysis treats travel time and activity time as separate, albeit acknowledging an interdependency. The paper challenges these approaches by exploring how travel time can be, and is, being used 'productively' as activity time, and what enhancements to time use might be emerging in the 'information age'. These sustainable mobility policies are called mobility management or travel/transportation demand management (TDM) to stress that the focus is not on infrastructure supply but on managing the de-mand-side, i.e. using the transport system in the most optimal way to fulfil our lifestyle needs (Frändberg and Vilhelmson, 2010; #CITATION_TAG).Such benefits are founded on the assumption that travel time is unproductive, wasted time in-between 'real' activities and which should be minimised.
        The rapid growth of the literature on neuroimaging in humans has led to major advances in our understanding of human brain function but has also made it increasingly difficult to aggregate and synthesize neuroimaging findings. We verified the distinctness of these two networks using a large meta-analytic resting state database (www.neurosynth.org; #CITATION_TAG), that produced similar (though less extensive) networks, shown in Supplementary Figure S1.nan
        Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Human history, as written traditionally, leaves out the important ecological and climate context of historical events. But the capability to integrate the history of human beings with the natural history of the Earth now exists, and we are finding that human-environmental systems are intimately linked in ways we are only beginning to appreciate. In Sustainability or Collapse?, researchers from a range of scholarly disciplines develop an integrated human and environmental history over millennial, centennial, and decadal time scales and make projections for the future. History offers many lessons relevant to sustainability by exhibiting how humans and their societies have recognized and responded to challenges and opportunities of their natural environment (Redman 1999; Diamond 2005; #CITATION_TAG; Sinclair et al. 2010).The contributors focus on the human-environment interactions that have shaped historical forces since ancient times and discuss such key methodological issues as data quality.
        Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. Demand response (DR) measures typically aim at an improved utilization of power plant and grid capacities. In energy systems mainly relying on photovoltaic and wind power, DR may furthermore contribute to system stability and increase the renewable energy share. EPRI ( 2009) present an extensive review for the U.S., and #CITATION_TAG carries out a comprehensive comparative study on DSM potentials for 40 European countries.Special attention is given to temporal availability and geographic distribution of flexible loads.
        requires a greater understanding of characteristics of clients who may or may not benefit from this technology. It is important to note the predominance of case series or case study designs in the field representing only level IV evidence [#CITATION_TAG].We simulate microflares as small (either randomic or periodic) heating episodes which account for all the "stationary heating". We synthesize the emission in some lines and bands observable by X-ray instruments.
        Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The Health Development Agency (www.hda.nhs.uk) is the national authority and information resource on what works to improve people's health and reduce health inequalities in England. iii Contents Foreword iv Summary 1 What is the role of qualitative approaches in traditional trials and experimental studies? 4 At what point in the development of a field of knowledge is it appropriate to pull qualitative and quantitative learning together? 7 Are there hierarchies of evidence within the different types of qualitative investigation? 11 Advice from the NHS Centre for Reviews and Dissemination 11 The interpretive/integrative distinction 11 Narrative summary 12 Thematic analysis 15 Grounded theory 15 Meta-ethnography 17 Estabrooks, Field and Morse's aggregation of findings approach 19 Qualitative meta-analysis 19 Qualitative meta-synthesis 19 Meta-study 21 Miles and Huberman's cross-case data analysis techniques 22 Content analysis 23 Case survey 24 Qualitative comparative analysis 24 Bayesian meta-analysis 25 Meta-needs assessment 27 Discussion 28 Quantitising and qualitising 28 Issues in qualitative synthesis 28 Conclusions 31 References 32 Quant-and-qual.indd 3/26/2004, 10:10 AM 3 iv In 2000 the Health Development Agency (HDA) was established to, among other things, build the evidence base in public health, with particular reference to reducing inequalities in health (Department of Health, 2001). Since then the HDA has been engaged in a programme of developing methodologies and protocols to do precisely that (Swann et al., 2002). There are two important ideas behind the HDA's remit, political and scientific. The political imperative is a clear commitment to tackling the long-term problem at the heart of public health - that, as the health of the population as a whole continues to improve, at the same time the gradient in inequalities in health across the population, from the most to the least advantaged becomes worse (Acheson, 1998) There is, in other words, a long-standing problem of inequalities in health. The scientific principle is that the best available evidence should be used in order to ... [#CITATION_TAG, 18] By integrating individual qualitative research studies into a qualitative synthesis, new insights and understandings can be generated and a cumulative body of empirical work produced. [19] Such syntheses have proven useful to health policy and practice.6 Generating hypotheses and questions 6 Informing the selection of outcomes for review 6 Extending or guiding sampling 6 Providing explanations and informing conclusions 6 What constitutes good evidence from qualitative studies?
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. #CITATION_TAG found the importance of learning approach varied with assessment type.nan
        We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. A U(1) adiabatic phase is created by two laser beams for the tunneling of atoms between neighbor lattice sites. Near these crossing points the quasiparticles and quasiholes can be considered as massless Dirac fermions. We note that this dispersion relation gives rise to two 'Dirac points" for (k x, k y) = (±π/2d x, π/2d y) around which the dispersion relation is linear [13, #CITATION_TAG] (see also [49, 50, 51, 52, 53] for discussions of the Dirac points occurring in different settings, such as hexagonal lattices or non-Abelian gauge fields).Furthermore, the anisotropic effects of massless Dirac fermions are obtained in the present square lattice model. The Dirac fermions as well as the anisotropic behaviors realizeded in our system can be experimentally detected with the Bragg spectroscopy technique
        The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Scientometric mapping and analysis using bibliographic data is a well-established methodology, for which a variety of approaches, techniques, and automated tools have been developed (#CITATION_TAG).Different techniques and software tools have been proposed to carry out science mapping analysis.
        Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Genetic disorders that present with a high incidence of autism spectrum disorders (ASD) offer tremendous potential both for elucidating the underlying neurobiology of ASD and identifying therapeutic drugs and/or drug targets. Tuberous sclerosis complex (TSC) is one such genetic disorder that presents with ASD, epilepsy, and intellectual disability. Cell culture and mouse model experiments have identified the mTOR pathway as a therapeutic target in this disease. Typical TSC lesions include hypomelanic macules and facial angiofibromas, as well as brain cortical tubers, subependymal nodules, and subependymal giant cell astrocytomas (SEGAs) (Holmes et al, 2007; Curatolo et al, 2008; #CITATION_TAG).nan
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. This book is a successor to J Griffiths, A Bood and H Weyers, Euthanasia and Law in the Netherlands (Amsterdam University Press 1998) which was widely praised for its thoroughness, clarity, and accuracy. Does the Dutch experience with legalised euthanasia support the idea of a 'slippery slope' toward a situation in which life-especially of the more vulnerable members of society-is less effectively protected? Is it possible to explain and to predict when a society will decide to legalise euthanasia? Similarly, illegal clandestine end-of-life practices, performed without peer control, as documented in Belgium before the euthanasia law and elsewhere (Kuhse et al. 1997; Deliens et al. 2000; #CITATION_TAG) can be considered more worrying than even imperfectly regulated legal euthanasia.The new book emphasises recent legal developments and new research, and has been expanded to include a full treatment of Belgium, where since 2002 euthanasia has also become legal. The book also includes descriptions written by local specialists of the legal situation and what is known about actual practice in a number of other European countries (England and Wales, France, Italy, Scandinavia, Spain, Switzerland). It covers in detail: - the substantive law applicable to euthanasia, physician-assisted suicide, withholding and withdrawing treatment, use of pain relief in potentially lethal doses, palliative and terminal sedation, and termination of life without a request (in particular in the case of newborn babies); -the process of legal development that has led to the current state of the law; -the system of legal control and its operation in practice; -the results of empirical research concerning actual medical practice. A concluding part deals with some general questions that arise out of the material presented: Is the legalisation of euthanasia an example of the decline of law or should it, on the contrary, be seen as part and parcel of the increasing juridification of the doctor-patient relationship?
        It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. English language teaching (ELT) has been investigated from various angles including how English language teachers perceive what happens in an ELT classroom. How primary school English language learners perceive their experiences of ELT is rarely reported in the published literature, particularly from developing countries such as Bangladesh. This article reports on a study that examined Bangladeshi primary school learners' experience of English language classroom practices in which technology-enhanced communicative language teaching activities were promoted through a project called English in Action (EIA). EIA is a large-scale 9-year long international English language development project in Bangladesh, funded by the UK government. This contradicts studies by #CITATION_TAG in Bangladesh and Asafeh, Khwaile, and Alshbou (2012) in Jordan that report many EFL learners preferred to have more communicative activities to practice their English, although they showed positive attitudes to traditional activities such as drilling of grammar rules and vocabulary.A semi-structured group interview was conducted with 600 Grade 3 students from different regions of Bangladesh.
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. There has been much debate regarding the 'double-effect' of sedatives and analgesics administered at the end-of-life, and the possibility that health professionals using these drugs are performing 'slow euthanasia.' On the one hand analgesics and sedatives can do much to relieve suffering in the terminally ill. On the other hand, they can hasten death. According to a standard view, the administration of analgesics and sedatives amounts to euthanasia when the drugs are given with an intention to hasten death. Some were explicit in describing a 'grey' area between palliation and euthanasia, or a continuum between the two. The distinction between the intention of life-ending and compassionate intensification of symptom treatment is often blurred (#CITATION_TAG).In this paper we report a small qualitative study based on interviews with 8 Australian general physicians regarding their understanding of intention in the context of questions about voluntary euthanasia, assisted suicide and particularly the use of analgesic and sedative infusions (including the possibility of voluntary or non-voluntary 'slow euthanasia').
        Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. One of the paradigms about coronal heating has been the belief that the mean or summit temperature of a coronal loop is completely insensitive to the nature of the heating mechanisms. However, we point out that the temperature profile along a coronal loop is highly sensitive to the form of the heating. For example, when a steady state heating is balanced by thermal conduction, a uniform heating function makes the heat flux a linear function of distance along the loop, while T7/2 increases quadratically from the coronal footpoints; when the heating is concentrated near the coronal base, the heat flux is small and the T7/2 profile is flat above the base; when the heat is focused near the summit of a loop, the heat flux is constant and T7/2 is a linear function of distance below the summit. Despite various difficulties 8 and the rather slow development since the introduction of tensor models, some interesting results have been reported recently [9] [10] [11] [12] [13] [14] [#CITATION_TAG].In particular, we apply this philosophy to a preliminary analysis of Yohkoh observations of the large-scale solar corona. In addition, we suggest that the decline in coronal intensity by a factor of 100 from solar maximum to solar minimum is a natural consequence of the observed ratio of magnetic field strength in active regions and the quiet Sun; the altitude of the maximum temperature in coronal holes may represent the dissipation height of Alfven waves by turbulent phase mixing; and the difference in maximum temperature in closed and open regimes may be understood in terms of the roles of the conductive flux there.
        Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Rapid motorization and fuel cost hike over the past few years have made carpool a new mode of travel in Chinese cities. But transportation policy makers have been rather ambivalent, if not indifferent, about carpool. Unlike cities in highly motorized societies, little is known about carpooling behavior in emerging economies such as China. What are the current practice and issues of carpool in Chinese cities? How do carpools in China compare with those in the motorized Western cities? Can carpools help Chinese cities mitigate the negative impacts of rapid motorization? Are foreign policies such as High-Occupancy-Vehicle (HOV) lanes transferable to China? More recently, carpooling was also advocated during the 2008 Olympics in Beijing as a response to driving restrictions (#CITATION_TAG).Policy suggestions are proposed to Chinese decision makers.Chinese city Carpool HOV Transport policy
        Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Traffic congestion has been a pervasive problem in many urban areas of this country. Basic questions about this potential include the following. Can the current population density, origin-destination distribution, tolerable pick-up and drop-off delays, departure time distribution, and the tolerance for deviation from preferred departure time support a sizable carpooling population that can make a significant contribution to traffic demand reduction? Could the proportion of long trips that are likely candidates for carpooling (e.g., those long trips with same O-D) be so small that no significant traffic demand reduction could be expected from carpooling? The potential depends on many factors, some of which are more amenable to quantification than others. Under the assumptions made in the paper, carpooling among unrelated partners has little potential for demand reduction. Finding a carpool partner with the same origin and destination zone may be difficult, especially in low-density areas (#CITATION_TAG) and at larger distances from the destination.Our approach to assessing the potential is to separate such quantifiable factors from the rest, and then, based on these quantifiable factors, identify likely upper bounds for the potential. For our numerical study, we use the job and worker data of the city of Los Angeles to approximate the worker/job density. An entropy optimization model that is equivalent to the gravity model is used for trip distribution.
        The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. Recent studies have focused on the relationship between the marine fauna of the Eastern Atlantic and the Mediterranean Sea, but within the Atlantic, little is known about genetic relationships between populations of the Macaronesian islands. The genetic, biological, and ecological information obtained in this investigation, along with relevant published information [64, #CITATION_TAG], was used for identification of conservation units.We combined phylogeographic and coalescent approaches using the fast evolving mitochondrial control region gene. Migration across the three archipelagos was estimated and a prevailing northwest trend was detected.
        We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. Similar appeals to the utility of CL explanation are common in the Bayesian literature (see, e.g., #CITATION_TAG).The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks.
        The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. In recent years there has been widespread acceptance that cognitive behavior therapy (CBT) is the treatment of choice for bulimia nervosa. The cognitive behavioral treatment of bulimia nervosa (CBT-BN) was first described in 1981. Over the past decades the theory and treatment have evolved in response to a variety of challenges. Although several randomised controlled trials have shown that CBT is more effective than a wide range of alternative treatments [1, 2, 5, 11], amongst BN treatment completers, only 40-50 % have a full and lasting response [#CITATION_TAG] [13] [14].The treatment has been adapted to make it suitable for all forms of eating disorder-thereby making it "transdiagnostic" in its scope- and treatment procedures have been refined to improve outcome. The new version of the treatment, termed enhanced CBT (CBT-E) also addresses psychopathological processes "external" to the eating disorder, which, in certain subgroups of patients, interact with the disorder itself.
        Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Similar observations have been made in the past [30] [31] [#CITATION_TAG] [33] [34], although others have reported either no relationship or a negative association with SES [35].Ninety-three Israeli-born MS patients identified in country-wide studies and 94 age- and sex-matched controls were interviewed. The questionnaire covered a large span of factors at ages 0,10 and onset of the disease, with particular emphasis on socioeconomic status (SES) and sanitary conditions (SAN).
        Fifty per cent of responders had needed to consider MRE under GA. Aims To survey the perceived indications for magnetic resonance imaging of the small bowel (MRE) by experts, when MR enteroclysis (MREc) or MR enterography (MREg) may be chosen, and to determine how the approach to MRE is modified when general anaesthesia (GA) is required. In this article we will give a comprehensive literature review on sedation/general anaesthesia (S/GA) and discuss the international variations in practice and options available for S/GA for imaging children.The key articles were obtained primarily from PubMed, MEDLINE, ERIC, NHS Evidence and The Cochrane Library.Recently, paediatric radiology has seen a surge of diagnostic and therapeutic procedures, some of which require children to be still and compliant for up to 1 h. It is difficult and sometimes even impossible to obtain quick and high-quality images without employing sedating techniques in certain children. As with any medical procedure, S/GA in radiological practice is not without risks and can have potentially disastrous consequences if mismanaged. Advances in knowledge Imaging children under general anaesthesia is becoming routine and preferred by operators because it ensures patient conformity and provides a more controlled environment. SB distension with large volumes of orally administered fluids, however, is generally considered a contraindication in GA, or heavy sedation, given the risk of aspiration [#CITATION_TAG] [3] [4] [5].nan
        Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Employer transport plans (ETPs) are increasingly seen by transport planners as one of potential means to manage the demand for private transport. Drawing on US, UK and Dutch experience, it argues that only a minority of employers will voluntarily implement ETPs because they will be seen by the majority as an unnecessary and potentially costly diversion from their normal business activities. Finally, involving the private sector also reduces the burden of transport policies on the public budget (Cairns et al., 2008; Roby, 2010; #CITATION_TAG).Instead it urges the adoption of an approach which uses fiscal measures to encourage organizations to adopt ETPs
        Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Abstract In recent years, there has been a growing interest in a range of transport policy initiatives which are designed to influence people's travel behaviour away from single-occupancy car use and towards more benign and efficient options, through a combination of marketing, information, incentives and tailored new services. In transport policy discussions, these are now widely described as 'soft' factor interventions or 'smarter choice' measures or 'mobility management' tools. In 2004, the UK Department for Transport commissioned a major study to examine whether large-scale programmes of these measures could potentially deliver substantial cuts in car use. Finally, involving the private sector also reduces the burden of transport policies on the public budget (#CITATION_TAG; Roby, 2010; Rye, 2002).nan
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Self-regulation of cognition and behavior is an important aspect of student learning and academic performance in the classroom context (Corno & Mandinach, 1983; Corno & Rohrkemper, 1985). There are a variety of definitions of selfregulated learning, but three components seem especially important for classroom performance. There are two strands of expectancy motivation (Eccles & Wigfield, 2002; #CITATION_TAG):A correlational study examined relationships between motivational orientation, self-regulated learning, and classroom academic performance for 173 seventh graders from eight science and seven English classes. A self-report measure of student self-efficacy, intrinsic value, test anxiety, self-regulation, and use of learning strategies was administered, and performance data were obtained from work on classroom assignments. First, self-regulated learning includes students' metacognitive strategies for planning, monitoring, and modifying their cognition (e.g., Brown, Bransford, Campione, & Ferrara, 1983; Corno, 1986; Zim
        Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. Tensor models can be regarded as theories of dynamical fuzzy spaces, and provide back-ground independent theories of space. Their classical solutions correspond to classical back-ground spaces, and the small fluctuations around them can be regarded as fluctuations of fields in them. Tensor models (or generalized matrix models)1)-8) were originally considered as the generalization of the matrix models, which describe the two-dimensional simpli-cial quantum gravity, to higher-dimensional cases. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19] [20] [21] [#CITATION_TAG] [23] [24] [25] [26].Numerical analysis also suggests that the lowest-order effective action is composed of curvature square terms, which is consistent with general relativity in view of the form of the considered action of the tensor model. As in the matrix models, partition functions are given by a certain summation over simplicial manifolds, which are dua
        Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. The airway epithelium forms a continuous barrier from the nose to the alveoli and serves a variety of functions. Multiple functionally distinct cell types are involved in these processes. The innate defence functions require a patent airway epithelium, with infections often associated with epithelial defects and phenotypic alterations that are themselves associated with multiple lung diseases. Non-typeable Haemophilus influenzae (NTHi) and respiratory syncytial virus (RSV) are frequently identified in the airways in a range of respiratory diseases These pathogens often trigger exacerbations and worsening symptoms that often result in hospitalisation. This is particularly true in paediatric populations. Although mortality for NTHi and RSV infections alone are themselves low it remains unclear what role these infections play in mortality rates in complex chronic respiratory infections. These markers are representative of different epithelial cell types within the cultures. Macrophages are not only a key line of defence in the respiratory tract, responsible for phagocytosis and clearance of infectious organisms but are also orchestrators of the adaptive immune response through presentation of antigen and by the cytokines they release [4, #CITATION_TAG].In vitro airway models were established using lung derived cell lines, undifferentiated primary human bronchial epithelial (uHBE) cells and air-liquid interface (ALI) differentiated uHBE cell cultures. Following establishment of differentiation we validated ALI cultures using a number of markers, including for the putative innate defence PLUNC family proteins, gel-forming mucins and tubulin. Cultures were infected with NTHi or RSV for periods of time ranging from 1 hour to 7 days with a view to establishing chronic infections and allowing biofilm formation. Neutrophil products and trypsin were shown to degrade PLUNC proteins in ALI cell secretions.
        Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. ABSTRACT Background: Although advance directives may seem useful instruments in decision-making regarding incompetent patients, their validity in cases of dementia has been a much debated subject and little is known about their effectiveness in practice. Insight into the experiences and wishes of people with dementia regarding advance directives is totally lacking in empirical research. It is clear, however, that the use of advance directives in practice remains problematic, above all in cases of advance euthanasia directives, but to a lesser extent also when non-treatment directives are involved. The peasant families of the Río Auqui watershed cultivate an average area of around 3 hectares per family which are distributed in small plots over different altitudes of the valley (Fig. 1) in order to guarantee diversified production for each family (Sietz et al., 2012; #CITATION_TAG; Zimmerer, 2011).Methods: The relevant problems from the ethical debate on advance directives in cases of dementia are summarized and we discuss how these relate to what is known from empirical research on the validity and effectiveness of advance directives in the clinical practice of dementia care.
        Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. The number of bus-based Park and Ride schemes in the UK has grown substantially over the past 40 years as a result of its encouragement by the Government as a tool to deal with increasing traffic congestion and traffic-related pollution. Analogously, carpool parkings and park and ride facilities should be carefully planned since they often generate additional traffic and encourage car-oriented land use development outside urban areas (#CITATION_TAG; Parkhurst, 2000).The authors identify phases of development of Park and Ride since its emergence as a local solution to transport capacity constraints in historic towns. Policy goals are identified against which a review of literature is used to highlight its effectiveness.
        In Czech, German, and many other languages, part of the semantic focus of the utterance can be moved to the left periphery of the clause. Given elements may later receive (topic or contrastive) accents, which accounts for fronting in multiple focus/contrastive topic constructions. We propose that movement to the left periphery is generally triggered by an unspecific edge feature of C (Chomsky 2008) and its restrictions can be attributed to requirements of cyclic linearization, modifying the theory of cyclic linearization developed by Fox and Pesetsky (2005) . In the picture that we had in mind, the syntax is autonomous - ''it does what it does'' - but sometimes the result maps to an unusable phonological representation. In this sense, linearization acts logically as a filter on derivations. We know of no evidence that the syntax can predict which syntactic objects will be usable by the phonology, and we know of no clear evidence that the phonology communicates this information to the syntax. Accentuation comes into play indirectly only, its relevance for fronting stems from the fact that accentuation is a side-effect of cyclic linearization in the sense of #CITATION_TAG and Müller (2007).We thus attempted to identify certain deviant configurations that are not plausibly excluded for syntax-internal reasons, but are filtered out in the linearization process.
        It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Web services technology has generated a lot interest, but its adoption rate has been slow. In an attempt to obtain low molecular-weight derivatives from sulfated fuco-oligosaccharides, results based on mild acid hydrolysis were also reported (#CITATION_TAG).quality of services) are taken into account for the service discovery.
        ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. Caves in Ireland, as elsewhere, have been used for shelter and burial over much of recorded time. This consists of firms that have quadratic cost functions, Cobb-Douglas technologies as well as output constraints and that are price takers in the markets for production factors (see also #CITATION_TAG).nan
        First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. Banks and other lenders often transfer credit risk to liberate capital for further loan intermediation. 57 This difficulty is also stressed by #CITATION_TAG.After an overview of recent credit risk transfer activity, the following points are discussed: motivations for CRT by banks; risk retention; theories of CDO design; specialty finance companies. As an illustration of CLO design, an example is provided showing how the credit quality of the borrowers can deteriorate if efforts to control their default risks are costly for issuers.
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Death is often preceded by medical decisions that potentially shorten life (end-of-life decisions [ELDs]), for example, the decision to withhold or withdraw treatment. Almost all of the incompetent patients had previously stated that they wanted their family involved in case of incompetence, but half did not achieve this.In half of the cases, advanced lung cancer patients-or their families in cases of incompetence-were not involved in ELD making, despite the wishes of most of them. Physicians should openly discuss ELDs and involvement preferences with their advanced lung cancer patients.Copyright A(c) 2012 U.S. Cancer Pain Relief Committee. Some patients express wishes of abbreviation of suffering at the end of their life, but when the time comes, they do not want to be informed of the imminence of their death (Bernheim 1996 (Bernheim, 2001 #CITATION_TAG).When the patient died, the specialist and general practitioner were asked to fill in a questionnaire.Eighty-five patients who died within 18 months of diagnosis were studied.
        Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. Evidence that lichenized blue-green algae are among the principal agents of N2 fixation on drier terrain in the Arctic and Subarctic has prompted attempts to quantify N-input by lichens in these habitats. The nitrogenase activity in Stereocaulon paschale mats in spruce-lichen woodland of central subarctic Canada has been examined in relation to thallus water content, temperature and incident radiation. It is suggested that simple predictive models are not yet able to accurately describe levels of nitrogenase activity in nature and that estimates of N-input on an annual or seasonal basis may be precarious. Leaching of metabolites from, and decomposition of the thallus are the two principal potential pathways for nitrogen, subsequent to fixation by cyanophilic lichens. Quantitative information on the operation of these pathways in nature is required. Lichenized blue-green algae are probably among the principal agents of nitrogen (N2) fixation in drier terrestrial habitats of the Arctic and Subarctic (Schell & Alexander, 1973; Kallio & Kallio, 1975; Crittenden, 1975; Huss-Danell, 1977). Several investigators working in boreal-arctic systems have used measurements of nitrogenase activity in lichens obtained either under field conditions or in the laboratory to derive estimates of N-input per unit area by the lichen biomass in situ on a seasonal or annual basis. The major shortcoming of such estimates is that their accuracy is unknown: the results of in situ measurements of nitrogenase activity over long periods have not been compared with those of predictive procedures applied to the same period. We have recently been examining the N2 fixing capabilities of S. paschale mats in spruce-lichen woodland in the Abitau-Dunvegan Lakes region of the Northwest Territories (60?21'N, 106'54'W). NITROGEN FIXATION BY STEREOCAULON PASCHALE IN SPRUCE-LICHEN WOODLAND--A CASE STUDY The importance of Stereocaulon paschale as a major component of spruce-lichen woodland in central subarctic Canada has been reviewed by Kershaw (1977). The maintenance of large areas of open Stereocaulon-spruce woodland in this region is dependent upon the high frequency of forest fire, the latter being a natural facet of the environment in the boreal forest (see, e.g. In the AbitauDunvegan Lakes region the periodicity of fire is such that spruce woodland older than 200 y is infrequently encountered on drier terrain and the average reburn interval is less than 100 y (Maikawa & Kershaw, 1976). Stereocaulon paschale usually dominates the lichen synusia in spruce-lichen woodland of between 60 and 130 y old where almost pure carpets of this lichen may occur (Fig. The acetylene (C2H2-) reduction technique (Stewart, Fitzgerald & Burris, 1967) with modifications after Stewart et al. Intensive monitoring programs of this kind were conducted on five occasions: 24-25 June, 6-7, 10-11 and 18-19 August 1976 and 10-11 May 1977. On 17 August 1976 the lichen mat was wetted by rainfall commencing at 04.15 h and monitoring of nitrogenase activity began at 14.30 h the following day. Photosynthetically active radiation (Fig. These data together ascertain the significance of biological N 2 fixation to the N-limited vegetation in the Subarctic ( #CITATION_TAG and Kershaw 1978).In conclusion, our multispecies comparison of N 2 fixation rates of cryptogams, both under standardised conditions and in situ, applying a direct nitrogen detection method, has provided strong evidence for differential nitrogen input by cryptogam taxa in the Subarctic, and substantiates their importance to the nitrogen economy of these cold biomesSimilarly, Horne (1972) predicted annual N2 fixation by Collema pulposum (= C. tenax) on mossy gravel surfaces of Signy Island, Antarctica. This study was conducted while one of us (P.D.C.) The field studies were conducted in typical S. paschale woodland estimated by Maikawa (1976) to have been last burned in 1898. Sequential C2H2-reduction assays were performed during periods of natural diffuse radiation to avoid the acute problem of excessive elevation of thallus temperature within the incubation bottles that occurs at high levels of irradiance and each assay was begun within 2 min of removing the pseudopodetium from the lichen mat. Thallus temperature within the lichen mat was measured with micro-thermocouples (42 gauge wire) arranged in thermopiles and with the junctions embedded in the main stem of pseudopodetia within the upper 15 mm of the lichen canopy (Fig. A similar method was employed to monitor thallus temperature within the incubation bottles (Fig. 2b) thus providing a check on the extent of deviations from operative temperature in situ. 2b) was measured with a quantum sensor (Lambda Instruments Corporation), and pseudopodetia were weighed on an electrobalance immediately prior to incubation in order that water content could be determined after dry weight estimates had been obtained (Fig.
        Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. The research has shown that people with Asperger's syndrome have had difficulty in finding employment in most cases. The vast majority of people with Asperger's syndrome live in a partnership and almost...Tematem teto prace je "Spolecenske a pracovni uplatneni dospelych osob s Aspergerovym syndromem". Na zaver teoreticke casti je nastineno podporovane zamestnavani osob s Aspergerovym syndromem, chranena pracovni mista a pracovni rehabilitace. Adults with autism are more likely to switch jobs frequently, have difficulty adjusting to new job settings and earn lower wages compared with typically developing peers (Howlin, 2000; #CITATION_TAG; Jennes-Coussens et al., 2006; Müller et al., 2003).In the theoretical part of the work, a reader is acquainted with the term Asperger syndrome, with the development of expert terminology, symptoms, diagnosis and prevalence. Then a reader is acquainted with the social position of adults with this syndrome, specifically with a person with Asperger syndrome in the role of a friend, a partner and a parent. The theoretical part describes training and positive work characteristics of these people, but also difficulties that these people may have in the work process. The practical part of this work contains an interpretation of the quantitative research, which was conducted by means of an online questionnaire survey.
        This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . A different view can be found in the 'Chicago school' of political economy (Stigler, 1971; Peltzman, 1976; #CITATION_TAG) and studies such as Wittman (1989).The conceptual discussion is based on the analytical framework for change developed by Halinen, Salmi and Havila. This framework, compiling the mechanism, nature and forces of change in business networks, distinguishes between confined and connected change. It is suggested that mergers and acquisitions (M&As) may cause changes that spread in the business networks, and M&As are investigated as triggers of radical network change in particular.
        Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. The empirical analysis thereby builds on recent work by #CITATION_TAG to allow easy reference to existing findings.I pay particular attention to the theory and measurement of voluntary associations in promoting trust, hypothesizing that voluntary associations connected to other voluntary associations are more beneficial for the creation of generalized trust than associations isolated from other associations. The theory is tested with a multi-level, cross-national model, including both individual-level and country-level variables to predict the placement of trust.
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. BACKGROUND Controversy remains on whether the dual use of clopidogrel and proton-pump inhibitors (PPIs) affects clinical efficacy of clopidogrel. PATIENTS All patients discharged after first-time myocardial infarction from 2000 to 2006. In the target population, clopidogrel is usually prescribed with aspirin, and it has been suggested that inhibition of antiplatelet effect may result from an interaction of PPIs with aspirin absorption [44, 45], independent of the interaction with clopidogrel [#CITATION_TAG, 47].DESIGN A nationwide cohort study based on linked administrative registry data. Patients were examined at several assembly time points, including 7, 14, 21, and 30 days after myocardial infarction.
        The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. Islands in Oceania were some of the last habitable land masses on earth to be colonized by humans. Current archaeological evidence suggests that these islands were colonized episodically rather than continuously, and that bursts of migration were followed by longer periods of sedentism and population growth. The decision to colonize isolated, unoccupied islands and archipelagos was complex and dependent on a variety of social, technological and environmental variables. Unique among existing models, it can account for the episodic nature of certain aspects of the colonization process. We argue that intensive food production was one variable that contributed to decreasing suitability of island habitats, stimulating dispersal, and ultimately migrations to more distant islands in Oceania. One example that has entered the literature is costly signaling theory, as described above; other studies, more wide-ranging in their use of HBE theory, have begun to appear (e.g., Fitzhugh 2003, #CITATION_TAG.This ecological model provides a framework that considers the dynamic character of island suitability along with density-dependent and density-independent variables influencing migratory behavior.
        This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. When train-ing vocabulary reflected the English-like competition between regular (suffixed) and irregular verbs (e.g., go 3 went, hit 3 hit), the acquisition of regular verbs became increasingly sus-ceptible to injury, while the irregulars were learned quickly and were relatively impervious to damage. Patterns of gener-alization to novel forms conflicts with the assumption that this behavioral dissociation is indicative of selective impairment of the learning and generalization of the past tense rule, while the associative lexical-based mechanism is left intact. The single mechanism account maintains that one mechanism governs production of both regular and irregular forms (Bird et al., 2003; Braber et al., 2005; Bybee, 1995; Joanisse and Seidenberg, 1999; Juola and Plunkett, 2000; Lambon Ralph et al., 2005; #CITATION_TAG; Patterson et al., 2001; Rumelhart and McClelland, 1986; Stockall, & Marantz, 2006; Yang, 2000).When networks were trained only on phonological encodings of stem-suhed pairs similar to English regular verbs (e.g., walk 3 wulkd), long-term deficits (i.e., "critical period " effects) were not observed, yet there were substantive short-term effects of injury.
        The growth in computer games and wireless networks has catalyzed the production of a new generation of hand-held game consoles that support multi-player gaming over IEEE 802.11 networks. Understanding the traffic characteristics of network games running on these new hand-helds is important for building traffic models and adequately planning wireless network infrastructures to meet future demand. This paper examines the traffic characteristics of IEEE 802.11 network games on the Nintendo DS and the Sony PSP. In addition, the games and hand-held platforms differ in their ability to handle degraded wireless network conditions and in the amount of broadcast traffic sent. The traffic generated by one host on a WLAN can have dramatic impact on the performance of other hosts on the WLAN [1, #CITATION_TAG].nan
        A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Originality: This is the first study to examine the effects of flow consciousness on consumer behaviour after an impulse purchase. In particular, research has not analysed the effects that flow consciousness has on negative feelings experienced after the impulse purchase of a product. Flow states lead to increased impulse buying, and if consumers are made aware that they were in a flow state, it may reduce any regret they feel after the purchase #CITATION_TAG asked participants in a laboratory to signal with a key press every time their mind shifted to a new topic, which happened on average about 5 or 6 s apart.Methodology: The study applied a mixed methodology. First, the authors conducted two qualitative studies (focus groups) to establish the relationships between flow, flow consciousness, and regret. Second, the authors conducted a quantitative study using data collected through an online questionnaire. Participants were asked to recall a recent shopping experience. To conduct confirmatory factor analysis, the authors gathered data from 304 consumers who had searched for, and purchased, a product on Amazon (www.amazon.com). Structural equation modelling, based on covariance, was used to test the hypotheses.
        Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Similar observations have been made in the past [30] [#CITATION_TAG] [32] [33] [34], although others have reported either no relationship or a negative association with SES [35].In previous papers of this series, we explored the epidemiology of MS, examining the effects of race, sex, geography, latitude and climate, migration, age at onset, population ancestry, and individual ethnicity on the risk of MS, using an unusually large cohort of MS cases and pre-illness matched controls comprising US veterans of World War II (WWII) and the Korean Conflict (KC).
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Artificial neural networks (ANNs) have been proved to be successfully used in a variety of pattern recognition and data mining applications. However, training ANNs on large scale datasets are both data-intensive and computation-intensive. In this paper, we present cNeural, a customized parallel computing platform to accelerate training large scale neural networks with the backpropagation algorithm. Nonetheless, NNs performance has been found to be comparable with other statistical approaches, particularly when approximating complex patterns based on numeric input values (Sargent, 2001; #CITATION_TAG).Therefore, large scale ANNs are used with reservation for their time-consuming training to get high precision. Unlike many existing parallel neural network training systems working on thousands of training samples, cNeural is designed for fast training large scale datasets with millions of training samples. Secondly, it provides a parallel in-memory computing framework for fast iterative training. Third, we choose a compact, event-driven messaging communication model instead of the heartbeat polling model for instant messaging delivery.
        Cognitive neuroscience boils down to describing the ways in which cognitive function results from brain activity. Exactly how cognitive function inherits the physical dimensions of neural activity, though, is highly non-trivial, and so are generally the corresponding dimensions of cognitive phenomena. In spite of their general use, these assumptions hold true to a high degree of approximation for many cognitive (viz. fast perceptual) processes, but have their limitations for other ones (e.g., thinking or reasoning). Classical within-subject analysis in functional magnetic resonance imaging (fMRI) relies on a detection step to localize which parts of the brain are activated by a given stimulus type. The originality of this contribution is twofold. By nature, scaling analysis requires the use of long enough signals with high frequency sampling rate. The break-down of scale invariance (Ciuciu et al., 2011 (#CITATION_TAG Zilber et al., 2012) is tantamount to velocity changes.This is usually achieved using model-based approaches. First, we propose a synthetic, consistent, and comparative overview of the various stochastic processes and estimation procedures used to model and analyze scale invariance. Notably, it is explained how multifractal models are more versatile to adjust the scaling properties of fMRI data but require more elaborated analysis procedures. To this end, we make use of a localized 3-D echo volume imaging (EVI) technique, which has recently emerged in fMRI because it allows very fast acquisitions of successive brain volumes. A voxel-based systematic multifractal analysis has been performed over both kinds of data.
        Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. #CITATION_TAG This will also affect muscle function, which in turn will reduce masticatory efficiency.The oral impact of malocclusion was assessed using the Oral Impact on Daily Performance (OIDP), whereas clinical criteria were assessed using the Dental Aesthetic Index (DAI). Self-perception of dental aesthetics was assessed using the Oral Aesthetic Subjective Impact Scale (OASIS) and self-esteem was assessed using the Global Negative Self-Evaluation (GSE) scale. Other variables were assessed using questionnaires. The chi-square test, simple and multiple logistic regression analyses were used for the statistical analysis.Ninety five adolescents (24%) reported feeling embarrassed to smile (aesthetic impact).
        Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. The paper closes with an example of a multi-level land use, transport and environment model ranging from the European to the grid cell level. Although our main focus is on workplaces, a multilevel perspective is used (#CITATION_TAG).It starts with a history of urban transport and land-use models and observes a trend towards increasing conceptual, spatial and temporal resolution stimulated by improved data availability, higher computer speed and better theories about mobility and location of individual behaviour.
        According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Because of intensified competition from low wage economies, such firms have been forced to restructure production processes to heighten both their productive efficiency and attain greater flexibility at the plant level. Much of this change has involved the introduction of high performance work practices (HPWP), a central focus of much recent scholarship on post-Fordism. We describe the failure to implement HPWP as some firms seek efficiency gains from work restructuring rather than broader effectiveness goals that would have deepened employee participation. All forms of organizations are subject to risks of opportunism (Williamson, 1975), but opportunism will not disappear with common ownership (Eisenhardt, 1989; Schotter and Beamish, 2011; #CITATION_TAG).nan
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. It remains an active research topic (Buckingham Shum & Deakin Crick, 2012; #CITATION_TAG; Komarraju, Ramsey & Rinella, 2013), indicating the inherent difficulty in both measurement of learning (Knight, Buckinham Shum, & Littleton, 2013; Tempelaar et al., 2013), and modelling the learning process, particularly in tertiary education (Pardos et al., 2011).nan
        This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. We consider the challenges of lack  of data, incomplete knowledge and modelling in the context of a rapidly changing knowledge base. There is a mismatch in scale between  these cellular models and tissue structures that are affected by tumours, and bridging this gap requires substantial  computational resource. We present concurrent programming as a technology to link scales without losing  important details through model simplification. An effect of the uncovering of incomplete knowledge is viewed as necessarily making a model unfit for purpose: instead, the process of argumentation highlights where biological experimentation might be focused to improve understanding [#CITATION_TAG].Computer simulation can be used to inform in vivo and in vitro experimentation, enabling rapid, low-cost  hypothesis generation and directing experimental design in order to test those hypotheses. Here, we outline a  framework that supports developing simulations as scientific instruments, and we select cancer systems biology  as an exemplar domain, with a particular focus on cellular signalling models. Our  framework comprises a process to clearly separate scientific and engineering concerns in model and simulation  development, and an argumentation approach to documenting models for rigorous way of recording assumptions  and knowledge gaps.
        NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the "Western diet" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The "Selfish Brain" is a term coined by Robert L. DuPont in the title of his book where he wrote: "With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: "The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the "push" principle borrowed from the economic "push-pull" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: "According to the 'push' principle, the environment pushes excess amounts of energy into the organism" (Hill and Peters, 1998). According to DuPond, "What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both "pull" and "push" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this "push" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called "unique" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the "puSh" component of energy metaBolISm The environment in Western-type societies can be characterized as "pushing" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental "Western Diet" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: "antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental "push." In a recent article entitled "Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy," J. Zimmerman wrote: "In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. "Control" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. The young rat adjusts its food intake so precisely to its energy needs that its fat stores remain almost constant. Considerable variation in food intake is brought about in response to change in heat loss to the environment, or in loss of food through the mammary gland in lactation, without appreciable change of weight. Hypothalamic damage permits excessive intake and causes obesity. The degree of obesity and in general its rate of development, is a function of the degree of dam age to the region of the tuber cinereum , and is independent of changes of intake with environmental temperature. There is no disturbance of temperature regulation or acclimatization to changed environmental temperature in obese rats. In this system, either the size of fat depot (#CITATION_TAG; Woods and Ramsay, 2011) or glucose levels (Mayer, 1953) are being controlled.Reasons are advanced for regarding this as the limiting rate at which absorbed foodstuffs can be removed from the circulation, that is as some aspect of the synthesis or transport of fat.
        Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Autism spectrum disorders (ASDs) are highly prevalent neurodevelopmental disorders, but the underlying pathogenesis remains poorly understood. Recent studies have implicated the cerebellum in these disorders, with post-mortem studies in ASD patients showing cerebellar Purkinje cell (PC) loss, and isolated cerebellar injury has been associated with a higher incidence of ASDs. However, the extent of cerebellar contribution to the pathogenesis of ASDs remains unclear. Tuberous sclerosis complex (TSC) is a genetic disorder with high rates of comorbid ASDs that result from mutation of either TSC1 or TSC2, whose protein products dimerize and negatively regulate mammalian target of rapamycin (mTOR) signalling. However, the roles of Tsc1 and the sequelae of Tsc1 dysfunction in the cerebellum have not been investigated so far. However, the conversion to a psychiatric disorder and need for psychiatric care is probably mediated by environmental risk factors as well as factors such as coping style, cognitive biases and negative emotional affect (#CITATION_TAG).nan
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The nanoflare model was early applied to the heating of coronal loops observed by Yohkoh (#CITATION_TAG).The solar validation is carried out by using spatially resolved X-ray observations of the Sun obtained from the Yohkoh satellite. In particular, we show how this analysis procedure can be used in the context of archival Einstein, ROSAT, and EUVE data, as well as for Chandra and XMM-Newton data, as a complementary analysis tool to existing multithermal component models.
        Physicians must report each euthanasia case to the Federal Control and Evaluation Committee. Cases describing other ELDs were sometimes also labelled 'euthanasia'. Mislabelling of ELDs could impede societal control over euthanasia. Schoon (2008: 74) views them as indicators of 'family social position', citing the view that they can act as indicators of socio-economic resources and cultural characteristics (#CITATION_TAG).METHODS Five hypothetical cases of ELDs: intensified pain alleviation, palliative/terminal sedation, euthanasia with neuromuscular relaxants, euthanasia with morphine and life-ending without patient request were presented in a cross-sectional survey of 914 physicians in Belgium in 2009.
        INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Anyone who watches the television news has seen images of firefighters rescuing people from burning buildings and paramedics treating bombing victims. How do these individuals make the split-second decisions that save lives? Most studies of decision making, based on artificial tasks assigned in laboratory settings, view people as biased and unskilled. Gary Klein is one of the developers of the naturalistic decision-making approach, which views people as inherently skilled and experienced. Since 1985, Klein has conducted fieldwork to find out how people tackle challenges in difficult, nonroutine situations. Sources of Power is based on observations of humans acting under such real-life constraints as time pressure, high stakes, personal responsibility, and shifting conditions. The duality referred to by 'dualprocess' has many names, each with it's own story: associative vs. rulebased (Sloman 1996), heuristic vs. analytic (Evans 1984 (Evans, 1989, tacit thought vs. explicit thought (Evans and Over 1996), implicit cognition vs. explicit learning (Reber 1989), interactional vs. analytic (Levinson 1995), experiential vs. rational (Epstein 1994), quick and inflexive modules vs. intellection (Pollock 1991), intuitive cognition vs. analytical cognition (Hammond 1996), recognition primed choice vs. rational choice strategy (#CITATION_TAG), implicit inference vs. explicit inference (Johnson-Laird 1983), automatic vs. controlled processing (Shiffrin and Schneider 1977), automatic activation vs. conscious processing system (Posner and Snyder 1975, 2004), rationality vs. rationality (Evans & Over 1996, intuitive vs. reflective, model-based vs. model-free (Daw et al 2005) and system 1 vs. system 2 (see Stanovich and West 2000 for the first mention of these terms as well as a useful, albeit dated, list of dualprocess terminology; see also Frankish 2010 for a list of features commonly associated with System 1 and System 2).nan
        Recent challenges in information retrieval are related to cross media information in social networks including rich media and web based content. In those cases, the cross media content includes classical file and their metadata plus web pages, events, blog, discussion forums, comments in multilingual. This heterogeneity creates large complex problems in cross media indexing and retrieval for services that integrate qualified documents and user generated content together. This paper presents a model and an indexing and searching solution for cross media contents, addressing the above issues, developed for the ECLAP Social Network, in the domain of Performing Arts. The research aimed to cope with the complexity of a heterogeneous indexing semantic model, using stochastic optimization techniques, with tuning and discrimination of relevant metadata terms. In recent years, the growth of Social Network communities has posed new challenges for content providers and distributors. Digital contents and their rich multilingual metadata sets need improved solutions for an efficient content management. There is an increasing need of services for scaling digital services, searching and indexing. Despite the huge amount of contents, users want to easily find relevant unstructured documents in large repositories, on the basis of multilingual queries, with a limited waiting time. At the same time, digital archives have to be fully accessible even if a major restructuring is in progress, or without a significant downtime. The ECLAP information model for cross-media integrates sources coming from 35 different international institutions [9, #CITATION_TAG].Effectiveness and optimization analysis of the retrieval solution are presented with relevant metrics.
        In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Did, for instance, Homo habilis have language, Homo erectus self-awareness or Neanderthals the capacity for analogical reasoning? While fossil endocasts may inform about brain structure, the character of past cognition must be largely inferred from the archaeological record. And to draw such inferences archaeologists need to engage with, or rather become participants in, the cognitive sciences - just as Bloch (1991) has recently argued for anthropology. This is essential since we cannot pretend to understand the ancient mind without entering debates concerning the character of the modern mind. I consider whether the Middle/Upper Palaeolithic transition may have constituted a phase in human evolution during which there was a significant development from domain specific to generalized intelligence. In this theory (#CITATION_TAG), the domain-specific entities are understood to be specialized intelligences, along the lines of (Gardner, 1993).nan
        This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. In loop quantum gravity we now have a clear picture of the quantum geometry of space, thanks in part to the theory of spin networks. In general, a spin network is a graph with edges labeled by represen- tations and vertices labeled by intertwining operators. In a 'spin foam model' we describe states as linear combina- tions of spin networks and compute transition amplitudes as sums over spin foams. A reasonable hypothesis is that actual practices are not so different and that these are countries where the culture is more conducive to using the doctrine of double effect #CITATION_TAGSimilarly, a spin foam is a 2-dimensional complex with faces labeled by representations and edges labeled by intertwining operators.
        Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. The size of a firm determines the quality of its product: when network effects are positive, a larger firm is of higher quality; when the effects are negative, a larger firm's product is of lower quality. Consumers have heterogeneous preferences towards quality (firm size), and firms compete in prices. Equilibria are characterised: for example, in any asymmetric equilibrium, it must be that congestion is not too severe. One consequence of this feature is that an increase in the number of firms in the industry can raise individual firms' profits. Two factors can bound the number of firms in a free-entry equilibrium without fixed costs: expectations, and the 'finiteness' property (Shaked and Sutton, Review of Economic Studies 49 (1982) 3-13, Econometrica 51(5) (1983) 1469-1483) of price competition 20 #CITATION_TAG take the issue of expectations serious and point out that the results change dramatically if rationally responsive beliefs are used in their pricing game.nan
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Aspirin-clopidogrel antiplatelet dual therapy is widely prescribed worldwide, with PPIs frequently associated to prevent gastrointestinal bleeding. Concerns about PPI and clopidogrel interaction were raised when omeprazole was found to inhibit the antiplatelet effect of clopidogrel in an in vivo study of 124 patients undergoing elective coronary stent implantation [#CITATION_TAG].METHODS: In this double-blind placebo-controlled trial, all consecutive patients undergoing coronary artery stent implantation received aspirin (75 mg/day) and clopidogrel (loading dose, followed by 75 mg/day) and were randomized to receive either associated omeprazole (20 mg/day) or placebo for 7 days.
        Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. The total glacial area of the Cordillera Blanca, Peru, has shrunk by more than 30% in the period of 1930 to the present with a marked glacier retreat also in the recent decades. The strong increase in precipitation in the last 30 years probably did not balance the increase of temperature before the 1980s. It is suggested that recent changes in temperature and precipitation alone may not explain the glacial recession within the thirty years from the early 1980s to 2012. Glaciers in the Cordillera Blanca may be still reacting to the positive air temperature rise before 1980. Especially small and low-lying glaciers are characterised by a serious imbalance and may disappear in the near future Precipitation increases from August towards the October to April core wet season and is close to nil during June and July (e.g., Kaser and Osmaston, 2002; Mark et al., 2010; #CITATION_TAG).We documented a cooling trend for maximum daily air temperatures and an increase in precipitation of about 60 mm/decade since the early 1980s.
        Such theories provide the necessary computational means to explain the flexible nature of human behavior but in doing so introduce extreme degrees of freedom in accounting for data. More recently, the universality conjecture of Dubrovin (2006) has been proven in #CITATION_TAG for solutions to the KdV equation with analytic initial data vanishing at infinity.The authors assume that individuals adapt rationally to a utility function given constraints imposed by their cognitive architecture and the local task environment. The new approach narrows the space of predicted behaviors through analysis of the payoff achieved by alternative strategies, rather than through fitting strategies and theoretical parameters to data. It extends and complements established approaches, including computational cognitive architectures, rational analysis, optimal motor control, bounded rationality, and signal detection theory. The authors illustrate the approach with a reanalysis of an existing account of psychological refractory period (PRP) dual-task performance and the development and analysis of a new theory of ordered dual-task responses.
        Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Auditory verbal hallucinations (AVH) are complex experiences that occur in the context of various clinical disorders. However, their predictive value for specific psychiatric disorders is not entirely clear. This choice bias is especially interesting because it makes explanations of enhanced revaluation due to effort [33, #CITATION_TAG] less plausible for the effect of action on the dynamics of choice-induced preference change.AVH also occur in individuals from the general population who have no identifiable psychiatric or neurological diagnoses. Longitudinal studies suggest that AVH are an antecedent of clinical disorders when combined with negative emotional states, specific cognitive difficulties and poor coping, plus family history of psychosis, and environmental exposures such as childhood adversity.
        Biofuels and biodiversity in South Africa. v107i5/6.186 The South African government, as part of its efforts to mitigate the effects of the ongoing energy crisis, has proposed that biofuels should form an important part of the country's energy supply. The contribution of liquid biofuels to the national fuel supply is expected to be at least 2% by 2013. The Biofuels Industrial Strategy of the Republic of South Africa of 2007 outlines key incentives for reaching this target and promoting the development of a sustainable biofuels industry. The available and proposed processing technologies have important implications for land use and the use of different non-native plant species as desired feedstocks. South Africa has a long history of planting non-native plant species for commercial purposes, notably for commercial forestry. This paper discusses issues relating to this strategy as well as key drivers in biofuel processing with reference to potential impacts on South Africa's rich biological heritage. Today, the principal focus of anti-globalizers is not the effect of globalization on economic prosperity but its harm to social agendas such as the reduction of child labour and poverty, the maintenance of rich-country labour and environmental standards, the exercise of national sovereignty, the maintenance of local culture, and womenaEUR(tm)s rights and welfare. The contrary view, which I defend in this essay, is that economic globalization advances the achievement of that social agenda. 4, 5, #CITATION_TAG, 7 ch of the global debate on biofuels has focused on policy, economics, social issues (such as competition with food crops), and the potential of biofuels to reduce greenhouse gas emissions.nan
        Recent challenges in information retrieval are related to cross media information in social networks including rich media and web based content. In those cases, the cross media content includes classical file and their metadata plus web pages, events, blog, discussion forums, comments in multilingual. This heterogeneity creates large complex problems in cross media indexing and retrieval for services that integrate qualified documents and user generated content together. This paper presents a model and an indexing and searching solution for cross media contents, addressing the above issues, developed for the ECLAP Social Network, in the domain of Performing Arts. The research aimed to cope with the complexity of a heterogeneous indexing semantic model, using stochastic optimization techniques, with tuning and discrimination of relevant metadata terms. Other techniques make use of Fuzzy algorithms [33, 47], local context analysis [56], clustering [59], and ranking improvement [#CITATION_TAG].It is argued that the optimal ranking problem should be factorized into two distinct yet interrelated stages: the relevance prediction stage and ranking decision stage. During retrieval the relevance of documents is not known a priori, and the joint probability of relevance is used to measure the uncertainty of documents' relevance in the collection as a whole. The resulting optimization objective function in the latter stage is, thus, the expected value of the IR metric with respect to this probability measure of relevance. Through statistically analyzing the expected values of IR metrics under such uncertainty, we discover and explain some interesting properties of IR metrics that have not been known before. Our analysis and optimization framework do not assume a particular (relevance) retrieval model and metric, making it applicable to many existing IR models and metrics.
        requires a greater understanding of characteristics of clients who may or may not benefit from this technology. The expansion of evidence-based practice across sectors has lead to an increasing variety of review types. However, the diversity of terminology used means that the full potential of these review types may be lost amongst a confusion of indistinct and misapplied terms. A limited number of review types are currently utilized within the health information domain.Few review types possess prescribed and explicit methodologies and many fall short of being mutually exclusive. Notwithstanding such limitations, this typology provides a valuable reference point for those commissioning, conducting, supporting or interpreting reviews, both within health information and the wider health care domain. This review therefore was undertaken as a 'state of the art' review [#CITATION_TAG] to present an assessment of the current state of knowledge in the field.A simple analytical framework -- Search, AppraisaL, Synthesis and Analysis (SALSA) -- was used to examine the main review types.Fourteen review types and associated methodologies were analysed against the SALSA framework, illustrating the inputs and processes of each review type. A description of the key characteristics is given, together with perceived strengths and weaknesses.
        Phonological errors were scarce in both groups. Extant literature and suppliers interviewed for this study view a solution as a customized and integrated combination of goods and services for meeting a customer's business needs. In contrast, customers view a solution as a set of customer-supplier relational processes comprising (1) customer requirements definition, (2) customization and integration of goods and/or services and (3) their deployment, and (4) postdeployment customer support, all of which are aimed at meeting customers' business needs. The relational process view can help suppliers deliver more effective solutions at profitable prices. Supplier variables include c... Speech errors produced by different aphasic patient groups have formed the basis of debates about particular deficits and how they relate to the intact language system (Bates & Wulfeck, 1989; #CITATION_TAG; Rapp & Goldrick, 2000, Goldrick, 2006.nan
        The growing movement to establish professional counseling as a distinct profession, based on an increasingly narrow definition of professional identity, is particularly relevant to counseling psychologists and professional counselors and has implications for the broader field of psychology. These restrictions reduce services to the public and threaten the viability of counseling psychology and professional counseling in the U.S. These challenges also have significant implications for counseling psychologists in Europe and internationally given similar efforts. It is with great respect and appreciation for Hansen's work as well as our shared commitments to humanistic counseling and counseling as a profession that I offer the following response. HUMANISM: THE FOUNDATION OF PROFESSIONAL COUNSELING AND RELATED FIELDS As Hansen points out, numerous people and movements have been credited with the founding of counseling as a profession. Although Frank Parsons and early advocates for educational and career development are often credited with influencing the foundation of the diverse field of counseling (Brown & Trusty, 2005), historically, theoretically, and practically, Sigmund Freud and colleagues' psychoanalytic/psychodynamic theory is more often considered the first force in counseling and psychology. B. F. Skinner and colleagues' behaviorism, having developed later, which is similarly referred to as the second force (Ivey, D'Andrea, & Ivey, 2011), also influenced counseling and related fields. Yet, as Hansen (2012) notes, professional counseling is founded on humanism, the third force in counseling and psychology. Like its antecedents, humanistic counseling developed in a historical and sociopolitical context that is continuing to evolve. Humanistically oriented counselors consider people's strengths and possibilities, recognizing that all people have the capacity to develop, to grow, to heal, and to maximize their own potential, that is, to "actualize" (Raskin & Rogers, 1995, p. 128). Overall, humanists are interested in normal and optimal functioning as well as people's phenomenological subjective experience (Kirschenbaum, 2007). Humanistic counseling, then, is a holistic approach that emphasizes healthy human development, human strengths and wellness, and understanding people in their environmental contexts (Lundin, 1996). Using a variety of creative approaches centered on the counselor's attitude toward the client (Rogers, 1951), humanistically oriented counselors partner with clients, often attending to Rogers's (1957) "necessary and sufficient conditions" (p. ... We, therefore, suggest training programs intentionally incorporate professional history and social justice advocacy that extends beyond the curriculum (Brady-Amoon, #CITATION_TAG).Second, I counter Hansen's position that the counseling profession has and ideally ought to be grounded solely in the humanities. Third and last, I provide a further extension of Hansen's vision for the future of counseling, a vision that encompasses renewed respect for human complexity, multiple perspectives, and the optimal development of human potential.
        Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. (c) Finding Experts By Semantic Matching of User Profiles Rajesh Thiagarajan, Geetha Manjunath, Markus Stumptner HP Laboratories HPL-2008-172 semantics, similarity matching, ontologies, user profile Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Thiagarajan et al. [#CITATION_TAG] use a different strategy by representing user profiles as bags-ofwords and weighing each term according to the user interests derived from a domain ontology.However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results. However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results. However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results.
        Dose-limiting neurotoxicity is a major side effect of oxaliplatin treatment, producing initial acute neurotoxicity and chronic neuropathy with increasing exposure. The finding that neurotoxicity influences the patients' daily life is a similar result to earlier studies [14, 15, [#CITATION_TAG] [26] [27] [28] [29], but those studies had cross-sectional cohorts, other disease stages, and different study designs.nan
        The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. This corresponds to a lack of social pathways through which ideas can flow. But given enough connections, isolated groups may converge into a single mass, which encompasses the majority of nodes, and in which everyone is potentially connected to everyone else by some path (#CITATION_TAG).We consider two scientists to be connected if they have authored a paper together, and construct explicit networks of such connections using data drawn from a number of databases, including MEDLINE (biomedical research), the Los Alamos e-Print Archive (physics), and NCSTRL (computer science). We show that these collaboration networks form "small worlds" in which randomly chosen pairs of scientists are typically separated by only a short path of intermediate acquaintances.
        It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. Short stories are considered as good resources that can be used in language classrooms. The present study was conducted in a small class of junior secondary school students in order to investigate if they became more interested and more confident in English with the use of short stories. #CITATION_TAG and Megawati and Anugerahwati's (2012) studies with secondary school students in Hong Kong and Indonesia, respectively, investigated the use of stories to enhance students' motivation and their writing ability.And, the narrative (or storytelling) approach is believed to help students understand the story easily.
        Currently, cryptography is in wide use as it is being exploited in various domains from data confidentiality to data integrity and message authentication. Basically, cryptography shuffles data so that they become unreadable by unauthorized parties. However, clearly visible encrypted messages, no matter how unbreakable, will arouse suspicions. Fundamentally, steganography conceals secret data into innocent-looking mediums called carriers which can then travel from the sender to the receiver safe and unnoticed. This paper proposes a novel steganography scheme for hiding digital data into uncompressed image files using a randomized algorithm and a context-free grammar. Notations are presented for numbers, numerical variables, Boolean variables , relations, n-dimensional arrays, functions, operators and algebraic expressions. L'autcur caracterise brievement la syntaxe et les regles d'inter-pretation du langage algebrique international propose a la Conference de Zurich (ACM-GAMM) puis en donne un expose formel et complet. II indique les notations utilisees pour designer les nombres, les variables numeriques ou booIeennes, les relations, les agencements pluri-dimensionnels, les fonctions, les operateurs et les expressions algebriques. Ce langage permet d'exprimer difierentes operations: affectation de valeurs aux variables, execution conditionnelle des expressions, procedes iteratifs, formation d'expressions complexes a partir d'une suite d'expressions elementaires, definition de nouvelles expressions pour des operations arbitraires, reemploi et modification de certaines parties du programme. Le lang age envisage est conyu pour permettre d'exprimer la quasi totalite des procedes de calcul numerique de maniere com-mode et concise a l'aide d'un nombre relativement restreint de regles de syntaxe et d'expressions-types. Its formalism was originally set by Chomsky [14] and Backus [#CITATION_TAG], independently of each other.Means are provided in the language for the assignment of values to variables, conditional execution of statements , iterative procedures, formation of compound statements from sequences of statements, definition of new statements for arbitrary procedures, and the re-use and alteration of program segments.
        A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Recent blood oxygenation level dependent functional MRI (BOLD fMRI) studies of the human brain have shown that in the absence of external stimuli, activity persists in the form of distinct patterns of temporally correlated signal fluctuations. Raichle (2009) argues that the default-mode network is not coextensive with conscious mind-wandering, citing the continuation of the network's activity into lighter states of anesthesia and Stages 1 and 2 of sleep (#CITATION_TAG), when dreaming is most frequent, and the demonstration (Christoff et al., 2009) of executive-network elements during resting states.For this purpose, we performed BOLD fMRI on normal subjects during varying levels of consciousness, from resting wakefulness to light (non-slow wave) sleep. Depth of sleep was determined based on concurrently acquired EEG data.
        The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. &lt;p&gt;&lt;i&gt;Evaluating complex interventions is complicated. The Medical Research Council's evaluation framework (2000) brought welcome clarity to the task. In 2000, the Medical Research Council (MRC) published a framework&lt;sup&gt;1&lt;/sup&gt; to help researchers and research funders to recognise and adopt appropriate methods. The research design was based on Patton's [25] description of qualitative process evaluation and guidelines for the evaluation of complex interventions [#CITATION_TAG].They present various problems for evaluators, in addition to the practical and methodological difficulties that any successful evaluation must overcome. The framework has been highly influential, and the accompanying BMJ paper is widely cited.&lt;sup&gt;2&lt;/sup&gt; However, much valuable experience has since accumulated of both conventional and more innovative methods.
        The topic of discussion was their views on the implementation of inclusive education. In seeking this feedback, we were interested in seeing if they interpreted the sessions as being learning experiences -that is, as sessions that enabled the participants to learn from each other as well as from facilitators with a view to promoting mutual learning. Furthermore, our form of pragmatism veers in the direction of including 1 features associated with the transformative paradigm as elucidated by #CITATION_TAG. Mertens (2010) indicates that, within the transformative paradigmatic outlook, it is recognised that it is part of the researcher's responsibility to consider the uses that might be made of their work, and to take into consideration the way in which research outcomes can be linked to social justice.Abstract As teachers of mixed methods, we have a responsibility to nurture our students' abilities to think through their choices in terms of mixed methods research based on a critically examined understanding of their philosophical assumptions. The belief systems associated with the transformative paradigm are used to illustrate the importance of teaching philosophical frameworks as a part of mixed methods instruction.
        The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. The majority of these GSCM studies, however, use either quantitative approaches and methodologies by collecting and analysing large samples and testing hypotheses and models, or qualitative case studies following grounded theory inspired approaches (Binder and Edwards, 2010; #CITATION_TAG).Design/methodology/approach -To better signify such contribution, it takes insight from Merton's (1968) notion of middle-range theory as a means to create pathways of propositions that link substantive concepts and practices of OM in both context-specific and context-free operational environments.
        When reading pro and con arguments, participants (Ps) counterargue the contrary arguments and uncritically accept supporting arguments, evidence of a disconfirmation bias. Many disciplines discuss skepticism, including politics (e.g., #CITATION_TAG), philosophy (e.g., McGrath, 2011), sociology (e.g., Freudenburg et al., 2008), and psychology (e.g., Lilienfeld, 2012).Two experimental studies explore how citizens evaluate arguments about affirmative action and gun control, finding strong evidence of a prior attitude effect such that attitudinally congruent arguments are evaluated as stronger than attitudinally incongruent arguments. We also find a confirmation bias--the seeking out of confirmatory evidence--when Ps are free to self-select the source of the arguments they read.
        Identifying ancestry along each chromosome in admixed individuals provides a wealth of information for understanding the population genetic history of admixture events and is valuable for admixture mapping and identifying recent targets of selection. We present PCAdmix (available at https://sites.google.com/site/ pcadmix/home), a Principal Componentsbased algorithm for determining ancestry along each chromosome from a high-density, genome-wide set of phased single-nucleotide polymorphism (SNP) genotypes of admixed individuals. We describe the location, allele frequency and local haplotype structure of approximately 15 million single nucleotide polymorphisms, 1 million short insertions and deletions, and 20,000 structural variants, most of which were previously undescribed. Phasing errors are becoming less common as phasing methods improve and efforts such as the 1,000 Genomes Project (#CITATION_TAG) produce larger pools of genotypes that can be used as references during phasing; however, it would be valuable to extend PCAdmix to unphased data, as well as to investigate the effectiveness of iterative phasing and ancestry assignment, employing reference panels conditional on estimated local ancestry.We undertook three projects: low-coverage whole-genome sequencing of 179 individuals from four populations; high-coverage sequencing of two mother-father-child trios; and exon-targeted sequencing of 697 individuals from seven populations. From the two trios, we directly estimate the rate of de novo germline base substitution mutations to be approximately 10[superscript -8] per base pair per generation.
        Decision-making animals can use slow-but-accurate strategies, such as making multiple comparisons, or opt for simpler, faster strategies to find a 'good enough' option. Social animals make collective decisions about many group behaviours including foraging and migration. The key to the collective choice lies with individual behaviour. We present a case study of a collective decision-making process (house-hunting ants, Temnothorax albipennis), in which a previously proposed decision strategy involved both quality-dependent hesitancy and direct comparisons of nests by scouts. This highlights the need to carefully design experiments to detect individual comparison. This parsimonious mechanism could promote collective rationality in group decision-making. Many individual decisions are informed by direct comparison of the alternatives. In collective decisions, however, only certain group members may have the opportunity to compare options. How do they do this? Switching by ants that had the opportunity to compare nests had little effect on nest choice. Colony-level comparison and choice can emerge, without direct comparison by individuals. Although lone T. rugatulus ant workers are capable of comparing the attributes of nest-sites which are very close together [25], the evidence for individual ants making direct comparisons between nests during colony emigration is weak [#CITATION_TAG], and furthermore, ant colonies are able to choose a distant good nest over a nearby poor nest, when recruitment latency differences would be expected to be cancelled out by travel time (Fig. 1a) [26, 27].We used radio-frequency identification-tagged ants to monitor individual behaviour. When ants switched quickly between the two nests, colonies chose the good nest. Previously proposed mechanisms, recruitment latency and nest comparison, can be explained as side effects of this simple rule.
        This paper reviews a diverse set of social and interpersonal inuence approaches and techniques which could be relevant to designers seeking to inuence behaviour change for social and environmental benet. In previous literature, George Stigler asserts a law of diminishing returns to group size in politics: Beyond some point it becomes counterproductive to dilute the per capita transfer. NLP was developed by Richard Bandler and John Grinder in the context of understanding the patterns used by`successful' psychotherapists, e.g. the psychiatrist and hypnotist Milton H. Erickson (#CITATION_TAG; Grinder, Delozier and Bandler, 1977), and putting them into a form where they could be useful to others.Since the total transfer is endogenous, there is a corollary that dirninishing returns apply to the transfer as well, due both to the opposition provoked by the transfer and to the demand this opposition exerts on resources to quiet it. This is at one level, a detail, which is the way Stigler treated it, but a detail with some important implications -- for entry into regulation, and for the price-output structure that emerges from regulation.
        Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (#CITATION_TAG; Bond et al., 1997 Bond et al.,, 2008 Crowther et al., 2001; Graetz, 2010; Griffin et al., 1996; Heffernan and Pilkington, 2011; McCaughrin et al., 1993; Noble et al., 1991; Rhodes et al., 1987; Stevens and Martin, 1999).First, the eclectic approach to foreign entry mode, proposed by Hill, Hwang and Kim [1990] and Kim and Hwang [1992], is used to examine transaction-specific, organizational capability and strategic factors that influence channel choices in foreign markets. Second, the study empirically examines the performance consequences of channel integration.
        Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. The conventions for evaluating information systems case studies conducted according to the natural science model of social science are now widely accepted. However, while their criteria are useful in evaluating case study research conducted according to the natural science model of social science, the positivist criteria they suggest are inappropriate for interpretive research.The use of interpretive approach is relatively new to information systems field, the approach has emerged as a valid and important strand in information systems research and most mainstream IS journals now welcome interpretive research and significant groups of authors are working within the interpretive tradition (Walsham, 1995). The interpretive research does not subscribe to the idea that a pre-determined set of criteria can be applied in a mechanistic way, it does not follow that there are no standards at all by which interpretive research can be judged.Striving and ensuring rigor in interpretive study requires different criteria through which one views and judges the quality and completeness of the research process. Many researchers (Orlikowski et al, 1991; Walsham, 1993, 1995; Klein and Myers, 1999) have addressed qualitative research and they have shown how the nature and purpose of interpretive research differs from positivist research. At present, there are no agreed criteria for evaluating research of this kind. Nonetheless, there must be some criteria by which the quality of interpretive research can be evaluated. Myers (1997) and Klein and Myers (1999) have proposed a set of criteria for the conduct and evaluation of interpretive research in information systems.This study is not concerned with adhering to the scientific tenets of precision and replication, instead the study is concerned in seeking a theory that is compatible with evidence that is both rigorous and relevant and generally useful to other areas. The remainder of the paper is structured as follows. The importance of understanding the IT adoption decision making process in organizations has been highlighted by many researchers [2, #CITATION_TAG].Benbasat et al (1987), Lee (1989) and Yin (1994) formulated a set of methodological principles for case studies that were consistent with the conventions of positivism. It begins with an overview of case study method and a discourse on the use of case study in Information Systems. This will be followed by a description of the procedures involved in collecting and analyzing data in grounded theory method. Then the criteria proposed by Myers (1997) for evaluating interpretive research will be discussed in relation to this particular study.
        In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. When the school opened in 1971, the existing 86 medical schools all offered similar programs: two years of basic science training in lecture halls and laboratories, followed by two years of direct contact with patients in clinical settings. In the new school, students were taught didactically only during the first year. Hence, institutionalism emphasises the persuasive control over the practices, beliefs and belief systems of individuals or organisations through an institution's sway (#CITATION_TAG, in King et al., 1994.During the second year, each student was assigned to a community physician who acted as an advisor and who discussed with students those patients afflicted with the diseases the student was currently studying. The author found the case of this medical school to be of particular interest from an organizational viewpoint in that: (1) the early development of the school was shaped by the first dean's entrepreneurial activity, ambitions, visions, strengths, and weaknesses; (2) the uncertainty resulting from the school's novelty forced individuals to assume new roles and face unclear performance criteria; and (3) the transition of an innovative school to an institutionalized one was problematic because it modified the decision-making process. The author suggests that those things which lead to an organization's success during its early years are not the same as those that lead to longer-run success. A comparative analysis of the birth, life, and death of organization is advocated.
        Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. High levels of unemployment among persons with mental illness are a significant social disability. Evidence outside North America is more limited. Systematic review of studies of the effectiveness of IPS conducted principally in the United Kingdom. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (Beyer and Kilsby, 1996; Bond et al., 1997 Bond et al.,, 2008 Crowther et al., 2001; Graetz, 2010; Griffin et al., 1996; #CITATION_TAG; McCaughrin et al., 1993; Noble et al., 1991; Rhodes et al., 1987; Stevens and Martin, 1999).Methods.
        The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. The availability of sand for eolian transport is often mediated by the type and extent of vegetation cover on dunes, and may be further controlled by moisture availability, edaphic associations (cf. related to soil processes) and landscape disturbance (e.g., Sala et al., 1988; Mangan et al., 2004; Hugenholtz and Wolfe, 2005; #CITATION_TAG).By controlling the lattice anisotropy, one can realize both massive and massless Dirac fermions and observe the phase transition between them. Through explicit calculations, we show that both the Bragg spectroscopy and the atomic density profile in a trap can be used to demonstrate the Dirac fermions and the associated phase transition.
        The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Abstract Refactoring is the process of changing the design of a program without changing what it does. Typical refactorings, such as function extraction and generalisation, are intended to make a program more amenable to extension, more comprehensible and so on. Because of this, there is a need to give automated support to the process. While there are a number of refactoring tools available for Erlang programs, such as Wrangler [17, #CITATION_TAG] and RefactorErl [21], the number of refactorings for retrofitting concurrency is limited.Refactorings differ from other sorts of program transformation in being applied to source code, rather than to a 'core' language within a compiler, and also in having an effect across a code base, rather than to a single function definition, say. We begin by discussing what refactoring means for functional programming languages, first in theory, and then in the context of a larger example. Next, we address system design and details of system implementation as well as contrasting the style of refactoring and tooling for Haskell and Erlang. We also discuss various extensions to the core tools, including integrating the tools with test frameworks; facilities for detecting and eliminating code clones; and facilities to make the systems extensible by users.
        Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. While there is currently much buzz about the new field of learning analytics [19] and the potential it holds for benefiting teaching and learning, the impression one currently gets is that there is also much uncertainty and hesitation, even extending to scepticism. A clear common understanding and vision for the domain has not yet formed among the educator and research community. Tertiary education providers collect an ever-increasing volume of data on their students, particularly activity data from virtual learning environments and other online resources (#CITATION_TAG).To investigate this situation, we distributed a stakeholder survey in September 2011 to an international audience from different sectors of education. The survey was scaffolded by a conceptual framework on learning analytics that was developed based on a recent literature review. In this article, we first briefly introduce the learning analytics framework and its six domains that formed the backbone structure to our survey.
        A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. A fundamental, universal property of human language is that its phonology is combinatorial. We present a novel model to investigate the hypothesis that combinatorial phonology results from optimising signal systems for perceptual distinctiveness. Some have argued against the gestural theory on the grounds that it must have required an unlikely transition from a visuo-manual format to an auditory-vocal one (e.g., Burling, 2005; #CITATION_TAG).That is, one can identify a set of basic, distinct units (phonemes, syllables) that can be productively combined in many different ways. Our model differs from previous models in two important respects. First, signals are modelled as trajectories through acoustic space. Hence, both holistic and combinatorial signals have a temporal structure. Second, we use the methodology from evolutionary game theory. Crucially, we show a path of ever increasing fitness from holistic to combinatorial signals, where every innovation represents an advantage even if no-one else in a population has yet obtained it.
        A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. A thorough investigation of the neural effects of psychotherapy is needed in order to provide a neurobiological foundation for widely used treatment protocols. Cognitive behavioural therapy in phobia resulted in decreased activity in limbic and paralimbic areas. However, a one to one correspondence between change in depression and change in specific brain areas may be over stated (#CITATION_TAG; Frewen et al., 2008; Dichter et al., 2012).nan
        Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. In the twelve years since the first review article dealing with chemical constituents of the Hepaticae appeared in this series as Volume 42 (19), several short reviews concerned with chemical constituents of bryophytes have been published (22, 96, 144, 265, 271, 647, 649, 650). In 1988, a Symposium on Chemistry and Chemical Taxonomy of Bryophytes was organised on the behalf of the Phytochemical Society of Europe; the proceedings of this meeting appeared as a book entitled Bryophytes: Their Chemistry and Chemical Taxonomy (651). The physiological and biochemical aspects of bryophytes have also been described in a recent book Bryophytes Development: Physiology and Biochemistry (139). Liverworts are known for rich composition of secondary compounds (#CITATION_TAG 1995, Mues 2000), many of which exhibit antimicrobial propertiesnan
        Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Accurate and reliable measurement of physical activity plays an important role in assessing effective lifestyle interventions for obesity. While several studies have found that 2-6 days are required [5, 7, 8, 6, 30], other studies have shown that 12 [4] and 16-23 days are needed [#CITATION_TAG].They wore RT3 accelerometers during waking hours for 7 d at baseline and after a 6-month weight loss intervention that included diet and physical activity recommendations. Using 4 d of data with >or=6 h x d of wear time optimized the balance between ICC and participant burden in overweight and obese adults before and after a weight loss intervention.
        Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. An increased risk of death from all causes is not restricted to the most severe mental illnesses, but is also associated with conditions such as depression and anxiety disorders (#CITATION_TAG).I also discuss the health care that psychiatric patients receive, both in terms of recognition of physical illness and subsequent intervention, with particular reference to cardiovascular disease.
        The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. Some people who have a visible difference (disfigurement) experience psychosocial adjustment problems that can lead to social anxiety and isolation. Applied psychologists, including health, clinical, and counseling psychologists have been at the forefront of developing interventions to support people with psychological needs arising from visible differences (#CITATION_TAG; Bessell et al., 2012b), and in developing a clearer understanding of the differentiating factors and processes between those who adjust well, and those who struggle to cope and manage with differing appearances.Eighty-three participants were assessed at four time points using the Hospital Anxiety and Depression Scales, Derriford Appearance Scale-24, Body Image Quality of Life Inventory and Fear of Negative Evaluation (FNE). A remote-access, computer-based intervention offers the potential to provide psychosocial support more easily and in a cost-effective manner to adults with appearance-related distress.
        type of analysis to explore social responsibility phenomena (for exceptions, see #CITATION_TAG; Crilly et al., 2012).To identify how factors at different levels of analysis combine to shape attitudes toward social responsibility, I apply fuzzy-set qualitative comparative analysis (fsQCA) to survey and archival data from 335 managers of overseas subsidiaries of three Dutch corporations. Attention to the simultaneous effects of individual psychological factors, the organizational context, and the broader social context offers a configurational perspective on the micro and macrofoundations of social responsibility.
        It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Computer science as an engineering discipline has been spectacularly successful. Yet it is also a philosophical enterprise in the way it represents the world and creates and manipulates models of reality, people, and action. The phenomenological tradition emphasizes the primacy of natural practice over abstract cognition in everyday activity. The attracting interest in developing potential drugs for chronic diseases has been evidenced in a recent review on biofunction of marine oligosaccharides (#CITATION_TAG).nan
        Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. Different measures will yield different results, and it follows that it is crucial to match the measure to the true objectives. These and related issues are discussed further in [3, 4, #CITATION_TAG, 6].In predictive data mining, algorithms will be both optimized and compared using a measure of predictive performance. We define two measures, one based on minimizing the overall cost to the card company, and the other based on minimizing the amount of fraud given the maximum number of investigations the card company can afford to make. We also describe a plot, analogous to the standard ROC, for displaying the performance trace of an algorithm as the relative costs of the two different kinds of misclassification--classing a fraudulent transaction as legitimate or vice versa--are varied.
        The ecological interactions parasitic insects have with their hosts may contribute to their prodigious diversity, which is unrivaled among animals. Many insects assumed to be polyphagous generalists have been shown to consist of several differentiated races, each occupying a different host-niche. The sunflower maggot fly, Strauzia longipennis, has long been thought to consist of two or more races due to its substantial intraspecific morphological variation. Evidence that mitochondrial genomes and morphological traits have moved between lineages implies a model of speciation-with-gene-flow for S. longipennis races. The likelihood of speciation in the face of homogenizing gene flow (i.e. without complete geographical isolation) is one of the most debated topics in evolutionary biology. For example, weak genetic differentiation between taxa could be due to recent divergence, gene flow, or a combination of these factors. Nonetheless, a number of convincing examples of speciation with gene flow have recently emerged, owing in part to the development of new analytical methods designed to estimate gene flow specifically. A recent example of speciation with gene flow in salamanders ( Niemiller et al. Further, recent empirical and theoretical work demonstrating that populations can diverge even in the face of effective migration (#CITATION_TAG; Nosil and Feder 2012), as well as new information about the permeability of genomes to between-lineage gene flow (Turner et al. 1999; Michel et al. 2010; Gompert et al. 2010) both suggest that persistent hybridization between closely related lineages may occur without leading to lineage fusion or breakdown of reproductive barriers.2008 ) further advances our understanding of this phenonemon, by showing that gene flow between cave and spring salamanders was ongoing during speciation, rather than having occurred after a long period of allopatric divergence.
        This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. Location analysis is concerned with locating one or more service facilities while fulfilling some constraints such as the demand of the customers and minimizing the total cost. Despite the cost of transporting goods or services, there is a fixed cost associated with opening a given facility such as the cost of the land, taxes or trunking (or hauling) cost to supply product, services and labour. This cost may vary from one area to another. Simulated Annealing is one of the meta-heuristic methods derived from the annealing process of a solid. #CITATION_TAG assume that fixed costs are zone-dependent, where zones are non-overlapping convex polygons.Several parameters in SA will be tested such as initial starting points, initial temperature and cooling schedules. The problems of locating 2 to 15 facilities are solved by using C++.
        The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Purpose - To make their supply chains more socially responsible, many companies are implementing supplier assessment tools and collaborative practices. and "What are the enablers of these mechanisms? Firms that engage in establishing strong relationships with suppliers enjoy superior performance (Giannakis, 2007; Reuter et al., 2010; #CITATION_TAG; Burritt and Schaltegger, 2012).".Design/methodology/approach - A structured literature review is carried out that analyses published studies, evaluates contributions, summarises knowledge and identifies managerial implications and lines for further research.Findings - Both assessment and collaboration have a positive impact on environmental performance and corporate social responsibility, although the most recent collaborative paradigm stresses that assessment alone is not enough.
        Performance at 22- and 23-mm simulated insertion depths was always poorer than normal, and performance at 25 mm simulated insertion depth was, most generally, the same as normal. When vocoded speech is spectrally shifted upward to simulate relatively shallow CI electrode insertions, shifts in excess of 3 mm of basilar membrane distance have large acute effects on speech perception (#CITATION_TAG; Shannon et al., 1998).Normally hearing listeners were presented with vowels, consonants, and sentences for identification through an acoustic simulation of a five-channel cochlear implant with electrodes separated by 4 mm (as in the Ineraid implant). Insertion depth was simulated by outputting sine waves from each channel of the processor at a frequency determined by the cochlear place of electrodes inserted 22-25 mm into the cochlea.
        In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Drawing on his breakthrough research in comparative neuroscience, Terrence Deacon offers a wealth of insights into the significance of symbolic thinking: from the co-evolutionary exchange between language and brains over two million years of hominid evolution to the ethical repercussions that followed man's newfound access to other people's thoughts and emotions. This way of conceptualizing the transition to a symbolic style of thought has a close relation with Bickerton's proposal for a transition from 'on-line' to 'off-line' thinking (Bickerton, 1996), and also to #CITATION_TAG proposal for a progression from indexical to symbolic representation.nan
        Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. Smallholder agriculture in the Central Andes of Peru is based to large extent on rainfed cropping systems, is exposed to climatic risks and is expected to respond sensitively to increasing temperatures and shifts in the precipitation regime. Criterion 3a follows data presented in Table 1 in #CITATION_TAG which is the only study we know that presents typical precipitation values required for planting of different crop types in the region.A simple crop model is used to assess the effects of these changes on crop phenology and development.
        -Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. High on-clopidogrel platelet reactivity (HCPR) and high on-aspirin platelet reactivity (HAPR) are associated with atherothrombotic events following coronary stenting. There are, however, few data concerning high on-treatment platelet reactivity to both aspirin and clopidogrel simultaneously. HCPR and HAPR were established by receiver-operator characteristic curve analysis. The VASP index is considered as a specific test for evaluating P2Y12 inhibition, while light-transmission aggregometry is used to predict outcome during dual antiplatelet therapy, although both tests have a predictive value [31, 33, 39, #CITATION_TAG].nan
        The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Knowledge management is one of the most important strategic resources of the firm which has been ascertained to many organizations to acquire and apply it before their competitor for achieving competitive advantages. Similarly, due to rising environmental awareness among customers, governments, NGOs, and researchers, firms are facing increasing pressure to implement environmental management practices in their operations. Realising the need to incorporate sustainability and the triple bottom line (Kleindorfer et al., 2005) as part of their strategic intent, companies focus on assessing the economic, environmental, and social impact of their activities and highlighting the relationship between sustainability and performance (Leppelt et al., 2013; #CITATION_TAG; Burritt and Schaltegger, 2012; Subramanian and Gunasekaran, 2015).nan
        Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. A nerve cell receives multiple inputs from upstream neurons by way of its synapses. Neuron processing functions are thus influenced by changes in the biophysical properties of the synapse, such as long-term potentiation (LTP) or depression (LTD). One major obstacle is the high dimensionality of the neuronal input-output space, which makes it unfeasible to perform a thorough computational analysis of a neuron with multiple synaptic inputs. Granule cells have a small dendritic tree (on average, they receive only four mossy fiber afferents), which greatly bounds the input combinatorial space, reducing the complexity of information-theoretic calculations. These selective mechanisms may have important consequences on the encoding of cerebellar mossy fiber inputs and the plasticity and computation at the next circuit stage, including the parallel fiber-Purkinje cell synapses This peculiarity of GrCs response patterns lead to a low output variability [#CITATION_TAG] which, in turn, greatly reduces the complexity of calculations and the duration of recording sessions.Numerical simulations and LTP experiments quantified how changes in neurotransmitter release probability (p) modulated information transmission of a cerebellar granule cell.
        ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. Manuscript Type: Empirical    Research Issue: This study investigates the employment consequences of private equity acquisitions, in particular institutional buyouts (IBOs), in the UK. It involves a pre- and post-acquisition analysis of employment and performance characteristics for a sample of acquired firms and a matched sample of non-acquired firms. Two important theoretical issues emerge. This paper builds on an earlier pilot study conducted by the authors (#CITATION_TAG).The first is a need to conceptualize skills and human capabilities on a collective dimension, specific to a particular organizational setting, and the extent to which they contribute to the organization's performance.
        Tremor in Parkinson's disease has several mysterious features. Clinically, tremor is seen in only three out of four patients with Parkinson's disease, and tremor-dominant patients generally follow a more benign disease course than non-tremor patients. Pathophysiologically, tremor is linked to altered activity in not one, but two distinct circuits: the basal ganglia, which are primarily affected by dopamine depletion in Parkinson's disease, and the cerebello-thalamo-cortical circuit, which is also involved in many other tremors. While recent studies [39, #CITATION_TAG] might further help to clarify this controversy, by adding new concepts to the debate, our concluding remark would be focused to a simple take home message: the appropriate examination and investigation of patients with tremor should not be simply addressed to motor aspects but should also consider non-motor features and specifically the core and supportive features of DLB [1] [2] [3] i.e., cognitive, visuo-spatial and dysexecutive abnormalities, RBD, and EEG abnormalities, before reaching definite conclusions.We first describe clinical and pathological differences between tremor-dominant and non-tremor Parkinson's disease subtypes, and then summarize recent studies on the pathophysiology of tremor. We also discuss a newly proposed 'dimmer-switch model' that explains tremor as resulting from the combined actions of two circuits: the basal ganglia that trigger tremor episodes and the cerebello-thalamo-cortical circuit that produces the tremor.
        We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Computational models will play an important role in our understanding of human higher-order cognition. How can a model's contribution to this goal be evaluated? One can make many arguments for the utility of developing cognitive models within a cognitive architecture (see, e.g., #CITATION_TAG; Newell, 1990), but adopting the concept of cognitive architecture is in fact highly consistent with Marr's original approach to vision.Further, using analogies with other sciences, the history of cognitive science, and examples from modern-day research programs, this article identifies five activities that have been demonstrated to play an important role in our understanding of human higher-order cognition. These include modeling within a cognitive architecture, conducting artificial intelligence research, measuring and expanding a model's ability, finding mappings between the structure of different domains, and attempting to explain multiple phenomena within a single model.2008 Cognitive Science Society, Inc.
        Solar radiation and ambient temperature have acted as selective physical forces among populations and thereby guided species distributions in the globe. Circadian clocks are universal and evolve when subjected to selection, and their properties contribute to variations in fitness within specific environments. Because of their position in the hierarchy and repressive actions, cryptochromes are the key components of the feedback loops on which circadian clocks are built. Sleep deprivation (SD) results in increased electroencephalographic (EEG) delta power during subsequent non-rapid eye movement sleep (NREMS) and is associated with changes in the expression of circadian clock-related genes in the cerebral cortex. The increase of NREMS delta power as a function of previous wake duration varies among inbred mouse strains. It is strengthened further by those which show that the CRY2 gene expression is abnormal when inbred-strain mice with the intrinsic level of high anxiety are deprived of sleep (#CITATION_TAG), and when humans with bipolar type 1 disorder do remain depressed after the antidepressant sleep deprivation (Lavebratt et al., 2010).Cortical expression of clock genes subsequent to SD was proportional to the increase in delta power that occurs in inbred strains: the strain that exhibits the most robust EEG response to SD (AKR/J) exhibited dramatic increases in expression of bmal1, clock, cry2, csnkIepsilon, and npas2, whereas the strain with the least robust response to SD (DBA/2) exhibited either no change or a decrease in expression of these genes and cry1.
        Since 2010, there has been repository growth in East Asia, South America, and Eastern Europe, especially in Taiwan, Brazil, and Poland. During the period, some countries, including France, Italy, and Spain, have maintained steady growth, whereas other countries, notably China and Russia, have experienced limited growth. Globally, repositories are predominantly institutional, multidisciplinary and English-language based. They typically use open-source OAI-compliant software but have immature licensing arrangements. Although the size of repositories is difficult to assess accurately, available data indicate that a small number of large repositories and a large number of small repositories make up the repository landscape. Major factors affecting both the initial development of repositories and their take-up include IT infrastructure, cultural factors, policy initiatives, awareness-raising activity, and usage mandates. Mandates are likely to be crucial in determining future repository development. This paper reviews the worldwide growth of openaccess (OA) repositories, 2005 to 2012, using data collected by the OpenDOAR project. Cultural dissimilarities across countries have played a significant role in open access development. IDT has also been used at a different level by #CITATION_TAG to explain different adoption patterns of both OA journals and repositories worldwide.Both maps and tables are used to support the analysis. The diffusionist theory is reviewed and applied to the understanding of open access.Findings - The paper discovers that technology is not the only factor determining the diffusion pattern of information systems as discussed in the literature.
        Lionfish are representative venomous fish, having venomous glandular tissues in dorsal, pelvic and anal spines. Some properties and primary structures of proteinaceous toxins from the venoms of three species of lionfish, Pterois antennata, Pterois lunulata and Pterois volitans, have so far been clarified. Nevertheless, the lionfish hyaluronidases as well as the stonefish hyaluronidases almost maintain structural features (active site, glyco_hydro_56 domain and cysteine location) observed in other hyaluronidases. The first proposal, by Engle, Pereira and Rovelli (EPR) [20, #CITATION_TAG], aimed also to identify the spin foam boundary state space with that of loop quantum gravity spin networks; this model is also referred to as the "flipped" vertex model.The hyaluronidases of P. antennata and P. volitans were shown to be optimally active at pH 6.6, 37 degC and 0.1 M NaCl and specifically active against hyaluronan. The primary structures (483 amino acid residues) of the lionfish hyaluronidases were elucidated by a cDNA cloning strategy using degenerate primers designed from the reported amino acid sequences of the stonefish hyaluronidases.
        The chronological development of universities ranges from the state at which universities are considered to be knowledge accumulators followed by knowledge factories and finally the knowledge hubs. The various national systems of innovations are aligned with the knowledge hubs and it involves a substantial amount of research activities. The newly established Mbeya University of Science and Technology is recognised as a knowledge hub in some particular niches. However, there are a limited number of research activities conducted at the university and this study is an attempt to identify the reasons that limit research activities. Universities have assumed an expanded role in science and technology-based economic development that has become of interest to catch-up regions as well as to leading innovation locales. Central to the transformation of Georgia Tech as a knowledge hub is the emergence of new institutional leadership, programs, organizational forms and boundary-spanning roles that meditate among academic, educational, entrepreneurial, venture capital, industrial, and public spheres. The evolving university context and missions discussed by #CITATION_TAG show a timeline of three models of universities.nan
        Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Autism spectrum disorders (ASDs) are highly prevalent neurodevelopmental disorders, but the underlying pathogenesis remains poorly understood. Recent studies have implicated the cerebellum in these disorders, with post-mortem studies in ASD patients showing cerebellar Purkinje cell (PC) loss, and isolated cerebellar injury has been associated with a higher incidence of ASDs. However, the extent of cerebellar contribution to the pathogenesis of ASDs remains unclear. Tuberous sclerosis complex (TSC) is a genetic disorder with high rates of comorbid ASDs that result from mutation of either TSC1 or TSC2, whose protein products dimerize and negatively regulate mammalian target of rapamycin (mTOR) signalling. However, the roles of Tsc1 and the sequelae of Tsc1 dysfunction in the cerebellum have not been investigated so far. Others have also provided evidence that autistic-like behavior can be prevented with mTOR treatment in mouse models of TSC (#CITATION_TAG; Talos et al, 2012; Reith et al. 2013).nan
        The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. The items were derived from existing literature (Zhu and Sarkis, 2004; #CITATION_TAG; Schoenherr, 2012; Zhang and Wang, 2014; Dubey et al. 2015).Corporate environmental strategies are identified on the basis of their orientation towards shareholder value.
        This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Published online: 27 Apr 2017Comparative research on aphasia and aphasia rehabilitation is challenged by the lack of comparable assessment tools across different languages. In English, a large array of tools is available, while in most other languages, the selection is more limited. These subtests were taken from two sources: translated subtests of the Comprehensive Aphasia Test (CAT) (#CITATION_TAG) and subtests that have been developed in speech and language clinics in Jordan.Specifically, we focus on challenges and solutions related to the use of imageability, frequency, word length, spelling-to-sound regularity and sentence length and complexity as underlying properties in the selection of the testing material.For the work reported in this article, we were supported by various funding bodies.
        Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Although the province of Nova Scotia, Canada is located in a region considered to have a high prevalence of multiple sclerosis (MS), epidemiologic data are limited. In 2010, the age-standardized prevalence of MS per 100,000 population was 266.9 (95% CI: 257.1- 277.1) and incidence was 5.17 (95% CI: 3.78-6.56) per 100,000 persons/year. From 1990-2010 the prevalence of MS rose steadily but incidence remained stable. MS prevalence in Nova Scotia is among the highest in the world, similar to recent prevalence estimates elsewhere in Canada. We aimed to estimate the incidence and prevalence of MS in BC, Canada using previously validated case definitions of MS [10, #CITATION_TAG] based on health administrative data.Methods: We used provincial administrative claims data to identify persons with MS. We validated administrative case definitions using the clinical database of the province's only MS Clinic; agreement between data sources was expressed using a kappa statistic. We then applied these definitions to estimate the incidence and prevalence of MS from 1990 to 2010.
        The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [#CITATION_TAG].The use of complex system simulation as a research tool is facilitated by principled development; software quality assurance, an important part of fitness for purpose, can be assisted by use of model driven engineering (MDE) techniques.
        Previous research has shown that people form impressions of potential leaders from their faces and that certain facial features predict success in reaching prestigious leadership positions. However, much less is known about the accuracy or meta-accuracy of face-based leadership inferences. Here we examine a simple, but important, question: Can leadership domain be inferred from faces? However, people are surprisingly bad at evaluating their own performance on this judgment task: We find no relationship between how well judges think they performed and their actual accuracy levels. Recent research has shown that rapid judgments about the personality traits of political candidates, based solely on their appearance, can predict their electoral success. This suggests that voters rely heavily on appearances when choosing which candidate to elect. Yet, the human mind often relies on superficial cues to form judgments or make decisions, and the choice of which leader to select is no exception: A large and growing literature shows that facial appearances predict success in reaching prestigious leadership positions (Antonakis & Jacquart, 2013; #CITATION_TAG).We also reanalyze previous data to show that facial competence is a highly robust and specific predictor of political preferences. Finally, we introduce a computer model of face-based competence judgments, which we use to derive some of the facial features associated with these judgments.
        Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. Environmental policies, reduced manufacturing costs, and technology improvements have all contributed to the growing installation of wind turbines and solar photovoltaic arrays in the electric grid. While these new sources of renewable electrical power provide environmental and economic benefits to the electric grid, they also complicate the balancing of supply and demand required to reliably operate the grid. The seasonal, daily, and sub-hourly fluctuations in the energy output of wind and solar generators must be compensated by operating the existing power plant fleet more flexibly or by providing more flexible sources of electricity demand. Wind and solar impact flexibility requirements at different times of the day: wind tends to intensify demand-driven flexibility events by ramping up energy production at night when demand is decreasing and ramping down energy production in the morning when demand is increasing, while solar tends to intensify flexibility requirements due to its quick changes in energy output driven by the rising and setting sun. Adding wind to a system with large amounts of solar does not tend to increase flexibility requirements except for the daily volatility. This renewable mix produces 110 TWh of energy per year, 34% of the total electricity demand. Chapter 6 develops a mixed-integer linear program for modeling the optimal equipment capacity and dispatch of a central utility plant (CUP) in a residential neighborhood and its ability to improve rooftop solar integration. Building a clean, resilient, and reliable electric grid for the future is a worthwhile endeavor that will require innovative supply-side and demand-side solutions for integrating the intermittent power output of renewable generation into the electric grid. There are many opportunities to expand these analyses and explore new sources of grid flexibility in future work.Mechanical Engineerin DSM may help to increase power system efficiency by reducing peak generation capacity requirements and by improving the utilization of both generation and network assets (#CITATION_TAG).These topics are covered in the four chapters described below. Chapter 3 utilizes a unit commitment and dispatch (UC&D) model to simulate large solar generation assets with different geographic locations and orientations. The west-located, west-oriented solar simulation required greater system flexibility, but utilized more low-cost generators and fewer high-cost generators for energy production than other simulated scenarios. Chapter 4 develops a quantitative framework for calculating flexibility requirements and performs a statistical analysis of load, wind, and solar data from the Electric Reliability Council of Texas (ERCOT) to show how wind and solar capacity impacts these grid flexibility requirements. It also presents a framework for choosing CO2 prices by balancing increasing system cost and flexibility requirements with CO2 emissions reductions. The CUP equipment includes a microturbine, battery, chiller plant, and cooling storage. The CUP model is exposed to a variety of electricity rate structures to see how they influence its operation. The model finds the optimal capacity for each piece of CUP equipment, optimizing their hourly dispatch to meet neighborhood cooling and electric demand while maximizing profit. As a cohesive document, this dissertation communicates the scale and severity of the flexibility requirements that will be required to operate systems with large amounts of wind and solar generation and explores one demand-side method for providing that needed flexibility.
        The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [#CITATION_TAG] [13].It also explored the differential effectiveness of the intervention for subjects with specific diseases and comorbidities. METHODS The study was a six-month randomized, controlled trial at community-based sites comparing treatment subjects with wait-list control subjects. Participants were 952 patients 40 years of age or older with a physician-confirmed diagnosis of heart disease, lung disease, stroke, or arthritis. Health behaviors, health status, and health service utilization, as determined by mailed, self-administered questionnaires, were measured.
        Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. More than half of employment intermediated by this resource belongs to standard companies. Advantages include greater financial gains for the employees, wider social integration, increased worker satisfaction, higher self-esteem, more independent living, reduced family burden including a lower need for providing informal care, and service cost-savings (Beyer and Kilsby, 1996; Bond et al., 1997 Bond et al.,, 2008 #CITATION_TAG; Graetz, 2010; Griffin et al., 1996; Heffernan and Pilkington, 2011; McCaughrin et al., 1993; Noble et al., 1991; Rhodes et al., 1987; Stevens and Martin, 1999).nan
        Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. For the systems of order one or two we describe the local structure of singularities of a generic solution to the unperturbed system near the point of "gradient catastrophe" in terms of standard objects of the classical singularity theory; we argue that their perturbed companions must be given by certain special solutions of Painleve' equations and their generalizations. MI descends directly from response entropy and noise entropy [#CITATION_TAG], which are correlated to the variability of responses to separate inputs [13] or to the same input [14] [15] [16], respectively.nan
        The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. Location The eight study sites are located in southern New England in the states of Massachusetts and Connecticut. Tsuga canadensis and Fagus grandifolia are abundant in the upland area, while Quercus, Carya and Pinus species have higher abundances in the lowlands. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp. ), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000; Shuman et al., 2001; Shuman and Donnelly, 2006; #CITATION_TAG; Marsicek et al., 2013).The sites span a climatic and vegetational gradient from the lowland areas of eastern Massachusetts and Connecticut to the uplands of north-central and western Massachusetts. Methods We collected sediment cores from three lakes in eastern and north-central Massachusetts (Berry East, Blood and Little Royalston Ponds). Pollen records from those sites were compared with previously published pollen dat
        Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Protection of large natural forest landscapes is a highly important task to help fulfill different international strategic initiatives to protect forest biodiversity, to reduce carbon emissions from deforestation and forest degradation, and to stimulate sustainable forest management practices. The vast majority of IFL are found in two biomes: Dense Tropical and Subtropical Forests (45.3%) and Boreal Forests (43.8%). The IFL exist in 66 of the 149 countries that together make up the forest zone. Therefore the maps are most comparable in undisturbed forest areas, so all comparisons were performed in Intact Forest Landscape (IFL) (#CITATION_TAG) areas only, with the exception of the analysis of recent deforestation.We have created a global IFL map using existing fine-scale maps and a global coverage of high spatial resolution satellite imagery.
        Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. The poor knowledge of epilepsy among traditional healers is due to cultural prejudices and environment. The resultant deep-rooted misconceptions and myths negatively affect the attitudes and encourage traditional care with high morbidity and mortality. There were prevalent negative attitudes and perception about epilepsy among the healers, as 146 (88.0%) of them viewed it as contagious; 149 (89.8%) would decline either marrying or eating with epileptic persons. Although traditional healers are frequently involved in the care of epilepsy in our environment, they have little or no scientific knowledge about the condition. Adequate knowledge about epilepsy is essential for diagnosis and treatment. Only a small empirical literature exists on the use of subjective evaluation or discretion in incentive systems (e.g., #CITATION_TAG; Ittner, Larcker & Meyer 2003; Murphy & Oyer 2003; Gibbs et al. 2004 Gibbs et al., 2009, presumably because quantifying the concepts is difficult.One hundred and seventy three traditional healers from villages/communities in Uyo were assessed for knowledge; attitude and perception of epilepsy, using an interviewer assisted Attitude Questionnaire.
        Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Increasingly, information technology governance is being considered an integral part of corporate governance. There has been a rapid increase in awareness and adoption of IT governance as well as the desire to conform to national governance requirements to ensure that IT is aligned with the objectives of the organization. Although IT governance as a framework may improve controls with respect to the alignment of IT and business objectives, it pays less attention to how IT adoption decisions are made [#CITATION_TAG].nan
        This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . In early December 2006, the Fijian military seized power in a coup led by the Armed Forces commander Commodore Frank Bainimarama. It was a coup long expected, and Fiji's fourth since 1987. Internationally, the response was swift imposing sanctions and removing or delaying international aid programmes. This has a potentially significant impact on Fiji because it is one of the largest per capita recipients of developmental aid funding in the world. However, it may also have little impact because, despite such assistance, the Fijian GDP has stagnated with an average growth of under 1% for the last 20 years. However, to review all international developmental programmes across all sectors of Fijian society, while maintaining contemporary relevance and coherency, is untenable. The EU is one of the most influential partners for Fiji and is often overlooked by scholars, allowing this thesis to make a valuable contribution to developmental studies in the pacific region. This is because they are the sectors that the European Union is presently devoting most attention. Similar mechanisms that imply a different fiscal policy between a democracy and an autocracy are proposed by #CITATION_TAG and McGuire and Olson (1996).nan
        Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Subjects often rated harmful omissions as less immoral, or less bad as decisions, than harmful commissions. One possibility is this is related to an increased sense of causality and personal responsibility associated with overt actions [#CITATION_TAG].Subjects read scenarios concerning pairs of options. One option was an omission, the other, a commission. Intentions, motives, and consequences were held constant. Subjects either judged the morality of actors by their choices or rated the goodness of decision options. Such ratings were associated with judgments that omissions do not cause outcomes. The 'omission bias ' revealed in these experiments can be described as an overgeneralization of a useful heuristic to cases in which it is not justified.
        Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Moving from mainly techni cal issues in procurement to corporate IS governance presents OSS with new challenges beyond outlining a business case for a particular OSS application. Amongst other things, IT governance is tasked with deciding on how decision rights and accountability are distributed in organizations to avoid ad hoc decision making [#CITATION_TAG].We draw parallels to the business case for commercial software products (COTS).
        Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. However, there was a linear trend for the prevalence of CS-OIDP attributed to malocclusion, by level of normative orthodontic treatment need (P = 0.042). #CITATION_TAG] [100] Marshman and colleagues 101 have expressed concern that generic measures of OHQoL might not fully capture the impact that malocclusion has suggested the development of a malocclusion specific measure of OHQoL for young people with malocclusion.Two hundred 16- to 17-year-old adolescents were randomly selected from 957 children attending a public college in London, UK. During interviews, participants provided information about demographic variables and socio-dental impacts on quality of life attributed to malocclusions, using the Condition-Specific form of the Oral Impacts on Daily Performances (CS-OIDP) index. Statistical comparison by covariates was performed using chi-squared test and chi-squared test for trends.
        Biobanking, the large-scale, systematic collection of data and tissue for open-ended research purposes, is on the rise, particularly in clinical research. However, the positioning of biobanking infrastructures and transfer of tissue and data between research and care is not an innocuous go-between. Instead, it involves changes in both domains and raises issues about how distinctions between research and care are drawn and policed. Based on an analysis of the emergence and development of clinical biobanking in the Netherlands, this article explores how processes of bio-objectification associated with biobanking arise, redefining the ways in which distinctions between research and clinical care are governed. Biobanks and archived data sets collecting samples and data have become crucial engines of genetic and genomic research. Unresolved, however, is what responsibilities biobanks should shoulder to manage incidental findings and individual research results of potential health, reproductive, or personal importance to individual contributors (using "biobank" here to refer both to collections of samples and collections of data). This article reports recommendations from a 2-year project funded by the National Institutes of Health. For instance, Wolf and others consider that "findings that are analytically valid, reveal an established and substantial risk of a serious health condition, and are clinically actionable should generally be offered to consenting contributors" (#CITATION_TAG).We analyze the responsibilities involved in managing the return of incidental findings and individual research results in a biobank research system (primary research or collection sites, the biobank itself, and secondary research sites).
        Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. In financially constrained health systems across the world, increasing emphasis is being placed on the ability to demonstrate that health care interventions are not only effective, but also cost-effective. To account for the uncertainty around the input parameter point estimates, a probabilistic analysis was undertaken, in which input parameters were assigned probabilistic distributions [#CITATION_TAG].Particular emphasis is placed on the importance of the appropriate representation of uncertainty in the evaluative process and the implication this uncertainty has for decision making and the need for future research. This highly practical guide takes the reader through the key principles and approaches of modelling techniques. It begins with the basics of constructing different forms of the model, the population of the model with input parameter estimates, analysis of the results, and progression to the holistic view of models as a valuable tool for informing future research exercises. ABOUT THE SERIES: Economic evaluation of health interventions is a growing specialist field, and this series of practical handbooks will tackle, in-depth, topics superficially addressed in more general health economics books. Each volume will include illustrative material, case histories and worked examples to encourage the reader to apply the methods discussed, with supporting material provided online.
        Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Light curves in the EUV band have been analysed also with a different approach: they have been compared to simulated ones obtained from sequences of random pulses with powerlaw distribution (#CITATION_TAG).It also investigated whether acoustic-phonetic modifications made to counteract the effects of a challenging listening condition are tailored to the condition under which communication occurs. Forty talkers were recorded in pairs while engaged in "spot the difference" picture tasks in good and challenging conditions. In the challenging conditions, one talker heard the other (1) via a three-channel noise vocoder (VOC); (2) with simultaneous babble noise (BABBLE).
        The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. In 1998 the Veterans Health Administration (VHA) created the National Center for Patient Safety (NCPS) to lead the effort to reduce adverse events and close calls systemwide. NCPS has always aimed to develop a program that would be applicable both within the VA and beyond.NCPS's full patient safety program was tested and implemented throughout the VA system from November 1999 to August 2000. It was developed from the Total Quality Management movement where it was conceived primarily as an "organizational learning device" (#CITATION_TAG).Core concepts included a non-punitive approach to patient safety activities that emphasizes systems-based learning, the active seeking out of close calls, which are viewed as opportunities for learning and investigation, and the use of interdisciplinary teams to investigate close calls and adverse events through a root cause analysis (RCA) process. Program components included an RCA system for use by caregivers at the front line, a system for the aggregate review of RCA results, information systems software, alerts and advisories, and cognitive acids.
        While it finds no empirical basis for this orthodox standpoint it observes that long-term unemployment dampens aggregate production which in turn aggravates unemployment problem. This paper analysed the OECD data on employment protection for OECD countries over the time span 1990-2008 on the basis of alternative dynamic panel data models and panel causality tests and examines the validity of the neo-liberal argument that strictness of employment protection hurts labour through increased long-term and youth unemployment rates. In recent years, comparative economics experienced a revival, with a new focus on comparing capitalist economies. In the late 1990s La Porta and his collaborators (La Porta et al., 1997, 1998, 1999, 2000 2006, 2008 #CITATION_TAG; Shleifer, 2002, 2003; Beck et al., 2003a Beck et al.,, 2003b Botero et al., 2004) set in motion a series of systematic analysis of the relationships between legal and economic variables.The authors argue that, to understand capitalist institutions, one needs to understand the basic tradeoff between the costs of disorder and those of dictatorship. They then apply this logic to study the structure of efficient institutions, the consequences of colonial transplantation, and the politics of institutional choice.Labor Policies,Decentralization,National Governance,Environmental Economics&Policies,Economic Theory&Research,National Governance,Environmental Economics&Policies,Economic Theory&Research,Governance Indicators,Banks&Banking Reform
        5G technology is using millimeter-wave band to improve the wireless communication system. However, narrow transmitter and receiver beams have caused the beam coverage area to be limited. Due to propagation limitations of mm wave band, beam forming technology with multi-beam based communication system, has been focused to overcome the problem. This paper demonstrates several shapes of microstrip array antennas, such as rectangular and triangular patch antennas array. Modern wireless communication systems require a low profile, lightweight, high gain and simple structure antennas to ensure reliability, mobility, and high efficiency [#CITATION_TAG].Specifically, 4x1, 2x1, and single element of both shapes are designed and simulated by a full wave simulator (IE3d). Moreover, this paper presents a comparison between both rectangular and triangular antenna arrays.