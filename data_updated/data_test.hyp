It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Marine microalgae have been used for a long time as food for humans, such as Arthrospira (formerly, Spirulina), and for animals in aquaculture. The biomass of these microalgae and the compounds they produce have been shown to possess several biological applications with numerous health benefits. Scientific knowledge present in literature for each of these sources seems to differ, from the well-known seaweed (Rinaudo, 2007) to relatively new sources of polysaccharides such as extremophiles and microalgae (#CITATION_TAG). It goes through the most studied activities of sulphated polysaccharides (sPS) or their derivatives, but also highlights lesser known applications as hypolipidaemic or hypoglycaemic, or as biolubricant agents and drag-reducers.
Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. ABSTRACT: A province wide prevalence study on multiple sclerosis (MS) was conducted in British Columbia (B.C.). These rates are among the highest reported in Canada or elsewhere. neurologists made this study unique in its scope and accuracy of diagnosis. Other than a study based on self-reported MS [21], the last province-wide estimate of MS prevalence, which used clinically confirmed definitions, was 93.3/100,000 in 1982 [#CITATION_TAG]. 239,412 neurologists' files were hand searched by one researcher using modified Schumacher criteria for classification. Other sources used during the study for identifying MS patients were the MS Clinic, general practitioners, ophthalmologists, urologists, specialized facilities such as long term care facilities and rehabilitation centres, and patient self-referrals. A total of 4,620 non-duplicated cases were identified and classified.
We report a high-pressure single-crystal synchrotron x-ray diffraction on a LaAlO 3 single crystal. It appeared worthwhile to us to present a state-of-the-art look at the field of ferroelectrics. Explorations of pressure-temperature or pressure-substitution phase diagrams remain even rarer [#CITATION_TAG]. For each topic we will try to work out both current interesting approaches and an outlook into future challenges.
A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Changes in neural activity of cortical-limbic regions that subserve hypervigilance and emotion regulation may represent biologically oriented change mechanisms that mediate symptom improvement of CT for IBS. For example, very similar changes in those brain areas are seen after CBT and other psychological treatments for anxiety (Furmark et al., 2002;Paquette et al., 2003;Straube et al., 2006;Porto et al., 2009;Freyer et al., 2011), schizophrenia ( Wykes et al., 2002), eating disorders (Vocks et al., 2011) and Irritable Bowel Syndrome ( #CITATION_TAG et al., 2006) Five healthy controls and 6 Rome II diagnosed IBS patients underwent psychological testing followed by rectal balloon distention while brain neural activity was measured with O-15 water positron emission tomography (PET) before and after a brief regimen of CT. Pre-treatment resting state scans, without distention, were compared to post-treatment scans using statistical parametric mapping (SPM). Neural activity in the parahippocampal gyrus and inferior portion of the right cortex cingulate were reduced in the post-treatment scan, compared to pre-treatment (x, y, z coordinates in MNI standard space were -30, -12, -30, P=0.017; 6, 34, -8, P=0.023, respectively). Limbic activity changes were accompanied by significant improvements in GI symptoms (e.g., pain, bowel dysfunction) and psychological functioning (e.g., anxiety, worry).
In previous work the authors considered the asymmetric simple exclusion process on the integer lattice in the case of step initial condition, particles beginning at the positive integers. There it was shown that the probability distribution for the position of an individual particle is given by an integral whose integrand involves a Fredholm determinant. In one an apparently new distribution function arises and in another the distribution function F 2 arises. CONTEXT Previous studies may have underestimated the contribution of health behaviors to social inequalities in mortality because health behaviors were assessed only at the baseline of the study. DESIGN, SETTING, AND PARTICIPANTS Established in 1985, the British Whitehall II longitudinal cohort study includes 10 308 civil servants, aged 35 to 55 years, living in London, England. In previous work [#CITATION_TAG] the authors considered the asymmetric simple exclusion process (ASEP) on the integer lattice Z in the case of step initial condition, particles beginning at the positive integers Z +. Analyses are based on 9590 men and women followed up for mortality until April 30, 2009. Socioeconomic position was derived from civil service employment grade (high, intermediate, and low) at baseline.
The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The logic-based machine-understandable framework of the Semantic Web typically challenges casual users when they try to query ontologies. An often proposed solution to help casual users is the use of natural language interfaces. Such tools, however, suffer from one of the biggest problems of natural language: ambiguities. Furthermore, the systems are hardly adaptable to new domains. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [3] [4] 6, 9, 13, #CITATION_TAG, 16]. The approach allows queries in natural language, thereby asking the user for clarification in case of ambiguities.
In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. When the school opened in 1971, the existing 86 medical schools all offered similar programs: two years of basic science training in lecture halls and laboratories, followed by two years of direct contact with patients in clinical settings. In the new school, students were taught didactically only during the first year. Hence, institutionalism emphasises the persuasive control over the practices, beliefs and belief systems of individuals or organisations through an institution's sway (#CITATION_TAG, in King et al., 1994. During the second year, each student was assigned to a community physician who acted as an advisor and who discussed with students those patients afflicted with the diseases the student was currently studying. The author found the case of this medical school to be of particular interest from an organizational viewpoint in that: (1) the early development of the school was shaped by the first dean's entrepreneurial activity, ambitions, visions, strengths, and weaknesses; (2) the uncertainty resulting from the school's novelty forced individuals to assume new roles and face unclear performance criteria; and (3) the transition of an innovative school to an institutionalized one was problematic because it modified the decision-making process. The author suggests that those things which lead to an organization's success during its early years are not the same as those that lead to longer-run success. A comparative analysis of the birth, life, and death of organization is advocated.
The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . THE AREA UNDER CULTIVATION IN SOUTH Africa more than tripled during the twentieth century, while plantation area increased more than tenfold. Both domestic and global population growth partly underlie the increased demand for crop products over the past century. Increased production was initially achieved mainly by expanding the area under cultivation, and, from the 1960s onwards, principally through enhanced yields per hectare. In the latter period, nationally averaged productivity in a given year was related to fertilizer use, irrigation and the proportion of the country experiencing dry conditions. Tree geometry has been used for reduced modeling of turbulence intermittency [2, 4, 5, 14, 22], in some instances formulated as generalizations of the zero-dimensional shell models [6, #CITATION_TAG] in which the scale space of turbulence is parameterized by wavenumber modulus. Independent estimates of historical cultivated area at the national level were derived from estimates of production and productivity per hectare, presenting a method that could be used to obtain improved historical land-cover estimates in data-poor countries.
The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. We followed Chen et al. (2010) and subsequent studies (#CITATION_TAG) in that we conducted a manual scan and analysis of all the abstracts and a selection of the highly cited and review papers. We draw on Nelson's theorisation of the co-evolution of Physical and Social Technologies to redefine the SIS domain as a Complex Adaptive System (CAS) for the co-evolution of ICT and organisational capabilities and business models to create social and economic value. We conduct a meta-analysis of the domain based on a longitudinal review of SIS research over 33 years, and contrary to contemporaneous SIS literature which suggests that a paradigm shift may be necessary to address the increased turbulence, uncertainty and dynamism in the emerging competitive landscape, we find that the SIS research domain has the requisite adaptive capacity to evolve gracefully to address the challenges of the emerging networked competitive landscape. Drawing on complexity science and network theory we identify four priorities for the development of the domain for the future: conceptualisation of the SIS Domain as a CAS for the co-evolution of Physical and Social Technologies; the adoption of the network paradigm; access to a science of networks; and adoption of Complexity Science as an articulation device within SIS and across disciplines
Registration is an important component of medical image analysis and for analysing large amounts of data it is desirable to have fully automatic registration methods. Many different automatic registration methods have been proposed to date, and almost all share a common mathematical framework - one of optimising a cost function. To date little attention has been focused on the optimisation method itself, even though the success of most registration methods hinges on the quality of this optimisation. We extracted the brain from the skull using the BET toolbox for both the FLAIR and the structural T1 weighted images and these scans were registered to standard space using FLIRT (#CITATION_TAG). It is demonstrated that the use of local optimisation methods together with the standard multi-resolution approach is not sufficient to reliably find the global minimum. To address this problem, a global optimisation method is proposed that is specifically tailored to this form of registration. A full discussion of all the necessary implementation details is included as this is an important part of any practical method.
The fungicides used to control diseases in cereal production can have adverse effects on non-target fungi, with possible consequences for plant health and productivity. The fungal community on wheat leaves consisted mainly of basidiomycete yeasts, saprotrophic ascomycetes and plant pathogens. This study examined fungicide effects on fungal communities on winter wheat leaves in two areas of Sweden. Foliar fungal communities of plants are diverse and ubiquitous. In grasses endophytes may increase host fitness; in trees, their ecological roles are poorly understood. At the time of the sampling the poplars had been growing in a common garden for two years. Leucosporidium golubevii is a yeast discovered in freshwater [46], and has been reported from the phyllosphere of balsam poplar [#CITATION_TAG]. We sampled leaves from genotyped balsam poplars from across the species' range, and applied 454 amplicon sequencing to characterize foliar fungal communities. We found diverse fungal communities associated with the poplar leaves. The observed patterns may be explained by a filtering mechanism which allows the trees to selectively recruit fungal strains from the environment. Alternatively, host genotype-specific fungal communities may be present in the tree systemically, and persist in the host even after two clonal reproductions.
to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' "cogito"), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. Recommender systems learn about user preferences over time, automatically finding things of similar interest. Recommender systems do, however, suffer from cold-start problems where no initial information is available early on upon which to base recommendations.Semantic knowledge structures, such as ontologies, can provide valuable domain knowledge and user information. However, acquiring such knowledge and keeping it up to date is not a trivial task and user interests are particularly difficult to acquire and maintain. This approach is rooted in dynamical approaches to coordination that are levelagnostic, seeking to understand emergent phenomena at one level (e.g., the dyad) as arising through processes of self-organization from the constrained interaction of autonomous components at a lower level (the speaker/listeners) (Kelso, 1995; #CITATION_TAG). This reduces the burden of creating explicit queries. The ontology is used to address the recommender systems cold-start problem. The recommender system addresses the ontology's interest-acquisition problem.
The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Sustainable supplier relationship management (SSRM) has become crucial in companies' sustainability efforts. A firm's corporate image, in terms of economical, environmental and social behavior, heavily depends on its supply chain and the sustainability performance of each and every chain link, including suppliers and sub-suppliers. Realising the need to incorporate sustainability and the triple bottom line (Kleindorfer et al., 2005) as part of their strategic intent, companies focus on assessing the economic, environmental, and social impact of their activities and highlighting the relationship between sustainability and performance (#CITATION_TAG; Green et al., 2012; Burritt and Schaltegger, 2012; Subramanian and Gunasekaran, 2015). Additionally, we identified corporate strategy alignment, risk perception and the listing in sustainability indices as key influential factors, which foster and limit a focal firm's engagement in SSRM. The contribution of this paper is twofold: First, in-depth insights on how sustainability leaders within the chemical industry int roduce sustainability into their supplier relationship management processes are presented and compared to the practices of sustainability followers.
Unlike most previous research that focused on the relationship between rs-fcMRI and a single behavioral measure of EF, in the current study we examined the relationship of rs-fcMRI with individual differences in subcomponents of EF. Neuroimaging has revealed that almost all functional networks that support aspects of task related processing have a comparable resting state network (Smith et al., 2009), and the integrity of these networks varies across individuals in a manner that is predictive of complex forms of cognition such as meta cognitive accuracy (Baird et al., 2013), spontaneous thought (Gorgolewski et al., 2014), reading comprehension (Smallwood et al., 2013) and executive control (#CITATION_TAG). From these three measures, we derived estimates of common aspects of EF, as well as abilities specific to working memory updating and task shifting. Using Independent Components Analysis (ICA), we identified across the group of participants several networks of regions (Resting State Networks, RSNs) with temporally correlated time courses. We then used dual regression to explore how these RSNs covaried with individual differences in EF.
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Design Two year nationwide retrospective study, 2005-6 (SENTI-MELC study).Data collection via the sentinel network of general practitioners, an epidemiological surveillance system representative of all general practitioners in Belgium.1690 non-sudden deaths in practices of the sentinel general practitioners.Non-sudden deaths of patients (aged >1 year) reported each week. To a large extent receiving spiritual care was associated with higher frequencies of euthanasia or physician assisted suicide than receiving little spiritual care (18.5, 2.0 to 172.7).End of life decisions that shorten life, including euthanasia or physician assisted suicide, are not related to a lower use of palliative care in Belgium and often occur within the context of multidisciplinary care. The relationship between euthanasia and spiritual or existential caregiving to the patient has been examined in a study involving the last three months of life of patients of a representative panel of Belgian general practitioners (#CITATION_TAG). Multivariable regression analysis controlled for age, sex, cause, and place of death.Use of specialist multidisciplinary palliative care services was associated with intensified alleviation of symptoms (odds ratio 2.1, 95% confidence interval 1.6 to 2.6), continuous deep sedation forgoing food/fluid (2.9, 1.7 to 4.9), and the total of decisions explicitly intended to shorten life (1.5, 1.1 to 2.1) but not with euthanasia or physician assisted suicide in particular.
Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. What is perhaps different in STS is its predominant emphasis on the use of case studies to produce theory, rather than testing theory using cases; however, this is perhaps a difference in perspective and approach rather than an indication of sharp boundaries between STS and (for instance) anthropology and sociology [#CITATION_TAG]. It then turns to more recent developments in STS, outlining the importance of material semiotics to important traditions within the discipline including those influenced by actor network theory, feminism, and postcolonialism.
Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Introduction There is a growing need for largescale data and biobanks for biomedical research. The Radboud Biobank was conceived from the standards that were laid down in the String of Pearls Initiative (PSI), a unique partnership between the eight University Medical Centers (UMCs) in the Netherlands, that contributes to innovation in health care by facilitating biomedical research. The establishment of the RadboudBiobank creates an efficient and high quality facility for scientific research and medical innovation. Ratchet effects have received surprisingly little empirical study beyond Roy's (1952) famous description (#CITATION_TAG). These procedures are generic and established with a view on standardization, quality and efficiency, transcending the interests of single departments. Furthermore, (quality) standards are set in the field of ICT, legal and ethical aspects, communication and distribution.
The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. BACKGROUND Sickness absence costs the UK economy around PS20 billion per year. Senior personnel changes in organisations; shifts in organisational structures; changes in strategy; as well as acquisitions, mergers and bankruptcies can all cause such change (Halinen et al., 1999; #CITATION_TAG). METHODS A Markov model was developed to assess the cost-effectiveness of three interventions: a workplace intervention; a physical activity and education intervention and a physical activity, education and workplace visit intervention.
The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. The paper reviews what is known about outcome in adult life for more able individuals within the autistic spectrum. An initial study of five exporters and seven importers was undertaken in 2006 (Appendix A lists the companies involved) as part of a larger study (#CITATION_TAG). The stability of IQ and other measures over time, and variables related to outcome, are also investigated.
This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. We do not look at the MSWP without fixed costs, but refer the reader to #CITATION_TAG. The new solution methods are  discussed for both the well known multisource Weber problem and its counterpart the capacitated case.
It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Lionfish are representative venomous fish, having venomous glandular tissues in dorsal, pelvic and anal spines. Some properties and primary structures of proteinaceous toxins from the venoms of three species of lionfish, Pterois antennata, Pterois lunulata and Pterois volitans, have so far been clarified. Nevertheless, the lionfish hyaluronidases as well as the stonefish hyaluronidases almost maintain structural features (active site, glyco_hydro_56 domain and cysteine location) observed in other hyaluronidases. Natural quick action characterizing these biocatalysts is a very interesting feature for biocatalytic manipulation of these carbohydrate-related molecules (Madokoro et al., 2011; #CITATION_TAG) also in view of bioactivity expressed by the polymer and HA oligosaccharides (Ariyoshi et al., 2012). The hyaluronidases of P. antennata and P. volitans were shown to be optimally active at pH 6.6, 37degC and 0.1 M NaCl and specifically active against hyaluronan. The primary structures (483 amino acid residues) of the lionfish hyaluronidases were elucidated by a cDNA cloning strategy using degenerate primers designed from the reported amino acid sequences of the stonefish hyaluronidases.
Drawing on the classic model of balanced affect, the Francis Burnout Inventory (FBI) conceptulises good work-related psychological health among clergy in terms of negative affect being balanced by positive affect. The high scorer on the psychoticism scale is characterised by Eysenck and Eysenck (1976), in their study of psychoticism as a dimension of personality, as being 'cold, impersonal, hostile, lacking in sympathy, unfriendly, untrustful, odd, unemotional, unhelpful, lacking in insight, strange, with paranoid ideas that people were against him.' Lie scales were originally introduced into personality inventories to detect the tendency of some respondents to 'fake good' and so to distort the resultant personality scores (#CITATION_TAG). The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.
Synthetic biology aims at reconstructing life to put to the test the limits of our understanding. The use of this still elusive category permits us to interact with reality via construction of self-consistent models producing predictions which can be instantiated into experiments. While the present theory of information has much to say about the program, with the creative properties of recursivity at its heart, we almost entirely lack a theory of the information supporting the machine. The underlying heuristics explored here is that an authentic category of reality, information, must be coupled with the standard categories, matter, energy, space and time to account for what life is. Locke's account is inadequate because of its failure  to allow any role to an agent who creates or understands the representational relation. Representation occurs mysteriously by means of mediating entities. This inadequacy becomes even more evident in contrast with Aquinas' account of the activity of mind: representation for Aquinas is an act (not an object) of mind. Descartes' conception of 'ideas' is ambivalent between act  and object; and, in examining some of the problems of his account, we shall see the importance of an intentional interpretation of representation: that it is an act of thought about some object. It is not because humans have a sign system that they can achieve 'disengagement' from any particular 'here-and-now' context: other animals also use representative items, but not in the same way as humans do. Something more is required than just the availability of some physical item which can stand in for something else. There are various kinds of signifiers - broadly speaking, those which are used wittingly and those which are  unwittingly used - and what gives a signifier its particular structure is the manner of its use. What kind of act is it, and how is it possible? Trying to match models with reality allowed scientists to progress by producing better and better adequation with reality (Danchin 1992; #CITATION_TAG). A reading of Aquinas  also illuminates the reasons for the extreme passivity of Locke's account, since its origins are clearly to be seen in the Thomist account, but Locke's version has lost the most important feature of the Thomist account. Part II develops the issues raised earlier within the  particular context of signification and semiotic theory. The semiotic theories of Saussure, Peirce .and Piaget are examined, with occasional reference to Frege. Part II, then, deals with questions about the representative item and how it comes to represent other items.
This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. DAFFODIL is a digital library system targeting at strategic support during the information search process. The visualisation of stratagems is based on a strictly object-oriented tool-based model. This discrepancy supports previous studies which point to the fact that linking agrammatism to effects of regularity in the dual mechanism account needs to be revised (De Diego Balaguer et al., 2004; Laiacona and Caramazza, 2004; #CITATION_TAG). nan
In the service computing paradigm, a service broker can build new applications by composing network-accessible services offered by loosely coupled independent providers. In this paper, we address the admission control problem for a a service broker which offers to prospective users a composite service with a range of different Quality of Service (QoS) classes. However, a self-adaptive SOA system has to be carefully designed in order not to compromise the system scalability and availability. In this paper, to overcome the limitations of the aforementioned results we study the admission problem for the MOdel-based SElf-adaptation of SOA systems (MOSES) service broker we proposed in [#CITATION_TAG, 7], which manages a composite service offering differentiated QoS service classes. Theintroductionofself-adaptationandself-management techniques in a service-oriented system can allow to meet in a changing environment the levels of service formally defined with the system users in a Service Level Agreement (SLA). To evaluate theperformance of the brokering service, we have carried out an extensive set of experiments on different implementations of the system architecture using workload generators that are based on open and closed system models.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. El objetivo del presente estudio fue evaluar las propiedades psicometricas del cuestionario de procesos de estudio revisado - 2 factores (CPE-R-2F) en estudiantes de ciencias de la salud en Cartagena, Colombia. Para determinar el numero de factores que explicaban el constructo se condujo analisis de factores (exploratorio). El analisis de factores confirmatorio determino la validez de constructo y el alfa de Cronbach la consistencia interna del instrumento. Learning style (deep or shallow) and selfregulated learning strategies are also relevant, and have been shown to mediate between other factors (such as factors of personality and factors of motivation) and academic performance (#CITATION_TAG; Entwhistle, 2005; Swanberg & Martinsen, 2010). The number of factors that explained the construct was determined using exploratory factor analysis. R-SPQ-2F is a scale with acceptable internal consistency and two-factor structure with questionable construct validity.
OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of "clusters" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). Plant sensitive factor attachment protein receptors (SNAREs) encoded by genes of the same sub-family are generally considered as redundant in promoting vesicle-associated membrane fusion events. Nonetheless, the application of innovative experimental approaches highlighted that members of the same gene sub-family often have different functional specificities. Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (#CITATION_TAG). When transiently overexpressed, the SYP51 and the SYP52 distributed between the TGN and the tonoplast.
Decision-making animals can use slow-but-accurate strategies, such as making multiple comparisons, or opt for simpler, faster strategies to find a 'good enough' option. Social animals make collective decisions about many group behaviours including foraging and migration. The key to the collective choice lies with individual behaviour. We present a case study of a collective decision-making process (house-hunting ants, Temnothorax albipennis), in which a previously proposed decision strategy involved both quality-dependent hesitancy and direct comparisons of nests by scouts. This highlights the need to carefully design experiments to detect individual comparison. This parsimonious mechanism could promote collective rationality in group decision-making. Prior studies of this decision-making process indicate that swarms attempt to use the best-of-N decision rule: sample some number (N) of alternatives and then select the best one. Although a honey bee swarm has bounded rationality (e.g., it lacks complete knowledge of the possible nesting sites), through its capacity for parallel processing it can choose a nest site without greatly reducing either the breadth or depth of its consideration of the alternative sites. In contrast, bees use a graded process, whereby scouts initially discovering a new nest-site almost always recruit [41], but the duration and rate of the recruiting waggle-dances are dependent on nest quality [#CITATION_TAG]. We tested how well swarms implement this decision rule by presenting them with an array of five nest boxes, only one of which was a high-quality (desirable) nest site; the other four were medium-quality (acceptable) sites. A dancing bee tunes her dance strength by adjusting the number of waggle-runs/dance, and she adjusts the number of waggle-runs/dance by changing both the duration and the rate of her waggle-run production. Differences in return-phase duration underlie the impression that dances differ in liveliness.
The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. Some people who have a visible difference (disfigurement) experience psychosocial adjustment problems that can lead to social anxiety and isolation. Applied psychologists, including health, clinical, and counseling psychologists have been at the forefront of developing interventions to support people with psychological needs arising from visible differences (#CITATION_TAG; Bessell et al., 2012b), and in developing a clearer understanding of the differentiating factors and processes between those who adjust well, and those who struggle to cope and manage with differing appearances. Eighty-three participants were assessed at four time points using the Hospital Anxiety and Depression Scales, Derriford Appearance Scale-24, Body Image Quality of Life Inventory and Fear of Negative Evaluation (FNE). A remote-access, computer-based intervention offers the potential to provide psychosocial support more easily and in a cost-effective manner to adults with appearance-related distress.
Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Abstract In recent years, there has been a growing interest in a range of transport policy initiatives which are designed to influence people's travel behaviour away from single-occupancy car use and towards more benign and efficient options, through a combination of marketing, information, incentives and tailored new services. In transport policy discussions, these are now widely described as 'soft' factor interventions or 'smarter choice' measures or 'mobility management' tools. In 2004, the UK Department for Transport commissioned a major study to examine whether large-scale programmes of these measures could potentially deliver substantial cuts in car use. Finally, involving the private sector also reduces the burden of transport policies on the public budget (#CITATION_TAG; Roby, 2010; Rye, 2002). nan
-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. BACKGROUND Controversy remains on whether the dual use of clopidogrel and proton-pump inhibitors (PPIs) affects clinical efficacy of clopidogrel. PATIENTS All patients discharged after first-time myocardial infarction from 2000 to 2006. In the target population, clopidogrel is usually prescribed with aspirin, and it has been suggested that inhibition of antiplatelet effect may result from an interaction of PPIs with aspirin absorption [44, 45], independent of the interaction with clopidogrel [#CITATION_TAG, 47]. DESIGN A nationwide cohort study based on linked administrative registry data. Patients were examined at several assembly time points, including 7, 14, 21, and 30 days after myocardial infarction.
A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. A realizability-notion akin to Kleene's slash [18, 19] was extended to various intuitionistic set theories by Myhill [27, #CITATION_TAG], whereby he also drew on work by Moschovakis [24]. The upshot is that CZF+CC+FT possesses the same strength as CZF, or more precisely, that CZF+CC+FTis conservative over CZF for 02 statements of arithmetic, whereas the addition of a restricted version of bar induction to CZF (called decidable bar induction, BID) leads to greater proof-theoretic strength in that CZF+BID proves the consistency of CZF
Cognitive neuroscience boils down to describing the ways in which cognitive function results from brain activity. Exactly how cognitive function inherits the physical dimensions of neural activity, though, is highly non-trivial, and so are generally the corresponding dimensions of cognitive phenomena. In spite of their general use, these assumptions hold true to a high degree of approximation for many cognitive (viz. fast perceptual) processes, but have their limitations for other ones (e.g., thinking or reasoning). Classical within-subject analysis in functional magnetic resonance imaging (fMRI) relies on a detection step to localize which parts of the brain are activated by a given stimulus type. The originality of this contribution is twofold. By nature, scaling analysis requires the use of long enough signals with high frequency sampling rate. The break-down of scale invariance (Ciuciu et al., 2011 (#CITATION_TAG Zilber et al., 2012) is tantamount to velocity changes. This is usually achieved using model-based approaches. First, we propose a synthetic, consistent, and comparative overview of the various stochastic processes and estimation procedures used to model and analyze scale invariance. Notably, it is explained how multifractal models are more versatile to adjust the scaling properties of fMRI data but require more elaborated analysis procedures. To this end, we make use of a localized 3-D echo volume imaging (EVI) technique, which has recently emerged in fMRI because it allows very fast acquisitions of successive brain volumes. A voxel-based systematic multifractal analysis has been performed over both kinds of data.
It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Chitin is one the most abundant polymers in nature and interacts with both carbon and nitrogen cycles. Processes controlling chitin degradation are summarized in reviews published some 20 years ago, but the recent use of culture-independent molecular methods has led to a revised understanding of the ecology and biochemistry of this process and the organisms involved. The case of chitinases is illustrative (#CITATION_TAG). Principal environmental drivers of chitin degradation are identified which are likely to influence both community composition of chitin degrading bacteria and measured chitin hydrolysis activities.
Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Fluctuating fuel prices, rising congestion, longer commutes, and related environmental and human health effects have combined to once more draw the interest of governments, commuters, and firms toward the concept of travel demand management (TDM). While TDM is not new, the proliferation of mobile telephony, fixed Internet, and associated applications has created fresh prospects for the implementation of commuter focused TDM strategies. One recent example is Carpool Zone, an on-line carpool-matching tool deployed and managed by the TDM group at Metrolinx, the regional transportation planning agency within Canada's largest metropolitan region, the Greater Toronto and Hamilton Area. In the literature most authors distinguish between householdbased and non-household-based carpools which are also called internal and external carpools respectively (#CITATION_TAG; Correia and Viegas, 2011; Ferguson, 1997a; Morency, 2007; Teal, 1987). nan
Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. The use of physical activity monitors in population-based research has increased dramatically in the past decade. However, such study designs have received critique for possibly leaving to optimistic results and should be interpreted with caution [23] [#CITATION_TAG] [25]. We also update and extend previous recommendations for use of these instruments in large-scale studies, particularly with respect to selecting monitor systems in the context of technological advances that have occurred in recent years. A checklist and flowchart are provided so that investigators have more guidance when reporting key elements of monitor use in their studies.
Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. An academic scientist's professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. Likewise, recent interest in data uploading and retention (e.g., #CITATION_TAG) has received little systematic attention in cardiac psychophysiology so far, even though a) data retention is a American Psychological Association requirement (American psychological association [APA], 2001) and b) the ability to broadly access raw data is a potentially excellent control for the methodological and analytical issues outlined here, as well as a test bed for the development of future HRV metrics and meta-analysis. nan
The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. The majority of these GSCM studies, however, use either quantitative approaches and methodologies by collecting and analysing large samples and testing hypotheses and models, or qualitative case studies following grounded theory inspired approaches (Binder and Edwards, 2010; #CITATION_TAG). Design/methodology/approach -To better signify such contribution, it takes insight from Merton's (1968) notion of middle-range theory as a means to create pathways of propositions that link substantive concepts and practices of OM in both context-specific and context-free operational environments.
Reports of firms ' behaviors with regard to corporate social responsibility (CSR) are often contrary to their stated standards of social responsibility. Corporate wrongdoing attracts the attention of the media and watchdog organizations, triggering questions about why companies engage in CSR and how they contribute to social well-being (Bielak et al., 2007; #CITATION_TAG). nan
A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Individuals with bipolar disorder manifest the full spectrum of emotions ranging from depression to mania. The stimuli produced activation in both patients and comparison subjects in brain regions previously implicated in the generation and modulation of affect, in particular the prefrontal and anterior cingulate cortices. The differential patterns of activation inform us about bipolar depression and have potential diagnostic and therapeutic significance. There is decreased activity in the limbic system, especially (Benson et al., 1999) Cornelissen et al., 2013 the amygdala, with dorsolateral prefrontal cortex becoming relatively more active and orbitomedial and cingulate cortex less so; a move toward normality from patterns observed before treatment (Ochsner et al., 2002; Goldapple et al., 2004; #CITATION_TAG; Ritchey et al., 2011; Hflich et al., 2012) and consistent with what is know of the processing of emotional stimuli (Simpson et al., 2000; Northoff et al., 2004; Leppnen, 2006; Beck, 2008). We therefore examined ten depressed female subjects with bipolar affective disorder, and ten age-matched and sex-matched healthy comparison subjects using functional magnetic resonance imaging (fMRI) while viewing alternating blocks of captioned pictures designed to evoke negative, positive or no affective change. The activation paradigm involved the presentation of the same visual materials over three experiments alternating (experiment 1) negative and reference; (experiment 2) positive and reference and (experiment 3) positive and negative captioned pictures. The activation in patients, when compared with healthy subjects, involved additional subcortical regions, in particular the amygdala, thalamus, hypothalamus and medial globus pallidus.
Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. We expect better resolution switching from narrow-band instruments with few channels (Weber et al., 2005) to spectrometers (#CITATION_TAG; Landi et al., 2012b), but it is difficult to achieve a temperature resolution better than  log  0.05 (Landi et al., 2012b). We also test the effects of 4) atomic data uncertainties on the results, and 5) the number of ions whose lines are available for the DEM reconstruction. Also, the DEM curves obtained using lines calculated with an isothermal plasma and with a Gaussian distribution with FWHM of log T = 0.05 are very similar. The availability of small sets of lines also does not worsen the performance of the MCMC technique, provided these lines are formed in a wide temperature range.
Time Series Forecasting (TSF) uses past patterns of an event in order to predict its future values and is a key tool to support decision making. In the last decades, Computational Intelligence (CI) techniques, such as Artificial Neural Networks (ANN) and more recently Support Vector Machines (SVM), have been proposed for TSF. In this work, we propose a novel Evolutionary SVM (ESVM) approach for TSF based on the Estimation Distribution Algorithm to search for the best number of inputs and SVM hyperparameters. Barcelona, Spain.Accurate time series forecasting are important for displaying the manner in which the past continues to affect the future and for planning our day to-day activities. In recent years, a large literature has evolved on the use of evolving artificial neural networks (EANNs) in many forecasting applications. Evolving neural networks are particularly appealing because of their ability to model an unspecified nonlinear relationship between time series variables. In [#CITATION_TAG], EDA was used as the search engine of an EANN, outperforming a GA based EANN. A comparative study between these two methods, with a set of referenced time series will be shown.
INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. The lack of gender parity in philosophy has garnered serious attention recently. Previous empirical work that aims to quantify what has come to be called "the gender gap" in philosophy focuses mainly on the absence of women in philosophy faculty and graduate programs. There are a variety of views about why this is the case, but there is some evidence that implicit bias at various stages in the undergraduate, graduate, and career-level training and selection processes could result in fewer opportunities for advancement for women (#CITATION_TAG Tiberius 2012, Stier and Yaish 2014). First, the biggest drop in the proportion of women in philosophy occurs between students enrolled in introductory philosophy classes and philosophy majors.
Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. Long-term potentiation (LTP) is a synaptic change supposed to provide the cellular basis for learning and memory in brain neuronal circuits. Although specific LTP expression mechanisms could be critical to determine the dynamics of repetitive neurotransmission, this important issue remained largely unexplored. A mathematical model of mossy fiber-granule cell neurotransmission showed that increasing release probability efficiently modulated the first-spike delay. Independent regulation of spike burst initiation and frequency during LTP may provide mechanisms for temporal recoding and gain control of afferent signals at the input stage of cerebellar cortex Whole-cell recordings from GrCs were obtained with patch-clamp technique [20, 29, #CITATION_TAG] by using an Axopatch 200B amplifier (Molecular Devices, Union City, CA, USA) (-3dB; cut-off frequency = 2 kHz). In agreement with a presynaptic expression caused by increased release probability, similar changes were observed by raising extracellular [Ca(2+)]. Glutamate spillover, by causing tonic NMDA and AMPA receptor activation, accelerated excitatory postsynaptic potential (EPSP) temporal summation and maintained a sustained spike discharge.
We report a high-pressure single-crystal synchrotron x-ray diffraction on a LaAlO 3 single crystal. Abstract Algal and fungal symbionts of the lichenized genus Thamnolia typically co-disperse through thallus fragmentation, which may be expected to lead to fungal associations with a restricted range of algal symbionts. Phylogenetic analyses of internal transcribed spacer rDNA (ITS) sequences suggest that Trebouxia algae associated with T. vermicularis are not monophyletic. The pressure scale used was the equation given by Jacobsen [#CITATION_TAG]. Additionally, as a species, T. vermicularis associates with a range of algae equal to or greater than that of many other fungal taxa.
The data stems from studies of searchers interaction with an XML information retrieval system. In this paper we present findings from a comparative study of data collected from client and server side loggings. The purpose is to see what factors of effort can be captured from the two logging methods. Searchers were asked to perform two search sessions, to solve one fact-finding task and one research task, each task was formulated as a simulated work task [#CITATION_TAG]. nan
The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. A U(1) adiabatic phase is created by two laser beams for the tunneling of atoms between neighbor lattice sites. Near these crossing points the quasiparticles and quasiholes can be considered as massless Dirac fermions. The quartz fraction was isolated by density separations (at 2.55 and 2.70 g/cc) using the heavy liquid Na-polytungstate and a 40-min immersion in HF (40%) was applied to etch the outer 10 m of grains, which is affected by alpha radiation (#CITATION_TAG). Furthermore, the anisotropic effects of massless Dirac fermions are obtained in the present square lattice model. The Dirac fermions as well as the anisotropic behaviors realized in our system can be experimentally detected with the Bragg spectroscopy technique.
First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. The global financial crisis (GFC) caused catastrophic losses in the highly regulated banking sector. In contrast, the largely unregulated global hedge fund industry navigated through the crisis relatively unscathed. As a consequence of the GFC, there is the tidal wave of opinion calling for reform of the global financial architecture, with a specific emphasis on tightened oversight of hedge funds. 88 For theoretical treatments of the problem, see Carletti (2006, 2008), #CITATION_TAG (2006). 89 International Monetary Fund (2008a, pp. 65f). nan
Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. The diagnosis of autonomic neuropathy frequently depends on results of tests which elicit reflex changes in heart rate. Few well-documented normal ranges are available for these tests. In view of the decline in heart rate variation with increasing age, normal ranges for tests of autonomic function must be related to the age of the subject. physical activity levels (Britton et al., 2007; Soares-Miranda et al., 2014), and age (#CITATION_TAG). A computerised method of measurement of R-R interval variation was used to study heart rate responses in 310 healthy subjects aged 18-85 years. Normal ranges (90% and 95% confidence limits) for subjects aged 20-75 years were calculated for heart rate difference (max-min) and ratio (max/min) and standard deviation (SD).
Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. Frequency analysis of the electrocardiographic RR interval is a common method of quantifying autonomic outflow by measuring the beat-to-beat modulation of the heart (heart rate variability; HRV). Few papers ideally control for medication, food, and water consumption, bladder filling, time of day, and other extraneous factors (Tak et al., 2009; #CITATION_TAG). nan
Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Secondary pneumococcal pneumonia is a serious complication during and shortly after influenza infection. This increased susceptibility to secondary bacterial pneumonia is at least in part caused by excessive IL-10 production and reduced neutrophil function in the lungs. Subsequent studies have suggested that it is the cytokines released by lymphocytes, such as IFN and IL-10 that mediate the suppressive effects of virus [1, [#CITATION_TAG] [23] [24]. C57BL/6 mice were intranasally inoculated with 10 median tissue culture infective doses of influenza A (A/PR/8/34) or PBS (control) on day 0.
Norm theory's prediction that actions are more mutable than inactions and, as a consequence, will elicit more regret following a negative outcome was tested in two experiments. However, many ideas and methods of Dubrovin (2006) (see also #CITATION_TAG) play an important role in our considerations. Action and inaction mutation frequencies were assessed in a design where both types of behaviors were presented together and when information about only one type was described. In Experiment 1, Kahneman and Tversky's (1982a) original scenario, where the action or inaction was instrumentally related to the negative outcome, was employed.
This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Four political systems and a qualitative index of political rights account for differences in political institutions. Pluralistic systems are associated with higher agricultural protection levels, although in a nonlinear fashion. All studies but one, exploit the cross-country variation in the data and find mixed and often weak evidence on the effect of democracy on agricultural protection (see #CITATION_TAG; Swinnen et al. 2000; Olper, 2001) 4. The analysis incorporates the effects of development, of constraints on tax collection feasibility, and of comparative advantages and terms of trade.
The rapid growth of the literature on neuroimaging in humans has led to major advances in our understanding of human brain function but has also made it increasingly difficult to aggregate and synthesize neuroimaging findings. We verified the distinctness of these two networks using a large meta-analytic resting state database (www.neurosynth.org; #CITATION_TAG), that produced similar (though less extensive) networks, shown in Supplementary Figure S1. nan
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. For example, models predicting academic performance that include factors of motivation (e.g., self-efficacy, goal setting) with cognitive ability yield a lower error variance than models of cognitive ability alone, particularly at tertiary level (reviewed in Boekaerts, 2001; #CITATION_TAG). On the basis of educational persistence and motivational theory models, the PSFs were categorized into 9 broad constructs: achievement motivation, academic goals, institutional commitment, perceived social support, social involvement, academic self-efficacy, general self-concept, academic-related skills, and contextual influences. Two college outcomes were targeted: performance (cumulative grade point average; GPA) and persistence (retention).
Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Biotic interactions and their dynamics influence species' relationships to climate, and this also has important implications for predicting future distributions of species. It is already well accepted that biotic interactions shape species' spatial distributions at local spatial extents, but the role of these interactions beyond local extents (e.g. 10 km(2)  to global extents) are usually dismissed as unimportant. Simplified ecosystems where there are relatively few interacting species and sometimes a wealth of existing ecosystem monitoring data (e.g. arctic, alpine or island habitats) offer settings where the development of modelling tools that account for biotic interactions may be less difficult than elsewhere. Climate influences species distributions directly through species' physiological tolerances or indirectly through its effect on available habitats, food resources and biotic interactions such as the presence of competitors (Ara ujo & Peterson, 2012, #CITATION_TAG. In this review we consolidate evidence for how biotic interactions shape species distributions beyond local extents and review methods for integrating biotic interactions into species distribution modelling tools. A range of species distribution modelling tools is available to quantify species environmental relationships and predict species occurrence, such as: (i) integrating pairwise dependencies, (ii) using integrative predictors, and (iii) hybridising species distribution models (SDMs) with dynamic models. These methods have typically only been applied to interacting pairs of species at a single time, require a priori ecological knowledge about which species interact, and due to data paucity must assume that biotic interactions are constant in space and time. To better inform the future development of these models across spatial scales, we call for accelerated collection of spatially and temporally explicit species data. Ideally, these data should be sampled to reflect variation in the underlying environment across large spatial extents, and at fine spatial resolution.
Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Antigen-presenting cells (APC) are considered to play a critical role in promoting the (re)activation of potentially autoreactive T cells in multiple sclerosis (MS), an inflammatory demyelinating disorder of the central nervous system (CNS). B7-H1 (PD-L1) is a novel member of the B7 family proteins which exert costimulatory and immune regulatory functions. B7-H1-dependent immune inhibition is in part mediated by CD4/CD25+ regulatory T cells. Since type I IFN expression was observed in response to influenza infection and previous studies have suggested that these cytokines could induce PDL1 expression [18, #CITATION_TAG], we investigated whether rhIFN could induce PDL1 gene expression in our MDM model (Fig. 7A). B7-H1 is constitutively expressed on monocytes and differentially matured DC, but not on B cells.
A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. These negative women\u27s stereotypes continue to persist in dominant popular culture, and this doublebind is overcome only by the impossible perfection of vampirism. These constructions are particularly dangerous in Young Adult literature and particularly inspirational in fanfiction There is also an extended version of this theory (#CITATION_TAG) that adds an emotional component. Through close-textual analysis of The Twilight Saga, I demonstrate how the monstrous-feminine frames the hysterical teenage body, hypersexuality, and eternal motherhood as simultaneously unacceptable and unavoidable. The monstrous-feminine invites constructions of teenage bodies as unstable and unreliable, women\u27s sexuality as dangerous and impure, and motherhood as a requirement for a complete identity.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. While there is currently much buzz about the new field of learning analytics [19] and the potential it holds for benefiting teaching and learning, the impression one currently gets is that there is also much uncertainty and hesitation, even extending to scepticism. A clear common understanding and vision for the domain has not yet formed among the educator and research community. Tertiary education providers collect an ever-increasing volume of data on their students, particularly activity data from virtual learning environments and other online resources (#CITATION_TAG). To investigate this situation, we distributed a stakeholder survey in September 2011 to an international audience from different sectors of education. The survey was scaffolded by a conceptual framework on learning analytics that was developed based on a recent literature review. In this article, we first briefly introduce the learning analytics framework and its six domains that formed the backbone structure to our survey.
The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Alluvial rivers can show unpredictable channel changes and humans living along the river corridor repeatedly have to cope with the alterations of their physical environment. This was specifically the case in the mid-sixteenth century, when the Viennese were confronted with one major problem: the Danube River successively abandoned its main arm that ran close to the city and shifted further north. This was at the time when Vienna became the permanent residence of the Holy Roman Empire (except from 1583 to 1611 when it was moved to Prague), due to the Habsburgs holding the crown. Vienna was also the capital of the so-called Danube Monarchy, which came into being in the early sixteenth century. The city assumed increasing significance, being home to and hosting important authorities and persons. In particular after the first siege by the Ottoman army in 1529, the resource need for a complex fortification system was extremely high. As noted by #CITATION_TAG, the increasing number of NHS investigations can be explained in the context of "a return to big government" (p.380). nan
People aging with HIV might have different health conditions compared with people who seroconverted at older ages. The study objective was to assess the prevalence of, and risk factors for, individual co-morbidities and multimorbidity (MM) between HIV-positive patients with a longer duration of HIV infection, and patients who seroconverted at an older age. Better understanding of the intersection of HIV, aging and health is an urgent issue due to the increasing number of people aging with HIV [1, #CITATION_TAG] as the synergistic result of two concurrent phenomenon: the increased life expectancy of people with HIV undergoing HAART, extensively demonstrated both in high [3, 4] and middle-and low-income countries [5, 6], but also the increasing number of people seroconverting HIV at an older age, as the result of a lower perception of sexual risk in older people [7, 8]. nan
Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Due to their powerful knowledge representation formalism and associated inference mechanisms, ontology-based systems are emerging as a natural choice for the next generation of KMSs operating in organizational, interorganizational as well as community contexts. User models, often addressed as user profiles, have been included in KMSs mainly as simple ways of capturing the user preferences and/or competencies. Razmerita et al. [#CITATION_TAG] describe OntobUM, an ontology-based recommender that integrates three ontologies: i) the user ontology, which structures the characteristics of users and their relationships, ii) the domain ontology, which defines the domain concepts and their relationships, and iii) the log ontology, which defines the semantics of the user interactions with the system. We extend this view by including other characteristics of the users relevant in the KM context and we explain the reason for doing this. The proposed user modeling system relies on a user ontology, using Semantic Web technologies, based on the IMS LIP specifications, and it is integrated in an ontology-based KMS called Ontologging. We are presenting a generic framework for implicit and explicit ontology-based user modeling.
The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. A Climate Change Damage Function (CCDF) is a reduced form relationship linking macroeconomic aggregates (e.g., potential GDP) to climate indicators (e.g., average temperature levels). This function is used in a variety of studies about climate change impacts and policy analysis. However, despite the fact that this function is key in determining results in many integrated assessment models, it is not typically calibrated in a consistent and rigorous way. The developments in archaeology are part of broader trends in anthropology and psychology more generally (see e.g., #CITATION_TAG, Cronk et al. 2000, Dunbar & Barrett 2007, Durham 1991, Smith & Winterhalder 1992b, Sperber 1996, but the types of data dealt with by archaeologists and the diachronic questions they generally address have led to an emphasis on some theoretical perspectives rather than others and to the development of specifically archaeological methodologies for obtaining information relevant to testing evolutionary hypotheses. nan
Recent challenges in information retrieval are related to cross media information in social networks including rich media and web based content. In those cases, the cross media content includes classical file and their metadata plus web pages, events, blog, discussion forums, comments in multilingual. This heterogeneity creates large complex problems in cross media indexing and retrieval for services that integrate qualified documents and user generated content together. This paper presents a model and an indexing and searching solution for cross media contents, addressing the above issues, developed for the ECLAP Social Network, in the domain of Performing Arts. The research aimed to cope with the complexity of a heterogeneous indexing semantic model, using stochastic optimization techniques, with tuning and discrimination of relevant metadata terms. In recent years, the growth of Social Network communities has posed new challenges for content providers and distributors. Digital contents and their rich multilingual metadata sets need improved solutions for an efficient content management. There is an increasing need of services for scaling digital services, searching and indexing. Despite the huge amount of contents, users want to easily find relevant unstructured documents in large repositories, on the basis of multilingual queries, with a limited waiting time. At the same time, digital archives have to be fully accessible even if a major restructuring is in progress, or without a significant downtime. The ECLAP information model for cross-media integrates sources coming from 35 different international institutions [9, #CITATION_TAG]. Effectiveness and optimization analysis of the retrieval solution are presented with relevant metrics.
requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Recent analysis of the EUV emission of the solar Active Region observed with TRACE (Sakamoto et al. One further paper relating to adults with non-progressive disorders [#CITATION_TAG] described the use of a laptop computer with word processing software for a male following total glossectomy and laryngectomy. This was considered as a signature of a multiple-strand structure of coronal loops that originates from numerous sporadic coronal heating events (nanoflares). Methods. Simultaneous TRACE and Yohkoh/SXT observations were interpreted by using theoretical predictions of the forward modelling of the nanoflare heating.
This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. We take the approach that it is most useful to report all available model results, since any supposed technical or economic infeasibility can be assessed best in a comparison across model results (see also #CITATION_TAG). We suggest a procedure that addresses this partiality.
Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Nevertheless, evidence suggests that, in constrast to pharmacological interventions, which are characterised by a relatively high relapse risk at 6 months of maintenance treatment [26] [27] [28] [29] [30], the effect of psychological interventions is well-maintained in the long-term, after end of treatment [#CITATION_TAG, 50]. Method In this study 375 patients with social phobia were randomised to treatment with sertraline or placebo for 24 weeks, with or without the addition of exposure therapy Fifty-two weeks after inclusion, 328 patients were evaluated by the same psychometric tests as at baseline and the end of treatment (24 weeks).
Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Several studies suggest an increasing prevalence of multiple sclerosis (MS) in Canada. From 1998 to 2006, the average age- and sex-adjusted annual incidence of MS per 100,000 population was 11.4 (95% confidence interval [CI] 10.7-12.0). The age-adjusted prevalence of MS per 100,000 population increased from 32.6 (95% CI 29.4-35.8) in 1984 to 226.7 (95% CI 218.1-235.3) in 2006, with the peak prevalence shifting to older age groups.The prevalence of multiple sclerosis (MS) in Manitoba is among the highest in the world. We aimed to estimate the incidence and prevalence of MS in BC, Canada using previously validated case definitions of MS [#CITATION_TAG, 11] based on health administrative data. To validate the case definition, questionnaires were mailed to 2,000 randomly selected persons with an encounter for demyelinating disease, requesting permission for medical records review. We used diagnoses abstracted from medical records as the gold standard to evaluate candidate case definitions using administrative data.From 1984 to 1997, cases of MS using claims data were defined as persons with > or = 7 medical contacts for MS. From 1998 onward, cases were defined as persons with > or = 3 medical contacts.
The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. Numerous geomorphic studies along coasts of the European North Atlantic implicate increased storminess in the Holocene with the reactivation of coastal dune system (e.g., Clarke and Rendell, 2009; #CITATION_TAG). We demonstrate how to create artificial external non-Abelian gauge potentials acting on cold atoms in optical lattices. The method employs atoms with k internal states, and laser assisted state sensitive tunneling, described by unitary k x k matrices. The single-particle dynamics in the case of intense U2 vector potentials lead to a generalized Hofstadter butterfly spectrum which shows a complex mothlike structure.
Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. Tensor models can be interpreted as theory of dynamical fuzzy spaces. It is found that the momentum distribution of the low-lying low-momentum spectra is in agreement with that of the metric tensor modulo the general coordinate transformation in the general relativity at least in the dimensions studied numerically, i.e. 18, and the subsequent studies mainly in numerical methods have supported the validity of this basic idea [19] [#CITATION_TAG] [21] [22] [23] [24] [25] [26]. nan
Phonological errors were scarce in both groups. In order for such comparisons to be meaningful, however, the instruments used to measure the theoretical constructs of interest have to exhibit adequate cross-national equivalence. For example, closed class words are semantically 'shallow', having fewer semantic features than open class items, which could make them more vulnerable to damage within the semantic system or between semantics and word form representations (Allen & Seidenberg, 1999; Goodglass & Menn, 1985; #CITATION_TAG). We review the various forms of measurement invariance that have been proposed in the literature, organize them into a coherent conceptual framework that ties different requirements of measure equivalence to the goals of the research, and propose a practical, sequential testing procedure for assessing measurement invariance in cross-national consumer research. The approach is based on multisample confirmatory factor analysis and clarifies under what conditions meaningful comparisons of construct conceptualizations. construct means, and relationships between constructs are possible. An empirical application dealing with the single-factor construct of consumer ethnocentrism in Belgium, Great Britain, and Greece is provided to illustrate the procedure.
This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service "free at the point of access", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland "outcomes frameworks" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. Priebe et al (#CITATION_TAG) has reported the results of subjective quality of life items in DIALOG (see Table 1), a structured communication tool in mental health services. Abstract Background DIALOG is an intervention to structure the communication between patient and key worker, which has been shown to improve patient outcomes in community mental health care. As part of the intervention, patients provide ratings of their subjective quality of life (SQOL) on eight Likert type items and their treatment satisfaction on three such items. Method Data were taken from 271 patients who received the DIALOG intervention. All patients were diagnosed with schizophrenia or a related disorder and treated in community mental health care. For SQOL and treatment satisfaction as assessed in the DIALOG intervention, we established the internal consistency (Cronbach's alpha), the convergent validity of SQOL items (correlation with Manchester Short Assessment of Quality of Life [MANSA]) and treatment satisfaction items (correlation with Client Satisfaction Questionnaire [CSQ]), the concurrent validity (correlations with Positive and Negative Syndrome Scale [PANSS]) and the sensitivity to change (t-test comparing ratings of the first and last intervention). We also explored the factorial structure of the eight SQOL items. SQOL items loaded on two meaningful factors, one capturing health and personal safety and one reflecting other life domains.
Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Recommender systems have the effect of guiding users in a personalized  way to interesting objects in a large space of possible options. Content-based  recommendation systems try to recommend items similar to those a given user has  liked in the past. Indeed, the basic process performed by a content-based recommender  consists in matching up the attributes of a user profile in which preferences  and interests are stored, with the attributes of a content object (item), in order to  recommend to the user new interesting items. Content-based recommender systems [#CITATION_TAG] rely on a pre-existing domain knowledge to suggest items more similar to the ones that the user seems to like. The first part of the chapter presents the basic concepts and terminology of contentbased  recommender systems, a high level architecture, and their main advantages  and drawbacks. The second part of the chapter provides a review of the state of  the art of systems adopted in several application domains, by thoroughly describing  both classical and advanced techniques for representing items and user profiles. The most widely adopted techniques for learning user profiles are also presented. The last part of the chapter discusses trends and future research which might lead  towards the next generation of systems, by describing the role of User Generated  Content as a way for taking into account evolving vocabularies, and the challenge  of feeding users with serendipitous recommendations, that is to say surprisingly  interesting items that they might not have otherwise discovered
Conditions such as asterixis, epilepsia partialis continua, clonus, and rhythmic myoclonus can be misinterpreted as tremor. Generally rest tremor is considered specific for PD, while postural and action (or intentional) tremor are attributed to essential tremor (ET), although exceptions to this rule are clearly reported [#CITATION_TAG] [5] [6]. The classification is based on the distinction between rest, postural, simple kinetic, and intention tremor (tremor during target-directed movements). Additional data from a medical history and the results of a neurologic examination can be combined into one of the following clinical syndromes defined in this statement: enhanced physiologic tremor, classical essential tremor (ET), primary orthostatic tremor, task- and position-specific tremors, dystonic tremor, tremor in Parkinson's disease (PD), cerebellar tremor, Holmes' tremor, palatal tremor, drug-induced and toxic tremor, tremor in peripheral neuropathies, or psychogenic tremor. The features distinguishing these conditions from tremor are described. Controversial issues are outlined in a comment section for each item and thus reflect the open questions that at present cannot be answered on a scientific basis.
In their formulation, the causal conjuncture is a sequence of conditions or events. The main aim of the technique is to identify all necessary and sufficient conditions that lead to a specific outcome condition (#CITATION_TAG). This comment clarifies and corrects aspects of their analysis and present methods for assessing temporality that are more amenable to truth table analysis and the use of existing software, fsQCA. The methods presented utilize codings that indicate event order in addition to codings that indicate whether specific events occurred. They also demonstrate how to use ``don't care'' codings to bypass consideration of event sequences when they are not relevant (e.g., as when only a single event occurs).
Ternary algebras, constructed from ternary commutators, or as we call them, ternutators, defined as the alternating sum of products of three operators, have been shown to satisfy cubic identities as necessary conditions for their existence. The subject of ternary algebras, a special case of n-Lie algebras is generally attributed to Fillipov [#CITATION_TAG], but Filippov was following up on earlier studies that had appeared in the mathematics literature, primarily by Kurosh [2] (as remarked in [3]). nan
The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Stice's (1994, 2001) dual pathway model proposed a mediational sequence that links body dissatisfaction to lack of control over eating through dieting and negative affect. Van Strien et al. Although there is evidence that initial dietary restraint levels predict future onset of binge eating among asymptomatic individuals [24, 56, #CITATION_TAG], as in this study, prior research using clinical interviews or ecological momentary assessment failed to support the dietary restraint-binge eating relationship among bulimic-type ED patients [9, [58] [59] [60]. (2005) extended the negative affect pathway of the original dual pathway model by adding two additional intervening variables: interoceptive deficits and emotional eating. Data collected from 361 adolescent girls, who were interviewed and completed self-report measures annually over a 2-year period, were analysed using structural equation modeling.
-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. Patients receiving dual antiplatelet treatment with aspirin and clopidogrel are commonly treated with proton pump inhibitors (PPIs). Attenuating effects on platelet response to clopidogrel have been reported solely for the PPI omeprazole. Whether this occurs with all PPIs or is even of significant amplitude with omeprazole remains a matter of debate [9, [24] [25] [26] [27] [28] [#CITATION_TAG]. PPIs differ in their metabolisation properties as well as their potential for drug-drug interactions. In a cross-sectional observational study, consecutive patients under clopidogrel maintenance treatment (n = 1,000) scheduled for a control coronary angiography were enrolled. Adenosine diphosphate (ADP)-induced platelet aggregation (in AU*min) was measured with multiple electrode platelet aggregometry (MEA). Attenuating effects of concomitant PPI treatment on platelet response to clopidogrel were restricted to the use of omeprazole. Specifically designed and randomized clinical studies are needed to define the impact of concomitant PPI treatment on adverse events after percutaneous coronary intervention.
Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Considering spatial structure, the more congested downtown areas, associated with a high transit access, less parking availability and higher parking costs, are stronger correlated with a higher use of SOV alternatives (#CITATION_TAG). We review the literature on the following topics: 1) employee ridesharing behavior and attitudes, 2) relationships between workplace characteristics and ridesharing behavior, 3) impacts of public programs on ridesharing behavior and, 4) effectiveness of employer-based ridesharing programs. We begin with a brief introduction on the origins of the current policy interest in ridesharing and the development of Regulation XV.
The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. Mon and Sekkat (2005), Pellegrini and Gerlagh (2004), #CITATION_TAG, and Mauro (1995) find that it leads to lower investment. It starts from an endogenous growth model and extends it to account for the detrimental effects of corruption on the potentially productive components of government spending, namely military and investment spending. Second, allowing for the cyclical economic fluctuations in specific countries leaves the estimated elasticities close to those of the full sample. Third, there are significant conditioning variables that need to be taken into account, namely the form of  government, political instability and natural resource endowment.
Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. The demand for aggregation in evaluating managerial performance arises because reporting all the basic transactions and other nonfinancial information about performance is costly and impracticable (see Ashton [1982], Casey [1978], and Holmstrom and Milgrom [1987]). A buyer generally tries to avoid dependence on a particular supplier (#CITATION_TAG) but companies today tend to trade in some of their independence against cost savings by having fewer, heavily bound, high value suppliers (Swift & Coe, 1994). We identify necessary and sufficient conditions on the joint density function of the signals under which linear aggregation, a simple and commonly employed way to construct a performance evaluation measure, is optimal. This characterization suggests that the linear form of aggregation is optimal for a large class of situations. We interpret these weights in terms of statistical characteristics (sensitivity and precision) of the joint distribution of the signals.
Previous research has shown that people form impressions of potential leaders from their faces and that certain facial features predict success in reaching prestigious leadership positions. However, much less is known about the accuracy or meta-accuracy of face-based leadership inferences. Here we examine a simple, but important, question: Can leadership domain be inferred from faces? However, people are surprisingly bad at evaluating their own performance on this judgment task: We find no relationship between how well judges think they performed and their actual accuracy levels. Recent research has shown that rapid judgments about the personality traits of political candidates, based solely on their appearance, can predict their electoral success. This suggests that voters rely heavily on appearances when choosing which candidate to elect. Yet, the human mind often relies on superficial cues to form judgments or make decisions, and the choice of which leader to select is no exception: A large and growing literature shows that facial appearances predict success in reaching prestigious leadership positions (Antonakis & Jacquart, 2013; #CITATION_TAG). We also reanalyze previous data to show that facial competence is a highly robust and specific predictor of political preferences. Finally, we introduce a computer model of face-based competence judgments, which we use to derive some of the facial features associated with these judgments.
Solar radiation and ambient temperature have acted as selective physical forces among populations and thereby guided species distributions in the globe. Circadian clocks are universal and evolve when subjected to selection, and their properties contribute to variations in fitness within specific environments. Because of their position in the hierarchy and repressive actions, cryptochromes are the key components of the feedback loops on which circadian clocks are built. Sleep deprivation (SD) results in increased electroencephalographic (EEG) delta power during subsequent non-rapid eye movement sleep (NREMS) and is associated with changes in the expression of circadian clock-related genes in the cerebral cortex. The increase of NREMS delta power as a function of previous wake duration varies among inbred mouse strains. It is strengthened further by those which show that the CRY2 gene expression is abnormal when inbred-strain mice with the intrinsic level of high anxiety are deprived of sleep (#CITATION_TAG), and when humans with bipolar type 1 disorder do remain depressed after the antidepressant sleep deprivation (Lavebratt et al., 2010). Cortical expression of clock genes subsequent to SD was proportional to the increase in delta power that occurs in inbred strains: the strain that exhibits the most robust EEG response to SD (AKR/J) exhibited dramatic increases in expression of bmal1, clock, cry2, csnkIepsilon, and npas2, whereas the strain with the least robust response to SD (DBA/2) exhibited either no change or a decrease in expression of these genes and cry1.
The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. The type of corruption charge is an important determinant of vote loss. Welch and Hibbing (1997), Dimock and Jacobson (1995), and #CITATION_TAG provide similar evidence in the case of U.S. Chang et al. (2010), Fernndez-Vzquez and Rivero (2010), Manzetti and Wilson (2007) and Golden (2006) offer some explanations as to why this is so. This assessment includes a consideration of the victory or defeat of alleged or convicted corrupt candidates, and an examination of the impact of corruption charges on electoral turnout and percentage of votes polled by the accused candidates.
Especially in northern Wisconsin, bogs are relatively unaffected by humans, but naturally comprise \1% of the landscape. Bog specialist species composition varied by bog type (muskeg, kettlehole, coastal peatland). A number of bog specialists frequently occurred in numerous examples of bogs, including all three types. But virtually no specialist individuals occurred in nearby upland roadsides. Bogs have different vegetation types superimposed on each other, including bog, heath, forest, sedge meadow, and wet meadow associates in the same spots. Conservation management needs to avoid simplifying the vegetation to one layer, reducing specialist fauna. A fundamental lesson may be that aiming to conserve typical ecosystems, even if native, and their average processes, leads to average (generalist) butterflies. Looking for qualified reading sources? We have the vegetation of wisconsin an ordination of plant communities to review, not just check out, yet likewise download them and even read online. Once more, never miss out on to review online and download this publication in our website below. We have got a considerable collection of totally free of expense Book for people from every single stroll of life. Searching for most offered book or reading resource worldwide? Well, merely read online or download by registering in our site here. Although often viewed as a long-lived successional stage between open water and forest in glaciated landscapes, peatlands can get reset to an earlier successional stage (#CITATION_TAG). Obtain the documents in the types of txt, zip, kindle, word, ppt, pdf, and also rar. GO TO THE TECHNICAL WRITING FOR AN EXPANDED TYPE OF THIS THE VEGETATION OF WISCONSIN AN ORDINATION OF PLANT COMMUNITIES, ALONG WITH A CORRECTLY FORMATTED VERSION OF THE INSTANCE MANUAL PAGE ABOVE.
When reading pro and con arguments, participants (Ps) counterargue the contrary arguments and uncritically accept supporting arguments, evidence of a disconfirmation bias. Many disciplines discuss skepticism, including politics (e.g., #CITATION_TAG), philosophy (e.g., McGrath, 2011), sociology (e.g., Freudenburg et al., 2008), and psychology (e.g., Lilienfeld, 2012). Two experimental studies explore how citizens evaluate arguments about affirmative action and gun control, finding strong evidence of a prior attitude effect such that attitudinally congruent arguments are evaluated as stronger than attitudinally incongruent arguments. We also find a confirmation bias--the seeking out of confirmatory evidence--when Ps are free to self-select the source of the arguments they read.
In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Where it once emphasized the importance of representational multiplicity and centralized integration (e.g. #CITATION_TAG; Gregory, 1984) it now gives as much weight to exploitation of scaffolding and embodiment (e.g. Wheeler, 1994; Beer, 2000). And where it once committed to symbolic reasoning being the medium of high-level integration (e.g. Marr, 1977; Boden, 1977; Winston, 1984) it increasingly recognizes the greater potential (and neural plausibility) of probabilistic forms (e.g. Doya et al., 2007; Chater and Oaksford, 2009; Clark, 2008). The description of the main components and mechanisms of the architecture is followed by a discussion of several domains where CHREST has already been successfully applied, such as the psychology of expert behaviour, the acquisition of language by children, and the learning of multiple representations in physics. The characteristics of CHREST that enable it to account for empirical data include: self-organisation, an emphasis on cognitive limitations, the presence of a perception-learning cycle, and the use of naturalistic data as input for learning.
The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. The availability of sand for eolian transport is often mediated by the type and extent of vegetation cover on dunes, and may be further controlled by moisture availability, edaphic associations (cf. related to soil processes) and landscape disturbance (e.g., Sala et al., 1988; Mangan et al., 2004; Hugenholtz and Wolfe, 2005; #CITATION_TAG). By controlling the lattice anisotropy, one can realize both massive and massless Dirac fermions and observe the phase transition between them. Through explicit calculations, we show that both the Bragg spectroscopy and the atomic density profile in a trap can be used to demonstrate the Dirac fermions and the associated phase transition.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. E-Learning is embedded in learning and, without an understanding of what learning encompasses, it can be difficult for academics to develop into good teachers. It is suggested that, although this may appear to be a simple aim, it is not necessarily understood or applied by university academics in their teaching. Academics may have a 'philosophy of teaching', but in many cases even this may not be consciously held or successfully implemented. Openness, conscientiousness, and intrinsic motivation are correlated with a deep learning approach, while neuroticism, agreeableness, and extrinsic motivation are associated with a shallow learning approach (Busato et al., 1999; Duff et al., 2004; #CITATION_TAG). One inference is that university teachers need to develop a theory of learning and teaching.
This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. Solution techniques for location-allocation problems usually are not a part of microcomputer-based geoprocessing systems because of the large volumes of data to process and store and the complexity of algorithms. In Type II and Type III, the fixed costs were generated from discrete uniform distributions in the range of [2, #CITATION_TAG] for 50 fixed points, [50, 400] for 287 fixed points, [1000, 10000] for 654 fixed points, and [10000, 100000] for 1060 fixed points, see Luis (2008) for more details. The strategies used, preprocessing interpoint distance data as both candidate and demand strings, and use of them to update an allocation table, allow the solution of large problems (3000 nodes) in a microcomputer-based, interactive decisionmaking environment. Moreover, these strategies yield solution times which increase approximately linearly with problem size.
The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. The respective contribution of occupational and behavioural factors to social disparities in all-cause mortality has been studied very seldom. Occupational factors played a substantial role in explaining social disparities in mortality, especially for premature mortality and men. The key value of requirements in this context was also recently highlighted by #CITATION_TAG who demonstrated that requirements are socially constructed in a political context. Mortality was derived from register-based information and linked to the baseline data. Socioeconomic status was measured using occupation.
Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Over the years, the prevalence of auditory verbal hallucinations (AVHs) have been documented across the lifespan in varied contexts, and with a range of potential long-term outcomes. Initially the emphasis focused on whether AVHs conferred risk for psychosis. However, recent research has identified significant differences in the presentation and outcomes of AVH in patients compared to those in non-clinical populations. For this reason, it has been suggested that auditory hallucinations are an entity by themselves and not necessarily indicative of transition along the psychosis continuum. In children, need for care depends upon whether the child associates the voice with negative beliefs, appraisals and other symptoms of psychosis. For example, the existence of maladaptive coping strategies in patient populations is one significant difference between clinical and non-clinical groups which is associated with a need for care. Whether or not these mechanisms start out the same and have differential trajectories is not yet evidenced. Furthermore, childhood trauma and childhood sexual abuse is more often experienced by females (Janssen et al., 2004; Molnar et al., 2001) and is also one of the strongest risk factors for developing clinically impairing AVH and psychotic symptoms (Daalman et al., 2012; #CITATION_TAG; Janssen et al., 2004). The stages described include childhood, adolescence, adult non-clinical populations, hypnagogic/hypnopompic experiences, high schizotypal traits, schizophrenia, substance induced AVH, AVH in epilepsy, and AVH in the elderly. This includes features of the voices such as the negative content, frequency, and emotional valence as well as anxiety and depression, independently or caused by voices presence.
Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Abstract Recognition of the importance of land-use history and its legacies in most ecological systems has been a major factor driving the recent focus on human activity as a legitimate and essential subject of environmental science. Ecologists, conservationists, and natural resource policymakers now recognize that the legacies of land-use activities continue to influence ecosystem structure and function for decades or centuries--or even longer--after those activities have ceased. As a result, environmental history emerges as an integral part of ecological science and conservation planning. Current biodiversity and ecosystem services are conditioned by history, regional context and continuity (#CITATION_TAG). nan
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The Health Development Agency (www.hda.nhs.uk) is the national authority and information resource on what works to improve people's health and reduce health inequalities in England. iii Contents Foreword iv Summary 1 What is the role of qualitative approaches in traditional trials and experimental studies? 4 At what point in the development of a field of knowledge is it appropriate to pull qualitative and quantitative learning together? 7 Are there hierarchies of evidence within the different types of qualitative investigation? 11 Advice from the NHS Centre for Reviews and Dissemination 11 The interpretive/integrative distinction 11 Narrative summary 12 Thematic analysis 15 Grounded theory 15 Meta-ethnography 17 Estabrooks, Field and Morse's aggregation of findings approach 19 Qualitative meta-analysis 19 Qualitative meta-synthesis 19 Meta-study 21 Miles and Huberman's cross-case data analysis techniques 22 Content analysis 23 Case survey 24 Qualitative comparative analysis 24 Bayesian meta-analysis 25 Meta-needs assessment 27 Discussion 28 Quantitising and qualitising 28 Issues in qualitative synthesis 28 Conclusions 31 References 32 Quant-and-qual.indd 3/26/2004, 10:10 AM 3 iv In 2000 the Health Development Agency (HDA) was established to, among other things, build the evidence base in public health, with particular reference to reducing inequalities in health (Department of Health, 2001). Since then the HDA has been engaged in a programme of developing methodologies and protocols to do precisely that (Swann et al., 2002). There are two important ideas behind the HDA's remit, political and scientific. The political imperative is a clear commitment to tackling the long-term problem at the heart of public health - that, as the health of the population as a whole continues to improve, at the same time the gradient in inequalities in health across the population, from the most to the least advantaged becomes worse (Acheson, 1998) There is, in other words, a long-standing problem of inequalities in health. The scientific principle is that the best available evidence should be used in order to ... [#CITATION_TAG, 18] By integrating individual qualitative research studies into a qualitative synthesis, new insights and understandings can be generated and a cumulative body of empirical work produced. [19] Such syntheses have proven useful to health policy and practice. 6 Generating hypotheses and questions 6 Informing the selection of outcomes for review 6 Extending or guiding sampling 6 Providing explanations and informing conclusions 6 What constitutes good evidence from qualitative studies?
Phonological errors were scarce in both groups. Extant literature and suppliers interviewed for this study view a solution as a customized and integrated combination of goods and services for meeting a customer's business needs. In contrast, customers view a solution as a set of customer-supplier relational processes comprising (1) customer requirements definition, (2) customization and integration of goods and/or services and (3) their deployment, and (4) postdeployment customer support, all of which are aimed at meeting customers' business needs. The relational process view can help suppliers deliver more effective solutions at profitable prices. Supplier variables include c... Speech errors produced by different aphasic patient groups have formed the basis of debates about particular deficits and how they relate to the intact language system (Bates & Wulfeck, 1989; #CITATION_TAG; Rapp & Goldrick, 2000, Goldrick, 2006. nan
Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. Extracting data elements such as study descriptors from publication full texts is an essential step in a number of tasks including systematic review preparation (#CITATION_TAG), construction of reference databases (Kleinstreuer et al., 2016), and knowledge discovery (Smalheiser, 2012). We estimate and partition genetic variation for height, body mass index (BMI), von Willebrand factor and QT interval (QTi) using 586,898 SNPs genotyped on 11,586 unrelated individuals. We show that the variance explained by each chromosome is proportional to its length, and that SNPs in or near genes explain more variation than SNPs between genes.
Synthetic biology aims at reconstructing life to put to the test the limits of our understanding. The use of this still elusive category permits us to interact with reality via construction of self-consistent models producing predictions which can be instantiated into experiments. While the present theory of information has much to say about the program, with the creative properties of recursivity at its heart, we almost entirely lack a theory of the information supporting the machine. The underlying heuristics explored here is that an authentic category of reality, information, must be coupled with the standard categories, matter, energy, space and time to account for what life is. It is argued that the account of Savage-Rumbaugh's ape language research in Savage-Rumbaugh, Shanker and Taylor (1998. Apes, Language and the Human Mind. Oxford University Press, Oxford) is profitably read in the terms of the theoretical perspective developed in Clark (1997. The authors, though, make heavy going of a critique of what they take to be standard approaches to understanding language and cognition in animals, and fail to offer a worthwhile theoretical position from which to make sense of their own data. A great many works dealing with the study of the brain (Edelman 1987), or of cognition (#CITATION_TAG; Ryle 1949) has taken into account this type of information. This model of 'distributed' cognition helps makes sense of the lexigram activity of Savage-Rumbaugh's subjects, and points to a re-evaluation of the language behaviour of humans
Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. We study the general structure of formal perturbative solutions to the Hamiltonian perturbations of spatially one-dimensional systems of hyperbolic PDEs vt + [ph(v)]x = 0. Under certain genericity assumptions it is proved that any bi-Hamiltonian perturbation can be eliminated in all orders of the perturbative expansion by a change of coordinates on the infinite jet space depending rationally on the derivatives. Total charge transfer was calculated by measuring IPSCs area [#CITATION_TAG]. We also describe, following [35], the invariants of such bi-Hamiltonian structures with respect to the group of Miura-type transformations depending polynomially on the derivatives.
In poor countries, labor productivity in agriculture is considerably lower than in the rest of the economy. Third, labor productivity in agriculture is severely mis-measured in the US. Its unique feature is that it gives rise to an endogenously variable number of sectors. For example, Adamopoulous and Restuccia (2014) and Donovan (2014) point to the scale or risk of farming; Restuccia et al. (2008) and #CITATION_TAG to barriers of moving workers or intermediate goods between agriculture and non-agriculture; and Lagakos and Waugh (2013) to selection of the workers in the two sectors. Each new sector is created by a pervasive innovation, which creates a new market and into and out of which there are entry and exit of firms. In the construction of our model we found inspiration in a number of growth models, both endogenous and evolutionary as well as on empirical work on structural change. Within our model the ability to create new sectors at the right times is the crucial determinant of the growth potential of an economic system.
It draws upon recent social and archaeological theory around embodied memory; in particular, Connerton's (1989) division of memory claims into three kinds. It is argued that cognitive memory claims and habit-memory should be regarded as aspects of the same process of remembering; following Gell (1998) and Jones (2007), physical traces of past action are regarded as central to this act of memory. It is argued that all three share some of the attributes of a ritual performance. This article is concerned with archaeological evidence for the mechanisms by which group memory is transmitted. In general, a classical solution in a tensor model may be physically regarded as a background space, and small fluctuations about the solution as emergent fields on the space. The numerical analyses of the tensor models possessing Gaussian classical background solutions have shown that the low-lying long-wavelength fluctuations around the backgrounds are in one-to-one correspondence with the geometric fluctuations on flat spaces in the general relativity. It has also been shown that part of the orthogonal symmetry of the tensor model spontaneously broken by the backgrounds can be identified with the local translation symmetry of the general relativity. Human remains occur over a long timescale at many prehistoric cave sites, particularly from the Neolithic onwards (Chamberlain, 1996; #CITATION_TAG; Leach, 2008; Schulting, 2007). Thus the tensor model provides an interesting model of simultaneous emergence of space, the general relativity, and its local gauge symmetry of translation.
Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. Perhaps paradoxically, however, there is still little understanding of the circumstances under which certain methods will perform better than others. and is reported in [#CITATION_TAG], in a meta-analysis of classification studies, as being used in 'the vast majority' of comparative studies of classification rules (p19). Numerous methods have been designed in the past twenty years or so, but by different communities who had different approaches and interests. Comparative studies have thus been carried out to assess the relative merits of the methods.
OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of "clusters" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). In eukaryotic endomembrane systems, Qabc-SNAREs (soluble N-ethylmaleimide-sensitive factor attachment protein receptors) on one membrane and R-SNARE on the opposing membrane assemble into a trans-QabcR-SNARE complex to drive membrane fusion. However, it remains ambiguous whether pairing of Qabc- and R-SNAREs mediates membrane fusion specificity. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (#CITATION_TAG) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). nan
The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The Semantic Web presents the vision of a distributed, dynamically growing knowledge base founded on formal logic. Common users, however, seem to have problems even with the simplest Boolean expression. As queries from web search engines show, the great majority of users simply do not use Boolean expressions. Ginseng relies on a simple question grammar which gets dynamically extended by the structure of an ontology to guide users in formulating queries in a language seemingly akin to English. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [3] [#CITATION_TAG] 6, 9, 13, 14, 16]. We address this problem by presenting Ginseng, a quasi natural language guided query interface to the Semantic Web. Based on the grammar Ginseng then translates the queries into a Semantic Web query language (RDQL), which allows their execution.
It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. By online practicing, children were more involved in the storytelling and language acquisition process, chances rarely available in the regular class. In a similar vein, it has been argued that story genres are considered some of the most suitable for students learning a second language because of their emphasis on action and events, their strong tradition of oral, embodied performance, and their concern with common themes (#CITATION_TAG; Pennington, 2009; Tsou, Wang, & Tzeng, 2006). VoiceThread is used to present the story with audio and visual aids to help children review keywords and read aloud simple summarizing sentences. Twenty Taiwanese EFL children determined learning-at-risk took part in this one-year project; survey questionnaires, storytellers' ethnographic notes, and teacher interviews were collected to examine the progress in terms of their changes in attitude, motivation, and responsiveness to storytelling and English learning.
Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Despite its relatively recent emergence over the past few decades, oral health-related quality of life (OHRQoL) has important implications for the clinical practice of dentistry and dental research. It has wide-reaching applications in survey and clinical research. OHRQoL is an integral part of general health and well-being. In fact, it is recognized by the World Health Organization (WHO) as an important segment of the Global Oral Health Program (2003). [98]#CITATION_TAG[100] Marshman and colleagues 101 have expressed concern that generic measures of OHQoL might not fully capture the impact that malocclusion has suggested the development of a malocclusion specific measure of OHQoL for young people with malocclusion OHRQoL is a multidimensional construct that includes a subjective evaluation of the individual's oral health, functional well-being, emotional well-being, expectations and satisfaction with care, and sense of self. A supplemental Appendix contains a Medline and ProQuest literature search regarding OHRQoL research from 1990-2010 by discipline and research design (e.g., descriptive, longitudinal, clinical trial, etc.). The search identified 300 articles with a notable surge in OHRQoL research in pediatrics and orthodontics in recent years.
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The idea that most smokers quit without formal assistance is widely accepted, however, few studies have been referenced as evidence. Unassisted quit attempts ranged from a high of 95.3% in a study in Christchurch, New Zealand, between 1998 and 1999, to a low of 40.6% in a national Australian study conducted between 2008 and 2009. However, across and within countries over time, it appears that there is a trend toward lower prevalence of making quit attempts without reported assistance or intervention.Copyright (c) 2013 Elsevier Ltd. All rights reserved. [9, #CITATION_TAG] Many smokers also appear to quit unplanned as a consequence of serendipitous events, [11] throwing into question the predictive validity of some of these cognitive models. nan
The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. Location The eight study sites are located in southern New England in the states of Massachusetts and Connecticut. Tsuga canadensis and Fagus grandifolia are abundant in the upland area, while Quercus, Carya and Pinus species have higher abundances in the lowlands. Lacustrine sediment cores from the northeastern U.S. show a decline in Hemlock (Tsuga sp. ), a decrease in lake level, or an increase in clastic sedimentation ca. 5.5-3.8 ka and is associated with a broad scale drying (e.g., Newby et al., 2000; Shuman et al., 2001; Shuman and Donnelly, 2006; #CITATION_TAG; Marsicek et al., 2013). The sites span a climatic and vegetational gradient from the lowland areas of eastern Massachusetts and Connecticut to the uplands of north-central and western Massachusetts. Methods We collected sediment cores from three lakes in eastern and north-central Massachusetts (Berry East, Blood and Little Royalston Ponds). Pollen records from those sites were compared with previously published pollen dat
Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Subjects often rated harmful omissions as less immoral, or less bad as decisions, than harmful commissions. One possibility is this is related to an increased sense of causality and personal responsibility associated with overt actions [#CITATION_TAG]. Subjects read scenarios concerning pairs of options. One option was an omission, the other, a commission. Intentions, motives, and consequences were held constant. Subjects either judged the morality of actors by their choices or rated the goodness of decision options. Such ratings were associated with judgments that omissions do not cause outcomes. The 'omission bias ' revealed in these experiments can be described as an overgeneralization of a useful heuristic to cases in which it is not justified.
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The reality is that one needs to be motivated to prompt action to stop smoking, but this is not sufficient in and of itself to ensure that cessation is maintained. [#CITATION_TAG, 54] Motivation has been identified as critical to explaining cessation success. METHODS Data are from three wave-to-wave transitions of the International Tobacco Control Four (ITC-4) country project. Smokers' responses at one wave were used to predict the likelihood of making an attempt and among those trying the likelihood of maintaining an attempt for at least a month at the next wave. For both outcomes, hierarchical logistic regressions were used to explore the predictive capacity of seven measures of motivation to quit smoking, controlling for a range of other known or possible predictors.
In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. The role of local governments in attracting roots tourists is one of most important factors analyzed in the studies of diaspora tourism. Governments of several countries have actively sought to promote varied forms of roots tourism in order to attract members of their respective diasporas. In contrast, African American roots tourism in Brazil is marked by the almost complete inaction of the government, at both the state and federal levels. This type of tourism was initiated and continues to develop largely as the result of tourist demand, and with very little participation on the part of the state. Our hope is that, by focusing on 'agency' alongside 'structure' (Orlikowksi and Barley, 2001), more light will be shed on the process of technological and organisational change (#CITATION_TAG) such that, eventually, the diffusion of egovernment will be better understood. The chapter also analyzes whether the left-leaning Workers' Party, then in charge of the state government, challenged the longstanding discourse of baianidade (Bahianness) that has predominantly represented blackness (in tourism and other realms) through domesticated and stereotypical images.
Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. The electron plasma frequency $\omega_{pe}$ and electron gyrofrequency $\Omega_e$ are two parameters that allow us to describe the properties of a plasma and to constrain the physical phenomena at play, for instance, whether a maser instability develops. The importance of the maser instability in coronal active regions depends on the complexity and topology of the magnetic field configurations.Comment: 10 pages, 7 figures, 1 appendix (with 1 figure This problem emerged dramatically when the analysis of the same large loop structure observed with Yohkoh/SXT on the solar limb led to three Living Reviews in Solar Physics http://www.livingreviews.org/lrsp-2014-4 different results depending mostly on the different ways to treat the background (Priest et al., 2000; #CITATION_TAG; Reale, 2002a). We perform an in-depth analysis of the $\omega_{pe}$/$\Omega_e$ ratio for simple theoretical and complex solar magnetic field configurations. Using the combination of force-free models for the magnetic field and hydrostatic models for the plasma properties, we determine the ratio of the plasma frequency to the gyrofrequency for electrons. For the sake of comparison, we compute the ratio for bipolar magnetic fields containing a twisted flux bundle, and for four different observed active regions. We also study how $\omega_{pe}$/$\Omega_e$ is affected by the potential and non-linear force-free field models. We demonstrate that the ratio of the plasma frequency to the gyrofrequency for electrons can be estimated by this novel method combining magnetic field extrapolation techniques and hydrodynamic models.
Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is associated with low direct costs compared to other anxiety disorders while indirect costs tend to be high. Mental comorbidities have been identified to increase costs, but the role of symptom severity is still vague. They also incur considerable healthcare costs, especially relating to the use of primary care services, experience high levels of productivity losses and receive higher social benefits compared with people in the general population [6] [7] [#CITATION_TAG]. Costs were calculated based on health care utilization and lost productivity.
Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. A great many tools have been developed for supervised clas- sification, ranging from early methods such as linear discriminant anal- ysis through to modern developments such as neural networks and sup- port vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the di!erence in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm. These and related issues are discussed further in [3, #CITATION_TAG, 5, 6]. nan
Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Tuberous sclerosis complex (TSC) is an autosomal dominant genetic disorder that occurs upon mutation of either the TSC1 or TSC2 genes, which encode the protein products hamartin and tuberin, respectively. The discovery that the products of the TSC genes regulate mTOR signaling (#CITATION_TAG) has paved the way to current experimental mTORC inhibitor-based treatment approaches. First, coexpression of hamartin and tuberin repressed phosphorylation of 4E-BP1, resulting in increased association of 4E-BP1 with eIF4E; importantly, a mutant of TSC2 derived from TSC patients was defective in repressing phosphorylation of 4E-BP1. Third, hamartin and tuberin blocked the ability of amino acids to activate S6K1 within nutrient-deprived cells, a process that is dependent on mTOR.
Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. More recently, (#CITATION_TAG) have utilized Word2Vec and Doc2Vec embeddings for unsupervised sentiment classification in medical discharge summaries. We applied unsupervised learning since the data sets did not have sentiment annotations. Note that unsupervised learning is a more realistic scenario than supervised learning which requires an access to a training set of sentiment-annotated data. We used SentiWordNet to establish a gold sentiment standard for the data sets and evaluate performance of Word2Vec and Doc2Vec methods.
This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Published online: 27 Apr 2017Comparative research on aphasia and aphasia rehabilitation is challenged by the lack of comparable assessment tools across different languages. In English, a large array of tools is available, while in most other languages, the selection is more limited. These subtests were taken from two sources: translated subtests of the Comprehensive Aphasia Test (CAT) (#CITATION_TAG) and subtests that have been developed in speech and language clinics in Jordan. Specifically, we focus on challenges and solutions related to the use of imageability, frequency, word length, spelling-to-sound regularity and sentence length and complexity as underlying properties in the selection of the testing material.For the work reported in this article, we were supported by various funding bodies.
The eolian sand depositional record for a dune field within Cape Cod National Seashore, Massachusetts is posit as a sensitive indicator of environmental disturbances in the late Holocene from a combination of factors such as hurricane/storm and forest fire occurrence, and anthropogenic activity. Stratigraphic and sedimentologic observations, particularly the burial of Spodosol-like soils, and associated C and OSL ages that are concordant indicate at least six eolian depositional events at ca. 3750, 2500, 1800, 960, 430, and <250 years ago. The two oldest events are documented at just one locality and thus, the pervasiveness of this eolian activity is unknown. Thus, local droughts are not associated with periods of dune movement in this mesic environment. Latest eolian activity on outer Cape Cod commenced in the past 300-500 years and may reflect multiple factors including broad-scale landscape disturbance with European colonization, an increased incidence of forest fires and heightened storminess. Eolian systems of Cape Cod appear to be sensitive to landscape disturbance and prior to European settlement may reflect predominantly hurricane/storm disturbance, despite generally mesic conditions in past 4 ka. The structure or composition of all vegetation types in the region have been shaped by past land-use, fire, or other disturbances, and vegetation patterns will con-tinue to change through time. Conservation efforts aimed at maintaining early succes-sional vegetation types may require intensive management comparable in intensity to the historical disturbances that allowed for their widespread development The current migration of dunes on Cape Cod is inferred to reflect a legacy of landscape disturbance, specifically forest clear-cutting, grazing and agricultural practices, associated with European settlement starting in the early 17th century and continuing into the 20th century (McCaffrey and Stilgoe, 1981; Rubertone, 1985; #CITATION_TAG; Eberhardt et al., 2003; Forman et al., 2008). Methods Historical changes in land-use and land-cover across the study region were determined from historical maps and documentary sources. Modern vegetation and soils were sampled and land-use and fire history determined for 352 stratified-random study plots. Ordination and classification were used to assess vegetation variation, and G-tests of independence and Kruskal-Wallis tests were used to evaluate relationships among individual species distributions, past land-use, surficial landforms and edaphic condi-tions.
This article presents the synthesis of results from the Stanford Energy Modeling Forum Study 27, an inter-comparison of 18 energy-economy and integrated assessment models. Limiting the atmospheric greenhouse gas concentration Climatic Change The study investigated the importance of individual mitigation options such as energy intensity improvements, carbon capture and storage (CCS), nuclear power, solar and wind power and bioenergy for climate mitigation. Decarbonization will lead to improved air quality, thereby reducing energy-related health impacts worldwide. At the same time, low-carbon technologies and energy-efficiency improvements can help to further the energy security goals of individual countries and regions by promoting a more dependable, resilient, and diversified energy portfolio. The cost savings of these climate policy synergies are potentially enormous Such measures (partially) describe the direct costs of mitigation, but neither include climate benefits nor as adverse side effects and co-benefits of climate policy (#CITATION_TAG; Riahi et al. 2012). We explain how the common practice of narrowly focusing on singular issues ignores potentially enormous synergies, quite often leading to the implementation of short-sighted solutions that may have unnecessarily costly, long-term consequences. Our analysis of a large ensemble of alternate energy-climate futures, developed using MESSAGE, an integrated assessment model with considerable technological detail of the global energy system, shows that climate change policy offers a strategic entry point along the path to energy sustainability in several dimensions.
This is a repository copy of Morpho-syntactic processing of Arabic plurals after aphasia: dissecting lexical meaning from morpho-syntax within word boundaries. Most roots in Arabic are made up of three consonants which are referred to as triliteral roots or consonantal roots (Zabbal, 2002; #CITATION_TAG). The observed shifting behavior allows for examining the question of predictability. The observed shifting patterns lead to the development of a new model for information seeking behavior that describes multiple search stages, aspects and dimensions. The model represents a non-linear process of multiple re-iterative cycles that make up the overall flow of communication with the information system. The model expands significantly earlier theoretical and empirical models as it represents a series of re-iterative processes that characterize the information seeking interaction.
Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Practically everywhere one looks these days the concept of "civil society" is in vogue. Neo-Tocquevillean scholars argue that civil society plays a role in driving political, social, and even economic outcomes. This new conventional wisdom, however, is flawed. It is simply not true that democratic government is always strengthened, not weakened, when it faces a vigorous civil society. To know when civil society activity will take on oppositional or even antidemocratic tendencies, one needs to ground one's analyses in concrete examinations of political reality. 2 Given the importance of formal and informal institutions for governing people's behaviour (e.g., North, 1990; Thelen, 1999) and earlier findings linking the institutional environment to the development of specific types of voluntary associations (e.g., #CITATION_TAG; Kriinen & Lehtonen, 2006; Schofer & Fourcade-Gourinchas, 2001), it is surprising that both lines of argument ignore the socio-political and institutional environment within which the individual and the association exist. nan
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Self-regulation of cognition and behavior is an important aspect of student learning and academic performance in the classroom context (Corno & Mandinach, 1983; Corno & Rohrkemper, 1985). There are a variety of definitions of selfregulated learning, but three components seem especially important for classroom performance. There are two strands of expectancy motivation (Eccles & Wigfield, 2002; #CITATION_TAG): A correlational study examined relationships between motivational orientation, self-regulated learning, and classroom academic performance for 173 seventh graders from eight science and seven English classes. A self-report measure of student self-efficacy, intrinsic value, test anxiety, self-regulation, and use of learning strategies was administered, and performance data were obtained from work on classroom assignments. First, self-regulated learning includes students' metacognitive strategies for planning, monitoring, and modifying their cognition (e.g., Brown, Bransford, Campione, & Ferrara, 1983; Corno, 1986; Zim
Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Secondary pneumococcal pneumonia is a serious complication during and shortly after influenza infection. This increased susceptibility to secondary bacterial pneumonia is at least in part caused by excessive IL-10 production and reduced neutrophil function in the lungs. Also, the peak of the coronal emission measure of active regions -where the loops are brightest -is above 2 MK, which is best observed in X-rays (e.g., Peres et al., 2000; Reale et al., 2009a; #CITATION_TAG). C57BL/6 mice were intranasally inoculated with 10 median tissue culture infective doses of influenza A (A/PR/8/34) or PBS (control) on day 0.
It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. There has been significant recent interest in the commercial utilisation of algae based on their valuable chemical constituents many of which exhibit multiple bioactivities with applications in the food, cosmetic, agri- and horticultural sectors and in human health. Compounds of particular commercial interest include pigments, lipids and fatty acids, proteins, polysaccharides and phenolics which all display considerable diversity between and within taxa. The chemical composition of natural algal populations is further influenced by spatial and temporal changes in environmental parameters including light, temperature, nutrients and salinity, as well as biotic interactions. As reported bioactivities are closely linked to specific compounds it is important to understand, and be able to quantify, existing chemical diversity and variability. The high biodiversity of the latter and how they can serve the biotechnological field has been recently pointed out (#CITATION_TAG; Barra et al., 2014). nan
Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Tropical tree height-diameter (H:D) relationships may vary by forest type and region making large-scale estimates of above-ground biomass subject to bias if they ignore these differences in stem allometry. Annual precipitation coefficient of variation (PV), dry season length (SD), and mean annual air temperature (TA) emerged as key drivers of variation in H:D relationships at the pantropical and region scales. Forests in Asia, Africa and the Guyana Shield all have, on average, similar H:D relationships, but with trees in the forests of much of the Amazon Basin and tropical Australia typically being shorter at any given D than their counterparts elsewhere. Some of the plot-to-plot variability in H:D relationships not accounted for by this model could be attributed to variations in soil physical conditions. The relationship between diameter and height also varies across the basin, but with more complexity than wood density, mostly related to climatic factors (#CITATION_TAG). 2. to ascertain if the H:D relationship is modulated by climate and/or forest structural characteristics (e.g. 3. to develop H:D allometric equations and evaluate biases to reduce error in future local-to-global estimates of tropical forest biomass.
Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category "toys to pick up before bed" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the "just previous" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, "boring" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term "categorization." Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as "how do prototype representations compare between 6 and 8 months of age?" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. Recent research on early word learning suggests that children's behavior when-generalizing novel nouns integrates their prior vocabulary knowledge with the specifics of the task. Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (#CITATION_TAG; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In ***1 condition, we used a combination of training and stimulus factors predicted to produce a bias to generalize nouns by shape similarity. We then reduced this shape bias and amplified a bias to generalize nouns by material similarity via manipulations of training and stimuli across 3 other conditions.
EEG abnormalities have been reported for both dementia with Lewy bodies (DLB) and Alzheimer's disease (AD). Although it has been suggested that variations in mean EEG frequency are greater in the former, the existence of meaningful differences remains controversial. No evidence is as yet available for Parkinson's disease with dementia (PDD). If revised consensus criteria for DLB diagnosis are properly applied (i.e. All patients received a full neurological, neuropsychological, neuroimaging and neuropsychiatric evaluation, as reported in Online Resource 1, and as described in previous studies on the same cohort [#CITATION_TAG] [30] [31] [32]. To improve clinical diagnostic accuracy, special emphasis was placed on identifying cognitive fluctuations and REM-sleep behaviour disorder. EEG variability was assessed by mean frequency analysis and compressed spectral arrays (CSA) in order to detect changes over time from different scalp derivations. Graded according to the presence of alpha activity, five different patterns were identified on EEG CSA from posterior derivations. A pattern with dominant alpha bands was observed in patients with AD alone while, in those with DLB and PDD, the degree to which residual alpha and 5.6-7.9 bands appeared was related to the presence and severity of cognitive fluctuations. Of interest, in four patients initially labelled as having AD, in whom the occurrence of fluctuations and/or REM-sleep behaviour disorder during the 2-year follow-up had made the diagnosis of AD questionable, the initial EEG was characterized by the features observed in the DLB group. emphasizing the diagnostic weight of fluctuations and REM sleep behaviour disorder), EEG recording may act to support discrimination between AD and DLB at the earliest stages of dementia, since characteristic abnormalities may even precede the appearance of distinctive clinical features.
Most existing Information Technology (IT) adoption models such as the Technology Acceptance Model (TAM) only consider individual behaviour and views on technology adoption, without providing mechanisms to accommodate multiple stakeholder perspectives in an organization. In this paper we propose an IT adoption framework, expected to assist an organization in resolving problem situations from multiple perspectives. Irrigation farmers in the lower reaches of the Vaal and Riet Rivers are experiencing substantial yield reductions in certain crops and more profitable crops have been withdrawn from production, hypothesised, as a result of generally poor but especially fluctuating water quality. Leaching is justified financially and there is a strong motivation for a change in the current water pricing system. Organizations have been urged to view IT adoption decision making as a social phenomenon which needs systems approaches to reveal competing interests among stakeholders [#CITATION_TAG]. In this paper secondary data is used in a linear programming model to test this hypothesis by calculating the potential loss in farm level optimal returns. Linear crop-water quality production functions (Ayers & Westcot, 1983; adapted from Maas & Hoffmann, 1977) are used to calculate net returns for the eight most common crops grown.
Because accurate diagnosis lies at the heart of medicine, it is important to be able to evaluate the effectiveness of diagnostic tests. One particularly widely used measure is the AUC, the area under the Receiver Operating Characteristic (ROC) curve. This measure has a well-understood weakness when comparing ROC curves which cross. However, it also has the more fundamental weakness of failing to balance different kinds of misdiagnosis effectively. This is not merely an aspect of the inevitable arbitrariness in choosing a performance measure, but is a core property of the way the AUC is defined. Different measures will yield different results, and it follows that it is crucial to match the measure to the true objectives. These and related issues are discussed further in [3, 4, #CITATION_TAG, 6]. In predictive data mining, algorithms will be both optimized and compared using a measure of predictive performance. We define two measures, one based on minimizing the overall cost to the card company, and the other based on minimizing the amount of fraud given the maximum number of investigations the card company can afford to make. We also describe a plot, analogous to the standard ROC, for displaying the performance trace of an algorithm as the relative costs of the two different kinds of misclassification--classing a fraudulent transaction as legitimate or vice versa--are varied.
Adults with autism face high rates of unemployment. Supported employment enables individuals with autism to secure and maintain a paid job in a regular work environment. In secondary analyses that incorporated potential cost-savings, supported employment dominated standard care (i.e. The objective of this study was to assess the cost-effectiveness of supported employment compared with standard care (day services) for adults with autism in the United Kingdom. The analysis considered two measures of outcome: the total number of weeks in employment and the quality-adjusted life year (QALY; #CITATION_TAG). Much of the support for the QALY is based on its simplicity as a tool for resolving complex choices.
This paper offers a short history of routine clinical outcomes measurement (RCOM) in UK mental health services. Within the general embrace of a health service "free at the point of access", the United Kingdom (UK) has no single national health service (NHS). Scotland, Northern Ireland and, since 2001, Wales have separate arrangements for health service policy, management and delivery. In Scotland there is no mandated or national system for RCOM, although large patient outcomes surveys have been carried out. In Wales and Scotland "outcomes frameworks" have been developed to measure the impact of policies on the mental health of the whole population, for instance the average scores of the Warwick-Edinburgh Mental Well-being Scale (WEMWBS: see Table 1 ) (Tennant et al., 2007) from the Scottish Health Survey. In Northern Ireland the emphasis has been on measuring mental health recovery, but without yet clear agreement of how this can be done. What follows therefore predominately relates to England. Of great importance in the use of rating scales in any context are their psychometric properties. There have been relatively few studies in UK clinical populations of psychometric properties of measures coming into widespread use, such as the Health of the Nation Outcome Scales. One reason for this could be an assumption that once the properties are established in one population, that this is likely to generalise to others. However, contexts can vary, and just as randomised controlled trials of treatments need to be replicated in different settings, so too should evaluations of psychometric properties. Given the breadth of measures and scarcity of relevant evidence, psychometric properties are not provided in this paper. Mental Health services in the UK and their patients Services are provided by the NHS in primary care settings (often but not always involving initial contact with general medical practitioners), in secondary specialist mental health services (usually after referral from general practitioner), and in tertiary services such as secure forensic milieus (Deakin & Bhugra, 2012) . Most mental health issues occur in and are dealt with in primary care (King et al., 2008) , either through informal self-funded counselling, private psychotherapy services, charitable organisations e.g., for relationship or bereavement problems, or funded counselling services attached to general practices, schools, colleges, universities and some workplaces. Depressive and anxiety disorders predominate. Severe mental illness is usually initially treated in secondary care by state-funded NHS services, but few with short-term illnesses such as major depressive or bipolar disorder and only a small proportion of patients with chronic severe illness remain in secondary caremany are discharged back into the care of their general practitioner once any acute phase has passed. Secondary care is community-orientated with patients assessed and treated in Introduction: Definitions and circumspections Routine clinical outcomes measurement (RCOM) is taken here to mean the measurement of health status change (i.e., between at least two points in time) in a service-user population, usually with the intention of inferring how muchor littleclinical interventions have helped. Purpose Commissioning has been a central plank of health and social are policy in England for many years now, yet there are still debates about how effective it is in delivering improvements in care and outcomes. Social inclusion of people with experience of mental health is one of the goals that commissioners would like to help services to improve but such a complex outcome for people can often be undermined by contractual arrangements that fragment service responses rather than deliver holistic support. In the UK, however, the word "outcomes" has recently been heard at every level of government (#CITATION_TAG) and, as ever imperfectly translated into action or actual resources, it is helping drive the RCOM process forwards, as is excitement about "value" (Porter et al., 2006). Design/methodology/approach The paper is a conceptual discussion and case description of the use of Alliance Contracts to improve recovery services and social inclusion in mental health care in one locality.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. INTRODUCTION The manner in which students are admitted for third level education and in particular the points system have become matters of increasing public controversy in recent years. While the subject is understandably emotive, a sad feature of many of the arguments presented to date has been the very limited factual support. A student who passes this examination has certainly been fully accepted by the university in accordance with its own criteria for assessment and any subsequent failure to graduate is indicative of faults in this assessment rather than any pre-university assessments. Such analyses cannot, of course, provide definitive answers to many important questions concerning entry to third level, as many of these are essentially political in character. SOME RELATED WORK 2.1 The literature on the predictive value of school leaving, scholastic aptitude and related tests for subsequent university performance is immense. Virtually all of this work has been undertaken outside Ireland and the only detailed research published in the Irish context has been Nevin (1974). Factors affecting academic performance have been the focus of research for many years (Farsides & Woodfield, 2003; Lent, Brown, & Hacket, 1994; #CITATION_TAG). The restriction to first year has also the important advantage of allowing the analysis of very recent university and school examinations to be presented. Our review of such work will be confined to what we consider most relevant and useful in the Irish context.
Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. This is the second in a series of reports from UK ERC's Energy 2050 project. Evidence from energy systems analysis suggests that the desirability of hydrogen as a decarbonisation option depends strongly on success in bringing down costs, for both fuel cell technologies and hydrogen production, delivery and storage systems [5, 6, #CITATION_TAG]. The technologies analysed include a number of renewables (marine, bioenergy, wind and solar PV) and also other low carbon options (carbon capture and storage, nuclear power and fuel cells).Technology acceleration is analysed firstly by devising detailed technology-by technology accounts of accelerated development, and then system-level modelling of the potential impacts of this acceleration on the UK energy system from now to 2050.
Performance at 22- and 23-mm simulated insertion depths was always poorer than normal, and performance at 25 mm simulated insertion depth was, most generally, the same as normal. When vocoded speech is spectrally shifted upward to simulate relatively shallow CI electrode insertions, shifts in excess of 3 mm of basilar membrane distance have large acute effects on speech perception (#CITATION_TAG; Shannon et al., 1998). Normally hearing listeners were presented with vowels, consonants, and sentences for identification through an acoustic simulation of a five-channel cochlear implant with electrodes separated by 4 mm (as in the Ineraid implant). Insertion depth was simulated by outputting sine waves from each channel of the processor at a frequency determined by the cochlear place of electrodes inserted 22-25 mm into the cochlea.
Typologies are an important way of organizing the complex cause-effect relationships that are key building blocks of the strategy and organization literatures. Types and typologies are ubiquitous, both in every-day social life and in the language of the social sciences. Everybody uses them, but almost no one pays any attention to the nature of their construction.-McKinney (1969: 4) The notion of causality plays a key role in both the strategy and organization literatures. For in-stance, cause-effect relationships are the central way in which strategic decisions and organization-al structures are understood and communicated in organizations (Ford, 1985; Huff, 1990; Huff &amp; Jen-kins, 2001). Building on this insight, the cognitive strategy literature has aimed to map and explain the causal reasoning of managers regarding both organizational performance and competitive envi-ronments (e.g., Barr, Stimpert,  &amp; Huff, 1992; Nad-karni &amp; Narayanan, 2007a, 2007b; Reger &amp; Huff, 1993). Similarly, cause-effect relationships are the main building blocks of the organizational design literature and have recently received increasing at-tention (e.g., Burton &amp; Obel, 2004; Grandori &amp; Fur The present study builds on the existing literature that underscores the value of fuzzy-set qualitative comparative analysis (fsQCA) (e.g., #CITATION_TAG; Woodside, 2013; Woodside & Zhang, 2013) and shows that the proposed methodological tool offers much in terms of understanding causal relationships, by virtue of providing information that is unique in comparison with the information that conventional correlational methods provide. Using data on high-technology firms, I empirically investigate configurations based on the Miles and Snow typology using fuzzy set qualitative comparative analysis (fsQCA).
The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. Studies from across the world have shown that clinical mistakes are a major threat to the safety of patient care (World Health Organisation 2004). For the National Health Service (NHS) of England and Wales it is estimated that one in ten hospital patients experience some form of error, and each year these cost the service over PS2billion in remedial care (Department of Health 2000). Unsurprisingly, 'patient safety' is now a major international health policy priority, questioning the efficacy of existing regulatory practices and proposing a new ethos of learning. Within England and Wales, the National Patient Safety Agency (NPSA) has been created to lead policy development and champion service-wide learning, whilst throughout the NHS the National Reporting and Learning System (NRLS) has been introduced to enable this learning (NPSA 2003). We are also aware that RCA has been interpreted as an emerging form of self-surveillance (#CITATION_TAG) and potentially extending the principle of concertive control among healthcare practitioners (Iedema, Jorm, Long et al., 2006). nan
The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. This grounded the mirror system hypothesis of Rizzolatti and Arbib (1998) which offers the mirror system for grasping as a key neural "missing link" between the abilities of our nonhuman ancestors of 20 million years ago and modern human language, with manual gestures rather than a system for vocal communication providing the initial seed for this evolutionary process. The present article, however, goes "beyond the mirror" to offer hypotheses on evolutionary changes within and outside the mirror systems which may have occurred to equip Homo sapiens with a language-ready brain. Crucial to the early stages of this progression is the mirror system for grasping and its extension to permit imitation. Imitation is seen as evolving via a so-called simple system such as that found in chimpanzees (which allows imitation of complex "object-oriented" sequences but only as the result of extensive practice) to a so-called complex system found in humans (which allows rapid imitation even of complex sequences, under appropriate conditions) which supports pantomime. It is argued that these stages involve biological evolution of both brain and body. By contrast, it is argued that the progression from protosign and protospeech to languages with full-blown syntax and compositional semantics was a historical phenomenon in the development of Homo sapiens, involving few if any further biological changes. For example, #CITATION_TAG warn that closure and consensus are often enemies of the capacity of organisations to learn from incidents that require them "to confront the possibility that the story being told is simultaneously a tale of disorder in which the reality of danger masquerades as safety and a tale of order in which the reality masquerades as danger" (p. 456). The starting point is the observation that both premotor area F5 in monkeys and Broca's area in humans contain a "mirror system" active for both execution and observation of manual actions, and that F5 and Broca's area are homologous brain regions. This is hypothesized to have provided the substrate for the development of protosign, a combinatorially open repertoire of manual gestures, which then provides the scaffolding for the emergence of protospeech (which thus owes little to nonhuman vocalizations), with protosign and protospeech then developing in an expanding spiral.
Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. In the recent past, availability of large data sets of species presences has increased by orders of magnitude. This, together with developments in geographical information systems and statistical methods, has enabled scientists to calculate, for thousands of species, the environmental conditions of their distributional areas. Climate is generally recognized as a chief driver of species' distributions at large spatial scales (Ara ujo & Peterson, 2012), although the broad distributional limits governed by climate may be modified by factors such as habitat availability, biotic interactions and dispersal limitations (#CITATION_TAG). I use set theory notation and analogies derived from population ecology theory to obtain formal definitions of areas of distribution and several types of niches.
In PD, comparisons among diabetic and nondiabetic and anuric patients and patients with residual renal function are frequent, but comparisons between patients undergoing PD as first option versus PD as a second option after haemodialysis are scarce [#CITATION_TAG] [8] [9] [10]. nan
Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. The paper closes with an example of a multi-level land use, transport and environment model ranging from the European to the grid cell level. Although our main focus is on workplaces, a multilevel perspective is used (#CITATION_TAG). It starts with a history of urban transport and land-use models and observes a trend towards increasing conceptual, spatial and temporal resolution stimulated by improved data availability, higher computer speed and better theories about mobility and location of individual behaviour.
Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. This creates a price differential between services that are identical for the consumer and generates network externalities despite network interconnection. #CITATION_TAG find that if firms set linear prices but can discriminate between on-and off-net calls, then above cost termination charges induce 3 In the initial stages of wireless telecommunication the most important regulatory issue was the fixedto-mobile (FTM) termination rate, i.e. the price to be paid by the incumbent land-line operator for calls terminating on a mobile network. Our companion article developed a clear conceptual framework of negotiated or regulated interconnection agreements between rival operators and studied competition between interconnected networks, under the assumption of nondiscriminatory pricing.
A hallmark of many an intuitionistic theory is the existence property, EP, i.e., if the theory proves an existential statement then there is a provably definable witness for it. However, there are well known exceptions, for example, the full intuitionistic Zermelo-Fraenkel set theory, IZF, does not have the existence property, where IZF is formulated with Collection. However, in this paper it is shown that several well known intuitionistic set theories with Collection have the weak existence property. As a result, the culprit preventing the weak existence property from obtaining must consist of a combination of Collection and unbounded Separation. However, this requires a new form of ordinal analysis for theories with Power Set and Exponentiation (cf. Rathjen (2011) [39]) and is beyond the scope of the current paper. Constructive Zermelo-Fraenkel Set Theory, CZF, has emerged as a standard reference theory that relates to constructive predicative mathematics as ZFC relates to classical Cantorian mathematics. A hallmark of this theory is that it possesses a type-theoretic model. In [#CITATION_TAG, 36, 37] the author of the present paper developed a different machinery for showing the DP and the NEP (and several other properties) directly for extensional set theories. Aczel showed that it has a formulae-as-types interpretation in Martin-Lof's intuitionist theory of types [14, 15]. It is shown that Kleene realizability provides a self-validating semantics for CZF, viz. this notion of realizability can be formalized in CZF and demonstrably in CZF it can be verified that every theorem of CZF is realized. This semantics, then, is put to use in establishing several equiconsistency results. Specifically, augmenting CZF by well-known principles germane to Russian constructivism and Brouwer's intuitionism turns out to engender theories of equal proof-theoretic strength with the same stock of provably recursive functions
The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [12] [#CITATION_TAG]. The major hypothesis is that during the 2-year period CDSMP participants will experience improvements or less deterioration than expected in health status and reductions in health care utilization. Design.Longitudinal design as follow-up to a randomized trial. Participants.Eight hundred thirty-one participants 40 years and older with heart disease, lung disease, stroke, or arthritis participated in the CDSMP. Health status (self-rated health, disability, social/role activities limitations, energy/fatigue, and health distress), health care utilization (ER/outpatient visits, times hospitalized, and days in hospital), and perceived self-efficacy were measured.
Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Chronic hepatitis B was characterized by fluctuant immune response to infected hepatocytes resulting in hepatic inflammation and virus persistence. Recently, Programmed Death-1 (PD-1) and its ligand PD-L1 have been demonstrated to play an essential role in balancing antiviral immunity and inflammation in the livers of acute hepatitis B patients, significantly influencing disease outcome. PD-1 up-regulation in peripheral T cells is associated with immune dysfunction in chronic hepatitis B patients. In particular, whereas the loop total emission measure distribution should steepen above the canonical 1.5 (Jordan, 1980; #CITATION_TAG; Peres et al., 2001) dependence for temperature above 1 MK. nan
This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. However, many existing safety cases, in their attempt to manage potentially complex arguments, are poorly structured, presented and understood. This creates problems in developing and maintaining safety cases, and in capturing successful safety arguments for use on future projects. Drawing on safety-case argumentation, we create a diagrammatic summary of the structured argument of fitness for purpose, using a visual notation closely based on the standard safety-critical argumentation notation, goal structuring notation (GSN) [#CITATION_TAG, 29]. A safety case should present a clear, comprehensive and defensible argument that a system is acceptably safe to operate within a particular context. This approach is based upon a graphical technique -- the Goal Structuring Notation (GSN) -- and has three strands. Firstly, a method for the use of GSN is defined together with an approach to supporting incremental safety case development. Thirdly, the concept of `Safety Case Patterns&apos; is defined as a means of supporting and promoting the reuse of successful safety arguments between safety cases. Examples of the approach are provided throughout.
A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. When we speak of consciousness we are referring to the sum total of events in awareness. The term by no means exhausts the realm of things psychological, but it does encompass all of an individual's direct experience. When we speak of the flow of consciousness we are referring to the changes that take place in consciousness over time. The events of consciousness are, of course, extremely complex and varied. They do not contain the imagery of current perceptual activity but they contain imaginai qualities that one can describe in terms of forms, colors, sounds, words, smells, tastes, temperatures, and the like. There are dream-like segments in waking states -in one study 25% of waking thought samples were rated by participants as having at least a trace of dream-like qualities Cox, 1987-1988), which agrees approximately with other results (Foulkes and Fleisher, 1975; #CITATION_TAG Klinger, -1979) -as well as there being waking-like cognitive content in dreams. They embrace images in every sensory modality and in every degree of vividness, realism, and believability, including inner dialogue, hallucinations, reveries, and dreamlike sequences; and they also embrace qualities that are at the same time less figured and more pervasive than these--the affects.
Phonological errors were scarce in both groups. However, such relationships help business service firms to resist price pressures from their customers and add more value to their services over time. In contrast, closed class elements may be chosen once competition amongst grammatical features is resolved: in other words, competitive selection would apply to grammatical feature specifications but not to closed class elements (La Heij, Mak, Sander, & Willeboordse, 1998; Levelt et al., 1999; #CITATION_TAG; Schriefers, Jescheniak, & Hantsch, 2005). nan
Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Citizens today are increasingly expected to be knowledgeable about and prepared to engage with biomedical knowledge. This is not based on an assumption that non-scientists are 'ignorant' and are thus unable to 'appropriately' use or debate science; rather, it is underpinned by an empirically-grounded observation that some individuals may be unfamiliar with certain specificities of particular modes of research and ethical frameworks, and, as a consequence, have their autonomy compromised when invited to participate in biomedical investigations. Public bioethics will have to play a key role in such an endeavour, and indeed will contribute in important ways to the opening up of new spaces of symmetrical engagement between bioethicists, scientists and wider publics-and hence to the democratisation of the bioethical enterprise. In so doing, opportunities for more democratic forms of bioethical deliberation are also restricted [#CITATION_TAG]. nan
It is widely acknowledged that the use of stories supports the development of literacy in the context of learning English as a first language. However, it seems that there are a few studies investigating this issue in the context of teaching and learning English as a foreign language. This action-oriented case study aims to enhance students' written narrative achievement through a pedagogical intervention that incorporates oral story sharing activities. Communicative language teaching (CLT) applicability to English as a foreign language (EFL) contexts has recently been debated extensively. Too, whereas EFL instruction met learners' preferences associated with FFI, it rarely responded to learners' MOI needs. This contradicts studies by Shrestha (2013) in Bangladesh and #CITATION_TAG in Jordan that report many EFL learners preferred to have more communicative activities to practice their English, although they showed positive attitudes to traditional activities such as drilling of grammar rules and vocabulary. The data were collected using a 41-item questionnaire and analyzed using descriptive and referential statistics.
The main problem with the state of the art in the semantic search domain is the lack of comprehensive evaluations. There exist only a few efforts to evaluate semantic search tools and to compare the results with other evaluations of their kind. In this paper, we present a systematic approach for testing and benchmarking semantic search tools that was developed within the SEALS project. The semantic web presents the vision of a distributed, dynamically growing knowledge base founded on formal logic. Common users, however, seem to have problems even with the simplest Boolean expressions. As queries from web search engines show, the great majority of users simply do not use Boolean expressions. Searching the Semantic Web lies at the core of many activities that are envisioned for the Semantic Web; many researchers have investigated means for indexing and searching the Semantic Web [1] [2] [#CITATION_TAG] [4] 6, 9, 13, 14, 16]. We address this problem by presenting a natural language interface to semantic web querying. The interface allows formulating queries in Attempto Controlled English (ACE), a subset of natural English. Each ACE query is translated into a discourse representation structure - a variant of the language of first-order logic - that is then translated into an N3-based semantic web querying language using an ontology-based rewriting framework.
Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. Naming ability on the PALPA54 subtest (#CITATION_TAG) indicated residual lexical capacity with scores of 59/60 for spoken (with no penalty for phonemic paraphasias as long as the target was recognizable) and 59/60 for written naming. Intended both as a clinical instrument and research tool, PALPA is a set of resource materials enabling the user to select language tasks that can be tailored to the investigation of an individual patient's impaired and intact abilities. The materials consist of sixty rigorously controlled tests of components of language structure such as orthography and phonology, word and picture semantics and morphology and syntax. The tests make use of simple procedures such as lexical decision, repetition and picture naming and have been designed to assess spoken and written input and output modalities. Each test is also accompanied by detailed instructions of how and why it was constructed, how to use it, and by presenter's forms and marking sheets.
Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Rationale: Decision-making involves two fundamental axes of control namely valence, spanning reward and punishment, and action, spanning invigoration and inhibition. We recently exploited a go/no-go task whose contingencies explicitly decouple valence and action to show that these axes are inextricably coupled during learning. Given that this bias is independent of valence it may reflect a general action bias observed in instrumental learning under uncertainty [24, [#CITATION_TAG] [31] [32]. Conversely, serotonin is implicated in motor inhibition and punishment processing. Methods: We combined computational modeling with pharmacological manipulation in 90 healthy human volunteers, using levodopa and citalopram to affect dopamine and serotonin, respectively.
Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. The taxonomic rank of subspecies remains highly contentious, largely because traditional subspecies boundaries have sometimes been contradicted by molecular phylogenetic data. However, the global generality of this phenomenon remains unclear due to this previous study's narrow geographic focus on continental Nearctic and Palearctic subspecies. The broader picture is that avian subspecies often provide an effective short-cut for estimating patterns of intraspecific genetic diversity, thereby providing a useful tool for the study of evolutionary divergence and conservation. Subspecies are generally based on discontinuities in the geographical distribution of phenotypic traits instead of molecular phylogenies, but can generally be considered useful proxies of patterns of divergence among populations (#CITATION_TAG). We suggest that the widespread impression that avian subspecies are not real arises from a predominance of studies focusing on continental subspecies in North America and Eurasia, regions which show unusually low levels of genetic differentiation.
Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. In addition, basal respiratory frequency has a non-linear relationship with spectral power as breathing rate falls below approximately 0.15 Hz (as it occasionally does in athletes; #CITATION_TAG). Fifteen male athletes were subjected to HRV measurements under six randomised breathing conditions: spontaneous breathing frequency (SBF) and five others at controlled breathing frequencies (CBF) (0.20; 0.175; 0.15; 0.125 and 0.10 Hz). The subjects were divided in two groups: the first group included athletes with SBF <0.15 Hz (infSBF) and the second athletes with SBF higher than 0.15 Hz (supSBF). In this study, BF was the main modulator of the LF/HF ratio in both controlled breathing and spontaneous breathing. During each CBF, all athletes presented spectral energy mainly concentrated around their BF.
In previous work the authors considered the asymmetric simple exclusion process on the integer lattice in the case of step initial condition, particles beginning at the positive integers. There it was shown that the probability distribution for the position of an individual particle is given by an integral whose integrand involves a Fredholm determinant. In one an apparently new distribution function arises and in another the distribution function F 2 arises. Although differential mortality decline for cardiovascular diseases has been suggested as an important contributory factor, it is not known what its quantitative contribution was, and to what extent other causes of death have contributed to the widening gap in total mortality. In most countries, mortality from cardiovascular diseases declined proportionally faster in the upper socioeconomic groups. In all countries with the exception of Italy (Turin), changes in cardiovascular disease mortality contributed about half of the widening relative gap for total mortality. For these causes, widening inequalities were sometimes due to increasing mortality rates in the lower socioeconomic groups. We found rising rates of mortality from lung cancer, breast cancer, respiratory disease, gastrointestinal disease, and injuries among men and/or women in lower socioeconomic groups in several countries. In this case the probability equals a probability in a unitary Laguerre random matrix ensemble #CITATION_TAG METHODS We collected data on mortality by educational level and occupational class among men and women from national longitudinal studies in Finland, Sweden, Norway, Denmark, England/Wales, and Italy (Turin), and analysed age-standardized death rates in two recent time periods (1981-1985 and 1991-1995), both total mortality and by cause of death. For simplicity, we report on inequalities in mortality between two broad socioeconomic groups (high and low educational level, non-manual and manual occupations).
A central component of mind wandering is mental time travel, the calling to mind of remembered past events and of imagined future ones. Mental time travel may also be critical to the evolution of language, which enables us to communicate about the non-present, sharing memories, plans, and ideas. Mental time travel is indexed in humans by hippocampal activity, and studies also suggest that the hippocampus in rats is active when the animals replay or pre play activity in a spatial environment, such as a maze. Mental time travel may have ancient origins, contrary to the view that it is unique to humans. The idea that memory is composed of distinct systems has a long history but became a topic of experimental inquiry only after the middle of the 20th century. Beginning about 1980, evidence from normal subjects, amnesic patients, and experimental animals converged on the view that a fundamental distinction could be drawn between a kind of memory that is accessible to conscious recollection and another kind that is not. Subsequent work shifted thinking beyond dichotomies to a view, grounded in biology, that memory is composed of multiple separate systems supported, for example, by the hippocampus and related structures, the amygdala, the neostriatum, and the cerebellum. Declarative memory, in turn, can be divided into episodic memory, which is personal memory for past episodes, and semantic memory, which is basic knowledge about the world (#CITATION_TAG). nan
We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. BACKGROUND AND OBJECTIVES The expansion of evidence-based practice across sectors has lead to an increasing variety of review types. However, the diversity of terminology used means that the full potential of these review types may be lost amongst a confusion of indistinct and misapplied terms. A limited number of review types are currently utilized within the health information domain. Notwithstanding such limitations, this typology provides a valuable reference point for those commissioning, conducting, supporting or interpreting reviews, both within health information and the wider health care domain. of allowed states in the combined lattice plus external trap, in striking contrast with the uniform case [#CITATION_TAG]. METHODS Following scoping searches, an examination was made of the vocabulary associated with the literature of review and synthesis (literary warrant). A simple analytical framework -- Search, AppraisaL, Synthesis and Analysis (SALSA) -- was used to examine the main review types. A description of the key characteristics is given, together with perceived strengths and weaknesses.
After at least a decade of parallel tool development, parallelization of scientific applications remains a significant undertaking. Typically parallelization is a specialized activity supported only partially by the programming tool set, with the programmer involved with parallel issues in addition to sequential ones. The aim of parallel programming tools is to automate the latter without sacrificing performance and portability, allowing the programmer to focus on algorithm specification and development. Computational molecular dynamics is an important application requiring large amounts of computing time. Parallel processing offers very high performance potential, but irregular problems like molecular dynamics have proven difficult to map onto parallel machines. In this paper, we describe the practicalities of porting a basic molecular dynamics computation to a distributedmemory machine. This set of algorithms represents an important class of unstructured problems in scientific .. The ease of implementation of an MD algorithm is important given the need for multiple algorithms to address the variability encountered in mapping molecular dynamics algorithms onto parallel architectures [#CITATION_TAG, 9]. We also argue that algorithm replacement may be necessary in parallelization, a task which cannot be performed automatically.
to a Cartesian view of distinct unobservable minds. Voice, it is suggested, necessarily gives rise to a temporally bound subjectivity, whether it is in inner speech (Descartes' "cogito"), in conversation, or in the synchronized utterances of collective speech found in prayer, protest, and sports arenas world wide. The notion of a fleeting subjective pole tied to dynamically entwined participants who exert reciprocal influence upon each other in real time provides an insightful way to understand notions of common ground, or socially shared cognition. It suggests that the remarkable capacity to construct a shared world that is so characteristic of Homo sapiens may be grounded in this ability to become dynamically entangled as seen, e.g., in the centrality of joint attention in human interaction. Empirical evidence of dynamic entanglement in joint speaking is found in behavioral and neuroimaging studies. A convergent theoretical vocabulary is now available in the concept of participatory sense-making, leading to the development of a rich scientific agenda liberated from a stifling metaphysics that obscures, rather than illuminates, the means by which we come to inhabit a shared world. Questioning this commitment leads us to recognize that the boundaries conventionally separating the linguistic from the non-linguistic can appear arbitrary, omitting much that is regularly present during vocal communication. The thesis is put forward that uttering, or voicing, is a much older phenomenon than the formal structures studied by the linguist, and that the voice has found elaborations and codifications in other domains too, such as in systems of ritual and rite. Synchronously read speech has shown to reduce a high degree of speaker variability of reading exhibited by speakers in laboratory recording; e.g., pause placement and duration, and speech rate. However, quantitative analysis of speech rate has rarely been found in studies on synchronous speech. Speech produced in these constrained laboratory settings is remarkably unremarkable, and the technique of having subjects speak in synchrony has been used as a device for obtaining unmarked speech in several phonetic studies (Krivokapi, 2007; #CITATION_TAG; O'Dell et al., 2010; Dellwo and Friedrichs, 2012). Consistency and variability of speech rate are compared in both reading types across repetitions within a subject, across subjects, and across dialects. This global pattern is consistent across dialects, and stylized local variation of speech rate over prosodic uni...
Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Every user has a distinct background and a specific goal when searching for information on the Web. Effective personalization of information access involves two important challenges: accurately identifying the user context and organizing the information in such a way that matches the particular context. They usually generate user models that describe user interests according to a set of features [#CITATION_TAG]. A spreading activation algorithm is used to maintain the interest scores based on the user's ongoing behavior.
The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Author's accepted version (post-print).Qualitative content analysis consists of conventional, directed and summative approaches for data analysis. Content analysis may be approached either inductively, in a purely exploratory context, or deductively when seeking to test known ideas or compare changes in content over time (#CITATION_TAG). They are used for provision of descriptive knowledge and understandings of the phenomenon under study. However, the method underpinning directed qualitative content analysis is insufficiently delineated in international literature. Various international databases were used to retrieve articles related to directed qualitative content analysis. A review of literature led to the integration and elaboration of a stepwise method of data analysis for directed qualitative content analysis. The proposed 16-step method of data analysis in this paper is a detailed description of analytical steps to be taken in directed qualitative content analysis that covers the current gap of knowledge in international literature regarding the practical process of qualitative data analysis. An example of "the resuscitation team members' motivation for cardiopulmonary resuscitation" based on Victor Vroom's expectancy theory is also presented. The directed qualitative content analysis method proposed in this paper is a reliable, transparent, and comprehensive method for qualitative researchers.
Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Sense of coherence (SOC) has been related to oral health behaviors and oral-health-related quality of life (OHRQoL) in observational studies. It has also been found that psychological factors might explain more about the impact of dental disorders upon individuals than their clinical symptoms [39] [#CITATION_TAG] [41] [42] [43]. Twelve primary schools were randomly allocated to intervention and control groups. The intervention was comprised of 7 sessions over 2 mos, focusing on child participation and empowerment. The first 4 sessions were classroom activities, and the last 3 involved working on healthy school projects. Trained teachers who received a one-day course delivered the intervention.
In many European countries, municipalities are becoming increasingly important as providers of electronic public services to their citizens. One of the horizons for further expansion is the delivery of personalised electronic services. In this paper, we describe the diffusion of personalised services in the Netherlands over the period 2006-2009 and investigate how and why various municipalities adopted personalised electronic services. In doing so, this article contributes to an institutional view on adoption and diffusion of innovations, in which (1) horizontal and vertical channels of persuasion and (2) human agency, rather than technological opportunity and rational cost-benefit considerations, account for actual diffusion of innovations. One respondent explained how one's own organisation could serve as a source of relevant knowledge (see also #CITATION_TAG): nan
The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. #CITATION_TAG have investigated first approaches towards large-scale requirements elicitation using social networks. A description of what they intend to measure is given together with how data are elicited and the advantages and limitation of the indicators. The glossary is divided into two parts for journal publication but the intention is that it should be used as one piece. The second part highlights a life course approach and will be published in the next issue of the journal.
A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. SummaryPrevious research has demonstrated that the emotional properties of words and their imaginability affect their recallability and that verbal material is recalled better when it is related to subjects' current concerns. Regarding the association of emotional arousal with goal-related cues, #CITATION_TAG computed 85 participants' intraindividual correlations between two kinds of their reactions to 40 words: the word's emotional "arousal potential" for them ("the strength of the subject's emotional reaction to the content of the word") and "the extent to which the word has to do with the subject's important concerns, problems, worries, or goals that currently preoccupy the subject." The two ratings were obtained in two different phases of the experiment, separated by a distractor task. Forty different words were presented visually under one of six orienting conditions that varied according to what the subject was asked to rate: their length, pronounceability, concreteness, defineability, the strength of emotion elicited by the word, and the relation of the word to personal concerns. Subjects were then asked to write as many words as they could recall.
Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. Abstract Many studies model the effects of the built environment on travel behaviour. Usually, results are controlled for socio-economic differences and sometimes socio-psychological differences among respondents. However, these studies do not mention why after all a relationship should exist between travel behaviour and spatial, socio-economic and personality characteristics. time geography, activity-based approach) and social psychology (e.g. Theory of Planned Behaviour, Theory of Repeated Behaviour). Standard mode choice research takes the individual or the household as the unit of observation since individual and household characteristics determine the choice process (#CITATION_TAG). Answering this query involves combining and linking theories stemming from transport geography (e.g. Comparable to customary theories in transport geography, this conceptual model considers travel behaviour as derived from locational behaviour and activity behaviour. But the conceptual model adds concepts such as 'lifestyle', 'perceptions', 'attitudes' and 'preferences' which indirectly influence travel behaviour.
The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Finally, as testing the significance of the mediation or indirect effects using bootstrap procedure has been recommended [#CITATION_TAG], Mplus [49] was specified to (a) create 5,000 bootstrap samples from the data set by random sampling with replacement and (b) generate indirect effects and bias-corrected confidence intervals (95 % CIs) around the indirect effects when analysing the (final) structural models (Fig. 2). Separate sections describe examples of moderating and mediating variables and the simplest statistical model for investigating each variable. The strengths and limitations of incorporating mediating and moderating variables in a research study are discussed as well as approaches to routinely including these variables in outcome research. The routine inclusion of mediating and moderating variables holds the promise of increasing the amount of information from outcome studies by generating practical information about interventions as well as testing theory.
Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Auditory verbal hallucinations (AVH) are complex experiences that occur in the context of various clinical disorders. However, their predictive value for specific psychiatric disorders is not entirely clear. This choice bias is especially interesting because it makes explanations of enhanced revaluation due to effort [33, #CITATION_TAG] less plausible for the effect of action on the dynamics of choice-induced preference change. AVH also occur in individuals from the general population who have no identifiable psychiatric or neurological diagnoses. Longitudinal studies suggest that AVH are an antecedent of clinical disorders when combined with negative emotional states, specific cognitive difficulties and poor coping, plus family history of psychosis, and environmental exposures such as childhood adversity.
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. BACKGROUND Healthcare professionals frequently advise patients to improve their health by stopping smoking. Such advice may be brief, or part of more intensive interventions. In some trials, subjects were at risk of specified diseases (chest disease, diabetes, ischaemic heart disease), but most were from unselected populations. The most common setting for delivery of advice was primary care. There was insufficient evidence, from indirect comparisons, to establish a significant difference in the effectiveness of physician advice according to the intensity of the intervention, the amount of follow up provided, and whether or not various aids were used at the time of the consultation in addition to providing advice. Only one study determined the effect of smoking advice on mortality. Yet, although these interventions are efficacious, [6] [7] [#CITATION_TAG] the majority of smokers who quit successfully do so without using them, choosing instead to quit unassisted, that is without pharmacological or professional support. SEARCH STRATEGY We searched the Cochrane Tobacco Addiction Group trials register and the Cochrane Central Register of Controlled Trials (CENTRAL). SELECTION CRITERIA Randomized trials of smoking cessation advice from a medical practitioner in which abstinence was assessed at least six months after advice was first provided. DATA COLLECTION AND ANALYSIS We extracted data in duplicate on the setting in which advice was given, type of advice given (minimal or intensive), and whether aids to advice were used, the outcome measures, method of randomization and completeness of follow up. We used the most rigorous definition of abstinence in each trial, and biochemically validated rates where available. Subjects lost to follow up were counted as smokers. Where possible, meta-analysis was performed using a Mantel-Haenszel fixed effect model. Other settings included hospital wards and outpatient clinics, and industrial clinics.
Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category "toys to pick up before bed" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the "just previous" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, "boring" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term "categorization." Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as "how do prototype representations compare between 6 and 8 months of age?" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. People are remarkably smart: they use language, possess complex motor skills, make non-trivial inferences, develop and use scientific theories, make laws, and adapt to complex dynamic environments. It is argued that conceptual development progresses from simple perceptual grouping to highly abstract scientific concepts. This paper reviews a large body of empirical evidence supporting this proposal. All categories are not created equal: categories vary in complexity and withincategory similarity (#CITATION_TAG). This proposal of conceptual development has four parts. First, it is argued that categories in the world have different structure. Third, these systems exhibit differential maturational course, which affects how categories of different structures are learned in the course of development.
The literature has identified antecedents and enablers for the adoption of GSCM practices. Nevertheless, there is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. *Research Highlights Green supply chain management enablers: Mixed methods research Research Highlights * This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. * There is relatively little research on building robust methodological approaches and techniques that take into account the dynamic nature of green supply chains. This paper contributes to the literature on green supply chain management (GSCM) by arguing for the use of mixed methods for theory building. Undertaking a review of the literature is an important part of any research project. However, traditional 'narrative' reviews frequently lack thoroughness, and in many cases are not undertaken as genuine pieces of investigatory science. Consequently they can lack a means for making sense of what the collection of studies is saying. These reviews can he hiased by the researcher and often lack rigour. Furthermore, the use of reviews of the available evidence to provide insights and guidance for intervention into operational needs of practitioners and policymakers has largely been of secondary importance. For practitioners, making sense of a mass of often-contrad ictory evidence has hecome progressively harder. The quality of evidence underpinning decision-making and action has heen questioned, for inadequate or incomplete evidence seriously impedes policy formulation and implementation. Over the last fifteen years, medical science has attempted to improve the review process hy synthesizing research in a systematic, transparent, and reproducihie manner with the twin aims of enhancing the knowledge hase and informing policymaking and practice. We conducted our systematic literature review (SLR) to identify the key enablers of GSCM and their interrelationships, following the principles set out by #CITATION_TAG, Rowley and Slack (2004) and were inspired by other prominent scholars (Burgess et al., 2006; Cousins et al., 2006) that have been used in recent reviews by Chen et al. (2014) and Gunasekaran et al. (2015). The researcher both maps and assesses the relevant intellectual territory in order to specify a research question which will further develop the knowledge hase.
However, it is found that in the regime of strong supersonic flows an appropriate limiting condition, which depends on the Prandtl number, must be imposed on the artificial conductivity SPH coefficients in order to avoid an unphysical amount of heat diffusion. This paper investigates the hydrodynamic performances of an SPH code incorporating an artificial heat conductivity term in which the adopted signal velocity is applicable when gravity is present. This article explores the relationship between the making of things and the making of people at the Bronze Age tell at Szazhalombatta, Hungary. Potters literally came into being as potters through repeated bodily enactment of potting skills. The creation of potters as a social category was essential to the ongoing creation of specific forms of material culture. This has motivated the development of adaptative mesh refinement (AMR) methods, in which the spatial resolution of the grid is locally refined according to some selection criterion (Berger & Colella 1989; Kravtsov, Klypin & Khokhlov 1997; #CITATION_TAG). Potters also gained their identity in the social sphere through the connection between their potting performance and their audience. We trace degrees of skill in the ceramic record to reveal the material articulation of non-discursive knowledge and consider the ramifications of the differential acquisition of non-discursive knowledge for the expression of different kinds of potter's identities. We examine the implications of altered potters' performances and the role of non-discursive knowledge in the construction of social models of the Bronze Age.
Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. Forest structure and dynamics vary across the Amazon Basin in an east-west gradient coincident with variations in soil fertility and geology. Basin-wide differences in stand-level turnover rates are mostly influenced by soil physical properties with variations in rates of coarse wood production mostly related to soil phosphorus status. A hypothesis of self-maintaining forest dynamic feedback mechanisms initiated by edaphic conditions is proposed. However, allometric equations that relate physical attributes of trees to their above-ground biomass normally rely on three parameters: in addition to tree height (H), tree diameter at 1.3 m (D) and wood density () are very important (Chave et al., 2005), and mean values and ratios between these parameters vary significantly between regions (Chave et al., 2005; Feldpausch et al., 2012; #CITATION_TAG), associated with different species communities (ter Steege et al., 2006). Soil samples were collected in a total of 59 different forest plots across the Amazon Basin and analysed for exchangeable cations, carbon, nitrogen and pH, with several phosphorus fractions of likely different plant availability also quantified. Physical properties were additionally examined and an index of soil physical quality developed. Bivariate relationships of soil and climatic properties with above-ground wood productivity, stand-level tree turnover rates, above-ground wood biomass and wood density were first examined with multivariate regression models then applied. Taking this into account, otherwise enigmatic variations in stand-level biomass across the Basin were then accounted for through the interacting effects of soil physical and chemical properties with climate.
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. The European Association for Palliative Care Task (EAPC) Force on the Development of Palliative Care in Europe was created in 2003 and the results of its work are now being reported in full, both here and in several other publications. Different models of service delivery have been developed and implemented throughout the countries of Europe. For example, in addition to the UK, the countries of Germany, Austria, Poland and Italy have a well-developed and extensive network of hospices. The model for mobile teams or hospital support teams has been adopted in a number of countries, most notably in France. Day Centres are a development that is characteristic of the UK with hundreds of these services currently in operation. The number of beds per million inhabitants ranges between 45-75 beds in the most advanced European countries, to only a few beds in others. The countries with the highest development of palliative care in their respective subregions as measured in terms of ratio of services per one million inhabitants are: Western Europe - UK (15); Central and Eastern Europe - Poland (9); Commonwealth of Independent States - Armenia (8). MA: Why have the development of palliative care and the drive for legal euthanasia been synergistic only in Belgium? How did Belgian "exceptionalism" come to be? That Belgium is one of the countries with the most developed palliative care systems (Centeno et al. 2007; #CITATION_TAG;) and the third country to have legally regulated euthanasia 5 is not paradoxical, as we have tried to make clear above, but it may also not be accidental. Four studies, each with different working methods, made up the study protocol: a literature review, a review of all the existing palliative care directories in Europe, a qualitative ;Eurobarometer' survey and a quantitative ;Facts Questionnaire' survey.
Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Some of the most successful pathogens of human, such as Mycobacterium tuberculosis (Mtb), HIV, and Leishmania donovani not only establish chronic infections but also remain a grave global threat. These pathogens have developed innovative strategies to evade immune responses such as antigenic shift and drift, interference with antigen processing/presentation, subversion of phagocytosis, induction of immune regulatory pathways, and manipulation of the costimulatory molecules. Costimulatory molecules expressed on the surface of various cells play a decisive role in the initiation and sustenance of immunity. Exploitation of the "code of conduct" of costimulation pathways provides evolutionary incentive to the pathogens and thereby abates the functioning of the immune system. Impairment by pathogens in the signaling events delivered by costimulatory molecules may be responsible for defective T-cell responses; consequently organisms grow unhindered in the host cells. In another study, hot monolithic loops visible with the Yohkoh/SXT were instead resolved as stranded cooler structures with TRACE at later times (#CITATION_TAG), although the large time delay (1 to 3 hours) is hardly compatible with the cooling time from SXT to TRACE sensitivity. nan
Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. An increased risk of death from all causes is not restricted to the most severe mental illnesses, but is also associated with conditions such as depression and anxiety disorders (#CITATION_TAG). I also discuss the health care that psychiatric patients receive, both in terms of recognition of physical illness and subsequent intervention, with particular reference to cardiovascular disease.
Phonological errors were scarce in both groups. The grammatical relations of noun phrases in sentences are ordered in a hierar-chy that is reflected in a wide array of linguistic phenomena. An experiment on the formulation of sentences examined the relationship betweeen conceptual acces-sibility and grammatical relations for three levels in the hierarchy, the subject, direct object, and indirect object. There was a strong and systematic influence of conceptual accessibility on the surface syntactic structure of sentences. Converting thoughts into language requires that elements of the nonlinguistic conceptual system be mapped onto syntactic roles in sentences. The nature of the cognitive and communicative features encoded in th Lexico-semantic representations are assigned to particular roles (e.g. the agent of an action) according to a conceptual message; thus this assignment cannot be independent of conceptual-semantic content (#CITATION_TAG; Bock & Warren, 1985). An experiment provides evidence that con-ceptual accessibility is linked to a hierarchy of grammatical relations that influences sentence formulation.
It is argued that ISE practices reinforced participants preexisting sense that museums and science centers were "not for us." This paper explores how people from low-income, minority ethnic groups perceive and experience exclusion from informal science education (ISE) institutions, such as museums and science centers. Drawing on qualitative data from four focus groups, interviews, four accompanied visits to ISE institutions, and field notes, this paper presents an analysis of exclusion from science learning opportunities during visits alongside participants' attitudes, expectations, and conclusions about participation in ISE. During the latter part of the Porfirio Diaz administration (1876-1910), the middle class grew as the city became a commercial and administrative center. Sociologists both criticized and praised the middle class and its role in the country's future. Members of the middle class distinguished themselves from the Porfirian elite and lower classes through bodily behaviors learned from urban conduct manuals and short stories. The Mexican Revolution (1910-1920) was a devastating blow to the middle class, which rallied around issues of housing, employment, and transportation. In the neighborhood of Santa Maria la Ribera, residents petitioned for urban services and infrastructure improvements. Continuing a long history of civic engagement, the city's middle class publicly organized in response to the anti-clerical policies of the Plutarco Calles administration (1924-1928). Economic and political difficulties hindered the efforts of post-revolutionary municipal and federal leaders to win state loyalty from Mexico City's public employees. At the same time, new mass media, fashions, and popular culture of the 1920s and 1930s challenged existing class distinctions and gender norms. Educational opportunities opened up wider prospects for the middle class, or those seeking middle-class status. Technical schools and the National Polytechnic School offered one set of possibilities. The Lazaro Cardenas administration (1936-1940) aimed to unite the middle class and the working class. As the state bureaucracy grew in the 1930s, Cardenas brought public employees into a close relationship with the National Revolutionary Party (PNR), which later became the Party of the Mexican Revolution (PRM). By the end of the Cardenas era, many sectors of the middle class felt politically marginalized. In contrast, middle-class public employees became beneficiaries of the country's new corporate state Here Thomas draws on his sense of the differences between "high" and "low" cultural practices, tastes, and class in terms of the forms of knowledge and behaviors that he believes would be more or less valued within an ISE institution (#CITATION_TAG; Bourdieu, 1984). nan
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. The study was a qualitative study carried out in 2002 by focus groups of 32 male secondary school students in Hong Kong who were either current smokers or had recently given up smoking. [30] Subsequent to this, no qualitative studies were identified that focused on unassisted cessation per se: the six post-2000 studies (from Hong Kong, US, UK, Canada and Norway) had as their primary focus either cessation in general [#CITATION_TAG, 34, [36] [37] [38] or health behaviour change. Subjects were students (grades 8-10) attending two full-day secondary schools in Hong Kong. Several barriers to quitting were reported, including boredom, peer influence, the urge to smoke, school work pressure, the wish to do something with their hands, difficulty in concentrating, and the ready availability of free cigarettes from peers.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. An approach to performance-based assessments that embeds assessments in digital games in order to measure how students are progressing toward targeted goals.To succeed in today's interconnected and complex world, workers need to be able to think systemically, creatively, and critically. Equipping K-16 students with these twenty-first-century competencies requires new thinking not only about what should be taught in school but also about how to develop valid assessments to measure and support these competencies. Many researchers have cited conscientiousness as compensating for lower cognitive intelligence (see Chamorro-Premuzic & Furnham, 2004, and it is a consistent predictor of academic performance across assessment type (Allick & Realo, 1997; Kappe & van der Flier, 2010; #CITATION_TAG). Embedding assessments within games provides a way to monitor players' progress toward targeted competencies and to use that information to support learning.Shute and Ventura discuss problems with such traditional assessment methods as multiple-choice questions, review evidence relating to digital games and learning, and illustrate the stealth-assessment approach with a set of assessments they are developing and embedding in the digital game Newton's Playground.
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Nicotine replacement therapies (NRTs) have been demonstrated to be effective in clinical trials but may have lower efficacy when purchased over-the-counter (OTC). [#CITATION_TAG] Further complicating the relationship, some regard commitment as a component of motivation, [57] operationalizing motivation as, for example, 'determination to quit' [58] or 'commitment to quit'. [59] The greater research interest in reasons for quitting or pros and cons of quitting (i.e., motivation) as opposed to commitment may be because motivation is simpler to measure, for example by asking people to rate or rank reasons, costs or benefits. nan
The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Diagnosis and Epidemiology of Binge-Eating Disorder. Binge-Eating Disorder and Obesity. Eating Behavior, Psychobiology, Medical Risks, and Pharmacotherapy of Binge-Eating Disorder. Binge-Eating Disorder and Bariatric Surgery. Psychotherapy for Binge-Eating Disorder. Part II: A Cognitive-Behavioral Treatment Program for Binge-Eating Disorder. Prior research comparing individuals with provisional DSM-IV BED to either DSM-IV BN purging or non-purging subtypes found significant differences in both current (age, BMI, and dietary restraint at the time of the assessment) and age-historical variables (age of onset) [#CITATION_TAG] [74] [75]. Clinical Features, Longitudinal Course, and Psychopathology of Binge-Eating Disorder.
Leading Edge Essay Distilling Pathophysiology from Complex Disease Genetics Aravinda Chakravarti,1,* Andrew G. Clark,2 and Vamsi K. Mootha3 1Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 2Cornell University, Ithaca, NY 14850, USA 3Massachusetts General Hospital, Boston, MA 02114, USA *Correspondence: aravinda@jhmi.edu http://dx.doi.org/10.1016/j.cell.2013.09.001 Technologies for genome-wide sequence interrogation have dramatically improved our ability to identify loci associated with complex human disease. However, a chasm remains between correlations and causality that stems, in part, from a limiting theoretical framework derived fromMendelian genetics and an incomplete understanding of disease physiology. It does not take much perspicacity to see that what really makes this difference is not the tall hat and the umbrella, but the wealth and nourishment of which they are evidence, and that a gold watch or membership of a club in Pall Mall might be proved in the same way to have the like sovereign virtues.. George Bernard Shaw, The Doctor's Dilemma (Preface), 1909 Distinguishing correlation from causality is the essence of experimental science. Nowhere is the need for this distinction greater today than in complex disease genetics, where proof that specific genes have causal effects on human disease phenotypes remains an enormous burden and challenge. This is particularly so in this age of routine -omic surveys, which can produce more false-positive than true-positive findings (Kohane et al., 2006). Moreover, genomic mapping and sequencing approaches that are invaluable for producing a list of unbiased candidates are, by themselves, insufficient for implicating specific gene(s) in a disease or biological process. We admit at the outset that the answers are not straightforward, and that there are serious technical and intellectual impediments to demonstrating causality for the common complex disorders of man where multiple interacting genes are involved. Nevertheless, the casual conflation of ''mapped locus'' to ''proven gene'' is a constant source of confusion and obfuscation in biology and medicine that requires remedy. Consider that two types of genomic surveys, one horizontal and the other vertical, are now routine for attempting to understand human biology and disease. In contrast, in vertical or deep surveys, we examine the effects of the genome as the DNA information gets processed, and its encoded functions get executed through its transcriptome, proteome, and effectors such as the metabolome. In turn, this implies that proving a gene's specific role in a biological process, either in wild-type or mutant form, may not be straightforward because its role may only be evident when examined in relation to its eptember 26, 2013 a2013 Elsevier Inc. 21 Box 1. biochemical partners, and in particular contexts of diet, pathogen exposure, etc. This is a particular problem in genetic studies of any outbred nonexperimental organism, such as the human, and studies of human disease, where investigations are observational not experimental. It is the strong belief of contemporary human geneticists that uncovering the genetic underpinnings of any disease, however complex, is the surest unbiased route to understanding its pathophysiology and, thus, enabling its future rational therapies (Brooke et al., 2008). Consequently, for this view to prevail, we should require experimental evidence, be it in cells, tissues, experimental models, or the rare patient, for the role of a specific gene in a disease process. We know that even in a simple model organism, budding yeast, synthetic lethality-- where death or some other phenotype occurs only through the conspiracy of mutations at two different genes--is widely prevalent (Costanzo et al., 2010). Interactions of greater complexity and involving more than two genes are also known in yeast (Hartman et al., 2001) and must be true for humans as well. A human genome will typically harbor 20 genes that are fully inactivated, without 22 Cell 155, September 26, 2013 a2013 Else any overt disease phenotype, presumably due to the buffering by other genes (MacArthur et al., 2012). Acknowledging this complexity, there are two general ways forward. The question then is how ''complex'' are complex traits and diseases? The New Genetics: Understanding the Function of Variation With the rediscovery of Mendel's rules of transmission more than 100 years ago, there was a vicious debate on the relative importance of single-gene versus multifactorial inheritance (Provine, 1971). Geneticists quickly, and successfully, focused on deciphering the specific mechanisms of gene inheritance and understanding the physiology of the gene in lieu of answering why some phenotypes had complex etiology and transmission. Nevertheless, the rare examples of deciphering the genetic basis of complex phenotypes, such as for truncate (wing) in Drosophila (Altenburg and Muller, 1920), clearly emphasized that traits were more than the additive properties of multiple genes. Today, it is quite clear that Mendelian inheritance of traits, including diseases, is the exception not the rule. Nevertheless, the entire language of genetics is in terms of individual genes for individual phenotypes, with one function, rather than the ensemble and emergent properties of genomes. This absence of a specific genetics language for the proper description of the multigenic architecture of traits (the ensemble) remains as an impediment to our understanding of the nature and degree of genetic complexity of the phenotype. The case of amyotrophic lateral sclerosis (ALS), a devastating, progressive motor neuron disease, illustrates this point (Ludolph et al., 2012). Despite the lack of evidence, we largely describe ALS as being ''heterogeneous'' and comprised of single-gene mutations that can individually lead to disease. In 1993, mutations in superoxide dismutase 1 (SOD1) were identified in an autosomal-dominant form of the disease; subsequently, the disorder has become synonymous with aberrant clearance of free radicals as its central pathology. What is often not appreciated, however, is that fewer than 10% of all cases of ALS are familial and even fewer follow an apparent Mendelian pattern. Even within this subset of cases, more than 20 distinct genes, spanning other pathways including RNA homeostasis, have been identified, and SOD1 represents a minority of cases. The molecular etiology for the majority of the sporadic forms of the disease remains unclear, and the scientific problem in understanding ALS is more than simply identification of additional genes. Are these the key rate-limiting steps to ALS or simply one of several required in concert? Is the aberrant clearance of free radicals the fundamental defect or one of many such pathologies or a common downstream consequence? Given the diversity and number of deleterious, even loss-of-function, genetic variants in all of our genomes (Abecasis et al., 2012; MacArthur et al., 2012) and, in the absence of stronger evidence bearing on these questions, it is fair to assume that ALS patients harbor multiple mutations with a plurality of molecular defects and that free radical metabolism is only one of a set of canonical pathophysiologies that define the disease. No doubt, this plurality is the case for cancer (Vogelstein et al., 2013), Crohn's disease (Jostins et al., 2012), and even rare developmental disorders such as Hirschsprung disease (McCallion et al., 2003). Molecular biology, genetics' twin, on the other hand, appears to have been far more successful in deciphering and describing not only its individual components (e.g., DNA, RNA, protein) but also their mutual relationships (e.g., DNAprotein interaction) and ensembles (e.g., transcriptional complex), although this is also far from complete (Watson et al., 2007). The consequences of the primary and interaction effects are often well understood, even though not completely described, at both the molecular and cellular levels (Alberts et al., 2007). Although the use of genetic tools and genetic perspectives are fundamental to this progress, these advances have not as yet led to a major revision of our understanding of trait or disease variation. The major reason for this discrepancy is that, with few exceptions (Raj et al., 2010), molecular and cell biology has focused on the impact of deleting or overexpressing genes and not grappled with the consequences of allelic variation. Classical Mendelian genetics has been a boon to uncovering biology from yeast to humans whenever a mutation with a simple inheritance pattern can be isolated. This approach has been revolutionary in the unicellular yeast, particularly because genetics (and gene manipulation), biochemistry, and cell biology were melded to understand function at a variety of levels. This kind of multilevel approach has been less straightforward, but still largely successful, for a metazoan such asDrosophilawheremore genes andmultiple specialized cells often rescue the effects of a mutation or enhance its minor effect. Success in this endeavor will require a synthesis of many biological disciplines that includes the role of genetic variation as intrinsic to the biological process, not an aspect to be ignored. Consequently, melding variation-based genetic and molecular biological thinking is of critical importance for both fields and is central to our understanding of mechanisms of trait variation, including interindividual variation in disease risk. If most disease, in most humans, is the consequence of the effects of variation at many genes, then knowledge of their functional relationships, rather than merely their identities, is central to understanding the phenotype. This is clearly a problem of ''Systems Biology'' but one that incorporates genetic variation directly. The ability to integrate the realities of such widespread genetic variation, which are ultimately at the causal root of disease mechanisms, with systems biology approaches to understand functional contingencies is central to the challenge of deciphering complex human disease. Genetic Dissection of Complex Phenotypes Genetic transmission rules imply that, even in an intractable species such as us, one can map genomic segments that must contain a disease or trait gene. Such mapping requires identification of the segregation of common sites of variation across the genome, now easy to identify through sequencing, and recognition of a genomic segment identical-by-descent in affected individuals, both within and between families. This task has become easier and more powerful as sequencing technology has improved to provide a nearly complete catalog of variants above 1% frequency in the population; further improvements to sample rarer variants are ongoing (Abecasis et al., 2012). Consequently, genetic mapping, once the province of rare Mendelian disorders, Cell 155, S is now applicable to any human trait or disease. For most complex traits examined, many such loci have been mapped, but the vast majority of the specific genes remain unidentified. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (Moon et al., 2004), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. In fact, most of the mapped loci underlying complex traits remain unresolved at the gene or mechanistic level. Despite the beginning clues to human disease pathophysiology that complex disease mapping is providing, and the slow identification of individual genes, it appears highly unlikely that we can understand traits and diseases this way. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (Yang et al., 2011). For Mendelian disorders, gene identification within a locus is made possible by each mutation being necessary and sufficient for the phenotype, being functionally deleterious and rare, and having an inheritance pattern consistent with the phenotype. It's the mutation that eventually reveals the biology and explains the phenotype. Any component locus for a complex disease has no such restriction, as the causal variants are neither necessary nor sufficient, nor coding (in fact, they are frequently noncoding and regulatory) nor rare (Emison et al., 2010; Jostins et al., 2012). Currently, the major attempts to overcome this impediment involve reliance on single severe mutations at the very same component genes and eptember 26, 2013 a2013 Elsevier Inc. 23 Genetic association studies in humans can synergize with prior knowledge and systems-level quantitative analysis to generate predictions of what pathways and modules are disrupted, where (anatomically), and when (developmentally) to yield a specificmorphological or biochemical phenotype. Consequently, these strategies themselves depend on the hidden biology we seek and are applicable only to the most common human diseases. It appears to us that ignorance of biology has become rate limiting for understanding disease pathophysiology, except perhaps for the Mendelian disorders. There are two ways to get out of this vicious cycle (Figure 1). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (Ilsley et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the vier Inc. complex inheritance problem (Yosef et al., 2013). One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013). Proving Causality: Molecular Koch's Postulates The evidence that a specific gene is involved in a particular human disease has historically been nonstatistical and based on our experience with identifying mutations in Mendelian diseases. Unfortunately, as already mentioned, all of these rules break down in complex phenotypes where neither cosegregation nor exclusivity to affecteds nor obviously deleterious alleles are likely; moreover, many mutations are suspected to be noncoding and in a diversity of regulatory RNA molecules. Consequently, statistical evidence of enrichment has been the mainstay, but this has two negative consequences: first, scanning across the genome or multiple loci covering tens to hundreds of megabases requires very large sample sizes and very strict levels of significance to guard against themany expected falsepositive findings; second, genetic effects that are small or genes with only a few causal alleles are notoriously difficult to detect, although they may be very important to understanding pathogenesis. This difficulty translates into a low power of detection, as common disease alleles cannot be distinguished from bystander associated alleles, whereas rare alleles are observed too infrequently to provide statistical significance. Consequently, although many genes are ''named'' as being responsible in a complex disease or disease process, proof of their involvement is either absent or circumstantial and not direct. In the late 19th century when bacteria were first shown to cause human disease, they were indiscriminately implicated in all manner of disease with little proof (Brown and Goldstein, 1992). One particularly embarrassing example was alcaptonuria, which Sir Archibald Garrod subsequently showed was inherited and which was his first ''inborn error of metabolism.'' We are likely to repeat this ''witch-hunt'' unless we are careful to note that mapping a locus is not equivalent to identifying the gene, and that identifying a gene and its mutations at a locus depends on numerous untested assumptions (mutational type, mutational frequency in cases and controls, coding or regulatory, cell autonomy). Inmicrobiology, Robert Koch set out three postulates that had to be satisfied to connect a specific bacterium (among the multitudes encountered, not unlike current genome analysis) to a disease: the agent had to be isolated from an affected subject, the agent had to produce disease when transmitted to an animal, and the agent had to be recoverable from an animal's lesion (Falkow, 1988). Simply because we cannot follow Koch to the letter in human patients does not absolve us from the responsibility of demonstrating a rigorous level of proof. This is particularly true if we are to pursue therapeutic targets for these diseases. It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-a-vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families Cell 155, S shows how environmental factors beyond diet can be examined even for congenital disorders (Sparrow et al., 2012). Despite these successes, pursuit of Koch's postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates. Ultimately, a lack of understanding of fundamental physiology is the biggest impediment to our understanding of genetically complex human disease. A unique aspect of genetics research seldom appreciated is that genetic effects are chronic biological exposures and as such can pinpoint the earliest stages of disease not readily studied otherwise. In reality, we still do not fully understand the pathogenesis stemming from some of the earliest identified human disease genes. With better understanding of disease mechanism, it seems likely that many disorders that we think of as ''genetic'' may have ameliorative diet, exercise, or other benign environmental ''treatments.'' Given the potential scientific and medical payoffs of disease gene discovery (Chakravarti, 2001), we argue in this Essay of the need for a rigorous examination of the assumptions under which we connect genes to phenotypes. Below we discuss the nature of the ''proof'' that we desire in order to make fundamental discoveries in human pathophysiology. Success in this difficult task requires us to solve a logical conundrum: how can we understand the genes underlying a phenotype if some of these component factors, in isolation, do not have recognizable phenotypes on their own? Both of these goals are approachable, particularly with recent advances in genome-editing technologies that allow the creation of multiple mutations within a single experimental organism (Wang et al., 2013). The second approach is to focus research on why the disease is complex in the first place. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. The chief criteria have been to demonstrate cosegregation with the phenotype in families, exclusivity of the mutation to affected individuals (rare alleles absent in controls), and the nature of themutation (a plausibly deleterious allele at a conserved site within a protein). We need to move beyond lists of plausible genes, to provide rigorous proof for their role in disease. But this goal is unlikely to be achieved in the absence of a superior understanding of the biology of hierarchical function within genomes, how variation alters these functions, and how these altered functions lead to human disease. Probabilistic models of cognition characterize the abstract computational problems underlying inductive inferences and identify their ideal solutions. Recent research employing this strategy has focused on the possibility that the Monte Carlo principle--which concerns sampling from probability distributions in order to perform computations--provides a way to link probabilistic models of cognition to more concrete cognitive and neural processes. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (#CITATION_TAG), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. To evaluate the theoretical implications of probabilistic models and increase their predictive power, we must understand the relationships between theories at these different levels of analysis.
The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. Erlang is gaining widespread adoption with the advent of multi-core processors and their new scalable approach to concurrency. This book helps you: Understand the strengths of Erlang and why its designers included specific features Learn the concepts behind concurrency and Erlang's way of handling it Write efficient Erlang programs while keeping code neat and readable Discover how Erlang fills the requirements for distributed systems Add simple graphical user interfaces with little effort Learn Erlang's tracing mechanisms for debugging concurrent and distributed systems Use the built-in Mnesia database and other table storage features Erlang Programming provides exercises at the end of each chapter and simple examples throughout the book. Introduction Erlang [3,#CITATION_TAG] is a functional programming language with builtin support for concurrency based on share-nothing processes and asynchronous message passing Written by leaders of the international Erlang community -- and based on their training material -- Erlang Programming focuses on the language's syntax and semantics, and explains pattern matching, proper lists, recursion, debugging, networking, and concurrency.
The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. In this paleoenvironment, the presence of the terrestrial mammals was favored by the availability of relatively abundant mesophytic vegetation, which was seemingly perturbed by the volcanic exhalations associated to the monogenetic volcanism of the Michoacan-Guanajuato volcanic field. The isolation of the Allotoca diazi complex from its common ancestor A. zacapuensis, does not contradict the hypothesis of lvarez (1972) [21] with respect to the connection between the Zacapu and Ptzcuaro basins during the Pleistocene, resulted from the formation of the El Zirate mountain and the northern Ptzcuaro Lake shoreline during the Late Pleistocene [#CITATION_TAG]. This column contains vegetation remnants and bone fragments, including a well preserved jaw of a gomphothere, which was recovered from a volcanic lahar deposit intercalated with fluvial deposits and a pyroclastic succession associated to the basaltic monogenetic volcanism of the Cerro Catio. The complete volcanic sequence was emplaced within a fluvial endorreic basin restricted to the northern portion of the Patzcuaro lake. This late Pleistocene age can be assigned to the gomphothere as well as to the basaltic volcanic event of the Cerro Catio. Moist climatic conditions favored the devitrification process of the volcanic ash and the weathering of other minerals, which provided good conditions for recovering of the mesophillic vegetation: Fraxinus, Acer, Corylus, Ulmus, Betula, and Juglans.
Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. (c) Finding Experts By Semantic Matching of User Profiles Rajesh Thiagarajan, Geetha Manjunath, Markus Stumptner HP Laboratories HPL-2008-172 semantics, similarity matching, ontologies, user profile Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Extracting interest profiles of users based on their personal documents is one of the key topics of IR research. Thiagarajan et al. [#CITATION_TAG] use a different strategy by representing user profiles as bags-ofwords and weighing each term according to the user interests derived from a domain ontology. However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results. However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results. However, when these extracted profiles are used in expert finding applications, only naive text-matching techniques are used to rank experts for a given requirement. In this paper, we address this gap and describe multiple techniques to match user profiles for better ranking of experts. We show that using these techniques, we can find an expert more accurately than other approaches, in particular within the top ranked results.
The latter stem, in part, from contradictions between potentially incompatible organizational agendas and social logics that drive the use of this approach. The presence of such diverse and partially contradictory aims creates tensions with the result that efforts are at times diverted from the aim of producing sustainable change and improvement. This paper examines the challenges of investigating clinical incidents through the use of Root Cause Analysis. While RCA is formally endorsed by policy makers in USA, UK, Australia, and Denmark (vretveit, 2005) and is in the process of being adopted by other countries, we have only a partial understanding of the challenges of using this approach, despite research suggesting it is not without problems (#CITATION_TAG; Iedema, Jorm, Long et al., 2006; Wallace, 2006; Wu, Lipshutz, & Pronovost, 2008). First, RCA team members find themselves in the unusual position of having to derive organizational-managerial generalizations from the specifics of in situ activity. Second, they are constrained by the expectation inscribed into RCA that their recommendations result in 'systems improvements' assumed to flow forth from an extension of formal rules and spread of procedures.
The fungicides used to control diseases in cereal production can have adverse effects on non-target fungi, with possible consequences for plant health and productivity. The fungal community on wheat leaves consisted mainly of basidiomycete yeasts, saprotrophic ascomycetes and plant pathogens. This study examined fungicide effects on fungal communities on winter wheat leaves in two areas of Sweden. The phyllosphere is a rich and varied microbial community comprising organisms with diverse functional types. Its composition is strongly influenced by both genotypic and environmental factors, many of which can be manipulated by breeding, agronomy and crop protection strategies in an agricultural context. The phyllosphere, defined as the total above-ground parts of plants, provides a habitat for many microorganisms [#CITATION_TAG]. nan
Home to work travel remains the prime focus of mobility management policies, in which the promotion of carpooling is one of the main strategies. Besides governments, employers are key players in this strive for a more sustainable commute. However, commuting research tends to focus on individual commuters and their place of residence, rather than on workplaces and company-induced measures. Therefore, this paper takes the workplace as research unit to analyse the popularity of carpooling in Belgium. It is found that carpooling is sensitive to tra%0c congestion reduction only when a congestion externality-based tolling scheme is implemented. Flexitime and the promotion of carpool are then seen as conflicting mobility management measures (Buliung et al., 2010; #CITATION_TAG). The logit-based stochastic model involves the consideration on preference option of mode choice.
Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. Evidence that lichenized blue-green algae are among the principal agents of N2 fixation on drier terrain in the Arctic and Subarctic has prompted attempts to quantify N-input by lichens in these habitats. The nitrogenase activity in Stereocaulon paschale mats in spruce-lichen woodland of central subarctic Canada has been examined in relation to thallus water content, temperature and incident radiation. It is suggested that simple predictive models are not yet able to accurately describe levels of nitrogenase activity in nature and that estimates of N-input on an annual or seasonal basis may be precarious. Leaching of metabolites from, and decomposition of the thallus are the two principal potential pathways for nitrogen, subsequent to fixation by cyanophilic lichens. Quantitative information on the operation of these pathways in nature is required. Lichenized blue-green algae are probably among the principal agents of nitrogen (N2) fixation in drier terrestrial habitats of the Arctic and Subarctic (Schell & Alexander, 1973; Kallio & Kallio, 1975; Crittenden, 1975; Huss-Danell, 1977). Several investigators working in boreal-arctic systems have used measurements of nitrogenase activity in lichens obtained either under field conditions or in the laboratory to derive estimates of N-input per unit area by the lichen biomass in situ on a seasonal or annual basis. The major shortcoming of such estimates is that their accuracy is unknown: the results of in situ measurements of nitrogenase activity over long periods have not been compared with those of predictive procedures applied to the same period. We have recently been examining the N2 fixing capabilities of S. paschale mats in spruce-lichen woodland in the Abitau-Dunvegan Lakes region of the Northwest Territories (60?21'N, 106'54'W). NITROGEN FIXATION BY STEREOCAULON PASCHALE IN SPRUCE-LICHEN WOODLAND--A CASE STUDY The importance of Stereocaulon paschale as a major component of spruce-lichen woodland in central subarctic Canada has been reviewed by Kershaw (1977). The maintenance of large areas of open Stereocaulon-spruce woodland in this region is dependent upon the high frequency of forest fire, the latter being a natural facet of the environment in the boreal forest (see, e.g. In the AbitauDunvegan Lakes region the periodicity of fire is such that spruce woodland older than 200 y is infrequently encountered on drier terrain and the average reburn interval is less than 100 y (Maikawa & Kershaw, 1976). Stereocaulon paschale usually dominates the lichen synusia in spruce-lichen woodland of between 60 and 130 y old where almost pure carpets of this lichen may occur (Fig. The acetylene (C2H2-) reduction technique (Stewart, Fitzgerald & Burris, 1967) with modifications after Stewart et al. Intensive monitoring programs of this kind were conducted on five occasions: 24-25 June, 6-7, 10-11 and 18-19 August 1976 and 10-11 May 1977. On 17 August 1976 the lichen mat was wetted by rainfall commencing at 04.15 h and monitoring of nitrogenase activity began at 14.30 h the following day. Photosynthetically active radiation (Fig. These data together ascertain the significance of biological N 2 fixation to the N-limited vegetation in the Subarctic ( #CITATION_TAG and Kershaw 1978).In conclusion, our multispecies comparison of N 2 fixation rates of cryptogams, both under standardised conditions and in situ, applying a direct nitrogen detection method, has provided strong evidence for differential nitrogen input by cryptogam taxa in the Subarctic, and substantiates their importance to the nitrogen economy of these cold biomes Similarly, Horne (1972) predicted annual N2 fixation by Collema pulposum (= C. tenax) on mossy gravel surfaces of Signy Island, Antarctica. This study was conducted while one of us (P.D.C.) The field studies were conducted in typical S. paschale woodland estimated by Maikawa (1976) to have been last burned in 1898. Sequential C2H2-reduction assays were performed during periods of natural diffuse radiation to avoid the acute problem of excessive elevation of thallus temperature within the incubation bottles that occurs at high levels of irradiance and each assay was begun within 2 min of removing the pseudopodetium from the lichen mat. Thallus temperature within the lichen mat was measured with micro-thermocouples (42 gauge wire) arranged in thermopiles and with the junctions embedded in the main stem of pseudopodetia within the upper 15 mm of the lichen canopy (Fig. A similar method was employed to monitor thallus temperature within the incubation bottles (Fig. 2b) thus providing a check on the extent of deviations from operative temperature in situ. 2b) was measured with a quantum sensor (Lambda Instruments Corporation), and pseudopodetia were weighed on an electrobalance immediately prior to incubation in order that water content could be determined after dry weight estimates had been obtained (Fig.
Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is common, debilitating and associated with high societal costs. Intervention costs of both treatments are offset by net societal cost reductions in a short time. Published economic analyses have explored the cost-effectiveness of a very limited range of interventions for social anxiety disorder and concluded that escitalopram [53], group CBT [54] [55] [#CITATION_TAG] and computer-based self-help [55] [56] [57] are cost-effective options. We conducted a 4-year follow-up study of participants who had received ICBT or CBGT for SAD within the context of a randomized controlled non-inferiority trial. The cost-effectiveness analyses were conducted taking a societal perspective.
requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Background: There has been a rapid growth in recent years of available technologies for individuals with communication difficulties. Research in the area is currently under-developed with practitioners having a limited body of work to draw on to guide the process of intervention. Concerns have been raised that this newly-developed technology may have limited functional usage. The synthesis of evidence describing views of users and providers, and the implementation of high tech AAC systems, can provide valuable data to inform intervention studies and functional outcome measures Findings regarding the qualitative studies are reported elsewhere [#CITATION_TAG]. Main Contribution: The review highlights the range of factors that can impact on provision and use of high tech AAC, which practitioners should consider and address as appropriate in the intervention process. The work outlines how qualitative synthesis review methods may be applied to the consideration of published material that is not reporting outcomes data, and how this may provide valuable information to inform future studies.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. In addition, including psychometric data in models of learning can provide useful feedback on the learning dispositions that assessment design rewards (#CITATION_TAG). It consisted of a series of five papers: one conceptual paper on ECD, three applied case studies that use ECD and EDM tools, and one simulation study that relies on ECD for its design and EDM for its implementation.
Heart rate variability (HRV) refers to various methods of assessing the beat-to-beat variation in the heart over time, in order to draw inference on the outflow of the autonomic nervous system. Easy access to measuring HRV has led to a plethora of studies within emotion science and psychology assessing autonomic regulation, but significant caveats exist due to the complicated nature of HRV. Secondly, experiments often have poor internal and external controls. In this review we highlight the interrelationships between HR and respiration, as well as presenting recommendations for researchers to use when collecting data for HRV assessment. In lieu of direct respiratory measures, established algorithms (Moody et al., 1985 (Moody et al.,, 1986) that have been successively improved (e.g., #CITATION_TAG; Langley et al., 2010) can also provide an appropriate surrogate measure of respiration from based on ECG signal morphology. The whole system consists of two-lead electrocardiogram acquisition using conductive textile electrodes located in bed, baseline fluctuation elimination, R-wave detection, adjustment of sudden change in R-wave area using moving average, and optimal lead selection. In order to solve the problems of previous algorithms for the ECG-derived respiration (EDR) signal acquisition, we are proposing a method for the optimal lead selection. An optimal EDR signal among the three EDR signals derived from each lead (and arctangent of their ratio) is selected by estimating the instantaneous frequency using the Hilbert transform, and then choosing the signal with minimum variation of the instantaneous frequency.
Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Building on social identity theory (e.g., Tajfel, 1978) and social network analysis (e.g., #CITATION_TAG), Paxton (2007, p. 51, italics in original) argues that the generalization of trust beyond a given voluntary association 'is critically dependent on whether an individual belongs to an association that is connected to other associations or one that is isolated'. We will then move on to Durkheim's organic view of society, to Marx's dialectical materialism, finishing with Weber's Verstehen sociology and ideal types of authority. We'll try to understand their theories not just as historical relics, but as living sets of ideas relevant to contemporary social issues.
Although general anesthetics are thought to modify critical neuronal functions, their impact on neuronal communication has been poorly examined. We have investigated the effect induced by desflurane, a clinically used general anesthetic, on information transfer at the synapse between mossy fibers and granule cells of cerebellum, where this analysis can be carried out extensively. Information Theory enables the quantification of how much information a neuronal response carries about external stimuli and is hence a natural analytic framework for studying neural coding. The main difficulty in its practical application to spike train analysis is that estimates of neuronal information from experimental data are prone to a systematic error (called "bias"). This bias is an inevitable consequence of the limited number of stimulus-response samples that it is possible to record in a real experiment. MI descends directly from response entropy and noise entropy [13], which are correlated to the variability of responses to separate inputs [13] or to the same input [14] [15] [#CITATION_TAG], respectively. In this paper, we first explain the origin and the implications of the bias problem in spike train analysis. We then review and evaluate some recent general-purpose methods to correct for sampling bias: the Panzeri-Treves, Quadratic Extrapolation, Best Universal Bound, Nemenman-Shafee-Bialek procedures, and a recently proposed shuffling bias reduction procedure. This provides information estimates with acceptable variance and which are unbiased even when the number of trials per stimulus is as small as the number of possible discrete neuronal responses.
The genetic diversity of green algal photobionts (chlorobionts) in soil crust forming lichens was studied as part of the SCIN-project (Soil Crust InterNational). The dominant lichen species at all four sites was Psora decipiens, often occurring with Buellia elegans, Fulgensia bracteata, F. fulgens and Peltigera rufescens. Regional droughts are common in North America, but pan-continental droughts extending across multiple regions, including the 2012 event, are rare relative to single-region events. During the Medieval Climate Anomaly (MCA), the central plains (CP), Southwest (SW), and Southeast (SE) regions experienced drier conditions and increased occurrence of droughts and the Northwest (NW) experienced several extended pluvials. Notably, megadroughts in these regions differed in their timing and persistence, suggesting that they represent regional events influenced by local dynamics rather than a unified, continental-scale phenomena. While relatively rare, pancontinental droughts are present in the paleo record and are linked to defined modes of climate variability, implying the potential for seasonal predictability. Peltigera rufescens, known to have a cyanobacterium as its primary photobiont (#CITATION_TAG), was also found to be associated with chlorobionts (Henskens et al. 2012). Positive values of the Southern Oscillation index (La Ni~ conditions) are linked to SW, CP, and SE (SW1CP1SE) droughts and SW, CP, and NW (SW1CP1NW) droughts, whereas CP, NW, and SE (CP1NW1SE) droughts are associated with positive values of the Pacific decadal oscillation and Atlantic multidecadal oscillation.
A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Little is known, however, about neural bases of the cognitive control of emotion. The present study employed functional magnetic resonance imaging to examine the neural systems used to reappraise highly negative scenes in unemotional terms. There is decreased activity in the limbic system, especially (Benson et al., 1999) Cornelissen et al., 2013 the amygdala, with dorsolateral prefrontal cortex becoming relatively more active and orbitomedial and cingulate cortex less so; a move toward normality from patterns observed before treatment (#CITATION_TAG; Goldapple et al., 2004; Malhi et al., 2004; Ritchey et al., 2011; Hflich et al., 2012) and consistent with what is know of the processing of emotional stimuli (Simpson et al., 2000; Northoff et al., 2004; Leppnen, 2006; Beck, 2008). Reappraisal of highly negative scenes reduced subjective experience of negative affect.
It is difficult to overvalue the importance of polysaccharides for the great number of applicative fields in which they appeared. Oligosaccharides are relatively short compounds that are prepared from the longer polysaccharides or could also be found as such in nature. The potential in bioactivity of marine polysaccharides is still considered under-exploited and these molecules, including the derived oligosaccharides, are an extraordinary source of chemical diversity. Sustainable ways to access marine oligosaccharides are particularly important in view of the huge list of the effects they play in cell events; enzymatic tools, on which these sustainable ways are based, and modern techniques for purification and for the investigation of chemical structures, will be shortly discussed indicating the most important recent literature. Alginate-derived oligosaccharides of 373-571 Da and chitooligosaccharides of 855-1671 Da obtained by enzymolysis with alginate lyase and chitosanase respectively, were investigated for cell regulation, erythrocytes haemolysis inhibition and antioxidant capacity (#CITATION_TAG). Furthermore, AOSC blocked the fibril formation of Abeta, which may be responsible for its anti-cytotoxic effects.
We studied choreographer Wayne McGregor's approach to movement creation through tasking, in which he asks dancers to create movement in response to task instructions that require a great deal of mental imagery and decision making. As part of a programme of research that is developing tools to enhance choreographic practice, an interdisciplinary team of cognitive scientists, neuroscientists and dance professionals collaborated on two studies examining the mental representations used to support movement creation. What is the contribution of the extrastriate body area (EBA) to this network? In particular, is the EBA involved in constructing a dynamic representation of observed actions? This region is thought to hold a human body representation (#CITATION_TAG) as well as a dynamic action representation (Downing et al., 2006) that may have contributed to both imagery creation and static movement creation. Abstract Numerous cortical regions respond to aspects of the human form and its actions. We scanned 16 participants with fMRI while they viewed two kinds of stimulus sequences. In the coherent condition, static frames from a movie of a single, intransitive whole-body action were presented in the correct order. In the incoherent condition, a series of frames from multiple actions (involving one actor) were presented. We suggest that the EBA response adapts when succeeding images depict relatively similar postures (coherent condition) compared to relatively different postures (incoherent condition).
NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the "Western diet" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The "Selfish Brain" is a term coined by Robert L. DuPont in the title of his book where he wrote: "With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: "The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the "push" principle borrowed from the economic "push-pull" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: "According to the 'push' principle, the environment pushes excess amounts of energy into the organism" (Hill and Peters, 1998). According to DuPond, "What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both "pull" and "push" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this "push" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called "unique" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the "puSh" component of energy metaBolISm The environment in Western-type societies can be characterized as "pushing" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental "Western Diet" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: "antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental "push." In a recent article entitled "Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy," J. Zimmerman wrote: "In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. "Control" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. The ability to store energy enables organisms to deal with temporarily harsh and uncertain conditions. Empirical studies have demonstrated that organisms adapted to fluctuating energy availability plastically adjust their storage strategies. So far, however, theoretical studies have investigated general storage strategies only in constant or deterministically varying environments. In environments with low variability and low predictability of energy availability, it is not optimal to store energy. As environments become more variable or more predictable, energy allocation to storage is increasingly favoured. The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (#CITATION_TAG). By varying environmental variability, environmental predictability, and the cost of survival, we obtain a variety of different optimal life-history strategies, from highly iteroparous to semelparous, which differ significantly in their storage patterns.
Many energy system optimization studies show that hydrogen may be an important part of an optimal decarbonisation mix, but such analyses are unable to examine the uncertainties associated with breaking the 'locked-in' nature of incumbent systems. Uncertainties around technical learning rates; consumer behaviour; and the strategic interactions of governments, automakers and fuel providers are particularly acute. System dynamics and agent-based models, and studies of historical alternative fuel transitions, have furthered our understanding of possible transition dynamics, but these types of analysis exclude broader systemic issues concerning energy system evolution (e.g. supplies and prices of low-carbon energy) and the politics of transitions. This paper presents a hybrid approach to assessing hydrogen transitions in the UK, by linking qualitative scenarios with quantitative energy systems modelling using the UK MARKAL model. In a quest for strategic and environmental benefits, the developed countries have been trying for many years to increase the share of alternative fuels in their transportation fuel mixes. They have met very little success though. Argentina's national programme to promote CNG vehicles relied on price controls on fuel, and has been relatively successful despite almost no government involvement in infrastructure provision [#CITATION_TAG]. We conducted interviews with a wide range of stakeholders and analyzed econometrically data collected in Argentina to investigate the factors, economic, political, and others that determined the high rate of adoption of this fuel.
Cognitive neuroscience boils down to describing the ways in which cognitive function results from brain activity. Exactly how cognitive function inherits the physical dimensions of neural activity, though, is highly non-trivial, and so are generally the corresponding dimensions of cognitive phenomena. In spite of their general use, these assumptions hold true to a high degree of approximation for many cognitive (viz. fast perceptual) processes, but have their limitations for other ones (e.g., thinking or reasoning). It covers a broad range of non-stationary aging and stationary driven systems such as structural glasses, spin glasses, coarsening systems, ferromagnetic models at criticality, trap models, models with entropy barriers, kinetically constrained models, sheared systems and granular media. Suitably modified versions of the FDT also hold for out-of-equilibrium systems (Cugliandolo et al., 1997; #CITATION_TAG; Pottier and Mauger, 2004; Allegrini et al., 2007; Aquino et al., 2007). The review is divided into four main parts: (1) an introductory section explaining basic notions related to the existence of the FDT in equilibrium and its possible extension to the glassy regime (QFDT), (2) a description of the basic analytical tools and results derived in the framework of some exactly solvable models, (3) a detailed report of the current evidence in favour of the QFDT and (4) a brief digression on the experimental evidence in its favour.
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Death is often preceded by medical decisions that potentially shorten life (end-of-life decisions [ELDs]), for example, the decision to withhold or withdraw treatment. Almost all of the incompetent patients had previously stated that they wanted their family involved in case of incompetence, but half did not achieve this.In half of the cases, advanced lung cancer patients-or their families in cases of incompetence-were not involved in ELD making, despite the wishes of most of them. Physicians should openly discuss ELDs and involvement preferences with their advanced lung cancer patients.Copyright A(c) 2012 U.S. Cancer Pain Relief Committee. Some patients express wishes of abbreviation of suffering at the end of their life, but when the time comes, they do not want to be informed of the imminence of their death (Bernheim 1996 (Bernheim, 2001 #CITATION_TAG). When the patient died, the specialist and general practitioner were asked to fill in a questionnaire.Eighty-five patients who died within 18 months of diagnosis were studied.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. The importance of accurate estimation of student's future performance is essential in order to provide the student with adequate assistance in the learning process. If this assumption is invalid, conditional probabilities between attributes can be modelled as a Bayesian Network (#CITATION_TAG). We presented empirical experiments on the prediction of performance with a data set of high school students containing 8 attributes.
This paper presents a new variant of the capacitated multi-source Weber problem that introduces fixed costs for opening facilities. Three types of fixed costs are considered and experimented upon. Reviews the research work and conceptual development of the International Marketing and Purchasing (IMP) group into the nature of buyer-seller relationships which has evolved during the past 20 years. The themes of interaction, relationships and networks encapsulate the major research thrusts of this group and underlie much of the contemporary academic research in Europe. For each cluster of customers, the location of its facility is found using the Weiszfeld algorithm (#CITATION_TAG). Addresses these themes, which represent the major phases of challenging conceptual and empirical research with which the IMP group has been concerned since its inception in 1976.
Cognitive neuroscience boils down to describing the ways in which cognitive function results from brain activity. Exactly how cognitive function inherits the physical dimensions of neural activity, though, is highly non-trivial, and so are generally the corresponding dimensions of cognitive phenomena. In spite of their general use, these assumptions hold true to a high degree of approximation for many cognitive (viz. fast perceptual) processes, but have their limitations for other ones (e.g., thinking or reasoning). These variants differ in the way the exchange interactions are treated. This is a consequence of the fluctuation-dissipation theorem (FDT), which establishes a general relationship between the (equilibrium) internal autocorrelation C(t) of fluctuations of some observable of the system in the absence of the disturbance and the (non-equilibrium) response R(t) of a system to small external perturbations (#CITATION_TAG). One of these variants, named dRPA-II, is original to this work and closely resembles the second-order screened exchange (SOSEX) method. We discuss and clarify the connections among different RPA formulations. We derive the spin-adapted forms of all the variants for closed-shell systems and test them on a few atomic and molecular systems with and without range separation of the electron electron interaction.
Several variant RARA translocations have been reported in acute promyelocytic leukemia (APL) of which the t(11;17)(q23;q21), which results in a ZBTB16-RARA fusion, is the most widely identified and is largely resistant to therapy with all-trans retinoic acid (ATRA). Acute promyelocytic leukemia (APL) is typified by the t(15;17) translocation, which leads to the formation of the PML/RARA fusion gene and predicts a beneficial response to retinoids. However, approximately 10% of all APL cases lack the classic t(15;17). To address this issue, a European workshop was held in Monza, Italy, during June 1997, and a morphologic, immunophenotypic, cytogenetic, and molecular review was undertaken in 60 cases of APL lacking t(15;17). This process led to the development of a novel morphologic classification system that takes into account the major nuclear and cytoplasmic features of APL. The bone marrow morphology of patients with ZBTB16-RARA APL tends to be distinct from those patients with either classical or the hypogranular variant of APL [#CITATION_TAG]. This group includes (1) cases with cryptic PML/RARA gene rearrangements and t(5;17) that leads to the NPM/RARA fusion gene, which are retinoid-responsive, and (2) cases with t(11;17)(q23;q21) that are associated with the PLZF/RARA fusion gene, which are retinoid-resistant.
Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. For example, women have been found to have more work absence days than men [#CITATION_TAG]. METHODS The study population was drawn from the Canadian Community Health Survey 1.2, a national population-based survey that gathered cross-sectional data on health status from 22,118 working respondents. The relationship between chronic work stress, chronic physical conditions, and psychiatric disorders and disability in the past 14 days was examined for working respondents by using logistic regressions controlling for sociodemographic characteristics, region, and occupation.
Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. In solid tumours, data on genetic alterations are  usually only available at a single point in time, allowing no direct insight into the  sequential order of genetic events. Another early model are distancebased trees [17, #CITATION_TAG], but their meaning is rather different, since the observed mutations are only placed in the leaves or terminal nodes of the tree, and the internal nodes are unobserved and unknown events, which precludes an interpretation in terms of order restrictions like "mutation A is required for mutation B". In our approach, genetic tumour development  and progression is assumed to follow a probabilistic tree model. We show how  maximum likelihood estimation can be used to reconstruct a tree model for the  dependencies between genetic alterations in a given tumour type. We illustrate the  use of the proposed method by applying it to cytogenetic data from 173 cases of  clear cell renal cell carcinoma, arriving at a model for the karyotypic evolution of  this tumour
ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. Despite the establishment of significant traditions of emotion research in many disciplines, there has been little discussion of the state and potential of the archaeological study of emotion. In archaeology, both the sociobiological approach and one based on empathy have serious problems. Finally, institutional buy outs (IBOs) involve private equity and other institutional investors; here managers do not hold any shares at all, unless this is part of their reward package (#CITATION_TAG). After reviewing and rejecting the dichotomy between emotions as entirely biological, universal, and hard-wired, on one hand, and entirely social and constructed, on the other, a view of emotions as historically specific and experientially embedied is advanced.
Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. To obtain sufficient variance for effective analysis, we randomly asked one half of the respondents to select a well-functioning relationship as the questionnaire subject and the other half a rather problematic one (for a similar procedure see #CITATION_TAG). A necessary and sufficient condition for imperfect information to improve on contracts based on the payoff alone is derived, and a characterization of the optimal use of such information is given.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. We describe an extraction of new features from both student data and behaviour data (or more precisely from social graph which we construct). Demographic data, such as age and gender, have been cited as significant (Naderi et al., 2009), as are data gathered from learner activity on online learning systems (#CITATION_TAG; Lpez et al., 2012). Then we introduce a novel method for learning classier for student failure prediction that employs cost-sensitive learning to lower the number of incorrectly classified unsuccessful students.
Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. Climatologically defined wet seasons (e.g., #CITATION_TAG) might typically end a few weeks earlier. nan
Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. Colorectal cancer (CRC) is the third most common cancer type, and third highest in mortality rates among cancer-related deaths in the United States. Originating from intestinal epithelial cells in the colon and rectum, that are impacted by numerous factors including genetics, environment and chronic, lingering inflammation, CRC can be a problematic malignancy to treat when detected at advanced stages. Chemotherapeutic agents serve as the historical first line of defense in the treatment of metastatic CRC. In recent years, however, combinational treatment with targeted therapies, such as vascular endothelial growth factor, or epidermal growth factor receptor inhibitors, has proven to be quite effective in patients with specific CRC subtypes. Current research into the efficacy of immunotherapy, particularly immune checkpoint inhibitor therapy (ICI) in mismatch repair deficient and microsatellite instability high (dMMR-MSI-H) CRC tumors have shown promising results, but its use in other CRC subtypes has been either unsuccessful, or not extensively explored. A CPC was defined according to the criteria of (international society of hematotherapy and genetic engineering) ISHAGE [11] and the expression of P504S according to the consensus of the american association of pathologists [#CITATION_TAG]. nan
This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Influencing more environmentally friendly and sustainable behaviour is a current focus of many projects, ranging from government social marketing campaigns, education and tax structures to designers' work on interactive products, services and environments. These approaches make different assumptions about 'what people are like': how users will respond to behavioural interventions, and why, and in the process reveal some of the assumptions that designers and other stakeholders, such as clients commissioning a project, make about human nature. While much focus has been given to the release of software tools that aid researchers in developing and analysing computational models [#CITATION_TAG][9][10][11][12][13], the same attention has not been given to providing researchers with a means of showing that their developed tool can adequately support the investigation of a specific biological research question: that the tool is fit for purpose. There is a wide variety of techniques and methods used, intended to work via different sets of cognitive and environmental principles. The models are characterised using systems terminology and the application of each model to design for sustainable behaviour is examined via a series of examples.
Time Series Forecasting (TSF) uses past patterns of an event in order to predict its future values and is a key tool to support decision making. In the last decades, Computational Intelligence (CI) techniques, such as Artificial Neural Networks (ANN) and more recently Support Vector Machines (SVM), have been proposed for TSF. In this work, we propose a novel Evolutionary SVM (ESVM) approach for TSF based on the Estimation Distribution Algorithm to search for the best number of inputs and SVM hyperparameters. TSF has become increasingly used in distinct areas such as Agriculture, Finance, Production or Sales [#CITATION_TAG]. A broad outline  follows. We begin with an introduction to smoothing in one dimension, followed by  a discussion of multi-dimensional smoothing methods. We then move on to review  and develop the array methods of Currie et al. (2006), and show how these methods  can be applied in additive models even when the data do not have a standard array  structure. Our main contributions are: firstly we extend the array methods of Currie et al. (2006) to cope with more general covariance structures; secondly we describe an additive  model of mortality which decomposes the mortality surface into a smooth twodimensional  surface and a series of smooth age dependent shocks within years; thirdly  we describe an additive model of mortality for data with a Lexis triangle structure
First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. Diversification within an intermediary serves to reduce these costs, even in a risk neutral economy. 24 In the theory of financial institutions, therefore, the paradigmatic model of viable financial intermediation, due to #CITATION_TAG, postulates an intermediary holding a fully diversified portfolio of assets, with outside finance taking entirely the form of debt, with claims that are independent of the returns which the intermediary earns on his portfolio: If the claims on the financial intermediary are independent of returns on the intermediary's assets and if diversification ensures that the probability of default is zero, any benefits of taking greater effort in managing assets, e.g., more thorough monitoring of loans clients, accrue entirely to the intermediary. It presents a characterization of the costs of providing incentives for delegated monitoring by a financial intermediary.
The growth in computer games and wireless networks has catalyzed the production of a new generation of hand-held game consoles that support multi-player gaming over IEEE 802.11 networks. Understanding the traffic characteristics of network games running on these new hand-helds is important for building traffic models and adequately planning wireless network infrastructures to meet future demand. This paper examines the traffic characteristics of IEEE 802.11 network games on the Nintendo DS and the Sony PSP. In addition, the games and hand-held platforms differ in their ability to handle degraded wireless network conditions and in the amount of broadcast traffic sent. Employment laws in India and Zimbabwe require employers to obtain permission from the government to retrench or lay off workers. However, in both countries a substantial decline in the demand for employees (other things equal) followed the new legislation. In Zimbabwe it is difficult to be precise about a causal connection between the drop in the demand for labor (allowing for concurrent increased wages) and the new legislation because enactment occurred simultaneously with Independence; however, the current economic climate induced high levels of investment in capital but not investments in long-term commitments to employees. Upon achieving independence in 1980, the government of Zimbabwe passed a new Employment Act, requiring employers to obtain permission from the Ministry of Labor to fire or lay off workers. Comparable regulations were imposed in India by the Industrial Disputes (Amendment) Act of 1976, requiring that written permission be obtained, normally from the relevant state government, either to close a plant or to retrench workers. Any addition to economic security in the lives of workers is clearly a laudable goal in its own right. But the question addressed in this article is whether these particular job security regulations have had undesirable side effects, which may even have thwarted the original goals of the legislation. The traffic generated by one host on a WLAN can have dramatic impact on the performance of other hosts on the WLAN [#CITATION_TAG, 8]. nan
This paper studies the effect of political regime transitions on public policy using a dataset on global agricultural distortions over 50 years (including data from 74 developing and developed countries over the period . Changes within networks receive less research attention, although considerable research exists on explaining business network structures in different research traditions. So far, the relevant literature discusses network pictures mainly as a theoretical concept. Furthermore, the direction of causation is hard to establish (see #CITATION_TAG; Gundlach and Paldam, 2009). The study is exploratory or iterative in the sense that revisions occur to the research question, method, theory, and context as an integral part of the research process. The study develops a concept of network change as well as an operationalization for comparing perceptions of change, where the study introduces a template model of dottograms to systematically analyze differences in perceptions. The study then applies the model to analyze findings from a case study of Norwegian/Japanese seafood distribution, and the chapter provides a rich description of a complex system facing considerable pressure to change. In-depth personal interviews and cognitive mapping techniques are the main research tools applied, in addition to tracer studies and personal observation. The dottogram method represents a valuable contribution to case study research as it enables systematic within-case and across-case analyses.
The chronological development of universities ranges from the state at which universities are considered to be knowledge accumulators followed by knowledge factories and finally the knowledge hubs. The various national systems of innovations are aligned with the knowledge hubs and it involves a substantial amount of research activities. The newly established Mbeya University of Science and Technology is recognised as a knowledge hub in some particular niches. However, there are a limited number of research activities conducted at the university and this study is an attempt to identify the reasons that limit research activities. Sexually Transmitted Infections (STI's), including HIV (Human Immunodeficiency Virus) mainly affects sexually active young people. Young adults aged 15-29 years, account for 32% of AIDS (Acquired Immunodeficiency Syndrome) cases reported in India and the number of young women living with HIV/AIDS is twice that of young men. About 30% of respondents considered HIV/AIDS could be cured, 49% felt that condoms should not be available to youth, 41% were confused about whether the contraceptive pill could protect against HIV infection and 32% thought it should only be taken by married women.Though controversial, there is an immense need to implement gender-based sex education regarding STIs, safe sex options and contraceptives in schools in India. For instance, in India the knowledge, perception, attitude of adolescent girls towards STIs/HIV and safer sex education were done and indicated the need to implement gender-based sex education (#CITATION_TAG). the self-administered questionnaire was completed by 251 female students from two senior secondary schools.More than one third of students in this study had no accurate understanding about the signs and symptoms of STIs other than HIV/AIDS.
Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. There are other medication classes with demonstrated efficacy in social phobia (benzodiazepines, antipsychotics, alpha-2-delta ligands), but due to limited published clinical trial data and the potential for dependence and withdrawal issues with benzodiazepines, it is unclear how best to incorporate these drugs into treatment regimens. There are very few clinical trials on the use of combined medications. There is some evidence, albeit limited to certain drug classes, that the combination of medication and cognitive behavior therapy may be more effective than either strategy used alone. Generalized social phobia is a chronic disorder, and many patients will require long-term support and treatment. Several studies have assessed the clinical effectiveness of psychological and pharmacological treatments for social anxiety disorder [10] [11] [#CITATION_TAG] [13]. An optimal treatment regimen would include a combination of medication and psychotherapy, along with an assertive clinical management program. For medications, selective serotonin reuptake inhibitors and dual serotonin-norepinephrine reuptake inhibitors are first-line choices based on their efficacy and tolerability profiles.
Pronounced hygric seasonality determines the regional climate and, thus, the characteristics of rainfed agriculture in the Peruvian Callejon de Huaylas (Cordillera Blanca). Peasants in the Cuenca Auqui on the eastern slopes above the city of Huaraz attribute recently experienced challenges in agricultural production mainly to perceived changes in precipitation patterns. Statistical analyses of daily precipitation records at nearby Recuay (1964Recuay ( to 2013 and Huaraz (1996 to 2013) stations do not corroborate the perceived changes. ABSTRACT Background: Although advance directives may seem useful instruments in decision-making regarding incompetent patients, their validity in cases of dementia has been a much debated subject and little is known about their effectiveness in practice. Insight into the experiences and wishes of people with dementia regarding advance directives is totally lacking in empirical research. It is clear, however, that the use of advance directives in practice remains problematic, above all in cases of advance euthanasia directives, but to a lesser extent also when non-treatment directives are involved. The peasant families of the Ro Auqui watershed cultivate an average area of around 3 hectares per family which are distributed in small plots over different altitudes of the valley (Fig. 1) in order to guarantee diversified production for each family (Sietz et al., 2012; #CITATION_TAG; Zimmerer, 2011). Methods: The relevant problems from the ethical debate on advance directives in cases of dementia are summarized and we discuss how these relate to what is known from empirical research on the validity and effectiveness of advance directives in the clinical practice of dementia care.
Tensor models are the generalization of matrix models, and are studied as models of quantum gravity in general dimensions. The algebraic structure is studied mainly from the perspective of 3-ary algebras. In this paper, I discuss the algebraic structure in the fuzzy space interpretation of the tensor models which have a tensor with three indices as its only dynamical variable. These generalizations are algebraic structures in which the two entries Lie bracket has been replaced by a bracket with n entries. Three-Lie algebras have surfaced recently in multi-brane theory in the context of the Bagger-Lambert-Gustavsson model. In the sequel, it is found that 3-ary algebras [#CITATION_TAG] [28] [29] describe the symmetries of the tensor models. Each type of n-ary bracket satisfies a specific characteristic identity which plays the r\^ole of the Jacobi identity for Lie algebras. Particular attention will be paid to generalized Lie algebras, which are defined by even multibrackets obtained by antisymmetrizing the associative products of its n components and that satisfy the generalized Jacobi identity (GJI), and to Filippov (or n-Lie) algebras, which are defined by fully antisymmetric n-brackets that satisfy the Filippov identity (FI). Because of this, Filippov algebras will be discussed at length, including the cohomology complexes that govern their central extensions and their deformations (Whitehead's lemma extends to all semisimple n-Lie algebras). When the skewsymmetry of the n-Lie algebra is relaxed, one is led the n-Leibniz algebras. The standard Poisson structure may also be extended to the n-ary case. We shall review here the even generalized Poisson structures, whose GJI reproduces the pattern of the generalized Lie algebras, and the Nambu-Poisson structures, which satisfy the FI and determine Filippov algebras.
The effectiveness of alcohol brief intervention (ABI) has been established by a succession of meta-analyses but, because the effects of ABI are small, null findings from randomized controlled trials are often reported and can sometimes lead to skepticism regarding the benefits of ABI in routine practice. This article first explains why null findings are likely to occur under null hypothesis significance testing (NHST) due to the phenomenon known as "the dance of the p-values." From the standpoint of scientific progress, the chief problem about null findings under the conventional NHST approach is that it is not possible to distinguish "evidence of absence" from "absence of evidence." In surveys of health professionals' attitudes to this work, one of the most commonly encountered obstacles is "lack of time" or "too busy" (#CITATION_TAG, 33). Qualitative focus group discussion method study applying the deductive framework approach. Six focus groups involving 18 general practitioners and 19 nurses were recruited from primary health care of the City of Tampere, Finland. Possible obstacles are: (1) confusion regarding the content of early-phase heavy drinking, (2) lack of self-efficacy among primary health care professionals, (3) sense of lacking time needed for carrying out brief intervention, (4) not having simple guidelines for brief intervention, (5) sense of difficulty in identifying of early-phase heavy drinkers, and (6) uncertainty about the justification for initiating discussion on alcohol issues with patients.
Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Inconsistent conclusions across studies might amongst other reasons arrive from unreliable measurements of SED, as most of these studies have included!3-5 days of measurement [12] [13] [14] [15] [16] [17], with some exceptions (!1 day [#CITATION_TAG];!6-7 days [19, 20]). Sedentary time during waking hours was measured by an accelerometer (5 min. A sedentary break was defined as an interruption in sedentary time (>=100 counts per minute). Metabolic syndrome was defined according to the Adult Treatment Panel (ATP) III criteria.
Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Social anxiety disorder (SAD) is highly prevalent and associated with a substantial societal economic burden, primarily due to high costs of productivity loss. Cognitive behavior group therapy (CBGT) is an effective treatment for SAD and the most established in clinical practice. Internet-based cognitive behavior therapy (ICBT) has demonstrated efficacy in several trials in recent years. No study has however investigated the cost-effectiveness of ICBT compared to CBGT from a societal perspective, i.e. Published economic analyses have explored the cost-effectiveness of a very limited range of interventions for social anxiety disorder and concluded that escitalopram [53], group CBT [54] [#CITATION_TAG] [56] and computer-based self-help [55] [56] [57] are cost-effective options. an analysis where both direct and indirect costs are included. We conducted a randomized controlled trial where participants with SAD were randomized to ICBT (n=64) or CBGT (n=62).
OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of "clusters" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). The continuous polarized vesicle secretion in pollen tubes is essential for tip growth but the location of endo- and exocytic sub-domains remains however controversial. The specific distribution of SYP124 at the plasma membrane is affected by changes in Ca2+ levels in agreement with the importance of this ion for exocytosis. Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (#CITATION_TAG; Yang et al., 2012). nan
Lipids comprise the bulk of the dry mass of the brain. In addition to providing structural integrity to membranes, insulation to cells and acting as a source of energy, lipids can be rapidly converted to mediators of inflammation or to signaling molecules that control molecular and cellular events in the brain. The advent of soft ionization procedures such as electrospray ionization (ESI) and atmospheric pressure chemical ionization (APCI) have made it possible for compositional studies of the diverse lipid structures that are present in brain. These include phospholipids, ceramides, sphingomyelin, cerebrosides, cholesterol and their oxidized derivatives. These proteins may be potential therapeutic targets since they transport lipids required for neuronal growth or convert lipids into molecules that control brain physiology. In this review, we examine the structure of the major lipid classes in the brain, describe methods used for their characterization, and evaluate their role in neurological diseases. The potential utility of characterizing lipid markers in the brain, with specific emphasis on disease mechanisms, will be discussed. Combining lipidomics and proteomics will enhance existing knowledge of disease pathology and increase the likelihood of discovering specific markers and biochemical mechanisms of brain diseases. Abstract: Phospholipase A2 (PLA2) is the name for the class of lipolytic enzymes that hydrolyze the acyl group from the sn-2 position of glycerophospholipids, generating free fatty acids and lysophospholipids. The products of the PLA2-catalyzed reaction can potentially act as second messengers themselves, or be further metabolized to eicosanoids, platelet-activating factor, and lysophosphatidic acid. The presence of PLA2 in the central nervous system, accompanied by the relatively large quantity of potential substrate, poses an interesting dilemma as to the role PLA2 has during both physiologic and pathologic states. Several different PLA2 enzymes exist in brain, some of which have been partially characterized. However, under pathological situations, increased PLA2 activity may result in the loss of essential membrane glycerophospholipids, resulting in altered membrane permeability, ion homeostasis, increased free fatty acid release, and the accumulation of lipid peroxides. These processes, along with loss of ATP, may be responsible for the loss of membrane phospholipid and subsequent neuronal injury found in ischemia, spinal cord injury, and other neurodegenerative diseases. The incorporation of PUFAs into phospholipid subclasses is highly choreographed such that most PUFAs are initially incorporated into 1,2-diacyl phospholipids subclasses before they are remodeled into the ether-linked phospholipids classes by coenzyme A (CoA)-dependent or CoA-independent transacylase activities [#CITATION_TAG, 84, 126, 165, 167]. They are classified into two subtypes, CA2+-dependent and Ca2+-independent, based on their catalytic dependence on Ca2+.
Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. However, it is unclear how often training should be performed to maximize its benefit. The present study investigated how the frequency of training contributed to normal-hearing listeners' adaptation to spectrally shifted speech. Although more frequent training may accelerate listeners' adaptation to spectrally shifted speech, there may be significant benefits from training as little as one session per week. It has been shown that time-dependent loop models must include a relatively thick, cool, and dense chromosphere and the transition region for a correct description of the mass transfer driven by transient heating (e.g., #CITATION_TAG) and to maintain the necessary numerical stability (Antiochos, 1979; Hood and Priest, 1980; Peres et al., 1982). Methods: Eighteen normal-hearing listeners were trained with spectrally shifted and compressed speech via an 8-channel acoustic simulation of cochlear implant speech processing. Five short training sessions (1 hr per session) were completed by each subject; subjects were trained at one of three training rates: five sessions per week, three sessions per week, or one session per week. Subjects were trained to identify medial vowels presented in a cVc format; depending on the level of difficulty, the number of response choices was increased and/or the acoustic differences between vowels were reduced. Vowel and consonant recognition was measured before and after training as well as at regular intervals during the training period. Sentence recognition was measured before and after training only.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Evidence-centered design (ECD) is a comprehensive framework for describing the conceptual, computational and inferential elements of educational assessment. At first blush, ECD and educational data mining (EDM) might seem in conflict: structuring situations to evoke particular kinds of evidence, versus discovering meaningful patterns in available data. Learning is a latent variable, typically measured as academic performance in assessment work and examinations (#CITATION_TAG). We first introduce ECD and relate its elements to the broad range of digital inputs relevant to modern assessment. We then discuss the relation between EDM and psychometric activities in educational assessment.
requires a greater understanding of characteristics of clients who may or may not benefit from this technology. Dragon Dictate is time-consuming to learn and demands a high level of motivation, but can be beneficial to a person who has profound dysarthria and great difficulties in accessing the computer. Another [#CITATION_TAG] assessing the use of DragonDictate reported that while 1 participant withdrew from the study, the other achieved gains in computer access efficiency of 40%. Single subject design was used and measures of computer access system effectiveness and speech production were used before, during and after intervention. The users' original switch access system was compared to a combination of their switch access system and speech recognition, by counting the number of correct entries. The other participant did not complete the intervention protocol.
Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. An important topic in the evolution of language is the kinds of grammars that can be computed by humans and other animals. Fitch and Hauser (F&H; 2004) approached this question by assessing the ability of different species to learn 2 grammars, (AB)(n) and A(n) B(n) . This article replicates F&H's data and reports new controls using either categories similar to those in F&H or less salient ones. Indeed, when familiarized with A(n) B(n) exemplars, participants failed to discriminate A(3) B(2) and A(2) B(3) from A(n) B(n) items, missing the crucial feature that the number of As must equal the number of Bs. There is strong evidence, including from self-reports after the experiment, that participants who learn non-configurational properties do so by counting and comparing the number of stimuli of each class, and participants without syntactic disorder more consistently attend to configurational (order) than nonconfigurational (counting) structure (de Vries, Monaghan, Knecht, & Zwitserlood, 2008; #CITATION_TAG; Perruchet & Rey, 2005;. Some non-impaired individuals learn the configurational structure only, but none learn only non-configurational structures. A(n) B(n) was taken to indicate a phrase structure grammar, eliciting a center-embedded pattern. (AB)(n) indicates a grammar whose strings entail only local relations between the categories of constituents. In their experiments, the A constituents were syllables pronounced by a female voice, whereas the B constituents were syllables pronounced by a male voice.
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Since its first publication in 2003, the World Health Organization's "Safe abortion: technical and policy guidance for health systems" has had an influence on abortion policy, law, and practice worldwide. To reflect significant developments in the clinical, service delivery, and human rights aspects of abortion care, the Guidance was updated in 2012. These themes not only connect the chapters into a coherent whole. They reflect the research and advocacy efforts of a growing field in women's health and human rights.Copyright (c) 2012 International Federation of Gynecology and Obstetrics. FIGO now states "neither society, nor members of the health care team responsible for counseling women, have the right to impose their religious or cultural convictions regarding abortion on those whose attitudes are different" (FIGO 2013, 104) and concludes that "the Committee recommend[s] that after appropriate counselling, a woman [has] the right to have access to medical or surgical induced abortion, and that the health care service [has] an obligation to provide such services as safely as possible" (FIGO 2013, 105; #CITATION_TAG). But also here, in addition to philosophical motives, pragmatic motives have been operating: Another important objective of FIGO was to lift abortion out of clandestine illegal activity, to let it be performed in medically correct conditions and so ensure that abortion is safe and accessible. nan
Background: Mild cognitive deficits, mainly in frontal-executive function and memory, have been reported in patients with essential tremor (ET). The clarification of the different tremor types and response to treatment in DLB could improve clinical recognition of DLB, but mostly, understanding tremor in DLB, would provide clarity to recent controversial debates [16] [17] [18] [#CITATION_TAG] [20] [21] [22] [23] [24] on the long term outcome of patients putatively affected by ET. Methods: Community-dwelling elders in northern Manhattan were enrolled in a prospective cohort study. Baseline ET diagnoses were assigned from handwriting samples. Dementia was diagnosed at baseline and follow-up using DSM-III-R criteria. Rather than attributing cognitive complaints in patients with ET to old age, assessment and possible treatment of dementia should be routinely incorporated into the treatment plan
Throughout Earth's history, life has increased greatly in abundance, complexity, and diversity. At the same time, it has substantially altered the Earth's environment, evolving some of its variables to states further and further away from thermodynamic equilibrium. For instance, concentrations in atmospheric oxygen have increased throughout Earth's history, resulting in an increased chemical disequilibrium in the atmosphere as well as an increased redox gradient between the atmosphere and the Earth's reducing crust. This is applied to the processes of planet Earth to characterize the generation and transfer of free energy and its dissipation, from radiative gradients to temperature and chemical potential gradients that result in chemical, kinetic, and potential free energy and associated dynamics of the climate system and geochemical cycles. Here, I present this hierarchical thermodynamic theory of the Earth system. This perspective allows us to view life as being the means to transform many aspects of planet Earth to states even further away from thermodynamic equilibrium than is possible by purely abiotic means. In this perspective pockets of low-entropy life emerge from the overall trend of the Earth system to increase the entropy of the universe at the fastest possible rate. International audienceAfter a career as a chemist and engineer, James Lovelock proposed the Gaia hypothesis in the 1970's with Lynn Margulis, a biologist. From the beginning Lovelock saw Gaia as a grand idea, challenging the way biology and geology should be carried out, and up to our very conception of nature. This chapter recalls the rich context in which the hypothesis was elaborated in the 1960's and 1970's. Wheras evolutionary biologists ridiculed it as a pseudo-metaphor comparing the Earth with an organism, Gais has generated new research programs in the Earth sciences and has been embraced by the environmental counterculture as a new conception of nature and of our relationships with the Earth The resulting view of the Earth system is then compared to the Gaia hypothesis of James Lovelock [40, #CITATION_TAG, 30] and the role of human activity in the global work budget is being discussed. It then traces Gaia's contrasted reception.
The recent progress in molecule-based magnetic materials exhibiting a large magnetocaloric effect at liquid-helium temperatures is reviewed. Theory and examples are presented with the aim of identifying those parameters to be addressed for improving the design of new refrigerants belonging to this class of materials. A failure in the degradation of these modified proteins might induce their accumulation and, thus, participation in lipofuscin or age pigments formation. Since then, magnetic refrigeration is a standard technique in cryogenics, which has shown to be useful to cool down from a few Kelvin [6, #CITATION_TAG]. The epoxyalkenal modified the primary structure of BSA as determined by lysine losses and formation of oxidative stress product epsilon-N-pyrrolylnorleucine (Pnl), which depended on the concentration of the aldehyde and the incubation time. These changes also modified secondary and tertiary structures of the protein, which were determined by studying protein denaturation and polymerization. In addition, all these modifications were parallel to the development of color and fluorescence, which were produced as a consequence of the formation and polymerization of pyrrole amino acid residues.
Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. BACKGROUND There is a growing body of research in the United States to suggest that men are less likely than women to seek help from health professionals for problems as diverse as depression, substance abuse, physical disabilities and stressful life events. Previous research has revealed that the principle health related issue facing men in the UK is their reluctance to seek access to health services. However, the growing body of gender-specific studies highlights a trend of delayed help seeking when they become ill. A prominent theme among white middle class men implicates "traditional masculine behaviour" as an explanation for delays in seeking help among men who experience illness. The reasons and processes behind this issue, however, have received limited attention. The effects of health problems may be compounded by different approaches to seeking help between men and women, with men seeking help less frequently than women (#CITATION_TAG), particularly for psychological problems (Smith et al 2006). METHOD The investigation of men's health-related help seeking behaviour has great potential for improving both men and women's lives and reducing national health costs through the development of responsive and effective interventions. A search of the literature was conducted using CINAHL, MEDLINE, EMBASE, PsychINFO and the Cochrane Library databases.
First, there was excessive maturity transformation through conduits and structured-investment vehicles (SIVs); when this broke down in August 2007, the overhang of asset-backed securities that had been held by these vehicles put significant additional downward pressure on securities prices. In thinking about regulatory reform, one must therefore go beyond considerations of individual incentives and supervision and pay attention to issues of systemic interdependence and transparency. The paper analyses the causes of the current crisis of the global financial system, with particular emphasis on the systemic elements that turned the crisis of subprime mortgage-backed securities in the United States, a small part of the overall system, into a worldwide crisis. The paper argues that these developments have not only been caused by identifiably faulty decisions, but also by flaws in financial system architecture. This supplement to the Swiss Journal of Economics and Statistics presents material from a conference on Capital Adequacy Rules as Instruments for the Regulation of Banks, which took place in Basle on July 5, 1996. As such it complements the December 1995 supplement, which presented papers from a conference on the same subject a year earlier. The area of banking regulation has not always been marked by intense interaction between academics and practitioners. Indeed, from the times of Regulation Q to the enactment of the 1988 Basle Accord on Capital Adequacy Requirements for Credit Risks, it has been an area where people were sure of their views, with little need to listen to each other. At the same time, not much academic research was actually done in the area; this has only changed with the various banking crises of the eighties and early nineties and the tightening of capital adequacy regulation of banks following the Basle Accord. However the certainties of yesteryear are disappearing. This is most obvious in the development of proposals to extend the 1988 Basle Accord to market risks. When the Basle Committee on Banking Supervision presented the first such proposal in April 1993, it seemed that the regulatory train was moving full speed ahead on a track built by traditional practice, with little analysis of the likely economic effects of the proposed measures. However the industry's reactions then made it clear that conceptually and procedurally this proposal was lagging behind some of the approaches to risk measurement and risk management that had been developed by financial institutions themselves. Because of these reactions, the Basle Committee's revised proposal of April 1995 allowed for the possibility that capital adequacy requirements for banks bearing market risks be based on the banks' own risk models rather than any exogenously set, rigid regulatory standard. #CITATION_TAG compares "the speed with which the regulatory community moved from the April 1993 and April 1995 proposals to the actual Amendment to the Capital Accord to Incorporate Market Risks of January 1996 to the time and expenses it takes for a private company to get a new drug approved for sale" and notes that "both the 1988 Accord and the 1996 Amendment to the 1988 Accord were enacted with hardly any evidence about the economic effects of capital requirements for banks." Both conferences were organized by NlKLAUS BLATTNER, of the University of Basle, and myself.
Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Current psychological theories state that the clinical outcome of hallucinatory experiences is dependent on the degree of associated distress, anxiety, and depression. Also, depressive mood has also been shown to increase the risk of transition to psychosis in individuals with hallucinatory experiences (#CITATION_TAG). nan
Phonological errors were scarce in both groups. Anomia is characteristic of SD and has received substantial research attention (see #CITATION_TAG, for an analysis of a large corpus of SD naming errors). Using a pool of 225 sets of picture naming data from 78 patients, we assessed the effects on naming accuracy of several characteristics of the target objects or their names: familiarity, frequency, age of acquisition and semantic domain (living/non-living). We also analysed the distribution of different error types according to the severity of the naming deficit.
Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. More specifically, we utilize representation learning methods (#CITATION_TAG), where words or phrases are embedded into the same vector space. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.
Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Understanding value stability and change is essential for understanding values of both individuals and cultures.Yet theoretical thinking and empirical evidence on this topic have been scarce. Hooghe (2003, p. 93) implies a similar idea when arguing that association membership is unlikely to 'introduce qualitatively new values, but enforces already existing values' (see also #CITATION_TAG; Katz & Lazersfeld, 1955). In this article, the authors suggest a model outlining processes of individual value change. They identify five facilitators of value change (priming, adaptation, identification, consistency maintenance, and direct persuasion) and consider the moderating role of culture in each. In addition, the authors discuss the roles of culture, personal values, and traits as general moderators of value change.
Since Brehm's (1956) initial free-choice experiment, psychologists have interpreted the spreading of alternatives as evidence for choice-induced attitude change. It is widely assumed to occur because choosing creates cognitive dissonance, which is then reduced through rationalization. Properties of various classes of solutions to this equation have been extensively studied both analytically and numerically (Bronski and Kutz 2002; Buckingham and Venakides 2007; Carles 2007; Ceniceros and Tian 2002; Forest and Lee 1986; Grenier 1998; Kamvissis 1996; Kamvissis et al. 2003; Klein 2006; Lyng and Miller 2007; #CITATION_TAG; Tovbis et al. 2004 Tovbis et al., 2006. After making a choice between 2 objects, people reevaluate their chosen item more positively and their rejected item more negatively (i.e., they spread the alternatives). Specifically, if people's ratings/rankings are an imperfect measure of their preferences and their choices are at least partially guided by their preferences, then the FCP will measure spreading, even if people's preferences remain perfectly stable. We show this, first by proving a mathematical theorem that identifies a set of conditions under which the FCP will measure spreading, even absent attitude change. We then experimentally demonstrate that these conditions appear to hold and that the FCP measures a spread of alternatives, even when this spreading cannot have been caused by choice. We discuss how the problem we identify applies to the basic FCP paradigm as well as to all variants that examine moderators and mediators of spreading.
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Increasing rates of smoking cessation is one of the most effective measures available to improve population health. Younger quitters were more likely to use unassisted methods such as cold turkey; older or less educated quitters were more likely to use assisted methods such as prescribed medication or advice from a general practitioner.The majority of recent quitters quit cold turkey or cut down before quitting, and reported that these methods were helpful. [#CITATION_TAG,46] It is widely accepted that searching the qualitative literature is difficult In this survey of recent quitters, we simultaneously examined rates of use and perceived helpfulness of various cessation methods.Recent quitters (within 12 months; n = 1097) completed a telephone survey including questions relating to 13 cessation methods. Indices of use and perceived helpfulness for each method were plotted in a quadrant analysis. Socio-demographic differences were explored using bivariate and multivariate analyses.From the quadrant analysis, cold turkey, NRT and gradual reduction before quitting had high use and helpfulness; GP advice had high use and lower helpfulness. Remaining methods had low use and helpfulness.
Phonological errors were scarce in both groups. questions in relationship marketing: why is relationship marketing so prominent now? Why do firms and consumers enter into relationships with other firms and consumers? Relationship marketing can take many forms and, as a result, relationship marketing theory has the potential to increase one's understanding of many aspects of business strategy.Research limitations/implications - The answ... In contrast, grammatical morphemes (both free and bound) are activated indirectly, via open class items, and they are intrinsic to the syntactic frame (Bock & Levelt, 1994; #CITATION_TAG; Garrett, 1984). nan
Processing of linear word order (linear configuration) is important for virtually all languages and essential to languages such as English which have little functional morphology. Damage to systems underpinning configurational processing may specifically affect word-order reliant sentence structures. We explore order processing in WR, a man with primary progressive aphasia. At test, they were told of the rule set, but not told what it was. AGL (#CITATION_TAG) is a commonly employed paradigm that tests processing of sequence structure. A single experiment is reported that investigated implicit learning using a conjunctive rule set applied to natural words. Participants memorized a training list consisting of words that were either rare-concrete and common-abstract or common-concrete and rare-abstract. Instead, they were shown all four word types and asked to classify words as rule-consistent words or not. Participants classified the items above chance, but were unable to verbalize the rules, even when shown a list that included the categories that made up the conjunctive rule and asked to select them. An analysis of the materials demonstrated that conscious micro rules (i.e., chunk knowledge) could not have driven performance.
The data stems from studies of searchers interaction with an XML information retrieval system. In this paper we present findings from a comparative study of data collected from client and server side loggings. The purpose is to see what factors of effort can be captured from the two logging methods. Charcoal records from lake sediment and fire-scar networks from long-lived tree species have improved our understanding of long-term relationships between fire events and climate. This work has primarily focused on historically fire-prone ecosystems and regions. In the northeastern USA, where wildfire has been relatively infrequent in historical times, fire-risk assessments have incorporated little-to-no pre-historical data and little is known about long-term fire-climate relationships. However, although prolonged droughts were widespread and associated with higher probability of fire, the fire events were rarely synchronous among sites, with the exception of ~550 years before present (yr BP) when all three sites experienced both drought and fire. While fire has been relatively uncommon in the northeastern USA during the past century, our records clearly highlight the potential vulnerability of the region to future drought and fire impacts. Numerous studies have been performed on searchers' interaction with IR systems, in non-web systems [1] [2] [3], but in particular with the advent of the Web [#CITATION_TAG] [5] [6]. We developed coupled, high-resolution records of moisture variability and fire from three ombrotrophic peatlands in Maine using testate amoebae and analysis of microscopic charcoal.
Spatially explicit predictions of invasion risk obtained through bioclimatic envelope models calibrated with native species distribution data can play a critical role in invasive species management. Forecasts of invasion risk to novel environments, however, remain controversial. Only when incorporating a measure of human modification of habitats within the native range do bioclimatic envelope models yield credible predictions of invasion risk for parakeets across Europe. Invasion risk derived from models that account for differing niche requirements of phylogeographic lineages and those that do not achieve similar statistical accuracy, but there are pronounced differences in areas predicted to be susceptible for invasion. Aim To mitigate the threat invasive species pose to ecosystem functioning, reliable risk assessment is paramount. Understanding how biotic and environmental factors facilitate their invasion success remains a challenge. Indeed, variance in laying dates between European and native (Asian) parakeet populations suggests that in Europe, parakeets are delaying their breeding in response to colder temperatures (#CITATION_TAG). Here, we assess the role of two major hypotheses explaining invasion success: (1) enemy-release, which argues that invasive species are freed from their native predators and parasites in the new areas; and (2) climate-matching, which argues that the climatic similarity between the exotic and native range determines the success of invasive populations.
Phonological errors were scarce in both groups. Although core service does not seem to have positive impact on switching costs, core ... One of the first models of speech production was based on an analysis of an extensive corpus of such errors in normal, healthy speakers (#CITATION_TAG). economic value, relational value, and core value) and investigates their relationships with buyers' perceptions of switching costs.
Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. The use of Ritalin and other stimulant drug treatments for attention-deficit hyperactivity disorder (ADHD) raises distinctive moral dilemmas for parents; these moral dilemmas have not been adequately addressed in the bioethics literature. The considerations of agency and autonomy that are so central to ethical appraisals of biomedical technologies are likewise key issues in relation to psychopharmaceuticals [15, #CITATION_TAG]. nan
This is a repository copy of Using argument notation to engineer biological simulations with increased confidence. They may be downloaded and/or printed for private study, or other acts as permitted by national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of the full text version. Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. Interface 12: 20141059. http://dx.doi.org/10.1098/rsif.2014.1059 Received: 23 September 2014 Accepted: 16 December 2014 Subject Areas: computational biology, systems biology Keywords: computational modelling, argumentation, simulation, ARTOO, immune system modelling Authors for correspondence: Kieran Alden e-mail: kieran.alden@york.ac.uk Mark C. Coles e-mail: mark.coles@york.ac.uk Jon Timmis e-mail: jon.timmis@york.ac.uk Using argument notation to engineer biological simulations with increased confidence Kieran Alden1,2,5, Paul S. Andrews1,3,4, Fiona A. C. Polack1,3,4, Henrique Veiga-Fernandes6, Mark C. Coles1,2,7 and Jon Timmis1,5,7 1York Computational Immunology Laboratory, 2Centre for Immunology and Infection, 3Department of Computer Science, 4York Centre for Complex Systems Analysis, and 5Department of Electronics, University of York, York, UK 6Faculdade de Medicina de Lisboa, Instituto de Medicina Molecular, Lisboa, Portugal 7SimOmics Ltd, The Catalyst, Baird Lane, Heslington, York, UK The application of computational and mathematical modelling to explore the mechanics of biological systems is becoming prevalent. To significantly impact biological research, notably in developing novel therapeutics, it is critical that the model adequately represents the captured system. We propose an approach based on argumentation from safety-critical systems engineering, where a system is subjected to a stringent analysis of compliance against identified criteria. Normal organogenesis requires co-ordinate development and interaction of multiple cell types, and is seemingly governed by tissue specific factors. Lymphoid organogenesis during embryonic life is dependent on molecules the temporal expression of which is tightly regulated. A subset of these cells expresses the receptor tyrosine kinase RET, which is essential for mammalian enteric nervous system formation. Although a basic model of tissue formation has been developed through laboratory experimentation [#CITATION_TAG, [34] [35] [36], the reasons for this emergent behaviour are not fully understood. During this process, haematopoietic 'inducer' cells interact with stromal 'organizer' cells, giving rise to the lymphoid organ primordia. Here we show that the haematopoietic cells in the gut exhibit a random pattern of motility before aggregation into the primordia of Peyer's patches, a major component of the gut-associated lymphoid tissue. To support this hypothesis, we show that the RET ligand ARTN is a strong attractant of gut haematopoietic cells, inducing the formation of ectopic Peyer's patch-like structures.
Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. Having a health problem is a strong determinant of sickness presenteeism (odds ratio = 3.32). For any given health status, there are certain other factors (personally and work-related demands) that impact on the risk of sickness presence, such as difficulties in staff replacement, time pressure, insufficient resources, and poor personal financial situation. However, the greater proportion of the total economic burden of MDD lies in reduced productivity, or presenteeism, in which the depressed individual remains in the work setting but with productivity suffering both in quality and quantity [#CITATION_TAG, 8]. Methods: The study group comprised a random sample of 3136 persons who responded to a questionnaire administered in conjunction with Statistics Sweden's labor market survey. Logistic regressions were used in the analyses.
Knowing the prevalence and characteristics of auditory verbal hallucinations (AVH) in adolescents is important for estimations of need for mental health care and assessment of psychosis risk. Autism spectrum disorders (ASDs) are highly prevalent neurodevelopmental disorders, but the underlying pathogenesis remains poorly understood. Recent studies have implicated the cerebellum in these disorders, with post-mortem studies in ASD patients showing cerebellar Purkinje cell (PC) loss, and isolated cerebellar injury has been associated with a higher incidence of ASDs. However, the extent of cerebellar contribution to the pathogenesis of ASDs remains unclear. Tuberous sclerosis complex (TSC) is a genetic disorder with high rates of comorbid ASDs that result from mutation of either TSC1 or TSC2, whose protein products dimerize and negatively regulate mammalian target of rapamycin (mTOR) signalling. However, the roles of Tsc1 and the sequelae of Tsc1 dysfunction in the cerebellum have not been investigated so far. However, the conversion to a psychiatric disorder and need for psychiatric care is probably mediated by environmental risk factors as well as factors such as coping style, cognitive biases and negative emotional affect (#CITATION_TAG). nan
Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category "toys to pick up before bed" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the "just previous" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, "boring" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term "categorization." Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as "how do prototype representations compare between 6 and 8 months of age?" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. Although vocabulary acquisition requires children learn names for multiple things, many investigations of word learning mechanisms teach children the name for only one of the objects presented. This is problematic because it is unclear whether children's performance reflects recall of the correct name-object association or simply selection of the only object that was singled out by being the only object named. Children introduced to one novel name may perform at ceiling as they are not required to discriminate on the basis of the name per se, and appear to rapidly learn words following minimal exposure to a single word. As dozens of studies illustrate, "boring" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (#CITATION_TAG) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007;. For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). We introduced children to four novel objects. For half the children, only one of the objects was named and for the other children, all four objects were named. Only children introduced to one word reliably selected the target object at test.
In that case, the measurement error is related to the variability between persons. Consequently, reliability parameters are highly dependent on the heterogeneity of the study sample, while the agreement parameters, based on measurement error, are more a pure characteristic of the measurement instrument. #CITATION_TAG he Bland-Altman method provides insight into the distribution of differences in relation to mean values. Agreement parameters assess how close the results of the repeated measurements are, by estimating the measurement error in repeated measurements. Reliability parameters assess whether study objects, often persons, can be distinguished from each other, despite measurement errors.
Major academic publishers need to be able to analyse their vast catalogue of products and select the best items to be marketed in scientific venues. This is a complex exercise that requires characterising with a high precision the topics of thousands of books and matching them with the interests of the relevant communities. In Springer Nature, this task has been traditionally handled manually by publishing editors. However, the rapid growth in the number of scientific publications and the dynamic nature of the Computer Science landscape has made this solution increasingly inefficient. We have addressed this issue by creating Smart Book Recommender (SBR), an ontologybased recommender system developed by The Open University (OU) in collaboration with Springer Nature, which supports their Computer Science editorial team in selecting the products to market at specific venues. Increasingly, organizations are adopting ontologies to describe their large catalogues of items. These ontologies need to evolve regularly in response to changes in the domain and the emergence of new requirements. This operation needs to take into account a variety of factors and in particular reconcile user requirements and application performance. This API supports a number of applications, including Smart Book Recommender, Smart Topic Miner [5], the Technology-Topic Framework [6], a system that forecasts the propagation of technologies across research communities, and the Pragmatic Ontology Evolution Framework [#CITATION_TAG], an approach to ontology evolution that is able to select new concepts on the basis of their contribution to specific computational tasks. An important step of this process is the selection of candidate concepts to include in the new version of the ontology. Current ontology evolution methods focus either on ranking concepts according to their relevance or on preserving compatibility with existing applications. In this paper, we propose the Pragmatic Ontology Evolution (POE) framework, a novel approach for selecting from a group of candidates a set of concepts able to produce a new version of a given ontology that (i) is consistent with the a set of user requirements (e.g., max number of concepts in the ontology), (ii) is parametrised with respect to a number of dimensions (e.g., topological considerations), and (iii) effectively supports relevant computational tasks. Our approach also supports users in navigating the space of possible solutions by showing how certain choices, such as limiting the number of concepts or privileging trendy concepts rather than historical ones, would reflect on the application performance.
Die Dokumente auf EconStor durfen zu eigenen wissenschaftlichen Zwecken und zum Privatgebrauch gespeichert und kopiert werden. Sie durfen die Dokumente nicht fur offentliche oder kommerzielle Zwecke vervielfaltigen, offentlich ausstellen, offentlich zuganglich machen, vertreiben oder anderweitig nutzen. Terms of use: Documents in EconStor may be saved and copied for your personal and scholarly purposes. You are not to copy documents for public or commercial purposes, to exhibit the documents publicly, to make them publicly available on the internet, or to distribute or otherwise use the documents in public. Rational models of cognition typically consider the abstract computational problems posed by the environment, assuming that people are capable of optimally solving those problems. DSM can further provide a means of accommodating growing power generation from fluctuating renewable sources (#CITATION_TAG) and may also help to address carbon emissions constraints (Bergaentzl et al. 2014). This differs from more traditional formal models of cognition, which focus on the psychological processes responsible for behavior. In particular, we argue that Monte Carlo methods provide a source of rational process models that connect optimal solutions to psychological processes. We support this argument through a detailed example, applying this approach to Anderson's (1990, 1991) rational model of categorization (RMC), which involves a particularly challenging computational problem. Drawing on a connection between the RMC and ideas from nonparametric Bayesian statistics, we propose 2 alternative algorithms for approximate inference in this model. The algorithms we consider include Gibbs sampling, a procedure appropriate when all stimuli are presented simultaneously, and particle filters, which sequentially approximate the posterior distribution with a small number of samples that are updated as new data become available.
A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. Research on the determinants of waking thought content (Klinger, 1978) has demonstrated the influence of current concerns on cognitive processes. The concept of current concern refers to the state of an organism between the time it becomes committed to a particular goal and the consummation or abandonment of the goal. Responsiveness to external stimulation has been shown to occur in sleeping subjects. Subjects can be induced to perform physical acts in response to cues introduced while they are asleep (Evans, Gustafson, O'Connell, Orne, & Shor, 1970; Oswald, Taylor, & Treisman, 1960; Williams, Morlock, & Morlock, 1966). Both the cues and the responses used in these investigations varied considerably: stereotyped hand movements to taperecorded names (Oswald et al., 1960), overlearned natural responses to suggestions about subjects' physical states, such as scratching one's nose in response to the suggestion that it was itchy (Evans et al., 1970), or finger movements in response to conditioned auditory stimuli (Williams et al., 1966). One investigation in a sleep laboratory using standard electroencephalography (EEG) and eye movement measures (#CITATION_TAG) administered a modified Concern Dimensions Questionnaire (CDQ; Klinger et al., 1980 Klinger et al.,, 1981 to assess seven participants' goals, followed by four consecutive nights, an adaptation night and three experimental nights, during which, five to seven times per night during Stage 1-rapid eye movement (REM) or Stage 2 sleep, the experimenters Hoelscher et al. (1981). The hypothesis that cues related to subjects' current concerns can control attentional and cognitive processes during sleeping and dreaming was examined by presenting concern- and nonconcern-related verbal stimuli to seven male subjects during sleep Stages 2 and REM. During dichotic listening, cues related to subjects' current concerns exerted a controlling effect on attention, recall, and thought content.
Research published in this series may include views on policy, but the institute itself takes no institutional policy positions. The Institute for the Study of Labor (IZA) in Bonn is a local and virtual international research center and a place of communication between science, politics and business. IZA is an independent nonprofit organization supported by Deutsche Post Foundation. IZA Discussion Papers often represent preliminary work and are circulated to encourage discussion. Citation of such a paper should account for its provisional character. A revised version may be available directly from the author. The IZA research network is committed to the IZA Guiding Principles of Research Integrity. IZA engages in (i) original and internationally competitive research in all fields of labor economics, (ii) development of policy concepts, and (iii) dissemination of research results and concepts to the interested public. Is the way that people make risky choices, or tradeoffs over time, related to cognitive ability? This interpretation suggests that the positive correlation between risk aversion and IQ emphasized, among others, by #CITATION_TAG, could be an artifact of the format of the price list, as already argued by Andersson et al. (2013). We conduct choice experiments measuring risk aversion, and impatience over an annual time horizon, for a randomly drawn sample of roughly 1,000 German adults. Subjects also take part in two different tests of cognitive ability, which correspond to sub-modules of one of the most widely used IQ tests. Interviews are conducted in subjects' own homes. We perform a series of additional robustness checks, which help rule out other possible confounds.
ail addresses: goergenm@cardiff.ac.uk (M. Go a b s t r a c t There is a growing controversy as to the impact of private equity acquisitions, especially in terms of their impact on employment and subsequent organizational performance. It has been suggested that closer owner supervision and the injection of a new management team revitalize the acquired organization and unlock dormant capabilities and value. However, both politicians and trade unionists suggest that private equity acquirers may significantly reallocate value away from employees to short term investors, typically through layoffs and reduced wages, which may undermine future organizational sustainability. This article investigates this in the context of a sample of institutional buy outs (IBOs) undertaken in the UK between 1997 and 2006. (2001, 2002) in investigating the employment consequences of regular takeovers. The report surveys the activity of private equity and other financial investors in the water, waste and healthcare sectors in Europe. Based on US evidence, #CITATION_TAG find an initial decline in jobs, but that this is not lasting. nan
A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. There is evidence that inducing negative moods increases mindwandering, perhaps because it potentiates personal concerns (#CITATION_TAG). Positive, neutral, and negative moods were induced in participants prior to them completing a sustained attention task. Mind wandering was measured by using the frequencies of both behavioral lapses and retrospective indices of subjective experience. Relative to a positive mood, induction of a negative mood led participants to make more lapses, report a greater frequency of task irrelevant thoughts, and become less inclined to reengage attentional resources following a lapse.
The fungicides used to control diseases in cereal production can have adverse effects on non-target fungi, with possible consequences for plant health and productivity. The fungal community on wheat leaves consisted mainly of basidiomycete yeasts, saprotrophic ascomycetes and plant pathogens. This study examined fungicide effects on fungal communities on winter wheat leaves in two areas of Sweden. Seed is among the most key input for improving crop production and productivity. Increasing the quality of seeds can increase the yield potential of the crop by significant folds. In recent years seed has become an international commodity used to exchange germplasm around the world. Seed is, however, also an efficient means of introducing plant pathogens into a new area as well as providing a means of their survival from one cropping season to another. Seed borne mycroflora are significant destroyers of food stuffs and grains during storage rendering them unfit for human consumption by retarding their nutritive value and often by producing mycotoxins. Seed-borne pathogens have been involved in seed rots during germination and seedling mortality leading to poor crop stand reduction in plant growth and productivity of crops. The seed-borne pathogens associated with seeds externally or internally may cause seed abortion, seed rot, seed necrosis, reduction or elimination of germination capacity, as well as seedling damage resulting in development of disease at later stages of plant growth by systemic or local infection. Infected seeds play considerable role in the establishment of economically important plant diseases in the field resulting in heavy reduction of crop yields. The ascomycete A. pullulans is one of the most common inhabitants of the phyllosphere of many crops [48] and is also present in many other habitats [#CITATION_TAG]. It also discusses the detection mechanism and implies some management strategies that are implemented to reduce the loss due to seed borne fungi.
As other authors have noted, as a concept, resilience involves some potentially serious conflicts or contradictions, for example between stability and dynamism, or between dynamic equilibrium (homeostasis) and evolution. This may be the case for disaster risk reduction, which involves transformation rather than preservation of the "state of the system". "An arrow never lodges in a stone: often it recoils upon its sender." 347-407), Archbishop of Constantinople The historical etymology of the term resilience The word resilience, together with its various derivatives, has a long and diverse history. This paper examines the development over historical time of the meaning and uses of the term resilience. The objective is to deepen our understanding of how the term came to be adopted in disaster risk reduction and resolve some of the conflicts and controversies that have arisen when it has been used. The paper traces the development of resilience through the sciences, humanities, and legal and political spheres. In order to gain a deeper and more Published by Copernicus Publications on behalf of the European Geosciences Union. The debate on disaster resilience has continued to grow, albeit at a slow pace, since the 2005 World Conference on Disaster Reduction held in Kobe, Hyogo, Japan. One of the most important and striking aspects is that despite the conceptual differences, the resilience and vulnerability paradigms are still locked together and are increasingly being treated as if they are one and the same. The reason for this is not a difficult one. However, the notion of "bounce back" differentiates resilience from vulnerability. The "bounce back" notion is important to the extent that it liberates resilience from the vulnerability conundrum. Yet, the "bounce back" notion does not seem to acknowledge that disasters are accompanied by change. Three arguments are presented in this paper. It stems from resilire, resilio, Latin for "bounce" -hence the idea of "bouncing back" (#CITATION_TAG). Resilience and vulnerability are viewed as opposite sides of the same coin (Twigg 2007). First, the "bounce forward" ability conceptualisation of resilience has implications on disaster research and scholarship.
The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. This article examines the use of seasonal climate forecasting in public and private efforts to mitigate the impacts of drought in Ceara, Northeast Brazil. Here, forecasts have been directed towards small scale, rainfed agriculturalists as well as state and local level policymakers in the areas of agriculture, water management, and emergency drought relief. On the other hand, climate forecasting has the potential to offer a dramatic opportunity for state and local level bureaucracies to embark on a path of proactive drought planning. It is not clear from our analysis that such work is being done yet in the NACP (#CITATION_TAG; Lemos et al., 2002). First, the current level of skill of the forecasts is inadequate for the needs of policy development and farmer decisionmaking. Second, forecast information application has been subject to distortion, misinterpretation and political manipulation. Third, focus on the forecast as a product until recently neglected to take into account end users' needs and decisionmaking behavior.
INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Do epistemic intuitions tell us anything about knowledge? Stich has argued that we respond to cases according to our contingent cultural programming, and not in a manner that tends to reveal anything significant about knowledge itself. I've argued that a cross-culturally universal capacity for mindreading produces the intuitive sense that the subject of a case has or lacks knowledge. I argue that existing work on cross-cultural variation in mindreading favors my position over Stich's Perhaps this is why philosophers will argue, explicitly or implicitly, that premises can be considered true or false in virtue of their intuitive appeal-viz., the premise just seems to be true or false (Audi 2004, Bealer 1998, Huemer 2005, #CITATION_TAG. nan
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. BACKGROUND Telephone services can provide information and support for smokers. Counselling may be provided proactively or offered reactively to callers to smoking cessation helplines. Participants were mostly adult smokers from the general population, but some studies included teenagers, pregnant women, and people with long-term or mental health conditions. Most studies (100/104) assessed proactive telephone counselling, as opposed to reactive forms.Among trials including smokers who contacted helplines (32,484 participants), quit rates were higher for smokers receiving multiple sessions of proactive counselling (risk ratio (RR) 1.38, 95% confidence interval (CI) 1.19 to 1.61; 14 trials, 32,484 participants; I2 = 72%) compared with a control condition providing self-help materials or brief counselling in a single call. There is currently insufficient evidence to assess potential variations in effect from differences in the number of contacts, type or timing of telephone counselling, or when telephone counselling is provided as an adjunct to other smoking cessation therapies. Yet, although these interventions are efficacious, [6] [#CITATION_TAG] [8] the majority of smokers who quit successfully do so without using them, choosing instead to quit unassisted, that is without pharmacological or professional support. SEARCH METHODS We searched the Cochrane Tobacco Addiction Group Specialised Register, clinicaltrials.gov, and the ICTRP for studies of telephone counselling, using search terms including 'hotlines' or 'quitline' or 'helpline'. SELECTION CRITERIA Randomised or quasi-randomised controlled trials which offered proactive or reactive telephone counselling to smokers to assist smoking cessation. DATA COLLECTION AND ANALYSIS We used standard methodological procedures expected by Cochrane. We pooled studies using a random-effects model and assessed statistical heterogeneity amongst subgroups of clinically comparable studies using the I2 statistic. In trials including smokers who did not call a quitline, we used meta-regression to investigate moderation of the effect of telephone counselling by the planned number of calls in the intervention, trial selection of participants that were motivated to quit, and the baseline support provided together with telephone counselling (either self-help only, brief face-to-face intervention, pharmacotherapy, or financial incentives).
The developments in archaeology are part of broader trends in anthropology and psychology and are characterized by the same theoretical disagreements. There are two distinct research traditions: one centered on cultural transmission and dual inheritance theory and the other on human behavioral ecology. Word representation is a means of representing a word as mathematical entities that can be read, reasoned and manipulated by computational models. The representation is required for input to any new modern data models and in many cases, the accuracy of a model depends on it. Because animal body size is correlated with handling costs and is readily assessable using archaeological faunal data, the proportion of large-bodied vs. small-bodied animal bones has very frequently been used as a diet-breadth measure (e.g., Broughton 1994; see also Ugan 2005; for within-species size variation see, e.g., #CITATION_TAG). In this paper, we analyze various methods of calculating vector space for Nepali words and postulate a word to vector model based on the Skip-gram model with NCE loss capturing syntactic and semantic word relationships.
Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Abstract Background The number of days of pedometer or accelerometer data needed to reliably assess physical activity (PA) is important for research that examines the relationship with health. While this important research has been completed in young to middle-aged adults, data is lacking in older adults. When examining time spent in specific intensities of PA, fewer days of data are needed for accurate prediction of time spent in that activity for ActiGraph but more for the PA log. However, estimates of how many days of monitoring that should be included to obtain a reliable result vary considerably between studies [3-7, 2, 8], and might also vary between outcome variables of interest [6, #CITATION_TAG]. Methods Participants (52 older men and women; age = 69.3 +- 7.4 years, range= 55-86 years) wore a Yamax Digiwalker SW-200 pedometer and an ActiGraph 7164 accelerometer while completing a PA log for 21 consecutive days. Mean differences each instrument and intensity between days of the week were examined using separate repeated measures analysis of variance for with pairwise comparisons. Spearman-Brown Prophecy Formulae based on Intraclass Correlations of .80, .85, .90 and .95 were used to predict the number of days of accelerometer or pedometer wear or PA log daily records needed to represent total PA, light PA, moderate-to-vigorous PA, and sedentary behaviour. To accurately predict average daily time spent in sedentary behaviour, five days of ActiGraph data are needed.
The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. This article conceptualizes community cultural wealth as a critical race theory (CRT) challenge to traditional interpretations of cultural capital. Various forms of capital nurtured through cultural wealth include aspirational, navigational, social, linguistic, familial and resistant capital. These forms of capital draw on the knowledges Students of Color bring with them from their homes and communities into the classroom. Recent research has stressed the importance of network structures in understanding business exchanges (#CITATION_TAG; Mller & Rajala, 2007). CRT shifts the research lens away from a deficit view of Communities of Color as places full of cultural poverty disadvantages, and instead focuses on and learns from the array of cultural knowledge, skills, abilities and contacts possessed by socially marginalized groups that often go unrecognized and unacknowledged.
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Physicians who deny the existence of a transcendent power and hardly attend religious services are more likely to approve of euthanasia even in the case of minors or demented patients. This was not at all so in Belgium: An initial major motive for the introduction of palliative care was also to promote the acceptability of euthanasia (Bernheim 1990 (#CITATION_TAG Distelmans 2010). Materials and Methods: An anonymous self administered questionnaire approved by Flemish Palliative Care Federation and its ethics steering group was sent to all physicians(n-147) working in Flemish Palliative Care. Questionnaire consisted of three parts. In first part responded were requested to provide demographic information. In second part the respondents were asked to provide information concerning their religion or world view through several questions enquiring after religious or ideological affiliation, religious or ideological self-definition, view on life after death, image of God, spirituality, importance of rituals in their life, religious practice, and importance of religion in life. The third part consisted of a list of attitudinal statements regarding different treatment decisions in advanced disease on which the respondents had to give their opinion using a five-point Likert scale.99 physician responded.
At least since the time of Popper, scientists have understood that science provides falsification, but not "proof." In the world of environmental and technological controversies, however, many observers continue to call precisely for "proof," often under the guise of "scientific certainty." Given that most scientific findings are inherently probabilistic and ambiguous, if agencies can be prevented from imposing any regulations until they are unambiguously "justified," most regulations can be defeated or postponed, often for decades, allowing profitable but potentially risky activities to continue unabated. Many disciplines discuss skepticism, including politics (e.g., Taber & Lodge, 2006), philosophy (e.g., McGrath, 2011), sociology (e.g., #CITATION_TAG), and psychology (e.g., Lilienfeld, 2012). An exploratory examination of previously documented controversies suggests that SCAMs are more widespread than has been recognized in the past, and that they deserve greater attention in the future. In the first section of the article, we draw on science and technology studies to underscore the point that science is often characterized not by certainty, but by uncertainty--meaning that the outcomes of scientific/technological controversies may depend less on which side has the "best science" than on which side enjoys the benefit of the doubt in the face of scientific ambiguity. In the second section, we note that the benefits of doubts may be distributed in ways that are not merely random: Although there is clearly a need for more extensive research, a series of riskrelated controversies, over a period of nearly a century, indicate that industrial interests have often managed to delay or prevent legislative and/or regulatory actions even in "tough" cases--those where the preponderance of scientific evidence had indicated significant reasons for concern.
Background Social anxiety disorder is one of the most persistent and common anxiety disorders. Individually delivered psychological therapies are the most effective treatment options for adults with social anxiety disorder, but they are associated with high intervention costs. Therefore, the objective of this study was to assess the relative cost effectiveness of a variety of psychological and pharmacological interventions for adults with social anxiety disorder. Research deficits lie in a lack of data for most EU countries and in a lack of studies in children and the elderly. No data are available addressing met and unmet needs for intervention and costs, and data for vulnerability and risk factors of malignant course are scarce Social anxiety disorder is one of the most persistent and common anxiety disorders, with a lifetime prevalence estimated to range between 3.9% and 13.7% in Europe [#CITATION_TAG]. Social phobia was shown to be a persistent condition with a remarkably high degree of comorbid conditions, associated impairment and disability.
These arguments have been described extensively in Kimsma and Van Leeuwen (Asking to die. Inside the Dutch debate about euthanasia, Kluwer Academic Publishers, Dordrecht, 1998). Analyses of intergenerational class mobility in Britain (e.g. Goldthorpe and Mills, 2008; Kuha and Goldthorpe, 2010) still tend to focus on father's occupation or the 'dominant' parental occupation (Erikson, 1984); however, internationally, authors examining occupational outcomes have highlighted the desirability of taking into account both parents' characteristics (#CITATION_TAG; Lampard, 2007a; Marks, 2009; Schoon, 2008). After some general introductory descriptions, by way of formulating a frame of reference, I shall describe the effects of this practice on patients, physicians and families, followed by a more philosophical reflection on the significance of these effects for the assessment of the authenticity of a request and the nature of unbearable suffering, two key concepts in the procedure towards euthanasia or physician-assisted suicide.
Traditional theories of forgetting are wedded to the notion that cue-overload interference procedures (often involving the A-B, A-C list-learning paradigm) capture the most important elements of forgetting in everyday life. However, findings from a century of work in psychology, psychopharmacology, and neuroscience converge on the notion that such procedures may pertain mainly to forgetting in the laboratory and that everyday forgetting is attributable to an altogether different form of interference. In contrast, theories grounded in the notion of retroactive interference (#CITATION_TAG; see also Mednick, Cai, Shuman, Anagnostaras, & Wixted, 2011) insist that during slow-wave sleep synaptic plasticity in the hippocampus is null. nan
Background In adults, a minimum of 3-5 days of accelerometer monitoring is usually considered appropriate to obtain reliable estimates of physical activity (PA). However, a longer period of measurement might be needed to obtain reliable estimates of sedentary behavior (SED). The aim of this study was to determine the reliability of objectively assessed SED and PA in adults. Background: In recent years there has been a growing interest in the relationship between sedentary behaviour (sitting) and health outcomes. Only recently have there been studies assessing the association between time spent in sedentary behaviour and the metabolic syndrome. Reducing sedentary behaviours is potentially important for the prevention of metabolic syndrome The possible impaired reliability for SED compared to other variables may be of critical importance, given the increased interest in SED in the primary and secondary prevention of a range of chronic diseases as well as premature death [9] [#CITATION_TAG] [11]. Reference lists of relevant articles and personal databases were hand searched. Inclusion criteria were: (1) cross sectional or prospective design; (2) include adults &ge;18 years of age; (3) self-reported or objectively measured sedentary time; and (4) an outcome measure of metabolic syndrome. Odds Ratio (OR) and 95% confidence intervals for metabolic syndrome comparing the highest level of sedentary behaviour to the lowest were extracted for each study. Data were pooled using random effects models to take into account heterogeneity between studies. Ten cross-sectional studies (n = 21393 participants), one high, four moderate and five poor quality, were identified.
We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Perhaps just as critically, #CITATION_TAG argues that interpretation of the neural data from reasoning experiments requires a re-evaluation of psychological theories of reasoning, with the neuroscience evidence indicating that human reasoning involves separate systems for dealing with (a) familiar and unfamiliar material, (b) conflicting information and belief bias, and (c) certain and uncertain information. Psychological type was assessed by the Francis Psychological Type Scales which provide classification in terms of orientation (extraversion or introversion), perceiving (sensing or intuition), judging (thinking or feeling) and attitude toward the outer world (extraverted judging or extraverted perceiving). Work-related psychological health was assessed by the Francis Burnout Inventory which distinguishes between positive affect (the Satisfaction in Ministry Scale) and negative affect (the Scale of Emotional Exhaustion in Ministry). Strategies are suggested for enabling introverted clergy to cope more effectively and more efficiently with the extraverted demands of ministry.
Orthodontic treatment is as popular as ever. Orthodontists frequently have long lists of people wanting treatment and the cost to the NHS in England was PS261m in 2013-14 (approximately 11% of the NHS annual spend on dentistry). It is important that clinicians and healthcare commissioners constantly question the contribution of interventions towards improving the health of the population. The authors would like to point out that this is not a comprehensive and systematic review of the entire scientific literature. Using a technique of altering the arrangements of the teeth on standardised photographs of young people smiling, Shaw and colleagues showed that the appearance of teeth could influence the social judgements made by their peers about the person in the photograph #CITATION_TAG, 48; however dental appearance did not affect the judgements made by teachers. Black and white photographs of an attractive boy and girl and an unattractive boy and girl were obtained and modified so that, for each face, five different photographic versions were available. In each version, the child's face was standardized except that a different dentofacial arrangement was demonstrated. These were normal incisors, prominent incisors, a missing lateral incisor, severely crowded incisors, and unilateral cleft lip. Each photograph was viewed by a different group of forty-two children and forty-two adults, equally divided as to sex. Their impressions of the depicted child's social attractiveness were recorded on visual analogue scales. The experimental procedure was such that the effect and interaction of different levels of facial attractiveness, different dentofacial arrangements, sex of the photographed child, and sex of the judge could be analyzed.
OPINION ARTICLE published: 16 April 2013 doi: 10.3389/fpls.2013.00099 Defining new SNARE functions: the i-SNARE Gian-Pietro Di Sansebastiano* Laboratory of Botany, DiSTeBA, University of Salento, Lecce, Italy *Correspondence: gp.disansebastiano@unisalento.it Edited by: Markus Geisler, University of Fribourg, Switzerland Reviewed by: Markus Geisler, University of Fribourg, Switzerland Frantisek Baluska, University of Bonn, Germany Giovanni Stefano, Michigan State University, USA SNAREs (N-ethylmaleimide-sensitive factor adaptor protein receptors) have been often seen to have a dishomogeneous distribution on membranes and are apparently present in excess of the amount required to assure correct vesicle traffic. It was also shown in few cases that SNARE on the target membrane (t-SNARE) with a fusogenic role, can become non-fusogenic when overexpressed. SNARE ABUNDANCE AND INFLUENCE OF DISTRIBUTION ON THEIR FUSOGENIC ROLE SNAREs are relatively small polypeptides (~200-400-amino-acids) characterized by the presence of a particular domain, the SNARE motif (Jahn and Scheller, 2006), consisting of heptad repeats that can form a coiled-coil structure. Via heterooligomeric interactions, these proteins form highly stable protein-protein interactions organized in a SNARE-complex that help to overcome the energy barrier required for membrane fusion. Even after considering all these potential interactors, in living cells, most SNARE molecules are apparently present in excess and concentrated in clusters, thus constituting a spare pool not readily available for interactions. About the alteration of SNARE function, it is essential to remember that antibodies or recombinant SNARE fragments, showing inhibitory or dominant negative (DN) effect, for example, on syntaxin 13 (Bethani et al., 2009), induce effects that are very different: antibodies cause the depletion of active domains while SNARE fragments cause the competitive saturation of the interacting partners. SNAREs (precisely t-SNAREs) have been visualized to form apparent clusters using fluorescence and confocal microscopy. This inhomogeneous distribution was initially proposed to provide a localized pool of t-SNAREs to facilitate and enhance membranes fusion (van den Bogaart et al., 2011) but recently, using super-resolution microscopy techniques, Yang and coworkers (2012) showed that secretory vesicles were preferentially targeted to membrane areas with a low density of SNAREs. Vesicles do not preferentially target these microdomains. Several mechanisms have been proposed to explain protein clustering in micro-domains and the t-SNARE distribution seems to depend both on lipidic and proteic contributions (Yang et al., 2012). Regulating t-SNARE distribution the cell could dynamically modulate vesicle fusion probabilities and consequently the kinetics of the cellular response (Silva et al., 2010; Yang et al., 2012). Recently we observed for Arabidopsis SYP51 and SYP52 a double localization associated to two different functions (De Benedictis et al., 2012). Also in Petunia hybrida, the single SYP51 gene cloned up to now (Faraco, 2011) seems to define in petal epidermal cells a very well defined vacuolar compartments separated from www.frontiersin.org April 2013 | Volume 4 | Article 99 | 1 the central vacuole and already observed with other vacuolar markers (Verweij et al., 2008). The discovery of new structural roles for SNAREs, eventually related to the interaction with still unknown partners, may shed light on vacuolar complex organization and it is not surprising that results about vacuolar SNAREs still appear contradictory. Bethani and co-workers (2009) discussed interesting points proving SNARE specificity. Little attention is generally paid to the need of the cell to keep very similar compartments separated, because this need may not be evident among endosomes as much as among larger vacuolar structures typical of only few plant cells (Epimashko et al., 2004; Verweij et al., 2008). Proteolipidic composition appears determinant (Strasser et al., 2011). From new data about vacuolar fusion in yeast, it seems that different SNAREs actively bind to different V-ATPase subunits, influencing their interaction with the proteolipid cylinder so promoting, or inhibiting, the lipid reorientation for the formation of a lipidic fusion pore (Strasser et al., 2011). It is extremely interesting a recent report on SNAREs interaction with proteolipid (Di Giovanni et al., 2010). It was suggested that this interaction had the effect to concentrate SNAREs in some areas to enhance their fusogenic potential but it is now evident that more regulatory events than simple localization is involved. i-SNAREs At the moment, in plants, it was observed that SYP21 (Foresti et al., 2006), SYP51, and SYP52 (De Benedictis et al., 2012) inhibit vacuolar traffic when overexpressed. Varlamov and co-workers (2004) suggested that non-fusogenic SNARE complexes, including the i-SNARE partners, have the physiological function at the level of the Golgi apparatus to increase the polarity of this organelle. Mammalian and yeast i-SNAREs (syntaxin 6/Tlg1, GS15/Sft1, and rBet1/Bet1) were found functionally conserved but i-SNARE characterization in plants is still poor. A mechanism for the i-SNARE effect of yeast Qc-SNAREs is described by the competition between endosomal (Tlg1 and Syn8) and vacuolar form (Vam7) of the proteins (Izawa et al., 2012) and because of their ability to interact with V-ATPase subunits influencing membrane potential (Strasser et al., 2011). More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (Grefen et al., 2010). The speculations about the mechanism active in plant cells can include the mechanisms elucidated in yeast cells with the exception that in S. cerevisiae a single Qc-SNARE is active at each step but more than one are active in plants. Several sorting processes may be influenced by the higher concentration of specific SNAREs but the phenomena are simply not yet correlated. SNAREs can also be specifically localized and active as t-SNARE on intermediate compartments, such as for example SYP61, localized on the TGN membranes (2). The compartments indicated in the figure are generic; their identity may change in different experimental systems and in differentiated cells. pollen tubes (Wang et al., 2011) where SYP5s are expressed at higher levels than in all other tissues (Lipka et al., 2007; De Benedictis et al., 2012). The equilibrium between fusogenic (tSNARE) and non fusogenic (i-SNARE) activity of specific SNAREs may reside on their localization, as highlighted for SYP51 and SYP52 (De Benedictis et al., 2012) but also on the formation of "clusters" in cholesterol-containing microdomains (Sieber et al., 2006, 2007). In this manuscript I discuss data obtained in various eukaryotic models that leave open different possibilities for the action mechanism of the i-SNAREs in plants. It supported the idea that a small number of SNAREs is needed to drive a single fusion event and that the proteins not engaged in classic fusion events are maintained, by yet undefined mechanisms, in membrane micro-domains with a non-random molecular composition. These have been proposed to belong to a new functional class of SNAREs (Varlamov et al., 2004). The SNARE protein of Arabidopsis, SYP121, contributes to vesicle traffic and also controls the gating of K+ channels for K+ uptake by binding to the KC1 channel subunit. The SNARE (for soluble N-ethylmaleimide-sensitive factor protein attachment protein receptor) protein SYP121 (=SYR1/PEN1) of Arabidopsis thaliana facilitates vesicle traffic, delivering ion channels and other cargo to the plasma membrane, and contributing to plant cell expansion and defense. More proteins potentially able to interact with SNAREs can have a direct influence on membrane potential such as ion channels, as shown in the case of SYP121, able to interact and control the K(+) channel KC1 (#CITATION_TAG). Here, we report isolating a minimal sequence motif of SYP121 prerequisite for its interaction with KC1. We made use of yeast mating-based split-ubiquitin and in vivo bimolecular fluorescence complementation assays for protein-protein interaction and of expression and electrophysiological analysis.
A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. A thorough investigation of the neural effects of psychotherapy is needed in order to provide a neurobiological foundation for widely used treatment protocols. Cognitive behavioural therapy in phobia resulted in decreased activity in limbic and paralimbic areas. However, a one to one correspondence between change in depression and change in specific brain areas may be over stated (#CITATION_TAG; Frewen et al., 2008; Dichter et al., 2012). nan
Coronal loops are the building blocks of the X-ray bright solar corona. They owe their brightness to the dense confined plasma, and this review focuses on loops mostly as structures confining plasma. Quiescent loops and their confined plasma are considered and, therefore, topics such as loop oscillations and flaring loops (except for non-solar ones, which provide information on stellar loops) are not specifically addressed here. Special attention is devoted to the question of loop heating, with separate discussion of wave (AC) and impulsive (DC) heating. Widespread evidence for outward propagation of Alfvn waves is reported from ground optical polarimetric observations (#CITATION_TAG), and non-thermal broadening has been shown to correlate with swaying motions detected in the corona from SDO/AIA data (speed of  20 km s -1 and periods of few minutes) (McIntosh et al., 2011). Treatment of DC with intact IgG or Fab of mAb DF272 enhanced their T cell stimulatory capacity. By using a retrovirus-based cDNA expression library generated from DC, we cloned and sequenced the mAb DF272-defined cell surface receptor and could demonstrate that it is identical with B7-H1 (programmed death-1 ligand), a recently identified new member of the B7 family of costimulatory molecules.
We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Background Epidemiological research has shown that hallucinations and delusions, the classic symptoms of psychosis, are far more prevalent in the population than actual psychotic disorder. These symptoms are especially prevalent in childhood and adolescence. Longitudinal research has demonstrated that psychotic symptoms in adolescence increase the risk of psychotic disorder in adulthood. There has been a lack of research, however, on the immediate clinicopathological significance of psychotic symptoms in adolescence. Adolescents who reported psychotic symptoms were at particularly high risk of having multiple co-occurring diagnoses. Conclusions Psychotic symptoms are important risk markers for a wide range of non-psychotic psychopathological disorders, in particular for severe psychopathology characterised by multiple co-occurring diagnoses. This has resulted in recent demonstrations that the importance sampling algorithm can be implemented by exemplar-based memory mechanisms (#CITATION_TAG; Shi & Griffiths, 2009; Shi et al., 2010). Method Data from four population studies were used: two early adolescence studies (ages 11-13 years) and two mid-adolescence studies (ages 13-16 years). Studies 1 and 2 involved school-based surveys of 2243 children aged 11-16 years for psychotic symptoms and for emotional and behavioural symptoms of psychopathology. Studies 3 and 4 involved in-depth diagnostic interview assessments of psychotic symptoms and lifetime psychiatric disorders in community samples of 423 children aged 11-15 years. These symptoms should be carefully assessed in all patients.
We studied choreographer Wayne McGregor's approach to movement creation through tasking, in which he asks dancers to create movement in response to task instructions that require a great deal of mental imagery and decision making. As part of a programme of research that is developing tools to enhance choreographic practice, an interdisciplinary team of cognitive scientists, neuroscientists and dance professionals collaborated on two studies examining the mental representations used to support movement creation. Creativity has long been a construct of interest to philosophers, psychologists and, more recently, neuroscientists. Recent efforts have focused on cognitive processes likely to be important to the manifestation of novelty and usefulness within a given social context. One such cognitive process - divergent thinking - is the process by which one extrapolates many possible answers to an initial stimulus or target data set. The distribution of brain regions, associated with both divergent thinking and creative achievement, suggests that cognitive control of information flow among brain areas may be critical to understanding creative cognition. Other studies have focused on the underlying neural mechanisms of creativity in realms other than dance (#CITATION_TAG), especially in music (Limb and Braun, 2008) and drawing (Bhattacharya & Petsche, 2005). Three independent judges ranked the creative products of each subject using the consensual assessment technique (Amabile, 1982) from which a "composite creativity index" (CCI) was derived. Structural magnetic resonance imaging was obtained at 1.5 Tesla Siemens scanner. Cortical reconstruction and volumetric segmentation were performed with the FreeSurfer image analysis suite. A region within the lingual gyrus was negatively correlated with CCI; the right posterior cingulate correlated positively with the CCI.
A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. The last two decades have seen a surge in the interest in research based on Functional Magnetic Resonance Imaging (fMRI) data. Decoding of cognitive states based on fMRI activation profiles has become a very active topic in this area. fMRI data is very high dimensional and noisy. However, there is a dearth of datasets to work on. Decoding of cognitive states using classifiers trained across multiple subjects is a challenging task because of differences in anatomy and cognition. Beginnings are starting to be made in reproducing data across as well as within subjects (#CITATION_TAG; Raizada and Connolly, 2012). Selecting features to analyze from the dataset is a key step in the analysis of fMRI data. In this paper we apply PCA, ICA and five non-linear dimensionality reduction techniques to the fMRI data. The reduced datasets are then used to train classifiers to solve a multiple-subject decoding problem.
Developments in immunological and quantitative real-time PCR-based analysis have enabled the detection, enumeration, and characterization of circulating tumor cells (CTCs). It is assumed that the detection of CTCs is associated with cancer, based on the finding that CTCs can be detected in all major cancer and not in healthy subjects or those with benign disease. Patients with chronic prostatitis may have circulating prostate cells detected in blood, which do not express the enzyme P504S and should be thought of as benign in nature. Prostate cancer is the most commonly diagnosed cancer in men and the second leading cause of cancer deaths. The serum prostate specific antigen (PSA) is the only biomarker routinely used in screening. The thirty women acting as controls were all CPC negative; in the 409 men, the frequency of malignant CPC detection increased significantly with age and PSA level and is associated with a biopsy positive for cancer [#CITATION_TAG]. For this purpose mononuclear cells were separated from blood using differential centrifugation and then prostate cells were identified by using standard immunocytochemical method.
Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. Named entity recognition is a crucial component of biomedical natural language processing, enabling information extraction and ultimately reasoning over and knowledge discovery from text. Much progress has been made in the design of rule-based and supervised tools, but they are often genre and task dependent. As such, adapting them to different genres of text or identifying new types of entities requires major effort in re-annotation or rule development. One such approach has been introduced by (#CITATION_TAG), who have proposed a model for unsupervised Named En-tity Recognition. A noun phrase chunker followed by a filter based on inverse document frequency extracts candidate entities from free text. Classification of candidate entities into categories of interest is carried out by leveraging principles from distributional semantics. Detailed error analysis provides a road map for future work.
The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. In the "stagnationist" macroeconomics, the long-term problem of growth and distribution has been discussed in a short-period Kaleckian framework which abstracts from labor market-output market interaction to determine real wages. However, even in this framework, one can challenge the stagnationist conclusion that there is no trade-off between wage and profit and between distribution and growth. Evidently, in a general framework, there is no unambiguous relationship between distribution and growth. Thus, stability is a prerequisite for change (Lundgren, 1992) and an inherent feature of a network (#CITATION_TAG). So distributive justice should be taken as a separate objective on its own merit.
A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Recent research in multivoxel pattern-based fMRI analysis has led to considerable success at decoding within individual subjects. However, the goal of being able to decode across subjects is still challenging: It has remained unclear what population-level regularities of neural representation there might be. On the contrary, to decode across subjects, it is beneficial to abstract away from subject-specific patterns of neural activity and, instead, to operate on the similarity relations between those patterns: Our new approach performs decoding purely within similarity space. Beginnings are starting to be made in reproducing data across as well as within subjects (Accamma and Suma, 2012; #CITATION_TAG). nan
Major depressive disorder (MDD) is associated with significant impairment in occupational functioning. This study sought to determine which depressive symptoms and medication side effects were perceived by patients with MDD to have the greatest interference on work functioning. The most common were allergies, arthritis/joint pain or stiffness, and back or neck disorders. However, the greater proportion of the total economic burden of MDD lies in reduced productivity, or presenteeism, in which the depressed individual remains in the work setting but with productivity suffering both in quality and quantity [7, #CITATION_TAG]. Methods: Using the Stanford Presenteeism Scale, information was collected from workers at five locations on work impairment and absenteeism based on self-reported "primary" chronic health conditions. Survey data were merged with employee demographics, medical and pharmaceutical claims, smoking status, biometric health risk factors, payroll records, and job type.
Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Information Theory enables the quantification of how much information a neuronal response carries about external stimuli and is hence a natural analytic framework for studying neural coding. The main difficulty in its practical application to spike train analysis is that estimates of neuronal information from experimental data are prone to a systematic error (called "bias"). This bias is an inevitable consequence of the limited number of stimulus-response samples that it is possible to record in a real experiment. An interesting implication of distortion and manipulability is that numeric performance measures may degrade over time (Courty & Marschke 2004, #CITATION_TAG. In this paper, we first explain the origin and the implications of the bias problem in spike train analysis. We then review and evaluate some recent general-purpose methods to correct for sampling bias: the Panzeri-Treves, Quadratic Extrapolation, Best Universal Bound, Nemenman-Shafee-Bialek procedures, and a recently proposed shuffling bias reduction procedure. This provides information estimates with acceptable variance and which are unbiased even when the number of trials per stimulus is as small as the number of possible discrete neuronal responses.
Choices are not only communicated via explicit actions but also passively through inaction. Additionally, the choice itself was biased towards action such that subjects tended to choose a photograph obtained by action more often than a photographed obtained through inaction. In this study we investigated how active or passive choice impacts upon the choice process itself as well as a preference change induced by choice. Since Brehm's (1956) initial free-choice experiment, psychologists have interpreted the spreading of alternatives as evidence for choice-induced attitude change. It is widely assumed to occur because choosing creates cognitive dissonance, which is then reduced through rationalization. One paradigm demonstrating how choice affects preferences is the free-choice paradigm wherein value judgements are gathered on options before and after subjects are forced to choose one and reject the other option [#CITATION_TAG, 7]. After making a choice between two objects, people evaluate their chosen item higher and their rejected item lower (i.e., they "spread " the alternatives). Specifically, if people's ratings/rankings are an imperfect measure of their preferences, and their choices are at least partially guided by their preferences, then the FCP will measure spreading, even if people's preferences remain perfectly stable. We show this, first, by proving a mathematical theorem that identifies a set of conditions under which the FCP will measure spreading, even absent attitude change. We then experimentally demonstrate that these conditions appear to hold, and that the FCP measures a spread of alternatives, even when this spreading cannot have been caused by choice. We discuss how the problem we identify applies to the basic FCP paradigm as well as to all variants that examine moderators and mediators of spreading.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Data mining is concerned with the extraction of novel and useful knowledge from large amounts of data. Simpler implementations (e.g., C5.0) limit each branch to value ranges from a single attribute, making this a linear classifier with a further restriction that each condition is an axis-parallel hyperplane (#CITATION_TAG). Topics include data preparation and feature selection, association rules, classification, clustering,evaluation and validation, scalability, mining of spatial/text/sequence/graph/time-series etc data, privacy, data mining applications, and other topics of interest.
The issue of how different actors in a network understand changes to their industry remains an underresearched but crucially important area. According to the industrial network approach, companies interact according to their perceptions of the relevant network environment and their subjective sensemaking of the network logic and exchange mechanisms relating to the activities, resources, and actor bonds. In financially constrained health systems across the world, increasing emphasis is being placed on the ability to demonstrate that health care interventions are not only effective, but also cost-effective. For example, strategic research has looked at strategic groups, either as defined by objective characteristics (McNamara, Deephouse, & Luce, 2003; #CITATION_TAG) or delineated by a shared understanding of different companies (Osborne, Stubbart, & Ramaprasad, 2001; Reger & Palmer, 1996). Particular emphasis is placed on the importance of the appropriate representation of uncertainty in the evaluative process and the implication this uncertainty has for decision making and the need for future research. This highly practical guide takes the reader through the key principles and approaches of modelling techniques. It begins with the basics of constructing different forms of the model, the population of the model with input parameter estimates, analysis of the results, and progression to the holistic view of models as a valuable tool for informing future research exercises. ABOUT THE SERIES: Economic evaluation of health interventions is a growing specialist field, and this series of practical handbooks will tackle, in-depth, topics superficially addressed in more general health economics books. Each volume will include illustrative material, case histories and worked examples to encourage the reader to apply the methods discussed, with supporting material provided online.
The contents and recommendations do not necessarily reflect the views of the Economic Research Forum. In the last few decades, the world has witnessed an enormous growth in the volume of foreign direct investment (FDI). The global stock of FDI reached US$ 7.5 trillion in 2003 and accounted for 11% of world Gross Domestic Product, up from 7% in 1990. Substantial FDI inflows went into transition countries. Inflows into one of the region's largest recipient, the Russian Federation, almost doubled, enabling Russia to become one of the five top FDI destinations in 2005-2006. FDI inflows in Russia have increased almost threefold from 13.6% in 2003 to 35% in 2007. In 2003, these flows were twice greater than those into China; whilst in 2007 they were six times larger. Efficient government institutions are argued by many economists to foster FDI and growth as a result. However, the magnitude of this effect has yet to be measured. This casts some doubt on the productivity of the investment in public capital in these regions as it might be that bureaucrats may prefer to use these infrastructural projects for rent extraction. Countries with high levels of public sector corruption are found to receive less foreign aid, by Alesina and Weder (2002), and less foreign direct investment, by #CITATION_TAG. Using a regional data-set it concentrates on three areas relating to FDI. Secondly, it quantifies the impact of perceived corruption on the volume of FDI stocks simultaneously estimating the impact of the investment in public capital such as telecommunications and transportation networks on FDI in the presence of corruption.
The version in the Kent Academic Repository may differ from the final published version. Users should always cite the published version of record. OBJECTIVE Negative affect precedes binge eating and purging in bulimia nervosa (BN), but little is known about factors that precipitate negative affect in relation to these behaviors. The final maintenance factor of mood intolerance (i.e. inability to appropriately cope with adverse affective states followed by dysfunctional impulsive behaviours) [13] is believed to directly affect and maintain binge eating [12, 17, [#CITATION_TAG] [22] [23] [24]. METHOD A total of 133 women with current BN recorded their mood, eating behavior, and the occurrence of stressful events every day for 2 weeks. Multilevel structural equation mediation models evaluated the relations among Time 1 stress measures (i.e., interpersonal stressors, work/environment stressors, general daily hassles, and stress appraisal), Time 2 negative affect, and Time 2 binge eating and purging, controlling for Time 1 negative affect.
Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Most quantitative research in social capital focuses on civic engagement in formal organisations. Data on social capital in informal social networks are harder to obtain and there has also been insufficient means for investigating this. Informal social networks, especially having good neighbourly relations, tend to foster greater trust than does formal civic engagement. Similarly, the 9/11 attack on the New York World Trade Centre, and the subsequent 2004 Madrid and 2005 London bombings had a significant impact on 'the thoughts, feelings and behaviors of ind ividuals' (Woods, 2011, p. 214; see also Best, Krueger, & Ladewig, 2006; Cols, 2010; Huddy, Khatib, & Capelos, 2002; #CITATION_TAG; Panagopoulos, 2006; Verkasalo, Goodwin, & Bezmenova, 2006; Yum & Schenck-Hamlin, 2005). The first two refer to informal social networks and the last to formal social networks. We use gllamm (Generalized Linear Latent and Mixed Models) to construct the latent variable scores from the categorical component variables. We also analyse the socio-cultural determinants of the three types of social capital and their impacts on social trust.
We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. Similar appeals to the utility of CL explanation are common in the Bayesian literature (see, e.g., #CITATION_TAG). The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks.
Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Constantinople is a city whose origin can be traced back to the establishment of Greek cities and colonies in early antiquity. Eventually it became the capital of the East Roman Empire, and since then its major role in the region has not diminished, whether under the rule of Byzantine emperors or Ottoman sultans. For more than 2000 years the city and its inhabitants have endured numerous changes and crises. Plague, war and economic regression have at times reduced its population to only a fraction of the previous size. The city has been subject to numerous sieges, the longest lasting eight years! Conquered only once prior to the major transformation in 1453, the city flourished again after each crisis and today it is still an important centre in this part of the world, on the border between the Mediterranean and the Black Sea. How could Constantinople maintain its leading position for such a long time, after suffering so many crises? The authors explore how the inhabitants of the ancient city of Constantinople managed to maintain a resilient food supply system. Constantinople differs in many ways from our modern cities, which are dependent on resources from a global hinterland that are transported using fossil fuels, and thus it can serve as an educational example for our time. At its first peak during the 6th century it was dependent on a complex grain transport system with ships travelling all the way to North Africa. This system collapsed in conjunction with the Arab expansion in the 7th century, and the collapse became a major part of a long recession that profoundly affected the city. That the city nonetheless survived cannot be explained by any single factor. A particularly interesting aspect, related to today's global transport system, is the urban agriculture system within and just outside the city walls. In our society, where the supply of food is considered as something obvious, one can question whether we lack memory as well as preparations for similar crises despite the fact that the food supply crisis of the Second World War is only 65 years behind us.Urban mind The most diffi cult blockade on the food supply lines, at the end of the fourteenth century AD, lasted an astonishing 8 years, but it did not succeed in starving out the urban population (#CITATION_TAG). They rather delimited an area with dispersed "sub-communities" and numerous acres of, for example, orchards and vineyards. This system was continuous and was maintained by the inhabitants' living memory as well as by important institutions.
INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Implicit learning is an inductive process whereby knowledge of a complex environment is acquired and used largely independently of awareness of either the process of acquisition or the nature of that which has been learned. The associationist about intuition will start by outlining a fairly well accepted tenet of cognitive science: whether or not we are aware of it, we are more or less constantly learning associations between things and properties of things via experience-e.g., if we frequently experience two things or properties of things together, then we will begin to associate them (Reber 1989, #CITATION_TAG, Talbot 2009. Characterized this way, implicit learning theory can be viewed as an attempt to come to grips with the classic epistemological issues of knowledge acquisition, representation and use. The argument is made that the process, despite its seeming cognitive sophistication, is of considerable evolutionary antiquity and that it antedates awareness and the capacity for conscious control of mentation. Various classic heuristics from evolutionary biology are used to substantiate this claim and several specific entailments of this line of argument are outlined.
INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. The differences model, which argues that males and females are vastly different psychologically, dominates the popular media. Gender differences can vary substantially in magnitude at different ages and depend on the context in which measurement occurs. Overinflated claims of gender differences carry substantial costs in areas such as the workplace and relationships. These results, as well as the results herein, corroborate the gender similarities hypothesis: "most psychological gender differences are in the close-to-zero (d  0.10) or small (0.11 < d < 0.35) range, a few are in the moderate range (0.36 < d < 0.65), and very few are large (d < 0.66-1.00) or very large (d < 1.00)" (#CITATION_TAG). nan
As neoadjuvant chemoradiation is established Electronic supplementary material The online version of this Article ( SETTING A district hospital and referral center in Basingstoke, England. This progress was due to standardizing surgical therapy [#CITATION_TAG] worldwide and by the implementation of multimodal therapy [4] [5] [6]. DESIGN A prospective consecutive case series. INTERVENTIONS Anterior resections (n = 465) with low stapled anastomoses (407 total mesorectal excisions), abdominoperineal resections (n = 37), Hartmann resections (n = 10), local excisions (n = 4), and laparotomy only (n = 3). Preoperative radiotherapy was used in 49 patients (7 with abdominoperineal resections, 38 with anterior resections, 3 with Hartmann resections, and 1 with laparotomy). In future clinical trials of adjuvant chemotherapy and radiotherapy, strategies should incorporate total mesorectal excision as the surgical procedure of choice.
Research linking civic engagement to citizens' democratic values, generalized trust, cooperative norms, and so on often implicitly assumes such connections are stable over time. This article argues that, due to changes in the broader institutional environment, the engagement-values relation is likely to generally lack temporal stability. Mismatches between institutions and social problems trigger reforms, but do not determine the options that policy makers finally choose. Frictions caused by emerging social risks interact with difficulties of established welfare regimes to cope with old risks to facilitate access to public agendas for reformist projects. Ultimately, however, reforms depend on the construction of pro-and anti-reform coalitions, shaped by two main forces: 1) lines of discrimination in the distribution of benefits by existing welfare regimes; 2) strategies of parties, interest groups, and bureaucracies, competing to activate those cleavages according to their interests. Selective pay-offs to appease privileged groups constitute the most direct determinants of the architecture of reforms. However, exogenous shocks may facilitate changes away from expected paths. Based on the idea that institutions get 'locked in' as a result of self-reinforcement, self-reproduction and path dependence (e.g., #CITATION_TAG; Mahoney, 2000; Pierson, 2000), institutions have long been viewed as stable and resistant to change until 'exogenous shocks (...) bring about radical institutional reconfigurations' (Mahoney & Thelen, 2010, p. 2). It theorizes the reforms by comparing pensions, health care, and social assistance policies. First, it provides an explanation of recent transformations of welfare regimes as resulting from the combined effects of gradual institutional change and exogenous socioeconomic transformations. Second, it explores the potentialities and limitations of historical institutionalism. Third, it identifies emerging patterns of governance. This requires combining strategies of blame-avoidance and credit-claiming that variably mix persuasion, exclusion, and division targeting potential opposition. In explaining the reforms, I discuss endogenous institutional change and how this results in fragmented social protection policies.
In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Wheeler's argument draws on analytic philosophy, continental philosophy, and empirical work to "reconstruct" the philosophical foundations of cognitive science in a time of a fundamental shift away from a generically Cartesian approach. Indeed, they are often seen to be philosophically flawed (#CITATION_TAG). Wheeler begins with an interpretation of Descartes. He defines Cartesian psychology as a conceptual framework of explanatory principles and shows how each of these principles is part of the deep assumptions of orthodox cognitive science (both classical and connectionist). Wheeler then turns to Heidegger's radically non-Cartesian account of everyday cognition, which, he argues, can be used to articulate the philosophical foundations of a genuinely non-Cartesian cognitive science.
Despite the established importance of buyer-seller relationships in B-to-B markets, research to determine the differential effects that keep suppliers and customers in a relationship has been scarce. Only with regard to relational tolerance and only for buyers do switching costs play a greater role than relationship value. Referring to transaction cost analysis, this study investigates how switching costs and relationship value as perceived by each side unfold their bonding forces in such a relationship. Extant literature and suppliers interviewed for this study view a solution as a customized and integrated combination of goods and services for meeting a customer's business needs. In contrast, customers view a solution as a set of customer-supplier relational processes comprising (1) customer requirements definition, (2) customization and integration of goods and/or services and (3) their deployment, and (4) postdeployment customer support, all of which are aimed at meeting customers ' business needs. The relational process view can help suppliers deliver more effective solutions at profitable prices. Customer variables include adaptiveness to supplier offerings and political an This description also applies for suppliers: they generally try to leverage an existing relationship by cross-selling or offering new services (Davies et al., 2007), providing capital, information, and dedicated staff or adapting their production and logistics to customer demands (#CITATION_TAG). Supplier variables include contingent hierarchy, documentation emphasis, incentive externality, customer interactor stability, and process articulation.
Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. Objective measures of performance are seldom perfect. In response, incentive contracts often include important subjective components that mitigate incentive distortions caused by imperfect objective measures. Naturally, objective and subjective measures often are substitutes, sometimes strikingly so: we show that if objective measures are sufficiently close to perfect then no implicit contracts are feasible (because the firm's fallback position after reneging on an implicit contact is too attractive). Finally, subjective evaluations have their own form of uncontrollable risk for the employee: they are difficult to verify and enforce contractually, so they require relational contracting and adequate trust of the supervisor (#CITATION_TAG). nan
Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. This book represents the first asymptotic analysis, via completely integrable techniques, of the initial value problem for the focusing nonlinear Schrodinger equation in the semiclassical asymptotic regime. This problem is a key model in nonlinear optical physics and has increasingly important applications in the telecommunications industry. (#CITATION_TAG) have shown that an important feature of Word2Vec embeddings is that similar words will have similar vectors because they appear in similar contexts. The authors exploit complete integrability to establish pointwise asymptotics for this problem's solution in the semiclassical regime and explicit integration for the underlying nonlinear, elliptic, partial differential equations suspected of governing the semiclassical behavior. To achieve this, the authors have extended the reach of two powerful analytical techniques that have arisen through the asymptotic analysis of integrable systems: the Lax-Levermore-Venakides variational approach to singular limits in integrable systems, and Deift and Zhou's nonlinear Steepest-Descent/Stationary Phase method for the analysis of Riemann-Hilbert problems. In particular, they introduce a systematic procedure for handling certain Riemann-Hilbert problems with poles accumulating on curves in the plane.
The aim of this study was to explore the health-related outcomes of a new health promotion intervention designed to be broadly applicable among people diagnosed with chronic illness. Research findings showed that the lay-led CDSME program resulted in improved health status and reduced health care costs among patients suffering from arthritis [#CITATION_TAG] [13]. It also explored the differential effectiveness of the intervention for subjects with specific diseases and comorbidities. METHODS The study was a six-month randomized, controlled trial at community-based sites comparing treatment subjects with wait-list control subjects. Participants were 952 patients 40 years of age or older with a physician-confirmed diagnosis of heart disease, lung disease, stroke, or arthritis. Health behaviors, health status, and health service utilization, as determined by mailed, self-administered questionnaires, were measured.
Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. The airway epithelium forms a continuous barrier from the nose to the alveoli and serves a variety of functions. Multiple functionally distinct cell types are involved in these processes. The innate defence functions require a patent airway epithelium, with infections often associated with epithelial defects and phenotypic alterations that are themselves associated with multiple lung diseases. Non-typeable Haemophilus influenzae (NTHi) and respiratory syncytial virus (RSV) are frequently identified in the airways in a range of respiratory diseases These pathogens often trigger exacerbations and worsening symptoms that often result in hospitalisation. This is particularly true in paediatric populations. Although mortality for NTHi and RSV infections alone are themselves low it remains unclear what role these infections play in mortality rates in complex chronic respiratory infections. These markers are representative of different epithelial cell types within the cultures. Macrophages are not only a key line of defence in the respiratory tract, responsible for phagocytosis and clearance of infectious organisms but are also orchestrators of the adaptive immune response through presentation of antigen and by the cytokines they release [4, #CITATION_TAG]. In vitro airway models were established using lung derived cell lines, undifferentiated primary human bronchial epithelial (uHBE) cells and air-liquid interface (ALI) differentiated uHBE cell cultures. Following establishment of differentiation we validated ALI cultures using a number of markers, including for the putative innate defence PLUNC family proteins, gel-forming mucins and tubulin. Cultures were infected with NTHi or RSV for periods of time ranging from 1 hour to 7 days with a view to establishing chronic infections and allowing biofilm formation. Neutrophil products and trypsin were shown to degrade PLUNC proteins in ALI cell secretions.
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Following the 2002 enactment of the Belgian law on euthanasia, which requires the consultation of an independent second physician before proceeding with euthanasia, the Life End Information Forum (LEIF) was founded which provides specifically trained physicians who can act as mandatory consultants in euthanasia requests. They tended to more often discuss the request with the attending physician (100% vs 95%) and with the family (76% vs 69%), and also more frequently helped the attending physician with performing euthanasia (44% vs 24%). Irrespective of whether euthanasia is a legal practice within a country, similar services may prove useful to also improve quality of consultations in various other difficult end-of-life decision-making situations. The value of specific training for the quality of second physician consultation was recently documented (#CITATION_TAG). This study assesses quality of consultations in Flanders and Brussels and compares these between LEIF and non-LEIF consultants.A questionnaire was sent in 2009 to a random sample of 3,006 physicians in Belgium from specialties likely involved in the care of dying patients. Several questions about the last euthanasia request of one of their patients were asked.
Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Searching for local-level causes of the disease may therefore not be as productive as investigating etiological factors operating at the population level. Similar observations have been made in the past [30] [31] [32] [33] [#CITATION_TAG], although others have reported either no relationship or a negative association with SES [35]. By using administrative health data, we identified all incident cases of MS in Manitoba from 1990 to 2006 (n = 2,290) and geocoded them to 230 neighborhoods in the City of Winnipeg and 268 municipalities in rural Manitoba. Age-standardized incidence rates for 1990-2006 (combined) were calculated for each region. By using the spatial scan statistic, we identified high-rate clusters in southwestern (incidence rate ratio (IRR) = 1.48) and central Winnipeg (IRR = 1.54) and low-rate clusters in north-central Winnipeg (IRR = 0.52) and northern Manitoba (IRR = 0.48).
We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. Speech output from speech-generating devices (SGD) and SGD software, such as talking word processors, has changed the landscape of options for aided communication. Combined with interatomic interactions, an entirely new class of superfluid or strongly correlated systems becomes accessible with ultracold atoms [34, 53, 63, #CITATION_TAG, 65]. Learner-oriented roles of speech output are summarized in terms of graphic symbol learning, communicative functions and social regulation, learner preference, challenging behaviors, natural speech production, comprehension, and literacy. Roles for the learner - partner dyad include changes to interaction patterns. Methodological issues are discussed and practical implications are drawn where appropriate.
According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? OBJECTIVES Patient safety has received increased attention in recent years, but mostly with a focus on the epidemiology of errors and adverse events, rather than on practices that reduce such events. The practices focused primarily on hospitalized patients, but some involved nursing home or ambulatory patients. For most practices, the project team required that the primary outcome consist of a clinical endpoint (i.e., some measure of morbidity or mortality) or a surrogate outcome with a clear connection to patient morbidity or mortality. Many patient safety practices drawn primarily from nonmedical fields (e.g., use of simulators, bar coding, computerized physician order entry, crew resource management) deserve additional research to elucidate their value in the health care environment. Such practices target a diverse array of safety problems. Within the empirical setting of this study, these costs occur inexorably out of intra-organizational coordination, but the same categories are also valid for inter-organizational coordination (#CITATION_TAG). SEARCH STRATEGY AND SELECTION CRITERIA Patient safety practices were defined as those that reduce the risk of adverse events related to exposure to medical care across a range of diagnoses or conditions. Potential patient safety practices were identified based on preliminary surveys of the literature and expert consultation. Protocols specified the inclusion criteria for studies and the structure for evaluation of the evidence regarding each practice. Pertinent studies were identified using various bibliographic databases (e.g., MEDLINE, PsycINFO, ABI/INFORM, INSPEC), targeted searches of the Internet, and communication with relevant experts. This criterion was relaxed for some practices drawn from the non-health care literature. The evidence supporting each practice was summarized using a prospectively determined format. The project team then used a predefined consensus technique to rank the practices according to the strength of evidence presented in practice summaries. A separate ranking was developed for research priorities. Appropriate use of prophylaxis to prevent venous thromboembolism in patients at risk; Use of perioperative beta-blockers in appropriate patients to prevent perioperative morbidity and mortality; Use of maximum sterile barriers while placing central intravenous catheters to prevent infections; Appropriate use of antibiotic prophylaxis in surgical patients to prevent postoperative infections; Asking that patients recall and restate what they have been told during the informed consent process; Continuous aspiration of subglottic secretions (CASS) to prevent ventilator-associated pneumonia; Use of pressure relieving bedding materials to prevent pressure ulcers; Use of real-time ultrasound guidance during central line insertion to prevent complications; Patient self-management for warfarin (Coumadin) to achieve appropriate outpatient anticoagulation and prevent complications; Appropriate provision of nutrition, with a particular emphasis on early enteral nutrition in critically ill and surgical patients; and Use of antibiotic-impregnated central venous catheters to prevent catheter-related infections.
Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. There is growing recognition that classifying terrestrial plant species on the basis of their function (into 'functional types') rather than their higher taxonomic identity, is a promising way forward for tackling important ecological questions at the scale of ecosystems, landscapes or biomes. These questions include those on vegetation responses to and vegetation effects on, environmental changes (e.g. changes in climate, atmospheric chemistry, land use or other disturbances). There is also growing consensus about a shortlist of plant traits that should underlie such functional plant classifications, because they have strong predictive power of important ecosystem responses to environmental change and/or they themselves have strong impacts on ecosystem processes. Large international research efforts, promoted by the IGBP-GCTE Programme, are underway to screen predominant plant species in various ecosystems and biomes worldwide for such traits. In contrast to the effort put into developing extensive international methodological protocols and databases for quantitative analysis of vascular plant traits ( #CITATION_TAG et al It features a practical handbook with step-by-step recipes, with relatively brief information about the ecological context, for 28 functional traits recognised as critical for tackling large-scale ecological questions.
In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Almost all the reviewers of Margaret Boden's Mind as machine have noted the obvious: at 2 volumes, 1452 pages, 134 pages of references, and seemingly infinite parenthetically cross-references, this book, longer than most editions of War and peace, is impractical, unwieldy, and inaccessible to readers. Boden did not intend Mind as machine to be a pleasant read for a weekend's leisure. Yet, according to her, this analogy, as well as its parallel "mind as machine", is of recent origin. It was only by the close of the nineteenth century that mechanistic theories of mind acquired respectability. These theories, however, were mere analogies; no one seriously contemplated consilience between the behaviours of machines and men. Still less did anyone outside science fiction circles propose that machines could be intelligent in the same way as humans. By the mid-1800s, Charles Babbage had invented an analytical engine, somewhat akin to a programme-controlled digital computer, but he never claimed it to have implications for psychology or biology, though perhaps his student Ada Lovelace hinted at the possibility. Thus, it was during the war years of the 1940s, at the height of collaborations between Anglo-American scientists, that computers began being developed, and with them, some investigators, such as Alan Turing, began to study questions about machine intelligence. In the 1950s, these claims led to the emergence of the multi-disciplinary field of the cognitive sciences, a discipline well provided for by philanthropic and institutional sources of support, stocked with new venues for publication, and bolstered by artificial intelligence research paradigms. It was, none the less, a field riddled with intellectual divides, which developed over the next half century. Behaviourism, then predominant, was on the wane. Seen as too universalist, it was criticized by Gestaltists, linguists, ethologists, proto-connectionists, anthropologists, and Noam Chomsky alike (the last comes bizarrely in Boden's narrative with a "health warning", p. 591). In this ferment, the "mind as machine" debate took different paths: cyberneticists, for example, assumed that the mind as a machine was identical with the body. Computational psychologists--little more than a smattering of research endeavours--treated the human mind as different from its body, and concerned themselves with questions about how the mind was different. The majority of psychologists, however, focused on what made the mind different. Always lurking in the background was the question of whether human thought was "constituted by, or identical with" symbolic processes (p. 702). Those questions especially plagued papers and programmes on artificial intelligence--even when their authors were uninterested in the answers. Artificial intelligence research bolstered this nascent field enormously during the last half of the century. AI research, however, was perhaps more tied to the geopolitical context of the Cold War period and the neo-Liberal period of the 1980s and 1990s than the cognitive sciences. While much AI work focused on developing programming languages and had modest goals (seek general intelligence but not human-like intelligence, appeared almost as an injunction), critics levelled numerous charges at AI-workers, despite the fact that few were seeking to understand the human mind as a machine. Seymour Papert, an early pioneer, for instance, used only simple programmes to understand thinking processes. Yet, as defence spending increased, AI's proponents and detractors became uncomfortable with the glib assertions being promulgated within policy and media exaggerations, especially the belief that enormous computer systems controlling weapon systems could be "bug" free in their script, and commonsensical in their behaviour. Connectionists, a new but inchoate group of psychologists, neuroscientists, and philosophers of the mind, also tore into the AI project. They argued that phenomena were represented within emerging networks (usually neurological) and not symbolic systems, which many within old-fashioned AI paradigms had claimed. In hindsight, all of AI's failed promises and faulty philosophical assumptions have led some to pronounce it a failed research programme. She observes that AI enormously advanced both itself and the cognitive sciences. In that sense, and contrary to its critics, AI continued as a fruitful area of research, but like its latest corollaries, computational neuroscience and artificial life, the field remains embryonic even today. Having read those chapters alongside M R Bennett and P M S Hacker's excellent Philosophical foundations of the neurosciences (2003), I find myself having misgivings about the conceptual foundations of much of the cognitive sciences project as outlined by Boden. In any case, Boden's volumes, despite their evident value, will aggravate many. Historians studying periods before 1945 will find fault both with her facts and pithy generalizations. Similarly, those still living cognitive scientists whose careers spanned 1945 and 2000 are bound not to recognize the caricatures of themselves, or people they knew, in her story. Such criticisms, which have already begun circulating about this work, strike me as unwarranted, especially because Boden's practitioner viewpoint brings with it the hindrances such life experience implies. Anyone failing to note Boden's polemical tone is just not awake. Putting it simply, the work is too large to be free of an agenda. However, for that same reason, criticisms of this work from other practitioners appear no less problematic. Cognitive science's reliance on computer simulation means it is not well equipped to give an answer (#CITATION_TAG). Boden begins by noting that some might mistake "man as machine" for an ancient idea.
Building on the varieties-of-capitalism approach, it is argued that competitive advantage in high-tech industries with radical innovation may be supported by combinations of certain institutional conditions: lax employment protection, weak collective bargaining coverage, extensive university training, little occupational training, and a large stock market. Furthermore, multinational enterprises engage in "institutional arbitrage": they allocate their activities so as to benefit from available institutional capital. A high share of university graduates and a large stock market are complementary institutions leading to strong export performance in high-tech. Employment protection is neither conducive nor harmful to export performance in high-tech. A high volume of cross-border mergers and acquisitions, as a form of institutional arbitrage leading to knowledge flows, acts as a functional equivalent to institutions that support knowledge production in the home economy. For example, in contrast with correlational techniques, which attempt to estimate the net effect of an independent variable on an outcome variable, fsQCA attempts to identify the conditions that lead to a given outcome (#CITATION_TAG). These hypotheses are tested on country-level data for 19 OECD economies in the period 1990 to 2003.
Biological theories of sexual orientation, typically presented in human sexuality classes, are considered by many social psychologists to cause reductions in students' sexual prejudice. Yet when biological theories were not presented to 36 psychology students in a 10-week seminar on lesbian, gay, bisexual and transgender (LGBT) psychology, both sexual prejudice and two forms of essentialist thinking reduced significantly. Psychological essentialism is an ordinary mode of category representation that has powerful social-psychological consequences. Why and when people engage in this mode of thinking remain open questions. Variability in essentialism across cultures, categories, and contexts suggests that this mode of representing human categories is rooted in a naturalistic theory of category origins, combined with a need to explain differences that cross category boundaries. Although belief in the biological determination of sexual orientation is correlated with tolerance towards lesbians and gay men, beliefs in biological determination are also correlated with prejudice and stereotyping of other minority groups (c.f., Bastian & Haslam, 2006; Keller, 2005; Martin & Parker, 1995; Morton, Postmes, Haslam, & Hornsey, 2009; #CITATION_TAG; Williams & Eberhardt, 2008; Yzerbyt, Rocher, & Schadron, 1997). nan
Externalities arise when firms discriminate between on-and off-net calls or when subscription demand is elastic. This literature predicts that profit decreases and consumer surplus increases in termination charge in a neighborhood of termination cost. This creates a puzzle since in reality we see regulators worldwide pushing termination rates down while being opposed by network operators. Economists have been paying increasing attention to the study of situations in which consumers face a discrete rather than a continuous set of choices. Such models are potentially very important in evaluating the impact of government programs upon consumer welfare. But very little has been said in general regarding the tools of applied welfare economics indiscrete choice situations. Consumer surplus in the Logit model has been derived by #CITATION_TAG as (up to a constant) This paper shows how the conventional methods of applied welfare economics can be modified to handle such cases.
Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. Abstract There is growing interest in the ecology of the Maya Forest past, present, and future, as well as in the role of humans in the transformation of this ecosystem. During the Archaic period, a time of stable climatic conditions 8,000-4,000 years ago, we propose that the ancestral Maya established an intimate relationship with an expanding tropical forest, modifying the landscape to meet their subsistence needs. This highly productive and sustainable system of resource management formed the foundation for the development of the Maya civilization, from 3,000 to 1,000 years ago, and was intensified during the latter millennia of a stable climatic regime as population grew and the civilization developed. These strategies of living in the forest evolved into the milpa cycle--the axis of the Maya Forest garden resource management system that created the extraordinary economic value recognized in the Maya Forest today. Remnant urban ecosystems and the rich levels of biodiversity found in the urban Yucatan today are hence viewed to be the products of a millennia-long co-evolution in cultural landscapes (Ford and Emery 2008; #CITATION_TAG). In particular, we consider the paleoenvironmental data from the Maya Forest area in light of interpretations of the precipitation record from the Cariaco Basin. This new adaptation, we suggest, was based on a resource management strategy that grew out of earlier landscape modification practices.
Remote sensing (RS) is currently the key tool for this purpose, but RS does not estimate vegetation biomass directly, and thus may miss significant spatial variations in forest structure. The use of single relationships between tree canopy height and above-ground biomass inevitably yields large, spatially correlated errors. This presents a significant challenge to both the forest conservation and remote sensing communities, because neither wood density nor species assemblages can be reliably mapped from space. Aim The accurate mapping of forest carbon stocks is essential for understanding the global carbon cycle, for assessing emissions from deforestation, and for rational land-use planning. At current status of negotiation five forest-related activities have been listed to be implemented as mitigation actions by developing countries, namely: reducing emissions from deforestation (which implies a land-use change) and reducing emissions from forest degradation, conservation of forest carbon stocks, sustainable management of forest, Enhancement of forest carbon stocks (all relating to carbon stock changes and GHG emissions within managed forest land use). The UNFCCC negotiations and related country submissions on REDD+ have advocated that methodologies and tools become available for estimating emissions and removals from deforestation and forest land management with an acceptable level of certainty. Amazonia contains half of all remaining tropical moist forest (#CITATION_TAG). nan
Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Background:                  Multiple sclerosis (MS) is the most common cause of neurological disability in young adults worldwide and approximately half of those affected are in Europe. Quality was generally higher in the more recent studies, which also tended to use current diagnostic criteria. Prevalence and incidence estimates tended to be higher in the more recent studies and were higher in the Nordic countries and in northern regions of the British Isles. Few studies examined ethnicity. Epidemiological data at the national level was uncommon and there were marked geographical disparities in available data, with large areas of Europe unrepresented and other regions well-represented in the literature. Both incidence and prevalence were calculated per 100,000 people using the BC mid-year population and were age and sex standardized to the 2001 Canadian population, for consistency to prior Canadian work [10, 11, #CITATION_TAG]. The assessment of differential incidence and prevalence across populations can reveal spatial, temporal and demographic patterns which are important for identifying genetic and environmental factors contributing to MS. However, study methodologies vary and the quality of the methods can influence the estimates. Methods:                  A comprehensive literature search was performed to obtain all original population-based studies of MS incidence and prevalence in European populations conducted and published between January 1985 and January 2011. Only peer-reviewed full-text articles published in English or French were included. All abstracts were screened for eligibility and two trained reviewers abstracted the data and graded the quality of each study using a tool specifically designed for this study.
The 2007-9 period saw an unprecedented crisis emerge in global financial markets with the collapse of several large western financial institutions, and the nearest moment of systemic crisis yet witnessed in the globalised financial system. The crisis has thus provoked a significant questioning of market theories, and in particular understandings of market within orthodox neoclassical economics. Within the social sciences, a significant element of this response has built on a growing heterodox socioeconomic literature which is heavily critical of hegemonic conceptions of the market within economics. However, whilst a small body of work in economic geography has begun to engage with this literature, geographical thinking has not directly sought to conceptualise the nature and significance of market spatiality. Utilising a cultural economy approach, this paper therefore argues that economic geographical theories need to foreground the concept of market rather than treat markets as a 'component' of wider processes. Drawing on the growing heterodox socioeconomic literature on markets, it thus proposes a practice-oriented 'socio-spatial approach' for framing conceptions of market spatiality, arguing that such a spatial epistemology opens up a range of theoretical possibilities for further contesting hegemonic neoclassical theories of the market beyond current socioeconomic critiques. It seeks to illustrate the utility of such a framework through a case study analysis of the limitations inherent in existing policy practices surrounding the early phase of the recent global financial crisis. Leadership turnover is managed by a selectorate - a group of individuals on whom the leader depends to hold onto power. This requires that the selectorate's hold on power is not too dependent on a specific leader being in office. In this view, the epistemological starting point is a recognition that markets 'do not simply fall out of thin air' (#CITATION_TAG Boeckler, 2007, 2009) but rather are phenomenon that are 'continually produced and constructed socially with the help of actors who are interlinked in dense and extensive webs of social relations' (Berndt and Boeckler, 2007: 536). The paper develops a simple theoretical model of accountability in the absence of regularized elections. Good policy is institutionalized when the selectorate removes poorly performing leaders from office. We use these case studies to identify the selectorate in specific instances of successful autocracy.
When sensory input allows for multiple, competing perceptual interpretations, observers' perception can fluctuate over time, which is called bistable perception. Imaging studies in humans have revealed transient responses in a right-lateralized network in the frontal-parietal cortex (rFPC) around the time of perceptual transitions between interpretations, potentially reflecting the neural initiation of transitions. Carefully controlling for the character of perceptual transitions has been shown to dramatically reduce the number of brain regions that are viable candidates for determining alternations (#CITATION_TAG). When replay, instead, depicts transitions with the actual durations reported during rivalry, yoked transitions and genuine rivalry transitions elicit equal activity.
Biological theories of sexual orientation, typically presented in human sexuality classes, are considered by many social psychologists to cause reductions in students' sexual prejudice. Yet when biological theories were not presented to 36 psychology students in a 10-week seminar on lesbian, gay, bisexual and transgender (LGBT) psychology, both sexual prejudice and two forms of essentialist thinking reduced significantly. Enrolled students reported increased exposure to issues of homosexuality since entering college, and many had sexual minority friends. #CITATION_TAG assessed students' prejudice and their interest in 26 topics at the beginning and end of a course titled 'The Psychology of Homosexuality'. We investigated who enrolled in a class about sexual diversity and what they most wanted to learn. Students left the class with significantly decreased homophobia.
Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. In the Andean region of South America, understanding communities' water perceptions is particularly important for water management as many rural communities must decide by themselves if and how they will protect their micro-watersheds and distribute their water. On the one hand, observed changes in land cover match perceptions of deforestation as the primary cause of increasing water scarcity. In contrast, binding of PDL1 to PD1 causes inhibition of TCR-mediated phosphatidylinositol-3-kinase (PI3Kinase) activation leading to inhibition of T cell proliferation and cytokine release [#CITATION_TAG]. Furthermore, water scarcity was perceived in regions where seasonal rainfall variability is higher but not in regions where annual rainfall is lower.
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Unreported cases were generally dealt with less carefully than reported cases: a written request for euthanasia was more often absent (87.7% v 17.6% verbal request only; P<0.001), other physicians and caregivers specialised in palliative care were consulted less often (54.6% v 97.5%; 33.0% v 63.9%; P<0.001 for both), the life ending act was more often performed with opioids or sedatives (92.1% v 4.4%; P<0.001), and the drugs were more often administered by a nurse (41.3% v 0.0%; P<0.001).One out of two euthanasia cases is reported to the Federal Control and Evaluation Committee. Most non-reporting physicians do not perceive their act as euthanasia. Countries debating legalisation of euthanasia should simultaneously consider developing a policy facilitating the due care and reporting obligations of physicians. In Flanders in 2007, slightly more than half the estimated total number of euthanasia cases (as according to the technical definition of the death certificate studies) were reported (#CITATION_TAG). Setting Flanders, Belgium.A stratified at random sample was drawn of people who died between 1 June 2007 and 30 November 2007.
-Several studies have suggested that proton-pump inhibitors (PPIs), mostly omeprazole, interact with clopidogrel efficacy by inhibiting the formation of its active metabolite via CYP2C19 inhibition. Whether this occurs with all PPIs is a matter of debate. The concomitant use of proton pump inhibitors (PPIs) with clopidogrel is suspected to be associated with an adverse impact on clinical outcomes in patients with coronary artery disease. Whether this occurs with all PPIs or is even of significant amplitude with omeprazole remains a matter of debate [9, [24] [#CITATION_TAG] [26] [27] [28] [29]. Patients were categorized into 2 groups: those taking a PPI [PPI (+), n=751] and those not taking a PPI [PPI (-), n=1900] at discharge. In addition, propensity-matched analysis was performed in 685 pairs of patients. The PPI (+) group was older and had more comorbid conditions than the PPI (-) group.
Dinitrogen fixation by cyanobacteria is of particular importance for the nutrient economy of cold biomes, constituting the main pathway for new N supplies to tundra ecosystems. It is prevalent in cyanobacterial colonies on bryophytes and in obligate associations within cyanolichens. Recent studies, ap-plying interspecific variation in plant functional traits to upscale species effects on ecosystems, have all but neglected cryptogams and their association with cyanobacteria. Cyanolichens and bryophytes differed significantly in their cyanobacterial N fixation capacity, which was not driven by microhabitat characteristics, but rather by morphology and physiology. Cyanolichens were much more prominent fixers than bryophytes per unit dry weight, but not per unit area due to their low specific thallus weight. The Sphagnum angustifolium stem transports nitrogen fixed and exuded by Nostoc muscorum upwards. Theoretical stoichiometric ratio of ethylene to nitrogen (3:2) is rarely attained and must always be corrected by parallel uptake of labelled 15 N 2 for each organism and possibly location (#CITATION_TAG 1980;Millbank 1981) nan
Leading Edge Essay Distilling Pathophysiology from Complex Disease Genetics Aravinda Chakravarti,1,* Andrew G. Clark,2 and Vamsi K. Mootha3 1Johns Hopkins University School of Medicine, Baltimore, MD 21205, USA 2Cornell University, Ithaca, NY 14850, USA 3Massachusetts General Hospital, Boston, MA 02114, USA *Correspondence: aravinda@jhmi.edu http://dx.doi.org/10.1016/j.cell.2013.09.001 Technologies for genome-wide sequence interrogation have dramatically improved our ability to identify loci associated with complex human disease. However, a chasm remains between correlations and causality that stems, in part, from a limiting theoretical framework derived fromMendelian genetics and an incomplete understanding of disease physiology. It does not take much perspicacity to see that what really makes this difference is not the tall hat and the umbrella, but the wealth and nourishment of which they are evidence, and that a gold watch or membership of a club in Pall Mall might be proved in the same way to have the like sovereign virtues.. George Bernard Shaw, The Doctor's Dilemma (Preface), 1909 Distinguishing correlation from causality is the essence of experimental science. Nowhere is the need for this distinction greater today than in complex disease genetics, where proof that specific genes have causal effects on human disease phenotypes remains an enormous burden and challenge. This is particularly so in this age of routine -omic surveys, which can produce more false-positive than true-positive findings (Kohane et al., 2006). Moreover, genomic mapping and sequencing approaches that are invaluable for producing a list of unbiased candidates are, by themselves, insufficient for implicating specific gene(s) in a disease or biological process. We admit at the outset that the answers are not straightforward, and that there are serious technical and intellectual impediments to demonstrating causality for the common complex disorders of man where multiple interacting genes are involved. Nevertheless, the casual conflation of ''mapped locus'' to ''proven gene'' is a constant source of confusion and obfuscation in biology and medicine that requires remedy. Consider that two types of genomic surveys, one horizontal and the other vertical, are now routine for attempting to understand human biology and disease. In contrast, in vertical or deep surveys, we examine the effects of the genome as the DNA information gets processed, and its encoded functions get executed through its transcriptome, proteome, and effectors such as the metabolome. In turn, this implies that proving a gene's specific role in a biological process, either in wild-type or mutant form, may not be straightforward because its role may only be evident when examined in relation to its eptember 26, 2013 a2013 Elsevier Inc. 21 Box 1. biochemical partners, and in particular contexts of diet, pathogen exposure, etc. This is a particular problem in genetic studies of any outbred nonexperimental organism, such as the human, and studies of human disease, where investigations are observational not experimental. It is the strong belief of contemporary human geneticists that uncovering the genetic underpinnings of any disease, however complex, is the surest unbiased route to understanding its pathophysiology and, thus, enabling its future rational therapies (Brooke et al., 2008). Consequently, for this view to prevail, we should require experimental evidence, be it in cells, tissues, experimental models, or the rare patient, for the role of a specific gene in a disease process. We know that even in a simple model organism, budding yeast, synthetic lethality-- where death or some other phenotype occurs only through the conspiracy of mutations at two different genes--is widely prevalent (Costanzo et al., 2010). Interactions of greater complexity and involving more than two genes are also known in yeast (Hartman et al., 2001) and must be true for humans as well. A human genome will typically harbor 20 genes that are fully inactivated, without 22 Cell 155, September 26, 2013 a2013 Else any overt disease phenotype, presumably due to the buffering by other genes (MacArthur et al., 2012). Acknowledging this complexity, there are two general ways forward. The question then is how ''complex'' are complex traits and diseases? The New Genetics: Understanding the Function of Variation With the rediscovery of Mendel's rules of transmission more than 100 years ago, there was a vicious debate on the relative importance of single-gene versus multifactorial inheritance (Provine, 1971). Geneticists quickly, and successfully, focused on deciphering the specific mechanisms of gene inheritance and understanding the physiology of the gene in lieu of answering why some phenotypes had complex etiology and transmission. Nevertheless, the rare examples of deciphering the genetic basis of complex phenotypes, such as for truncate (wing) in Drosophila (Altenburg and Muller, 1920), clearly emphasized that traits were more than the additive properties of multiple genes. Today, it is quite clear that Mendelian inheritance of traits, including diseases, is the exception not the rule. Nevertheless, the entire language of genetics is in terms of individual genes for individual phenotypes, with one function, rather than the ensemble and emergent properties of genomes. This absence of a specific genetics language for the proper description of the multigenic architecture of traits (the ensemble) remains as an impediment to our understanding of the nature and degree of genetic complexity of the phenotype. The case of amyotrophic lateral sclerosis (ALS), a devastating, progressive motor neuron disease, illustrates this point (Ludolph et al., 2012). Despite the lack of evidence, we largely describe ALS as being ''heterogeneous'' and comprised of single-gene mutations that can individually lead to disease. In 1993, mutations in superoxide dismutase 1 (SOD1) were identified in an autosomal-dominant form of the disease; subsequently, the disorder has become synonymous with aberrant clearance of free radicals as its central pathology. What is often not appreciated, however, is that fewer than 10% of all cases of ALS are familial and even fewer follow an apparent Mendelian pattern. Even within this subset of cases, more than 20 distinct genes, spanning other pathways including RNA homeostasis, have been identified, and SOD1 represents a minority of cases. The molecular etiology for the majority of the sporadic forms of the disease remains unclear, and the scientific problem in understanding ALS is more than simply identification of additional genes. Are these the key rate-limiting steps to ALS or simply one of several required in concert? Is the aberrant clearance of free radicals the fundamental defect or one of many such pathologies or a common downstream consequence? Given the diversity and number of deleterious, even loss-of-function, genetic variants in all of our genomes (Abecasis et al., 2012; MacArthur et al., 2012) and, in the absence of stronger evidence bearing on these questions, it is fair to assume that ALS patients harbor multiple mutations with a plurality of molecular defects and that free radical metabolism is only one of a set of canonical pathophysiologies that define the disease. No doubt, this plurality is the case for cancer (Vogelstein et al., 2013), Crohn's disease (Jostins et al., 2012), and even rare developmental disorders such as Hirschsprung disease (McCallion et al., 2003). Molecular biology, genetics' twin, on the other hand, appears to have been far more successful in deciphering and describing not only its individual components (e.g., DNA, RNA, protein) but also their mutual relationships (e.g., DNAprotein interaction) and ensembles (e.g., transcriptional complex), although this is also far from complete (Watson et al., 2007). The consequences of the primary and interaction effects are often well understood, even though not completely described, at both the molecular and cellular levels (Alberts et al., 2007). Although the use of genetic tools and genetic perspectives are fundamental to this progress, these advances have not as yet led to a major revision of our understanding of trait or disease variation. The major reason for this discrepancy is that, with few exceptions (Raj et al., 2010), molecular and cell biology has focused on the impact of deleting or overexpressing genes and not grappled with the consequences of allelic variation. Classical Mendelian genetics has been a boon to uncovering biology from yeast to humans whenever a mutation with a simple inheritance pattern can be isolated. This approach has been revolutionary in the unicellular yeast, particularly because genetics (and gene manipulation), biochemistry, and cell biology were melded to understand function at a variety of levels. This kind of multilevel approach has been less straightforward, but still largely successful, for a metazoan such asDrosophilawheremore genes andmultiple specialized cells often rescue the effects of a mutation or enhance its minor effect. Success in this endeavor will require a synthesis of many biological disciplines that includes the role of genetic variation as intrinsic to the biological process, not an aspect to be ignored. Consequently, melding variation-based genetic and molecular biological thinking is of critical importance for both fields and is central to our understanding of mechanisms of trait variation, including interindividual variation in disease risk. If most disease, in most humans, is the consequence of the effects of variation at many genes, then knowledge of their functional relationships, rather than merely their identities, is central to understanding the phenotype. This is clearly a problem of ''Systems Biology'' but one that incorporates genetic variation directly. The ability to integrate the realities of such widespread genetic variation, which are ultimately at the causal root of disease mechanisms, with systems biology approaches to understand functional contingencies is central to the challenge of deciphering complex human disease. Genetic Dissection of Complex Phenotypes Genetic transmission rules imply that, even in an intractable species such as us, one can map genomic segments that must contain a disease or trait gene. Such mapping requires identification of the segregation of common sites of variation across the genome, now easy to identify through sequencing, and recognition of a genomic segment identical-by-descent in affected individuals, both within and between families. This task has become easier and more powerful as sequencing technology has improved to provide a nearly complete catalog of variants above 1% frequency in the population; further improvements to sample rarer variants are ongoing (Abecasis et al., 2012). Consequently, genetic mapping, once the province of rare Mendelian disorders, Cell 155, S is now applicable to any human trait or disease. For most complex traits examined, many such loci have been mapped, but the vast majority of the specific genes remain unidentified. We can sometimes guess at a candidate gene within the locus (Jostins et al., 2012), sometimes implicate a gene by virtue of an abundance of rare variants among affected individuals (Jostins et al., 2012), in rare circumstances, use therapeutic modulation of a pathway to pinpoint the gene (Moon et al., 2004), and sometimes identify one by painstaking experimental dissection (Musunuru et al., 2010), but, generally, identification of the underlying gene has not become easier. In fact, most of the mapped loci underlying complex traits remain unresolved at the gene or mechanistic level. Despite the beginning clues to human disease pathophysiology that complex disease mapping is providing, and the slow identification of individual genes, it appears highly unlikely that we can understand traits and diseases this way. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (Yang et al., 2011). For Mendelian disorders, gene identification within a locus is made possible by each mutation being necessary and sufficient for the phenotype, being functionally deleterious and rare, and having an inheritance pattern consistent with the phenotype. It's the mutation that eventually reveals the biology and explains the phenotype. Any component locus for a complex disease has no such restriction, as the causal variants are neither necessary nor sufficient, nor coding (in fact, they are frequently noncoding and regulatory) nor rare (Emison et al., 2010; Jostins et al., 2012). Currently, the major attempts to overcome this impediment involve reliance on single severe mutations at the very same component genes and eptember 26, 2013 a2013 Elsevier Inc. 23 Genetic association studies in humans can synergize with prior knowledge and systems-level quantitative analysis to generate predictions of what pathways and modules are disrupted, where (anatomically), and when (developmentally) to yield a specificmorphological or biochemical phenotype. Consequently, these strategies themselves depend on the hidden biology we seek and are applicable only to the most common human diseases. It appears to us that ignorance of biology has become rate limiting for understanding disease pathophysiology, except perhaps for the Mendelian disorders. There are two ways to get out of this vicious cycle (Figure 1). Although we suspect that the numbers of pathways involved are fewer than the numbers of genes involved, this is merely suspicion. Although the genome is linear, its expression and biology are highly nonlinear and hierarchical, being sequestered in specific cells and organelles (Ilsley et al., 2013). Understanding this hierarchy, the province of systems biology, is critical to the solution of the vier Inc. complex inheritance problem (Yosef et al., 2013). One might counter that existing gene ontologies do precisely that, but, even in yeast, this appears to be highly incomplete (Dutkowski et al., 2013). Proving Causality: Molecular Koch's Postulates The evidence that a specific gene is involved in a particular human disease has historically been nonstatistical and based on our experience with identifying mutations in Mendelian diseases. Unfortunately, as already mentioned, all of these rules break down in complex phenotypes where neither cosegregation nor exclusivity to affecteds nor obviously deleterious alleles are likely; moreover, many mutations are suspected to be noncoding and in a diversity of regulatory RNA molecules. Consequently, statistical evidence of enrichment has been the mainstay, but this has two negative consequences: first, scanning across the genome or multiple loci covering tens to hundreds of megabases requires very large sample sizes and very strict levels of significance to guard against themany expected falsepositive findings; second, genetic effects that are small or genes with only a few causal alleles are notoriously difficult to detect, although they may be very important to understanding pathogenesis. This difficulty translates into a low power of detection, as common disease alleles cannot be distinguished from bystander associated alleles, whereas rare alleles are observed too infrequently to provide statistical significance. Consequently, although many genes are ''named'' as being responsible in a complex disease or disease process, proof of their involvement is either absent or circumstantial and not direct. In the late 19th century when bacteria were first shown to cause human disease, they were indiscriminately implicated in all manner of disease with little proof (Brown and Goldstein, 1992). One particularly embarrassing example was alcaptonuria, which Sir Archibald Garrod subsequently showed was inherited and which was his first ''inborn error of metabolism.'' We are likely to repeat this ''witch-hunt'' unless we are careful to note that mapping a locus is not equivalent to identifying the gene, and that identifying a gene and its mutations at a locus depends on numerous untested assumptions (mutational type, mutational frequency in cases and controls, coding or regulatory, cell autonomy). Inmicrobiology, Robert Koch set out three postulates that had to be satisfied to connect a specific bacterium (among the multitudes encountered, not unlike current genome analysis) to a disease: the agent had to be isolated from an affected subject, the agent had to produce disease when transmitted to an animal, and the agent had to be recoverable from an animal's lesion (Falkow, 1988). Simply because we cannot follow Koch to the letter in human patients does not absolve us from the responsibility of demonstrating a rigorous level of proof. This is particularly true if we are to pursue therapeutic targets for these diseases. It is clear that the majority of complex diseases do not harbor this level of proof today; neither do most monogenic disorders. Animal models are attractive because of the ability to do experimental manipulations that test predictions of gene function, but these experiments test the function of a gene in a context that is decidedly different from that with a human patient. However imperfect animal models are, progress in the direction of understanding causality has been very beneficial when gene disruptions alone, perhaps at more than one gene, have taught us fundamental lessons in pathophysiology (Farago et al., 2012). In many cases, investigators have also demonstrated that disease results only when combined with a potent environmental insult. When known, such as the effect of dietary cholesterol vis-a-vis genes involved in cholesterol metabolism in atherosclerosis, such environmental exposures to gene-deficient mouse models have provided a tight circle of proof (Plump et al., 1992). A recent example of gestational hypoxia modulating the effect of Notch signaling and leading to scoliosis in mice and in human families Cell 155, S shows how environmental factors beyond diet can be examined even for congenital disorders (Sparrow et al., 2012). Despite these successes, pursuit of Koch's postulates faces other challenges. For example, mutations in the same gene might not reveal an identical phenotype in humans and in an animal model even if molecular pathways are conserved. This is a particular problem for behavioral phenotypes where brain circuitry may have evolved quite differently in humans and other mammals, challenging our ability to model behavior accurately. Nevertheless, such an analysis might reveal an underlying neural phenotype or a molecular or cellular correlate that is in common and subject to testing of the postulates. Ultimately, a lack of understanding of fundamental physiology is the biggest impediment to our understanding of genetically complex human disease. A unique aspect of genetics research seldom appreciated is that genetic effects are chronic biological exposures and as such can pinpoint the earliest stages of disease not readily studied otherwise. In reality, we still do not fully understand the pathogenesis stemming from some of the earliest identified human disease genes. With better understanding of disease mechanism, it seems likely that many disorders that we think of as ''genetic'' may have ameliorative diet, exercise, or other benign environmental ''treatments.'' Given the potential scientific and medical payoffs of disease gene discovery (Chakravarti, 2001), we argue in this Essay of the need for a rigorous examination of the assumptions under which we connect genes to phenotypes. Below we discuss the nature of the ''proof'' that we desire in order to make fundamental discoveries in human pathophysiology. Success in this difficult task requires us to solve a logical conundrum: how can we understand the genes underlying a phenotype if some of these component factors, in isolation, do not have recognizable phenotypes on their own? Both of these goals are approachable, particularly with recent advances in genome-editing technologies that allow the creation of multiple mutations within a single experimental organism (Wang et al., 2013). The second approach is to focus research on why the disease is complex in the first place. This last aspect is critical: as we argue below, with our current state of knowledge, we are likely to have our greatest success with understanding how genes map onto pathways, and how pathways map onto disease, before a true quantitative understanding of disease biology emerges. The chief criteria have been to demonstrate cosegregation with the phenotype in families, exclusivity of the mutation to affected individuals (rare alleles absent in controls), and the nature of themutation (a plausibly deleterious allele at a conserved site within a protein). We need to move beyond lists of plausible genes, to provide rigorous proof for their role in disease. But this goal is unlikely to be achieved in the absence of a superior understanding of the biology of hierarchical function within genomes, how variation alters these functions, and how these altered functions lead to human disease. There is indeed evidence for scenarios in which variation in complex traits, including risk of complex disease, is mediated by a myriad of variants of minute effect, spread evenly across the genome (#CITATION_TAG). We estimate and partition genetic variation for height, body mass index (BMI), von Willebrand factor and QT interval (QTi) using 586,898 SNPs genotyped on 11,586 unrelated individuals. We show that the variance explained by each chromosome is proportional to its length, and that SNPs in or near genes explain more variation than SNPs between genes.
Lung macrophages are an important defence against respiratory viral infection and recent work has demonstrated that influenza-induced macrophage PDL1 expression in the murine lung leads to rapid modulation of CD8+ T cell responses via the PD1 receptor. Viral infection significantly increased cell surface expression of PDL1 on explant macrophages, lung macrophages and MDM but not explant epithelial cells. The aim of this study was to investigate the mechanisms of PDL1 regulation by human macrophages in response to viral infection. Unreported cases were generally dealt with less carefully than reported cases: a written request for euthanasia was more often absent (87.7% v 17.6% verbal request only; P<0.001), other physicians and caregivers specialised in palliative care were consulted less often (54.6% v 97.5%; 33.0% v 63.9%; P<0.001 for both), the life ending act was more often performed with opioids or sedatives (92.1% v 4.4%; P<0.001), and the drugs were more often administered by a nurse (41.3% v 0.0%; P<0.001). Most non-reporting physicians do not perceive their act as euthanasia. Countries debating legalisation of euthanasia should simultaneously consider developing a policy facilitating the due care and reporting obligations of physicians. This work demonstrated an additional role for MHC class II expressing cells and T helper cell responses [#CITATION_TAG] highlighting the potential importance of macrophage-T cell interactions in the control of influenza infection. Participants A stratified at random sample was drawn of people who died between 1 June 2007 and 30 November 2007. The certifying physician of each death was sent a questionnaire on end of life decision making in the death concerned.
Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. Major efforts to sequence cancer genomes are now occurring throughout the world. Though the emerging data from these studies are illuminating, their reconciliation with epidemiologic and clinical observations poses a major challenge. We model tumors as a discrete time branching process that starts with a single driver mutation and proceeds as each new driver mutation leads to a slightly increased rate of clonal expansion. Two of the models used, called here "Bozic" (as it is based on [#CITATION_TAG]) and "exp" have no density dependence and lead to exponential growth. Using the model, we observe tremendous variation in the rate of tumor development - providing an understanding of the heterogeneity in tumor sizes and development times that have been observed by epidemiologists and clinicians. Furthermore, the model provides a simple formula for the number of driver mutations as a function of the total number of mutations in the tumor.
The Derriford Appearance Scale24 (DAS24) is a widely used measure of distress and dysfunction in relation to self-consciousness of appearance. It has been used in clinical and research settings, and translated into numerous European and Asian languages. Hitherto, no study has conducted an analysis to determine the underlying factor structure of the scale. In order to be able to have a relevant, specific and well defined outcome variable to further assess these theoretical explorations, and also to make a meaningful assessment of interventions, a team of plastic surgeons and psychologists created the Derriford Appearance Scale 59 (#CITATION_TAG). DESIGN Cross-sectional survey designs using clinical (out-patient and in-patient) and general population samples. METHOD Twenty-five items were selected initially from the 59 items of the original DAS59. These were refined to 24 through item analyses and the scale was standardized on 535 patients with a range of problems of appearance and on a representative general population sample (N=1, 107). It is psychometrically robust and discriminates well between patient groups, between clinical and non-clinical populations, and within the general population between those concerned, and those not concerned, about their appearance.
In cognitive archeology, theories of cognition are used to guide interpretation of archeological evidence. But the implications that archeology has for cognitive science particularly relate to traditional proposals from the field involving modular decomposition, symbolic thought and the mediating role of language. There is a need to make a connection with more recent approaches, which more strongly emphasize information, probabilistic reasoning and exploitation of embodiment. Proposals from cognitive archeology, in which evolution of cognition is seen to involve a transition to symbolic thought need to be realigned with theories from cognitive science that no longer give symbolic reasoning a central role. The present paper develops an informational approach, in which the transition is understood to involve cumulative development of information-rich generalizations. Most research in computer vision has been directed towards minimalistic approaches, in which problems are addressed on how properties of the environment can be computed from as little information as possible. Although such approaches may be scientifically well motivated they have only resulted in limited progress towards our understanding of seeing systems. Ballard, Bajcsy and others have pointed out the importance of vision being an active process which is tightly connected to behaviors. Continuous operation over time and early use of three dimensional cues are important in this context. Conceptions of cognition in which symbolic reasoning takes charge are increasingly questioned (Thelen and Smith, 1993; #CITATION_TAG). We illustrate our proposed approach by some experiments on a real-time active system.
To understand price changes one must determine the relative impact of supply and demand shifts on price. The competition between farmed salmon and wild caught Pacific salmon has received some attention previously. However, this was before frozen Atlantic salmon emerged as an important product form in the market. L'article traite de l'importance de diverses sortes d'un meme type de produit et de leur provenance dans les etudes de structure de marche au sein du marche europeen du saumon. La concurrence entre le saumon d'elevage et le saumon sauvage du Pacifique a deja fait l'objet d'etudes a ce sujet, mais c'etait avant l'emergence du saumon de l'Atlantique surgele sur le marche. C'est une question importante parce que ce dernier produit a plus de chance de concurrence directement le saumon du Pacifique-le plus souvent vendu surgele - que le saumon de l'Atlantique frais. Nous avons utilise un systeme de demande quasi ideale pour estimer l'etat respectif de la demande de saumon de l'Atlantique frais et surgele et du saumon du Pacifique surgele au sein de l'Union europeenne. Previous measures of supply and demand shifts have specified these shifts relative to the expected quantity  or   (#CITATION_TAG; Marsh 2003; Kinnucan and Myrland 2006). nan
Humans and the institutions they devise for their governance are often successful at self-organizing to promote their survival in the face of virtually any environment challenge. However, from history we learn that there may often be unanticipated costs to many of these solutions with long-term implications on future societies. For example, increased specialization has led to increased surplus of food and made continuing In this chapter, we explore the historical dimension of urbanization and why the ecology of urbanization has, until recently, been missing. The Fortune at the Bottom of the Pyramid: Eradicating Poverty through Profits Pearson Education Inc., Wharton School Publishing, Upper Saddle River, New Jersey, 2004; Pages: 432; Price: US $ 29.99; ISBN: 0-13-146750-6. The treasure lying at the bottom of the world's economic pyramid, attracts companies with unimaginable prospects and profits. The economic pyramid of the world contains more than 400 crores of population at the bottom whose earnings and spendings per capita are low, but as an aggregate, are much larger than they are perceived to be. Hence, it is high time for the corporate managers to make an effort to locate the Alibaba's treasure lying hidden at the Bottom of the Pyramid (BOP). Broadly, the characteristics and spending patterns of the BOP segment are different as compared to the other segments of the society. In the high and medium range markets, consumers are able to buy bigger packages of consumables such as a 10 kg packet of detergents so that they need not shop frequently. But low-end BOP consumers do not have so much disposable cash to buy in bulk and store. Already in India, 30% of personal care and other consumables such as shampoo, tea, and cold medicines are sold in single serve packages and most single serve packages are priced at one rupee. Hindustan Lever Limited offers Clinic Plus shampoo priced at 50 Paisa. The major differences between the Bottom of the Pyramid and the Top of the Pyramid markets are differences in buying habits and cultural differences. The Bottom of the Pyramid is characterized by a low brand consciousness due to low economic position, and is mostly need driven. Major emerging BOP markets in the world, including China, India, Mexico, Russia, Brazil, South Africa, Thailand, Turkey and Indonesia, with a combined population of about 300 crores, represent 70% of the world BOP population. Such a huge jumbo size BOP market promises intense marketing opportunities for the companies. Even with NGOs working tirelessly and government support and international aid being circulated heavily, the problem of poverty still persists. According to the author, the main reason for this situation is that the private sector, which is huge and potentially capable of solving any problem in the world, is practically not involved. The World's Economic Pyramid contains wealthy people at the top with high spending capacities and habits. But the size of this segment is very small i.e., only about 7.5 to 10 crores of 600 crores plus population, whereas more than 400 crores of population live at the bottom of the pyramid earning less than $2 per day and in India alone around 40 crores of people earn less than Rs. THE BOOK PRESENTS CASES OF COMPANIES THAT ARE INVOLVED IN BOP MARKETING * The Solar Electric Light Fund (SELF): It established the Solar Electric Light Company (SELCO) in 1995 to market, install, and serve the Solar Home Systems (SHS) in south India. Strength, agility, and intelligence certainly were important, but which family, clan, and class one was born into set the limits on one's future potential in the age of early cities; to some extent, these constraints continue to operate today (Adams 1966; #CITATION_TAG; Scott 1998). Hence, it is suggested that this segment be provided with "single serve" packages. Part 2 describes 12 cases, in a variety of businesses where BOP is becoming an active market and is bringing benefits to its customers far beyond offering just products. Part 3 contains stories in video form-an attempt to present the prospects and prosperity underlying the BOP markets, and to bring the BOP markets into limelight.
Neuropsychiatric symptoms are very common in tuberous sclerosis complex (TSC). Autism is present in up to 60% of these patients, and TSC accounts for 1-4% of all cases of autism. Genetic disorders that present with a high incidence of autism spectrum disorders (ASD) offer tremendous potential both for elucidating the underlying neurobiology of ASD and identifying therapeutic drugs and/or drug targets. Tuberous sclerosis complex (TSC) is one such genetic disorder that presents with ASD, epilepsy, and intellectual disability. Cell culture and mouse model experiments have identified the mTOR pathway as a therapeutic target in this disease. Typical TSC lesions include hypomelanic macules and facial angiofibromas, as well as brain cortical tubers, subependymal nodules, and subependymal giant cell astrocytomas (SEGAs) (Holmes et al, 2007; Curatolo et al, 2008; #CITATION_TAG). nan
A great deal of the research and theorizing on consciousness and the brain, including my own on hallucinations for example (Collerton and Perry, 2011) has focused upon specific changes in conscious content which can be related to temporal changes in restricted brain systems. In this paper, I will review why psychotherapy is relevant to the question of how consciousness relates to brain plasticity. Little is known about how psychological treatments work. However, certain conceptual and practical difficulties arise when studying psychological treatments, most especially deciding how best to conceptualise the treatment concerned and how to accommodate the fact that most psychological treatments are implemented flexibly. However, evidence is lacking as to what specifically changes as a consequence of psychotherapy (see, for example, #CITATION_TAG). nan
While it finds no empirical basis for this orthodox standpoint it observes that long-term unemployment dampens aggregate production which in turn aggravates unemployment problem. This paper analysed the OECD data on employment protection for OECD countries over the time span 1990-2008 on the basis of alternative dynamic panel data models and panel causality tests and examines the validity of the neo-liberal argument that strictness of employment protection hurts labour through increased long-term and youth unemployment rates. In recent years, comparative economics experienced a revival, with a new focus on comparing capitalist economies. In the late 1990s La Porta and his collaborators (La Porta et al., 1997, 1998, 1999, 2000 2006, 2008 #CITATION_TAG; Shleifer, 2002, 2003; Beck et al., 2003a Beck et al.,, 2003b Botero et al., 2004) set in motion a series of systematic analysis of the relationships between legal and economic variables. The authors argue that, to understand capitalist institutions, one needs to understand the basic tradeoff between the costs of disorder and those of dictatorship. They then apply this logic to study the structure of efficient institutions, the consequences of colonial transplantation, and the politics of institutional choice.Labor Policies,Decentralization,National Governance,Environmental Economics&Policies,Economic Theory&Research,National Governance,Environmental Economics&Policies,Economic Theory&Research,Governance Indicators,Banks&Banking Reform
The DEM profile of flaring plasmas generally exhibits a double peak distribution in temperature, with a cold component around log T [?] However, the M7.7 flare on 19 July 2012 poses a very intriguing violation of this paradigm: the temperature decreases with altitude from the tip of the cusp toward the top of the arcade; the hottest region is slightly above the X-ray loop-top source that is For the past twenty years, drawing on the Industrial Network Approach, Industrial Marketing and Purchasing Group researchers have been trying to get a better understanding of organisational networks related issues. Researchers frequently highlight that whatever the researched phenomena, it is important to consider actors' subjective views of the world. The concept of Network Pictures as introduced in the IMP (Industrial Marketing and Purchasing) body of literature by Ford et al. (2002b), refers to those subjective views and despite its recognised importance no in-depth research had been conducted so far on the concept which has thus remained blurred. Ford et al. Here the events of interest are roughly categorized as standard or non-standard flares, depending on whether they were similar to the prototypical Tsuneta flare (#CITATION_TAG). The concept's theoretical foundations are uncovered by reviewing some principles from Sense-Making Theory. The method consisted of operationalising the construct of Network Pictures and then testing it in two different network contexts to see if it was usable and useful for carrying out research in organisational networks.
Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. #CITATION_TAG[4][5] More importantly, however, recent studies have shown substantial patient-reported disability and pain in patients up to 4 years after surgery Knee extensor and flexor strength was evaluated at four different velocities (60, 120, 180, and 240 degrees/sec) preoperatively and every 2 weeks from weeks 2-12 postsurgery. Eight subjects were evaluated on a Cybex II+ and 14 subjects were evaluated on a Cybex II isokinetic device. A repeated measures analysis of variance was used to determine possible side (involved and uninvolved), speed (60, 120, 180, and 240 degrees/sec), or time (preoperative, 2, 4, 6, 8, 10, and 12 weeks postoperatively) effects as well as possible interactions between these factors.
The North American Carbon Program (NACP) was formed to further the scientific understanding of sources, sinks, and stocks of carbon in Earth's environment. A CoP describes the communities formed when people consistently engage in shared communication and activities towards a common passion or learning goal. This investigation uses the conceptual framework of communities of practice (CoP) to explore the role that the NACP has played in connecting researchers into a carbon cycle knowledge network, and in enabling them to conduct physical science that includes ideas from social science. Food price is one way through which climate change may affect health. The complex and dynamic nature of pricing mechanisms makes it difficult to predict precisely how prices will be impacted. Should prices rise disproportionately among healthy foodstuffs compared to less healthy foods there may be adverse health outcomes if less expensive and less healthy foods are substituted. These investigations may also include the specific role humans play in the carbon cycle, such as the impact of human-generated emissions or the consequences of climate change to agriculture and food systems (Berthelot et al., 2002; #CITATION_TAG; Dempewolf et al., 2014; Shindell et al., 2012). We outline areas where there are particular vulnerabilities for food systems and food prices arising from climate change, particularly global commodity prices; agricultural productivity; short term supply shocks; and less direct factors such as input costs and government policies. We use Australia as a high-income country case study to consider these issues in more detail.
Background: Meniscus surgery is a high-volume surgery carried out on 1 million patients annually in the USA. A critical oversight of previous studies is their failure to account for the type of meniscal tears. Meniscus tears can be categorised as traumatic or nontraumatic. Traumatic tears (TT) are usually observed in younger, more active individuals in an otherwise 'healthy' meniscus and joint. Non-traumatic tears (NTT) (ie, degenerative tears) are typically observed in the middleaged (35-55 years) and older population but the aetiology is largely unclear. Knowledge about the potential difference of the effect of arthroscopic meniscus surgery on patient symptoms between patients with traumatic and NTT is sparse. Furthermore, little is known about the natural time course of patient perceived pain, function and quality of life after meniscus surgery and factors affecting these outcomes. The aim of this prospective cohort study is to investigate the natural time course of patient-reported outcomes in patients undergoing meniscus surgery, with particular emphasis on the role of type of symptom onset. Meniscectomy patients have an increased risk of developing knee OA. 2 7] [#CITATION_TAG] One explanation for the poor selfreported outcomes may be that the loss of meniscal function triggers other events that may cause knee pain. Reduced muscle strength is suggested as a risk factor for knee osteoarthritis (OA). The Knee Injury and Osteoarthritis Outcome Score (KOOS) was used to determine self-reported outcomes.No differences were detected in any muscle strength variables between the operated and nonoperated leg (mean +- SD quadriceps maximum voluntary contraction of 2.80 +- 0.10 for the operated leg and 2.88 +- 0.10 for the nonoperated leg), between patients and controls (mean +- SD torque of 2.70 +- 0.09 Nm x kg(-1) for the controls; P = 0.26 for main effect leg), or in objectively measured function (P >= 0.27).
Design and Implementation of Pay for Performance * A large, mature and robust economic literature on pay for performance now exists, which provides a useful framework for thinking about pay for performance systems. The poor knowledge of epilepsy among traditional healers is due to cultural prejudices and environment. The resultant deep-rooted misconceptions and myths negatively affect the attitudes and encourage traditional care with high morbidity and mortality. There were prevalent negative attitudes and perception about epilepsy among the healers, as 146 (88.0%) of them viewed it as contagious; 149 (89.8%) would decline either marrying or eating with epileptic persons. Although traditional healers are frequently involved in the care of epilepsy in our environment, they have little or no scientific knowledge about the condition. Adequate knowledge about epilepsy is essential for diagnosis and treatment. Only a small empirical literature exists on the use of subjective evaluation or discretion in incentive systems (e.g., #CITATION_TAG; Ittner, Larcker & Meyer 2003; Murphy & Oyer 2003; Gibbs et al. 2004 Gibbs et al., 2009, presumably because quantifying the concepts is difficult. One hundred and seventy three traditional healers from villages/communities in Uyo were assessed for knowledge; attitude and perception of epilepsy, using an interviewer assisted Attitude Questionnaire.
Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. ABSTRACT Public dialogue about science, technology and medicine is an established part of the activities of a range of charities, private corporations, governmental departments and scientific institutions. However, the extent to which these activities challenge or bridge the lay-expert divide is questionable. Expertise is contested, by the public and the community of scholars who study and/or facilitate public engagement. The colonization of lay positions by expert speakers and the hybrid positioning of lay-experts was characteristic of the consensus and conservatism that emerged. Yet, often an expert-lay divide is perpetuated which closes down opportunities for more reflexive debate [#CITATION_TAG]. We examine participants ' claims to expertise and consider how this relates to their claims to credibility and legitimacy and the way in which these events unfolded. Using a combination of ethnographic and discursive analysis, we found that participants supplemented technical expertise with other expert and lay perspectives. We can also link participants ' claims to expertise to their generally positive appraisal of genetic research and services.
The fluid parcels reside at the base of the tree. The tree structure partitions the fluid parcels into adjacent pairs (or more generally, p-tuples). Adjacent parcels intermix at rates governed by diffusion time scales based on molecular diffusivities and parcel sizes. Keywords Turbulence * Stochastic model * Mixing 1 Motivation Mixing closure in computational models of turbulent combustion is typically implemented by partially or fully intermixing pairs or groups of notional fluid parcels selected from a parcel population that discretely instantiates the joint probability distribution function (PDF) of the thermochemical variables that are time advanced by the model [10] . One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [26] . In modeling turbulent reactive flows based on the transport equation for the joint probability density function (jpdf) of velocity and composition, the change in fluid composition due to convection and reaction is treated exactly, while molecular mixing has to be modeled. In this model the change in particle composition is determined by particle interactions along the edges of a Euclidean minimum spanning tree (EMST) constructed in composition space. One such constraint that has proven effective is to intermix only parcel pairs that are close, by some criterion, in a metric space defined on the manifold of thermochemical states [#CITATION_TAG]. The model is applied to the diffusion flame test model problem proposed by Norris and Pope [Combust.
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. There is increasing recognition that the development of evidence-informed health policy is not only a technical problem of knowledge exchange or translation, but also a political challenge. This said, the "translation" of scientific evidence into health policy is a complex process that is subject to inertia and cultural impediments (#CITATION_TAG). Yet, while political scientists have long considered the nature of political systems, the role of institutional structures, and the political contestation of policy issues as central to understanding policy decisions, these issues remain largely unexplored by scholars of evidence-informed policy making.We conducted a systematic review of empirical studies that examined the influence of key features of political systems and institutional mechanisms on evidence use, and contextual factors that may contribute to the politicisation of health evidence. Eligible studies were identified through searches of seven health and social sciences databases, websites of relevant organisations, the British Library database, and manual searches of academic journals. Relevant political and institutional aspects affecting the use of health evidence included the level of state centralisation and democratisation, the influence of external donors and organisations, the organisation and function of bureaucracies, and the framing of evidence in relation to social norms and values.
This paper describes the scenario matrix architecture that underlies a framework for developing new scenarios for climate change research. Although the use of climate scenarios for impact assessment has grown steadily since the 1990s, uptake of such information for adaptation is lagging by nearly a decade in terms of scientific output. Nonetheless, integration of climate risk information in development planning is now a priority for donor agencies because of the need to prepare for climate change impacts across different sectors and countries. Up to this time the human signal, though detectable and growing, will be a relatively small component of climate variability and change. Such methods are commonly applied in IAVanalysis, although multiple criteria are normally applied in determining the final selection of representative climate scenarios to be used (IPCC-TGICA 2007; #CITATION_TAG). This implies the need for a twin-track approach: on the one hand, vulnerability assessments of social and economic strategies for coping with present climate extremes and variability, and, on the other hand, development of climate forecast tools and scenarios to evaluate sector-specific, incremental changes in risk over the next few decades. This review starts by describing the climate outlook for the next couple of decades and the implications for adaptation assessments. We then review ways in which climate risk information is already being used in adaptation assessments and evaluate the strengths and weaknesses of three groups of techniques. We assert that climate change scenarios can meet some, but not all, of the needs of adaptation planning. Even then, the choice of scenario technique must be matched to the intended application, taking into account local constraints of time, resources, human capacity and supporting infrastructure.
Health promotion is essential to improve the health status and quality of life of individuals. Promoting mental health at an individual, community and policy level is central to reducing the incidence of mental health problems, including self-harm and suicide. Men may be particularly vulnerable to mental health problems, in part because they are less likely to seek help from healthcare professionals. Although this article discusses mental health promotion and related strategies in general, the focus is on men's mental health. The NHS National Institute for Health and Clinical Excellence (NICE, 2009) publication - Depression in adults with a chronic physical health problem: treatment and management - is a clinical practice guideline for the UK which partially updates and extends the earlier depression management in primary and secondary care guideline (NICE, 2004). There are variations in the suicide rates for different parts of the world, with the highest rates in the Russian Federation, Baltic States, Sri Lanka and Japan, and the lowest rates in Latin America (#CITATION_TAG). Like other clinical guidelines, it has been systematically developed from the best available research evidence to assist clinicians and patients, and service commissioners and providers in making decisions about the most appropriate treatment and service organisation for this important area of health care need. The National Collaborating Centre for Mental Health (one of a number of centres established by NICE for the purpose of clinical guideline development) together with a guideline development group, comprising health and social care professionals, lay and patient representatives, and technical experts, worked on the guidance. The process from initial scope preparation to the production of the final guideline took over two years.
A few empirically supported principles can account for much of the thematic content of waking thought, including rumination, and dreams. The cues may be external or internal in the person's own mental activity. The responses may take the form of noticing the cues, storing them in memory, having thoughts or dream segments related to them, and/or taking action. Noticing may be conscious or not. Goals may be any desired endpoint of a behavioral sequence, including finding out more about something, i.e., exploring possible goals, such as job possibilities or personal relationships. The article briefly summarizes neurocognitive findings that relate to mind-wandering and evidence regarding adverse effects of mind-wandering on task performance as well as evidence suggesting adaptive functions in regard to creative problem-solving, planning, resisting delay discounting, and memory consolidation. The semantic processing of language and action has been linked to the N400 component of the event-related potential (ERP). EEG evidence with eventrelated potentials indicates that infants as young as nine months react with N400 deflections (negative deflections after 400 ms poststimulus) when sequences they observe end unexpectedly (#CITATION_TAG). The sequential nature of action ensures that an individual can anticipate the conclusion of an observed action via the use of semantic rules. The authors developed an ERP paradigm in which infants and adults observed simple sequences of actions. Adults and infants at 9 months and 7 months were assessed via the same neural mechanisms-the N400 component and analysis of the theta frequency.
NEUROENERGETICS Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain Tanya Zilberter* Infotonic Consultancy, Stockholm, Sweden *Correspondence: zilberter@gmail.com IntroductIon There is evidence that the brain favors consumption of carbohydrates (CHO) rather than fats, this preference resulting in glycolysis-based energy metabolism domination. This metabolic mode, typical for consumers of the "Western diet" (Cordain et al., 2005; Seneff et al., 2011), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). However, it is not CHO but fat that is often held responsible for metabolic pathologies. It is general knowledge that the glucose homeostasis possesses very limited buffering capacities, while energy homeostasis in its fat-controlling part enjoys practically unlimited energy stores. the SelfISh BraIn concept: two meanIngS There are two ways to look at the CHObiasing trait of the brain. (1) The "Selfish Brain" is a term coined by Robert L. DuPont in the title of his book where he wrote: "With respect to aggression, fear, feeding, and sexuality, the brain is selfish. The bad news is, in the long run the body can be harmed as the result. They wrote referring to DuPont's book: "The brain looks after itself first. Such selfishness is reminiscent of an earlier concept in which the brain's selfishness was addressed with respect to addiction. We chose our title by analogy but applied it in a different context, i.e., the competition for energy resources" (Peters et al., 2004). These two meaning of the Selfish Brain have important common points if we consider the addiction (highly non-homeostatic) as a result of the "push" principle borrowed from the economic "push-pull" paradigm of supply chains. As early as in 1998, Hill and Peters wrote: "According to the 'push' principle, the environment pushes excess amounts of energy into the organism" (Hill and Peters, 1998). According to DuPond, "What makes a drug addictive is not that it is 'psychoactive' but that it produces specific brain reward. It is not withdrawal that hooks the addict, it is reward" (DuPont, 2008). This reward is hard-wired in the brain, in the loci where both "pull" and "push" systems might be converging, something that is discussed within the Selfish Brain paradigm as the comforting effect of food (Peters et al., 2007), particularly, the CHO-rich foods (Hitze et al., 2010). puSh and pull partS of energy Supply control SyStem The role of depots, as determined by a general principle in economic supply chains, is energy buffering in unstable environments (Fischer et al., 2011). The surplus, naturally, goes into depots. Peters and Langemann, however, remained in doubt about this concept partly due to the fact that this "push" does not work invariably for all animal or human subjects (Martin et al., 2010; Cao et al., 2011). Indeed, the sizes of CHO and fat depots are incomparable. Among the most frequently reported consequences of HFD are features typical for metabolic syndrome - increased hunger/appetite, insulin resistance, elevated body fat deposition, and glucose intolerance along with decreased neuronal resistance to damaging conditions. The metabolic state caused by KD (Figure 1C) was called "unique" (Kennedy et al., 2007) and it closely resembles effects of calorie restriction (Domouzoglou and MaratosFlier, 2011). the KetogenIc ratIo and the "puSh" component of energy metaBolISm The environment in Western-type societies can be characterized as "pushing" the energy into our organisms via activation of reward and addiction circuits of our selfish brains. In the standard experimental "Western Diet" (5TJN) with KR close to 1:1, CHO proportion is high enough to continuously maintain glycolysis, overconsumption, and the subsequent chain of events resulting in metabolic disturbances detrimental for the brain (Langdon et al., 2011). The NHANES surveys of 1971-2006 (Austin et al., 2011) revealed that in the USA population, the trend toward increased CHO intake and decreased fat intake (KR shift from 0.716 to 0.620) resulted in the increase of obesity But why, then, it is the dietary fat that is blamed for overconsumption, obesity, and neuro-deteriorating effects? the role of macronutrIent compoSItIon Interestingly, the diet categorization (HFD, low-CHO, KD, etc.) A century ago, Woodyatt wrote: "antiketogenesis is an effect due to certain products which occur in the oxidation of glucose, an interaction between these products on the one hand and one or more of the acetone bodies on the other" (Woodyatt, 1910). Wilder and Winter (1922) defined the threshold of ketogenesis explaining it from the standpoint of condition where either ketone bodies or glucose can be oxidized. This is a very important point, not only methodologically, but also ideologically. On the other hand, ketogenesis introduces a fuel alternative to glucose, which can be crucial in metabolic pathologies. water-vitamin fast, with body fat as a sole energy source, has been reported (Stewart and Fleming, 1973). non-homeoStatIc effectS of cho verSuS fat From the teleological standpoint, the strong drive for CHO intake beyond homeostatic needs exists very likely due to limited CHOstoring capacities. For fat with its vast depots, there is less (or none at all) evidence for a drive of similar magnitude. Oral stimulation with both sweet and non-sweet CHO activated brain regions associated with reward - insula/frontal operculum, orbitofrontal cortex, and striatum. In humans, the intra-amniotic injection of fat (Lipiodol) reduced fetal drinking, while injection of sodium saccharin stimulated it; infants consumed the same amounts of milk formulas with different fat contents. CHO-rich food intake (buffet, KR 0.511:1) relieved neuroglycopenic and mood responses to stress independently from oral or i.v. administration of energy (Hitze et al., 2010). Besides, HFD often fails in inducing obesity. Consequently, it is not uncommon in diet-induced obesity experiments that obesity-resistant subjects are eliminated from analysis or CHO are added to the diet to encourage overeating. To sum it up, fat per se is neither as highly rewarding as CHO nor it is as addictive (Wojnicki et al., 2008; Avena et al., 2009; Pickering et al., 2009; Berthoud et al., 2011). Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | 2 obesity; it is CHO that is not limited enough in HFD; (2) KR may be an element of common language in experiments with different methodological approaches. Trends in carbohydrate, fat, and protein intakes and association with energy intake in normal-weight, overweight, and obese individuals: 1971-2006. Sugar and fat bingeing have notable differences in addictivelike behavior. Berthoud, H. R., Lenard, N. R., and Shin, A. C. (2011). Food reward, hyperphagia, and obesity. Lowcarbohydrate diets: what are the potential short- and long-term health implications? However, this is possible only in deterministic environments. In variable environments, energy storage becomes advantageous and approximately equal parts of energy are allocated for maintenance, reproduction, and depots (Fischer et al., 2011). Energy intake beyond rigid homeostatic regulation relies on behaviors with hedonic, rewarding, and addictive nuances more characteristic for CHO than for fat. To maximize energy stores, energy intake relies on CHO-driven behaviors to allow the environmental "push." In a recent article entitled "Using Marketing Muscle to Sell Fat: The Rise of Obesity in the Modern Economy," J. Zimmerman wrote: "In this paradigm, overeating results from more extensive advertising, new product development, increased portion sizes, and other tactics of food marketers that have caused shifts in the underlying demand for total food calories" (Zimmerman, 2011). On the other hand, the diets with KR of 2:1 or higher are repeatedly described as metabolically beneficial, non-addictive, hunger-reducing, and neuroprotective (Figure 1A). Nutrition and Alzheimer's disease: the detrimental role of a high carbohydrate diet. Bidirectional metabolic regulation of neurocognitive function. Fat substitutes promote weight gain in rats consuming high-fat diets. The Maillard reaction in the human body. The sour side of neurodegenerative disorders: the effects of protein glycation. Binge-type behavior in rats consuming trans-fat-free shortening. Food intake, metabolism and homeostasis. The action of glycol aldehyd and glycerin aldehyd in diabetes mellitus and the nature of antiketogenesis. Objects and methods of diet adjustment in diabetes. Using marketing muscle to sell fat: the rise of obesity in the modern economy. Citaiton: Zilberter T (2011) Carbohydrate-biased control of energy metabolism: the darker side of the selfish brain. This is an open-access article distributed under the terms of the Creative Commons Attribution Non Commercial License, which permits noncommercial use, distribution, and reproduction in other forums, provided the original authors and source are credited. metabolic state in mice. The role of depot fat in the hypothalamic control of food intake in the rat. Long-term exposure to high fat diet is bad for your brain: exacerbation of focal ischemic brain injury. "Control" laboratory rodents are metabolically morbid: why it matters. Fat taste and lipid metabolism in humans. Genetic, traumatic and environmental factors in the etiology of obesity. Neurobiology of overeating and obesity: the role of melanocortins and beyond. Build-ups in the supply chain of the brain: on the neuroenergetic cause of obesity and type 2 diabetes mellitus. Neuroenergetics 1:2. doi: 10.3389/neuro.14.002.2009 Peters, A., Pellerin, L., Dallman, M. F., Oltmanns, K. M., Schweiger, U., Born, J., and Fehm, H. L. (2007). Causes of obesity: looking beyond the hypothalamus. Peters, A., Schweiger, U., Pellerin, L., Hubold, C., Oltmanns, K. M., Conrad, M., Schultes, B., Born, J., and Fehm, H. L. (2004). The selfish brain: competition for energy resources. Withdrawal from free-choice high-fat high-sugar diet induces craving only in obesityprone animals. Puchowicz, M. A., Xu, K., Sun, X., Ivy, A., Emancipator, D., and Lamanna, J. C. (2007). Diet-induced ketosis increases capillary density without altered blood flow in rat brain. Puchowicz, M. A., Zechel, J. L., Valerio, J., Emancipator, D. S., Xu, K., Pundik, S., Lamanna, J. C., and Lust, W. D. (2008). Neuroprotection in diet-induced ketotic rat Cao, L., Choi, E. Y., Liu, X., Martin, A., Wang, C., Xu, X., and During, M. J. White to brown fat phenotypic switch induced by genetic and environmental activation of a hypothalamic-adipocyte axis. Origins and evolution of the Western diet: health implications for the 21st century. Domouzoglou, E., and Maratos-Flier, E. (2011). Fibroblast growth factor 21 is a metabolic regulator that plays a role in the adaptation to ketosis. The Selfish Brain: Learning from Addiction. When to store energy in a stochastic environment. Environmental contributions to the obesity epidemic. How the selfish brain organizes its supply and demand. A high-fat diet impairs cardiac high-energy phosphate metabolism and cognitive function in healthy human subjects. Effects of a highprotein ketogenic diet on hunger, appetite, and weight loss in obese men feeding ad libitum. A high-fat, ketogenic diet induces a unique Frontiers in Neuroenergetics www.frontiersin.org December 2011 | Volume 3 | Article 8 | This paper, based on analysis of experimental data, offers an opinion that the obesogenic and neurodegenerative effects of dietary fat in the high-fat diets (HFD) cannot be separated from the effects of the CHO compound in them. The role of glyoxalases for sugar stress and aging, with relevance for dyskinesia, anxiety, dementia and Parkinson's disease. Alzheimer's disease is a devastating disease whose recent increase in incidence rates has broad implications for rising health care costs. Huge amounts of research money are currently being invested in seeking the underlying cause, with corresponding progress in understanding the disease progression. This leads to cholesterol deficiency in neurons, which significantly impairs their ability to function. Over time, a cascade response leads to impaired glutamate signaling, increased oxidative damage, mitochondrial and lysosomal dysfunction, increased risk to microbial infection, and, ultimately, apoptosis. Other neurodegenerative diseases share many properties with Alzheimer's disease, and may also be due in large part to this same underlying cause.Copyright (c) 2011 European Federation of Internal Medicine. This metabolic mode, typical for consumers of the "Western diet" (Cordain et al., 2005; #CITATION_TAG), is characterized by over-generation of reactive oxygen species and advanced glycation products both of which are implicated in many of the neurodegenerative diseases (Tessier, 2010; Vicente Miranda and Outeiro, 2010; Auburger and Kurz, 2011). A first step in the pathophysiology of the disease is represented by advanced glycation end-products in crucial plasma proteins concerned with fat, cholesterol, and oxygen transport.
We studied choreographer Wayne McGregor's approach to movement creation through tasking, in which he asks dancers to create movement in response to task instructions that require a great deal of mental imagery and decision making. As part of a programme of research that is developing tools to enhance choreographic practice, an interdisciplinary team of cognitive scientists, neuroscientists and dance professionals collaborated on two studies examining the mental representations used to support movement creation. In artists, the higher synchrony in the low-frequency band is possibly due to the involvement of a more advanced long-term visual art memory and to extensive top-down processing. Other studies have focused on the underlying neural mechanisms of creativity in realms other than dance (Jung et al., 2010), especially in music (Limb and Braun, 2008) and drawing (#CITATION_TAG). To assess the underlying synchronization, which is assumed to be the platform for general cognitive integration between different cortical regions, three measures inspired by nonlinear dynamical system theory were applied as follows: (1) index based on generalized synchronization; (2) index based on mean phase coherence; and (3) index of phase synchrony based on entropy. Strong right hemispheric dominance in terms of synchronization was found in the artists.
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Artificial neural networks (ANNs) have been proved to be successfully used in a variety of pattern recognition and data mining applications. However, training ANNs on large scale datasets are both data-intensive and computation-intensive. In this paper, we present cNeural, a customized parallel computing platform to accelerate training large scale neural networks with the backpropagation algorithm. Nonetheless, NNs performance has been found to be comparable with other statistical approaches, particularly when approximating complex patterns based on numeric input values (Sargent, 2001; #CITATION_TAG). Therefore, large scale ANNs are used with reservation for their time-consuming training to get high precision. Unlike many existing parallel neural network training systems working on thousands of training samples, cNeural is designed for fast training large scale datasets with millions of training samples. Secondly, it provides a parallel in-memory computing framework for fast iterative training. Third, we choose a compact, event-driven messaging communication model instead of the heartbeat polling model for instant messaging delivery.
The potential of mobile technologies is not fully exploited by current software services. One of the most influencing reasons for this problem is the lack of novel software engineering methods and tools that can master the complexity of mobile environments. Looking at a person in a smart environment, where mobile technologies and sensors are installed to support daily activities, it is observed that informed decision-making with the help of mobile technologies is beyond what users can expect from current software services. In this paper we present a motivating scenario to highlight the limitations of current decision support approaches. This is a study of social mobility within the developing class structures of modern industrial societies based on a unique data-set constructed by John Goldthorpe and Robert Erikson. Approaches which allow end-user to give feedback on current context-aware services (#CITATION_TAG) and which allow them to document their ideas on services in situ (Seyff et al., 2010) build a basis to satisfy some of the depicted issues. The authors combine historical and statistical approaches in their analysis of both trends in mobility and of cross-national similarities and differences.
According to transaction cost and internalization theories of multinational enterprises, companies make foreign direct investments (FDI) when the combined costs of operations and governance are lower for FDI than for market or contract based options, such as exports and licensing. Yet, ex post governance costs remain a conjectural construct, which has evaded empirical scrutiny, and the lack of focus on the implications of these costs constitutes a challenge for management in multinational companies (MNCs). What effects does the ensuing establishment of subsidiaries abroad have in terms of governance costs? What factors drive these costs? Human communication is grounded in fundamentally cooperative, even shared, intentions. In this original and provocative account of the evolutionary origins of human communication, Michael Tomasello connects the fundamentally cooperative structure of human communication (initially discovered by Paul Grice) to the especially cooperative structure of human (as opposed to other primate) social interaction. Tomasello argues that human cooperative communication rests on a psychological infrastructure of shared intentionality (joint attention, common ground), evolved originally for collaboration and culture more generally. The basic motives of the infrastructure are helping and sharing: humans communicate to request help, inform others of things helpfully, and share attitudes as a way of bonding within the cultural group. Requesting help in the immediate you-and-me and here-and-now, for example, required very little grammar, but informing and sharing required increasingly complex grammatical devices. Conventional communication, first gestural and then vocal, evolved only after humans already possessed these natural gestures and their shared intentionality infrastructure along with skills of cultural learning for creating and passing along jointly understood communicative conventions. Trust begets trust, but for the process to activate it needs to be initiated (#CITATION_TAG). These cooperative motives each created different functional pressures for conventionalizing grammatical constructions.
Understanding how children develop in this complex environment will require a solid, theoretically-grounded understanding of how the child and environment interact-- both within and beyond the laboratory. Categories, like children, do not exist in isolation. Consequently, category learning cannot be easily separated from the learning context--nor should it be. According to a systems perspective of cognition and development, categorization emerges as the product of multiple factors combining in time (Thelen and Smith, 1994). To be as inclusive as possible, we consider any case in which a participant responds to how stimuli may be grouped as evidence of category learning. You may notice in these examples that we have not included children's ages because, according to a systems view, research should not be about age per se. Obviously, age must be taken into account in experimental design because age is generally (but not perfectly) correlated with developmental level (e.g., appropriate motor responses differ for a 2-year-old vs. 2-month-old). WHO IS INVOLVED IN LEARNING In the real world children learn through play and independent exploration (HirshPasek et al., 2009). However, in the lab children are seldom alone. This is important because children adjust their learning depending on who is providing information (e.g., the same or different experimenter, Goldenberg and Sandhofer, 2013; human or robot, O'Connell et al., 2009; mom or dad, Pancsofar and VernonFeagans, 2006). Children are also opportunistic and will look for any signal of what the right answer is. For example, children will track who is present when they hear a new word (e.g., Akhtar et al., 1996), whether the speaker has provided reliable information before (e.g., Jaswal and Neely, 2006) and whether a question is repeated (e.g., Samuel and Bryant, 1984). Moreover, who the child is also matters. WHAT IS BEING CATEGORIZED All categories are not created equal: categories vary in complexity and withincategory similarity (Sloutsky, 2010). Where children draw boundaries between categories is influenced by category (object) properties, including distinctive features (Hammer and Diesendruck, 2005), number of common features (Samuelson and Horst, 2007; Horst and Twomey, 2013), visual cues to animacy (Jones et al., 1991), the presence of category labels (Sloutsky and Fisher, 2004; Plunkett et al., 2008) and the presence of other objects (e.g., identical or nonidentical exemplars Oakes and Ribar, 2005; Kovack-Lesh and Oakes, 2007). In naturalistic environments, categories are often ad hoc and flexible (Barsalou, 1983). For example, the category "toys to pick up before bed" may be discussed every day, but each day it may include different items. Furthermore, the process of categorizing objects is not independent of the objects themselves: different objects may be more or less flexibly assigned to www.frontiersin.org January 2015 | Volume 6 | Article 46 | 1 different categories depending on the context (Mareschal and Tan, 2007) and information available (Horst et al., 2009). Where a child lives impacts what social categories they learn and the category choices they make. For example, Black Xhosa children in South Africa prefer own-race faces if they live in a primarily Black township, but prefer higher-status race faces if they live in a racially diverse city (Shutts et al., 2011). In the lab, location matters both in terms of where the child is and where the stimuli are. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (Perry et al., 2014). For example, yes/no questions lead to a stronger shape bias than forced-choice questions (Samuelson et al., 2009), various types of feedback differentially affect learning categories with highly salient features vs. less salient features (Hammer et al., 2012) and highly variable category members facilitate category name generalization (Perry et al., 2010) whereas less variable category members facilitate category name retention (Twomey et al., 2014). Categorization does not reflect static knowledge; rather, category learning unfolds over time and is a product of nested timescales. Children (and adults) are constantly learning: experimenters' distinction between learning vs. test trials is arbitrary with respect to the processes that operate within the task (McMurray et al., 2012). That is, learning continues even on test trials--in fact, participants may not realize the shift from learning to test trials. Consequently, different behaviors are observed depending on when during the categorization process category learning is assessed (Horst et al., 2005). Category learning is a product of nested timescales including (a) the current moment (e.g., how similar the stimuli are on the current trial, Horst and Twomey, 2013), (b) the "just previous" past (e.g., what happens during the intertrial interval, Kovack-Lesh and Oakes, 2007; whether stimuli on the first test trial are novel or familiar, Schoner and Thelen, 2006; and trial order effects Wilkinson et al., 2003; Vlach et al., 2008) and (c) developmental history (e.g., vocabulary level, Ellis and Oakes, 2006; Horst et al., 2009; Perry and Samuelson, 2011). Because children's behavior is never solely the product of a single timescale it is impossible to create an experiment that taps only into category learning in the moment or only knowledge children brought to the lab. For example, Kovack-Lesh et al. UNEXPECTED INFLUENCES If researchers view categorization as static knowledge, then neither the when or how should matter. Many researchers hold this view, which purports experiments are designed to test what a child knows upon arrival at the lab: trial order and trial types are largely trivial. Small variations in what children experience during category learning can have dramatic impact on how they form categories (e.g., sequential vs. simultaneous presentation, Oakes and Ribar, 2005; Lawson, 2014) and differences in testing contexts can lead to indications of what has been learned (Cohen and Marks, 2002). Subtle experimental design decisions, such as the number of test trials to include, may not seem theoretically significant, but they can have profound effects on children's behavior. As dozens of studies illustrate, "boring" factors like counterbalancing and stimuli choice during both learning and testing can have a profound effect on findings, including trial order (Wilkinson et al., 2003), how many targets (Axelsson and Horst, 2013) or competitors (Horst et al., 2010) are presented, or the color of the stimuli (Samuelson and Horst, 2007; Samuelson et al., 2007). For example, how broadly participants generalize a category label depends on where the exemplars are presented and if the exemplars are visible simultaneously (Spencer et al., 2011). In particular whether more or less diverse examples occur in the first block of trials influences later generalization (see Spencer et al., 2011, Supplementary Materials). Unexpected influences may not be of immediate theoretical interest to a given experimenter, but they are still often informative--even at times vital-- to the underlying processes at work (e.g., the influence of novelty on children's selection is informative for understanding how prior memory influences current learning). We recognize this can be impractical with populations that are costly to recruit, in which case such factors may Frontiers in Psychology | Cognition January 2015 | Volume 6 | Article 46 | 2 be controlled for statistically, for example with item-level analyses. OUTLOOK Category learning unfolds across both space and time, and small differences at one moment (e.g., shared features among the stimuli; whether exemplars are identical) can create a ripple of effects on real behavior. Behavior emerges from the combination of many factors, including those not explicitly manipulated or controlled by the experimenter. However, just as it is important to acknowledge these unexpected influences, we must not fail to see the forest for the trees. If a behavior such as category learning can only be captured in an ideal environment under carefully-controlled conditions, how much can we generalize to the contexts in which learning typically occurs? Theoretical accounts that neglect the rich influence of context in real time are too narrow to be applied outside the lab (Simmering and Perone, 2013). What we as researchers are ultimately trying to understand is how learning occurs in a real, cluttered world across time and a variety of contexts. Consequently, a solid, theoretically-grounded understanding of cognitive development will require understanding how the child (or adult) and environment interact. In this paper, we include many different types of behaviors under the umbrella term "categorization." Our goal is not to create a catalog of milestones; our goal is to understand the cognitive mechanisms driving change. Our point, however, is that we will learn more about category learning if we stop asking questions such as "how do prototype representations compare between 6 and 8 months of age?" Thus, in order to understand the process of categorization, researchers must ensure that the results they find in the lab are not too closely tied to the specific stimuli. Thus, it is vital to acknowledge the impact of such unexpected influences if we want to understand how categorization unfolds over time. We argue that what infants learn about naming nonsolid substances is contextually bound - most nonsolids that toddlers are familiar with are foods and thus, typically experienced when sitting in a highchair. For example, children are more likely to learn names for non-solid substances if introduced to the gooey items in a familiar highchair context (#CITATION_TAG). We examine developmental interactions between context, exploration, and word learning. We asked whether 16-month-old children's naming of nonsolids would improve if they were tested in that typical context. Furthermore, context-based differences in exploration drove differences in the properties attended to in real-time.
The evolutionary history of Mexican ichthyofauna has been strongly linked to natural events, and the impact of pre-Hispanic cultures is little known. The live-bearing fish species Allotoca diazi, Allotoca meeki and Allotoca catarinae occur in areas of biological, cultural and economic importance in central Mexico: Patzcuaro basin, Zirahuen basin, and the Cupatitzio River, respectively. The species are closely related genetically and morphologically, and hypotheses have attempted to explain their systematics and biogeography. The separation of A. diazi and A. meeki was dated to 400-7000 years ago, explained by geological and climate events. The isolation of A. catarinae occurred~1900 years ago. No geological events are documented in the area during this period, but the date is contemporary with P'urhepecha culture settlements. # Abstract Molecular clocks have profoundly influenced modern views on the timing of important events in evolutionary history. On the population genetic scale, we review advances in the incorporation of ancestral population processes into the estimation of divergence times between recently separated species. The isolation of A. diazi, A. meeki, and A. catarinae is reflected in the low genetic differences, non-monophyletic patterns, shared haplotypes, and genetic groups assignment, and we considered that the species complex consist in an incomplete lineage sorting pattern [72] [#CITATION_TAG] [74] [75]. On the phylogenetic scale, we address the complexities of DNA sequence evolution as they relate to estimating divergences, focusing on models of nucleotide substitution and problems associated with among-site and among-lineage rate variation. Throughout the review we emphasize new statistical methods and the importance of model testing during the process of divergence time estimation.
Background Unassisted cessationquitting without pharmacological or professional supportis an enduring phenomenon. Unassisted cessation persists even in nations advanced in tobacco control where cessation assistance such as nicotine replacement therapy, the stop-smoking medications bupropion and varenicline, and behavioural assistance are readily available. We review the qualitative literature on the views and experiences of smokers who quit unassisted. Motivation, although widely reported, had only one clear meaning, that is 'the reason for quitting'. Commitment was equated to seriousness or resoluteness, was perceived as key to successful quitting, and was often used to distinguish earlier failed quit attempts from the final successful quit attempt. Commitment had different dimensions. Ninety-five percent of ex-smokers have quit smoking without professional assistance. Research investigating self-initiated smoking cessation has increased to the point where a sizable data base on this phenomenon is now available. First, we were aware of a small but not unsubstantial body of quantitative evidence on smokers who quit unassisted; [#CITATION_TAG] [49] [50] [51] [52] and second, in the course of our literature search we had identified a considerable number of qualitative studies on smoking cessation. nan
INTUITIVE AND REFLECTIVE RESPONSES IN PHILOSOPHY by NICK BYRD B.A. IRB protocol #13-0678 Nick Byrd (M.A., Philosophy) INTUITIVE AND REFLECTIVE REASONING IN PHILOSOPHY Committee: Michael Huemer, Robert Rupert, and Michael Tooley Cognitive scientists have revealed systematic errors in human reasoning. There is disagreement about what these errors indicate about human rationality, but one upshot seems clear: human reasoning does not seem to fit traditional views of human rationality. This concern about rationality has made its way through various fields and has recently caught the attention of philosophers. Nonetheless, philosophers are not entirely immune to this systematic error, and their proclivity for this error is statistically related to their responses to a variety of philosophical questions. So, while the evidence herein puts constraints on the worries about the integrity of philosophy, it by no means eliminates these worries. I also owe a great deal to various faculty members in cognitive science and psychology. The concern is that if philosophers are prone to systematic errors in reasoning, then the integrity of philosophy would be threatened. In this paper, I present some of the more famous work in cognitive science that has marshaled this concern. Many philosophers have worried about what philosophy is. Often they have looked for answers by considering what it is that philosophers do. In this article we consider the philosophical temperament, asking an alternative question: What are philosophers like? Recognizing this aspect of the philosophical temperament, it is natural to wonder how philosophers came to be this way: Does philosophical training teach reflectivity or do more reflective people tend to gravitate to philosophy? One of the purposes in considering the various characterizations is to caution the reader from adopting one characterization without considering other possibilities (#CITATION_TAG). We then illustrate this tendency by considering what we know about the philosophizing of a few prominent philosophers.
We consider approaches to explanation within the cognitive sciences that begin with Marr's computational level (e.g., purely Bayesian accounts of cognitive phenomena) or Marr's implementational level (e.g., reductionist accounts of cognitive phenomena based only on neural-level evidence) and argue that each is subject to fundamental limitations which impair their ability to provide adequate explanations of cognitive phenomena. For this reason, it is argued, explanation cannot proceed at either level without tight coupling to the algorithmic and representation level. Even at this level, however, we argue that additional constraints relating to the decomposition of the cognitive system into a set of interacting subfunctions (i.e., a cognitive architecture) are required. Integrated cognitive architectures that permit abstract specification of the functions of components and that make contact with the neural level provide a powerful bridge for linking the algorithmic and representational level to both the computational level and the implementational level. Different types of psychotic symptoms may exist, some being normal variants and some having implications for mental health and functioning. Intermittent, infrequent psychotic experiences were common, but frequent experiences were not. Magical Thinking was only weakly associated with these variables. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas may represent expressions of underlying vulnerability to psychotic disorder, but Magical Thinking may be a normal personality variant. An alternative approach to solving the reverse inference problem has been developed by Poldrack and colleagues (e.g., #CITATION_TAG; Yarkoni, Poldrack, Nichols, Van Essen, & Wager, 2011). Method: Eight hundred and seventy-five Year 10 students from 34 schools participated in a cross-sectional survey that measured psychotic-like experiences using the Community Assessment of Psychic Experiences; depression using the Centre for Epidemiologic Studies Depression Scale; and psychosocial functioning using the Revised Multidimensional Assessment of Functioning Scale. Factor analysis was conducted to identify any subtypes of psychotic experiences. Bizarre Experiences, Perceptual Abnormalities and Persecutory Ideas were strongly associated with distress, depression and poor functioning.
We present a scheme that produces a strong U(1)-like gauge field on cold atoms confined in a two-dimensional square optical lattice. As in the proposal by Jaksch and Zoller [New Journal of Physics 5, 56 ( 2003 )], laser-assisted tunneling between adjacent sites creates an effective magnetic field. We discuss the observable consequences of the artificial gauge field on non-interacting bosonic and fermionic gases. In this context, g and e denoted two different hyperfine states in the ground state manifold, and the spin-dependent optical potential was obtained by exploiting the vector light-shift arising in a laser field with suitable polarization (see [57, #CITATION_TAG], for example). Individuals with mild and moderate dysarthria trained and then used 45 command words to input text independently into the PSDD. The PSDD system also adapted to the speech of two participants with different degrees of severe dysarthria, but they were unable to use this system independently.
Drawing on the classic model of balanced affect, the Francis Burnout Inventory (FBI) conceptulises good work-related psychological health among clergy in terms of negative affect being balanced by positive affect. Introduction: There is a high prevalence of burnout among emergency medicine (EM) residents. The Maslach Burnout Inventory - Human Services Survey (MBI-HSS) is a widely used tool to measure burnout. In an attempt to bring greater scientific objectivity to the conceptualisation and assessment of clergy work-related psychological health, a number of studies have employed the Maslach Burnout Inventory (MBI) established by #CITATION_TAG. A 2-Question Summative Score totaling &gt;3 correlated most closely with the primary definition of burnout (Spearman's rho 0.65 [95% confidence interval 0.62-0.68]).
CDT encourages the use of naturalistic idioms and allows the training talker to adapt their speaking style to perceptual difficulties (#CITATION_TAG). It also investigated whether acoustic-phonetic modifications made to counteract the effects of a challenging listening condition are tailored to the condition under which communication occurs. Forty talkers were recorded in pairs while engaged in "spot the difference" picture tasks in good and challenging conditions. In the challenging conditions, one talker heard the other (1) via a three-channel noise vocoder (VOC); (2) with simultaneous babble noise (BABBLE).
This article analyses domestic and foreign reactions to a 2008 report in the British Medical Journal on the complementary and, as argued, synergistic relationship between palliative care and euthanasia in Belgium. The earliest initiators of palliative care in Belgium in the late 1970s held the view that access to proper palliative care was a precondition for euthanasia to be acceptable and that euthanasia and palliative care could, and should, develop together. Advocates of euthanasia including author Jan Bernheim, independent from but together with British expatriates, were among the founders of what was probably the first palliative care service in Europe outside of the United Kingdom. In what has become known as the Belgian model of integral end-oflife care, euthanasia is an available option, also at the end of a palliative care pathway. This approach became the majority view among the wider Belgian public, palliative care workers, other health professionals, and legislators. The legal regulation of euthanasia in 2002 was preceded and followed by a considerable expansion of palliative care services. The Belgian model of so-called integral end-oflife care is continuing to evolve, with constant scrutiny of practice and improvements to procedures. It still exhibits several imperfections, for which some solutions are being developed. This article analyses this model by way of answers to a series of questions posed by Journal of Bioethical Inquiry consulting editor Michael Ashby to the Belgian authors. Although advance directives may seem useful instruments in decision-making regarding incompetent patients, their validity in cases of dementia has been a much debated subject and little is known about their effectiveness in practice. Insight into the experiences and wishes of people with dementia regarding advance directives is totally lacking in empirical research.Ethics and actual practice are two "different worlds" when it comes to approaching advance directives in cases of dementia. It is clear, however, that the use of advance directives in practice remains problematic, above all in cases of advance euthanasia directives, but to a lesser extent also when non-treatment directives are involved. is hotly debated (#CITATION_TAG). nan
Estimates of annual prevalence (1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008), and incidence (1996)(1997)(1998)(1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008); allowing a 5-year disease-free run-in period) were age and sex standardized to the 2001 Canadian population. From 1991-2008, MS prevalence increased by 4.7 % on average per year (p \ 0.001) from 78.8/100,000 (95 % CI 75.7, 82.0) to 179.9/100,000 (95 % CI 176.0, 183.8), the sex prevalence ratio increased from 2.27 to 2.78 (p \ 0.001) and the peak prevalence age range increased from 45-49 to 55-59 years. MS incidence and prevalence in BC are among the highest in the world. Neither the incidence nor the incidence sex ratio increased over time. Background:                  Multiple sclerosis (MS) is the most common cause of neurological disability in young adults worldwide and approximately half of those affected are in Europe. Quality was generally higher in the more recent studies, which also tended to use current diagnostic criteria. Prevalence and incidence estimates tended to be higher in the more recent studies and were higher in the Nordic countries and in northern regions of the British Isles. Few studies examined ethnicity. Epidemiological data at the national level was uncommon and there were marked geographical disparities in available data, with large areas of Europe unrepresented and other regions well-represented in the literature. It is estimated that more than two million people live with this disease worldwide [1], although the incidence and prevalence vary geographically [2] [#CITATION_TAG] [4]. The assessment of differential incidence and prevalence across populations can reveal spatial, temporal and demographic patterns which are important for identifying genetic and environmental factors contributing to MS. However, study methodologies vary and the quality of the methods can influence the estimates. Methods:                  A comprehensive literature search was performed to obtain all original population-based studies of MS incidence and prevalence in European populations conducted and published between January 1985 and January 2011. Only peer-reviewed full-text articles published in English or French were included. All abstracts were screened for eligibility and two trained reviewers abstracted the data and graded the quality of each study using a tool specifically designed for this study.
It is argued that ISE practices reinforced participants preexisting sense that museums and science centers were "not for us." This paper explores how people from low-income, minority ethnic groups perceive and experience exclusion from informal science education (ISE) institutions, such as museums and science centers. Drawing on qualitative data from four focus groups, interviews, four accompanied visits to ISE institutions, and field notes, this paper presents an analysis of exclusion from science learning opportunities during visits alongside participants' attitudes, expectations, and conclusions about participation in ISE. In examining the concepts of authoritative language and symbolic power we often look to one legitimate  language placed at the top of a hierarchy that serves to marginalize all others, and in so doing,  marginalizes its speakers. We look at subversive moments of resistance to challenge this structure,  but more often than not, it would seem, this hierarchical structure of language is the one that prevails. The situational context of the dual language system  of Malta provides just such juxtaposition, within which to study authoritative language, symbolic  power, and strategies of condescension, as it is a place where the normal rules of language hierarchy  do not apply. While strategies of condescension  in the use of English abound, there are still ways to negotiate moments of power and meet in the  middle.peer-reviewe Language practices can be understood as forms of capital that delineate the information, culture, and people who are more valued in a particular field (#CITATION_TAG). nan
Increasing college participation rates, and diversity in student population, is posing a challenge to colleges in their attempts to facilitate learners achieve their full academic potential. Learning analytics is an evolving discipline with capability for educational data analysis that could enable better understanding of learning process, and therefore mitigate these challenges. This study reviewed factors that could be used to predict academic performance, but which are currently not systematically measured in tertiary education. It focused on psychometric factors of ability, personality, motivation, and learning strategies. Important factors include learning style (e.g., Bruinsma, 2004; Chamorro-Premuzic & Furnham, 2008; Diseth, 2011; #CITATION_TAG) and self-regulation (e.g., Nasiriyan et al., 2011; Ning & Downing, 2010). The task involved a collaborative computer-based modeling task. In order to test the model, group measures of mastery-approach goal orientation, performance-avoidance goal orientation, self-efficacy, and achievement were employed. Students' cognitive processing was assessed using an online log-file measure.
Much bioethical scholarship is concerned with the social, legal and philosophical implications of new and emerging science and medicine, as well as with the processes of research that under-gird these innovations. Science and technology studies (STS), and the related and interpenetrating disciplines of anthropology and sociology, have also explored what novel technoscience might imply for society, and how the social is constitutive of scientific knowledge and technological artefacts. More recently, social scientists have interrogated the emergence of ethical issues: they have documented how particular matters come to be regarded as in some way to do with 'ethics', and how this in turn enjoins particular types of social action. In sum, engagements between STS and bioethics are increasingly important in order to understand and manage the complex dynamics between science, medicine and ethics in society. In this paper, I will discuss some of this and other STS (and STS-inflected) literature and reflect on how it might complement more 'traditional' modes of bioethical enquiry. Social scientists often lament the fact that philosophically trained ethicists pay limited attention to the insights they generate. Science and technology studies (STS) is one such tradition that is articulating with bioethics, though sometimes fractiously [9, #CITATION_TAG]. Increased awareness of differences in styles of reasoning and objects of research interest might help to overcome the hostility, and an anthropological project is presented as an invitation to a dialogue informed by awareness of such differences.
