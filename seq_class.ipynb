{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1743e9-b8a5-4e36-a91c-771970d1a901",
   "metadata": {},
   "source": [
    "### Combine the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "0367c1b6-1e9c-4b28-ba15-e687de3b85cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Documents/GitHub/KGG\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Documents/GitHub/KGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c07d88e-244c-4d3c-91ab-b272e00045f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob(\"./*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fda3499d-330b-43b9-90f1-1c860568d46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3032"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_o_res = pd.read_csv(\"./combined_abstracts_wo_res.csv\")\n",
    "len(w_o_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5da12e2-b8b4-4c29-bad5-a1a162df265b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3032 entries, 0 to 3031\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Unnamed: 0        3032 non-null   int64 \n",
      " 1   citing_abstracts  3032 non-null   object\n",
      " 2   cited_abstracts   3032 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 71.2+ KB\n"
     ]
    }
   ],
   "source": [
    "w_o_res.info()\n",
    "citing_abs = w_o_res[\"citing_abstracts\"].tolist()\n",
    "cited_abs = w_o_res[\"cited_abstracts\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee8d7761-e645-40de-8b32-1b0ef2053b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3032 entries, 0 to 3031\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  3032 non-null   int64 \n",
      " 1   reponses    3032 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.5+ KB\n"
     ]
    }
   ],
   "source": [
    "frame.info()\n",
    "responses = frame[\"reponses\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a916d964-2621-458b-865a-0076df530b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"citing_abstracts\": citing_abs, \"cited_abstracts\": cited_abs, \"meta\": responses}).to_csv(\"./abstract_w_meta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8556c-2d1d-4a1e-b4c7-590d1b63a67e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3bd62d8-04f2-4241-82aa-4d29a48cb5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Documents/GitHub/downloads/sequential_sentence_classification/data/CSAbstruct\n"
     ]
    }
   ],
   "source": [
    "# Check CSAbstruct data\n",
    "%cd /Users/quert/Documents/GitHub/downloads/sequential_sentence_classification/data/CSAbstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de88c617-8606-46aa-bd02-6ade73b874b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"./dev.jsonl\"\n",
    "data = []\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        data.append(json_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "124dd530-6402-455c-89ed-62998923fbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions.',\n",
       " 'The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function.',\n",
       " 'This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient.',\n",
       " 'To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy.',\n",
       " 'We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63a1f0d8-b9be-471e-8728-4e0a7d790bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe439fd-c854-4b78-a81d-262051fb3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0].keys()\n",
    "# dict_keys(['abstract_id', 'sentences', 'labels', 'confs'])\n",
    "\n",
    "# make the model to classify sentences in our data, then store them separately\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"typeof/distilbert_base_uncased_csabstruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de108e2f-22e4-4e62-991a-6b4d18afb981",
   "metadata": {},
   "source": [
    "### Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06e69304-34cd-4f95-acc1-ce057ca3ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Documents/GitHub/acl-research/data\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Documents/GitHub/acl-research/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a001ebca-0bce-4a72-84e2-b4e24ea032cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./abstract_w_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8310947-d872-42c3-b764-58c49cb1a2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citing_abstracts</th>\n",
       "      <th>cited_abstracts</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>As part of an interdisciplinary project on the...</td>\n",
       "      <td>Die Beziehungen der Kaiserstadt an der Donau z...</td>\n",
       "      <td>In comparing the newer and older abstracts, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>As part of an interdisciplinary project on the...</td>\n",
       "      <td>Some hydraulic characteristics of stream chann...</td>\n",
       "      <td>Based on the comparison of the two abstracts, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>As part of an interdisciplinary project on the...</td>\n",
       "      <td>Understanding contemporary urban landscapes re...</td>\n",
       "      <td>The newer paper presents several improvements ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>As part of an interdisciplinary project on the...</td>\n",
       "      <td>Channel planform change was investigated along...</td>\n",
       "      <td>Improvements or advancements in the newer pape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>As part of an interdisciplinary project on the...</td>\n",
       "      <td>In the relation between urban development and ...</td>\n",
       "      <td>Based on the given abstracts, there are severa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>As part of an interdisciplinary project on the...</td>\n",
       "      <td>In addition to objective climatic data, subjec...</td>\n",
       "      <td>In the newer paper compared to the older one, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Reproducibility of tender point examination in...</td>\n",
       "      <td>Background: Reproducibility concerns the degre...</td>\n",
       "      <td>In the newer paper, the improvement or advance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Reproducibility of tender point examination in...</td>\n",
       "      <td>We hypothesized that change in pain threshold ...</td>\n",
       "      <td>In the newer paper, there are several improvem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Many individual decisions are informed by dire...</td>\n",
       "      <td>In the newer paper, there have been several im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Abstract. Social insect colonies possess remar...</td>\n",
       "      <td>In the newer paper, there are several improvem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Evolutionary theory predicts that animal decis...</td>\n",
       "      <td>In comparing the two abstracts, it is evident ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>We tested the decision-making abilities of emi...</td>\n",
       "      <td>Improvements/Advancements in the Newer Paper:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Abstract. When its nest is damaged, a colony o...</td>\n",
       "      <td>In the newer paper, there are several advancem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Abstract. This study views a honey bee swarm a...</td>\n",
       "      <td>In the newer version of the abstract, there ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Abstract Western scrub-jays (Aphelocoma califo...</td>\n",
       "      <td>In the newer paper, there are several improvem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Empis borealisfemales form swarms, and males c...</td>\n",
       "      <td>Comparing the two abstracts, it appears that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Abstract.This paper examines the individual be...</td>\n",
       "      <td>Improvements or advancements made in the newer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>Foragers of the ant Lasius niger exploiting a ...</td>\n",
       "      <td>The newer version of the abstract has made sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>There are claims in the literature that certai...</td>\n",
       "      <td>Improvements/Advancements in the Newer Paper:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Decision-making animals can use slow-but-accur...</td>\n",
       "      <td>We show for the first time, to our knowledge, ...</td>\n",
       "      <td>In the newer paper, there are several improvem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                   citing_abstracts  \\\n",
       "0            0  As part of an interdisciplinary project on the...   \n",
       "1            1  As part of an interdisciplinary project on the...   \n",
       "2            2  As part of an interdisciplinary project on the...   \n",
       "3            3  As part of an interdisciplinary project on the...   \n",
       "4            4  As part of an interdisciplinary project on the...   \n",
       "5            5  As part of an interdisciplinary project on the...   \n",
       "6            6  Reproducibility of tender point examination in...   \n",
       "7            7  Reproducibility of tender point examination in...   \n",
       "8            8  Decision-making animals can use slow-but-accur...   \n",
       "9            9  Decision-making animals can use slow-but-accur...   \n",
       "10          10  Decision-making animals can use slow-but-accur...   \n",
       "11          11  Decision-making animals can use slow-but-accur...   \n",
       "12          12  Decision-making animals can use slow-but-accur...   \n",
       "13          13  Decision-making animals can use slow-but-accur...   \n",
       "14          14  Decision-making animals can use slow-but-accur...   \n",
       "15          15  Decision-making animals can use slow-but-accur...   \n",
       "16          16  Decision-making animals can use slow-but-accur...   \n",
       "17          17  Decision-making animals can use slow-but-accur...   \n",
       "18          18  Decision-making animals can use slow-but-accur...   \n",
       "19          19  Decision-making animals can use slow-but-accur...   \n",
       "\n",
       "                                      cited_abstracts  \\\n",
       "0   Die Beziehungen der Kaiserstadt an der Donau z...   \n",
       "1   Some hydraulic characteristics of stream chann...   \n",
       "2   Understanding contemporary urban landscapes re...   \n",
       "3   Channel planform change was investigated along...   \n",
       "4   In the relation between urban development and ...   \n",
       "5   In addition to objective climatic data, subjec...   \n",
       "6   Background: Reproducibility concerns the degre...   \n",
       "7   We hypothesized that change in pain threshold ...   \n",
       "8   Many individual decisions are informed by dire...   \n",
       "9   Abstract. Social insect colonies possess remar...   \n",
       "10  Evolutionary theory predicts that animal decis...   \n",
       "11  We tested the decision-making abilities of emi...   \n",
       "12  Abstract. When its nest is damaged, a colony o...   \n",
       "13  Abstract. This study views a honey bee swarm a...   \n",
       "14  Abstract Western scrub-jays (Aphelocoma califo...   \n",
       "15  Empis borealisfemales form swarms, and males c...   \n",
       "16  Abstract.This paper examines the individual be...   \n",
       "17  Foragers of the ant Lasius niger exploiting a ...   \n",
       "18  There are claims in the literature that certai...   \n",
       "19  We show for the first time, to our knowledge, ...   \n",
       "\n",
       "                                                 meta  \n",
       "0   In comparing the newer and older abstracts, it...  \n",
       "1   Based on the comparison of the two abstracts, ...  \n",
       "2   The newer paper presents several improvements ...  \n",
       "3   Improvements or advancements in the newer pape...  \n",
       "4   Based on the given abstracts, there are severa...  \n",
       "5   In the newer paper compared to the older one, ...  \n",
       "6   In the newer paper, the improvement or advance...  \n",
       "7   In the newer paper, there are several improvem...  \n",
       "8   In the newer paper, there have been several im...  \n",
       "9   In the newer paper, there are several improvem...  \n",
       "10  In comparing the two abstracts, it is evident ...  \n",
       "11  Improvements/Advancements in the Newer Paper:\\...  \n",
       "12  In the newer paper, there are several advancem...  \n",
       "13  In the newer version of the abstract, there ar...  \n",
       "14  In the newer paper, there are several improvem...  \n",
       "15  Comparing the two abstracts, it appears that t...  \n",
       "16  Improvements or advancements made in the newer...  \n",
       "17  The newer version of the abstract has made sev...  \n",
       "18  Improvements/Advancements in the Newer Paper:\\...  \n",
       "19  In the newer paper, there are several improvem...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7c5b587-5efc-4755-9456-58a784393621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Documents/GitHub/acl-research/outputs\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Documents/GitHub/acl-research/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2aaf4081-9191-41ae-8c07-404fdf686352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob(\"./hyps/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame_hyps = pd.concat(li, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f608cd3-e3b6-448f-8764-3425107c6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob(\"./refs/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame_refs = pd.concat(li, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7b52bc2c-9d3a-4ac1-9ad7-bee1cc418b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3033 entries, 0 to 3032\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  3033 non-null   int64 \n",
      " 1   hyps        3017 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.5+ KB\n"
     ]
    }
   ],
   "source": [
    "frame_hyps.info()\n",
    "hyp_lst = frame_hyps[\"hyps\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d58c49e5-7a15-4ddf-9a01-cc3b23141233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3033 entries, 0 to 3032\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  3033 non-null   int64 \n",
      " 1   refs        1209 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.5+ KB\n"
     ]
    }
   ],
   "source": [
    "frame_refs.info()\n",
    "refs_lst = frame_refs[\"refs\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20007039-6f03-4cff-92d7-84337706a079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the instances with no refs\n",
    "keep_idx = []\n",
    "for idx in range(len(refs_lst)):\n",
    "    if refs_lst[idx]!=\"\" and str(refs_lst[idx])!=\"nan\":\n",
    "        keep_idx.append(idx)\n",
    "len(keep_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a0712d9e-1a8f-4800-a843-e814de2e6fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 8, 12, 13, 20, 22, 24, 25, 26, 27]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "420e8ce4-63a4-403e-be54-b77c0133f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hyps = [hyp_lst[idx] for idx in keep_idx]\n",
    "filtered_refs = [refs_lst[idx] for idx in keep_idx]\n",
    "assert len(filtered_hyps) == len(filtered_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "14f3e905-eb6d-498a-b7a6-3f8c11be3099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Documents/GitHub/acl-research/outputs\n"
     ]
    }
   ],
   "source": [
    "# convert hyp and ref to txt\n",
    "%cd /Users/quert/Documents/GitHub/acl-research/outputs\n",
    "with open(\"./hyps.src\", \"w\") as f:\n",
    "    for hyp in filtered_hyps:\n",
    "        f.write(str(hyp).replace(\"\\n\", \"\\c\")+\"\\n\")\n",
    "with open(\"./refs.tgt\", \"w\") as f:\n",
    "    for ref in filtered_refs:\n",
    "        f.write(str(ref)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "408335ad-355f-4f8f-a5fa-de1bed164574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate lengths of hyps and refs\n",
    "import numpy as np\n",
    "hyp_len, ref_len = [], []\n",
    "for hyp in filtered_hyps:\n",
    "    hyp_len.append(len(str(hyp).split()))\n",
    "\n",
    "for ref in filtered_refs:\n",
    "    ref_len.append(len(str(ref).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8c643dbc-134f-44c3-bea1-ab7b1d322178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296.0, 29.0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(hyp_len), np.median(ref_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a96dfc-ae31-40d6-bd18-e31ad6641759",
   "metadata": {},
   "source": [
    "### Split data into train and test set (70%,30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df0ff52-0826-47a0-9a79-b3174b01fdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Documents/GitHub/KGG/outputs\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Documents/GitHub/KGG/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c37b6de-ea8d-42a3-aefa-e65079436325",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps, refs = [], []\n",
    "with open(\"./hyps.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        hyps.append(line.strip())\n",
    "with open(\"./refs.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        refs.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "566f3cec-63b8-4dfd-90eb-7217e73c10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "start = 0\n",
    "end = 1209\n",
    "# 0-1208\n",
    "\n",
    "num_to_pick = 847\n",
    "all_ids = [idx for idx in range(start, end)]\n",
    "train_ids = list(random.sample(range(start, end), num_to_pick))\n",
    "test_ids = [ele for ele in all_ids if ele not in train_ids]\n",
    "train_ids.sort()\n",
    "test_ids.sort()\n",
    "\n",
    "# extract ids\n",
    "pd.DataFrame({\"id\": train_ids}).to_csv(\"./train_id.csv\", index=False)\n",
    "pd.DataFrame({\"id\": test_ids}).to_csv(\"./test_id.csv\", index=False)\n",
    "\n",
    "# extract train and test sets\n",
    "train_hyps = [hyps[idx] for idx in train_ids]\n",
    "train_refs = [refs[idx] for idx in train_ids]\n",
    "test_hyps = [hyps[idx] for idx in test_ids]\n",
    "test_refs = [refs[idx] for idx in test_ids]\n",
    "pd.DataFrame({\"hyp\": train_hyps, \"ref\": train_refs}).to_csv(\"./train.csv\", index=False)\n",
    "pd.DataFrame({\"hyp\": test_hyps, \"ref\": test_refs}).to_csv(\"./test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396f446-7860-483b-a446-ffe37b6a3fb8",
   "metadata": {},
   "source": [
    "### re-structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f92ced79-eb7f-4633-a448-2aa7053821a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Documents/GitHub/KGG\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Documents/GitHub/KGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f651dc6-fba8-4609-9b51-7bda82982a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3032 entries, 0 to 3031\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  3032 non-null   int64 \n",
      " 1   background  3030 non-null   object\n",
      " 2   objective   2486 non-null   object\n",
      " 3   reference   3030 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 94.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data = pd.read_csv(\"./data_updated/raw_data.csv\")\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb62442b-e56b-4180-a215-d853480f46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_col = raw_data[\"reference\"].tolist()\n",
    "bg_col = raw_data[\"background\"].tolist()\n",
    "obj_col = raw_data[\"objective\"].tolist()\n",
    "\n",
    "rm_na_contents = [ele for ele in reference_col if ele!=\"[]\"]\n",
    "rm_na_ids = [idx for idx in range(len(reference_col)) if reference_col[idx]!=\"[]\"]\n",
    "len(rm_na_contents), len(rm_na_ids)\n",
    "\n",
    "cleaned_bgs = [bg_col[idx] for idx in rm_na_ids]\n",
    "cleaned_objs = [obj_col[idx] for idx in rm_na_ids]\n",
    "cleaned_refs = [reference_col[idx] for idx in rm_na_ids]\n",
    "\n",
    "pd.DataFrame({\"background\": cleaned_bgs, \"objective\": cleaned_objs, \"reference\": cleaned_refs}).to_csv(\"./data_updated/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50d322a7-b434-4ecb-a3b1-5db8f7a1825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "import random\n",
    "\n",
    "start = 0\n",
    "end = 1217\n",
    "\n",
    "num_to_pick = 365\n",
    "\n",
    "all_ids = [idx for idx in range(1217)]\n",
    "test_ids = random.sample(range(start, end), num_to_pick)\n",
    "train_ids = [idx for idx in all_ids if idx not in test_ids]\n",
    "len(all_ids), len(train_ids), len(test_ids)\n",
    "# train\n",
    "train_bgs = [cleaned_bgs[idx] for idx in train_ids]\n",
    "train_objs = [cleaned_objs[idx] for idx in train_ids]\n",
    "train_refs = [cleaned_refs[idx] for idx in train_ids]\n",
    "# test\n",
    "test_bgs = [cleaned_bgs[idx] for idx in test_ids]\n",
    "test_objs = [cleaned_objs[idx] for idx in test_ids]\n",
    "test_refs = [cleaned_refs[idx] for idx in test_ids]\n",
    "\n",
    "\n",
    "pd.DataFrame({\"idx\": train_ids, \"background\": train_bgs, \"objective\": train_objs, \"reference\": train_refs}).to_csv(\"./data_updated/data_train.csv\", index=False)\n",
    "pd.DataFrame({\"idx\": test_ids, \"background\": test_bgs, \"objective\": test_objs, \"reference\": test_refs}).to_csv(\"./data_updated/data_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7839761-fd8e-4d78-9b91-2f4ebacf47d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare hyps and refs for scoring on `data.csv` and `data_test.csv`\n",
    "all_data = pd.read_csv(\"./data_updated/data.csv\")\n",
    "test_data = pd.read_csv(\"./data_updated/data_test.csv\")\n",
    "# all_data.info(), test_data.info()\n",
    "all_hyp = [str(all_data.iloc[idx, 0]) + \" \" + str(all_data.iloc[idx, 1]) for idx in range(len(all_data))]\n",
    "all_ref = [all_data.iloc[idx, 2] for idx in range(len(all_data))]\n",
    "all_ref = [str(ref).replace(\"['\", \"\").replace(\"']\", \"\") for ref in all_ref]\n",
    "test_hyp = [str(test_data.iloc[idx, 1]) + \" \" + str(test_data.iloc[idx, 2]) for idx in range(len(test_data))]\n",
    "test_ref = [test_data.iloc[idx, 3] for idx in range(len(test_data))]\n",
    "test_ref = [str(ref).replace(\"['\", \"\").replace(\"']\", \"\") for ref in test_ref]\n",
    "\n",
    "\n",
    "with open(\"./data_updated/data.hyp\", \"w\") as f:\n",
    "    for line in all_hyp:\n",
    "        f.write(line+\"\\n\")\n",
    "with open(\"./data_updated/data.ref\", \"w\") as f:\n",
    "    for line in all_ref:\n",
    "        f.write(str(line)+\"\\n\")\n",
    "with open(\"./data_updated/data_test.hyp\", \"w\") as f:\n",
    "    for line in test_hyp:\n",
    "        f.write(line+\"\\n\")\n",
    "with open(\"./data_updated/data_test.ref\", \"w\") as f:\n",
    "    for line in test_ref:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3aaba134-d62e-4bb4-86fe-70cf52e7d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare `merge.json` for Vicuna\n",
    "\n",
    "dataset_data = [\n",
    "    {\n",
    "        \"instruction\": \"\"\"\n",
    "    I have gathered information regarding the existing BACKGROUND and OBJECTIVE of current scholarly research. You have to return the research methodology that effectively bridges the gap between this BACKGROUND knowledge and OBJECTIVE. Remember, the output length is restricted to about 35 tokens.\n",
    "    \n",
    "    \"\"\",\n",
    "        \"input\": f\"\"\"\n",
    "        ### BACKGROUND\n",
    "        {train_bgs[idx]}\n",
    "        ### OBJECTIVE\n",
    "        {train_objs[idx]}\n",
    "        \"\"\",\n",
    "        \"output\": f\"\"\"\n",
    "        {train_refs[idx]}\n",
    "        \"\"\"\n",
    "    }\n",
    "    for idx in range(len(train_bgs))\n",
    "]\n",
    "\n",
    "with open(\"./data_updated/merge_train.json\", \"w\") as f:\n",
    "    json.dump(dataset_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8f86f413-091d-4c61-b79e-93c36e1da2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data splitting, we have to convert \"all\" data to json format for lit-gpt\n",
    "dataset_data = [\n",
    "    {\n",
    "        \"instruction\": \"\"\"\n",
    "    I have gathered information regarding the existing BACKGROUND and OBJECTIVE of current scholarly research. You have to return the research methodology that effectively bridges the gap between this BACKGROUND knowledge and OBJECTIVE. Remember, the output length is restricted to about 35 tokens.\n",
    "    \n",
    "    \"\"\",\n",
    "        \"input\": f\"\"\"\n",
    "        ### BACKGROUND\n",
    "        {cleaned_bgs[idx]}\n",
    "        ### OBJECTIVE\n",
    "        {cleaned_objs[idx]}\n",
    "        \"\"\",\n",
    "        \"output\": f\"\"\"\n",
    "        {cleaned_refs[idx]}\n",
    "        \"\"\"\n",
    "    }\n",
    "    for idx in range(len(cleaned_bgs))\n",
    "]\n",
    "\n",
    "with open(\"./data_updated/merge_all.json\", \"w\") as f:\n",
    "    json.dump(dataset_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "202662eb-720e-4d9c-b22a-2dbc838436bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the `test.pt` into updated test data (hyp and ref formats) for scoring and GPT-3.5\n",
    "\n",
    "import torch\n",
    "\n",
    "test_pt = torch.load(\"./data_updated/test.pt\")\n",
    "hyps, refs = [], []\n",
    "for idx in range(len(test_pt)):\n",
    "    hyp = test_pt[idx][\"input\"].replace(\"\\n        ### BACKGROUND\\n        \", \"\").replace(\"\\n        ### OBJECTIVE\\n        \", \"\")\n",
    "    ref = test_pt[idx][\"output\"].replace(\"\\n        [\", \"\").replace(\"]\\n        \", \"\")\n",
    "    hyps.append(hyp), refs.append(ref)\n",
    "    \n",
    "with open(\"./data_updated/data_test_vicuna.hyp\", \"w\") as f:\n",
    "    for line in hyps:\n",
    "        f.write(line+\"\\n\")\n",
    "with open(\"./data_updated/data_test_vicuna.ref\", \"w\") as f:\n",
    "    for line in refs:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1e27f428-37a5-43d6-a7bf-749da1626b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call gpt in restricted output length\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # load environment variables from .env file\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def call_gpt(prompt):\n",
    "    inputs_for_gpt = \" \".join(prompt.split()[:15000])\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "         model = \"gpt-3.5-turbo-1106\",\n",
    "         messages = [\n",
    "             {\"role\": \"user\", \"content\": inputs_for_gpt}\n",
    "         ]\n",
    "     )\n",
    "    response = completion.choices[0].message.content\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475899a-348f-4992-860d-aa77e90d2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "outputs = []\n",
    "cnt = 0\n",
    "for idx in range(len(test_bgs)):\n",
    "    prompt = f\"\"\"\n",
    "    I have gathered information regarding the existing BACKGROUND and OBJECTIVE of current scholarly research. You have to return the research methodology that effectively bridges the gap between this BACKGROUND knowledge and OBJECTIVE. Remember, the output length is restricted to about 35 tokens.\n",
    "    ### BACKGROUND\n",
    "    {test_bgs[idx]}\n",
    "    ### OBJECTIVE\n",
    "    {test_objs[idx]}\n",
    "    \"\"\"\n",
    "    outputs.append(call_gpt(prompt).strip())\n",
    "    cnt += 1\n",
    "    if cnt % 5 ==0:\n",
    "        time.sleep(60)\n",
    "\n",
    "pd.DataFrame({\"GPT-3.5\": outputs}).to_csv(\"./data_update/data_test_gpt.hyp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "650aede1-01d6-4db9-bded-30093199a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"bgs\": test_bgs, \"objs\": test_objs}).to_csv(\"./zero_shot_gpt_3_5/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7b97ec25-1dce-4911-b2f6-a9d3b4c51cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check = pd.read_csv(\"./zero_shot_gpt_3_5/data.csv\")\n",
    "bgg = data_check[\"bgs\"].to_list()\n",
    "objj = data_check[\"objs\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f45a39-d10e-4952-876a-cb265e648328",
   "metadata": {},
   "source": [
    "### for netkup scripts to finetune Vicuna, have to convert the `train.pt` created by lit-gpt-Vicuna to `json` format (`merge.json` for netkup-Vicuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7f3bea2d-24b1-48d7-9481-f07df9e1df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_pt = torch.load(\"/Users/quert/Documents/GitHub/KGG/data_updated/train.pt\")\n",
    "instructions = [str(train_pt[idx][\"instruction\"]) for idx in range(len(train_pt))]\n",
    "inputs = [str(train_pt[idx][\"input\"]) for idx in range(len(train_pt))]\n",
    "outputs = [str(train_pt[idx][\"output\"]) for idx in range(len(train_pt))]\n",
    "assert len(instructions) == len(inputs) == len(outputs)\n",
    "\n",
    "data = [{\"instruction\": instr, \"input\": inp, \"output\": out} \n",
    "        for instr, inp, out in zip(instructions, inputs, outputs)]\n",
    "\n",
    "with open(\"/Users/quert/Documents/GitHub/KGG/data_updated/train.json\", 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4c13a322-d3dc-4844-b3e9-98552368ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "test_pt = torch.load(\"/Users/quert/Documents/GitHub/KGG/data_updated/test.pt\")\n",
    "ins = []\n",
    "for idx in range(len(test_pt)):\n",
    "    instruction = str(test_pt[idx][\"instruction\"])\n",
    "    input = str(test_pt[idx][\"input\"])\n",
    "    merged_txt = instruction + input\n",
    "    ins.append(merged_txt)\n",
    "pd.DataFrame({\"prompt\": ins}).to_csv(\"/Users/quert/Documents/GitHub/KGG/data_updated/test_in.csv\")\n",
    "# merge the instructions with inputs as test_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2cf48-cfbb-4756-96b1-6d0d188d7eec",
   "metadata": {},
   "source": [
    "### Prepare the validation of zero-shot GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6850206f-614b-4857-b202-363af29d0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_hyps = []\n",
    "with open(\"/Users/quert/Documents/GitHub/KGG/data_updated/data_test_vicuna.hyp\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        gt_hyps.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "25a523de-85da-419c-b310-df260dcde663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_gt = [gt for gt in hyps if gt != '        \\n']\n",
    "len(filtered_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6d5dd893-4e08-4c37-8f0e-e0305d5a75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/quert/Documents/GitHub/KGG/data_updated/data_test_vicuna_updated.hyp\", \"w\") as f:\n",
    "    for line in filtered_gt:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f0352-803f-4fa9-a9dc-25c07ba25c52",
   "metadata": {},
   "source": [
    "### Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36505a0b-2f66-474e-80c7-8e632d400d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"\"\"\n",
    "k-nearest-neighbor machine translation (kNNMT) (Khandelwal et al., 2021) boosts the translation performance of trained neural machine translation (NMT) models by incorporating example-search into the decoding algorithm. However, decoding is seriously time-consuming, i.e., roughly 100 to 1,000 times slower than standard NMT, because neighbor tokens are retrieved from all target tokens of parallel data in each timestep. In this paper, we propose “Subset kNN-MT”, which improves the decoding speed of kNN-MT by two methods: (1) retrieving neighbor target tokens from a subset that is the set of neighbor sentences of the input sentence, not from all sentences, and (2) efﬁcient distance computation technique that is suitable for subset neighbor search using a look-up table. Our subset kNNMT achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT’19 De-En translation task and the domain adaptation tasks in De-En and En-Ja.Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Vaswani et al., 2017) has achieved state-of-the-art performance and become the focus of many studies.We propose “Subset kNN-MT”, which improves the decoding speed of kNN-MT by two methods.Our subset kNN-MT achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT’19 German-to-English general domain translation task and the domain adaptation tasks in German-to-English and English-to-Japanese with open-domain settings. kNN-MT (Khandelwal et al., 2021) retrieves the k-nearest-neighbor target tokens in each timestep, computes the kNN probability from the distances of retrieved tokens, and interpolates the probability with the model prediction probability. \n",
    "\"\"\"\n",
    "related_work = \"\"\"\n",
    "The method consists of two steps: (1) datastore creation, which creates key–value translation memory, and (2) generation, which calculates an output probability according to the nearest neighborsof the cached translation memory. Datastore Construction A typical NMT model is composed of an encoder that encodes a source x sentence x = (x 1 , x 2 , . . . , x x | | ) ∈ V | X | and a decoder that generates target tokens y = y (y 1 , y 2 , . . . , y y | | ) ∈ V Y | | where | x | and | y | are the lengths of sentences x and y , respectively, and VX  and V Y are the vocabularies of the source language and target language, respectively. The t-th target token y t is generated according to its output probability P(y t | x, y <t ) over the target vocabulary, calculated from the source sentence x and generated target tokens y <t . kNN-MT stores pairs of Ddimensional vectors and tokens in a datastore, represented as key–value memory M ⊆ R D × V Y . The key (∈ R D ) is an intermediate representation of the ﬁnal decoder layer obtained by teacher forcing a parallel sentence pair (x, y ) to the NMT model, and the value is a ground-truth target token y t . The datastore is formally deﬁned as follows:where D is parallel data and f : V | X | × V t−1 Y → R D is a function that returns the D-dimensional intermediate representation of the ﬁnal decoder layer from the source sentence and generated target tokens. In our model, as in (Khandelwal et al., 2021), the key is the intermediate representation before it is passed to the ﬁnal feed-forward network. Generation During decoding, kNN-MT generates output probabilities by computing the linear interpolation between the kNN and MT probabili-ties, p kNN and p MT , as follows:where λ is a hyperparameter for weighting the kNN probability. Let f(x, y <t ) be the query vector at timestep t. The top i-th key and value in the k-nearest-neighbor are k i ∈ R D and v i ∈ V Y , respectively. Then p kNN is deﬁned as follows:where τ is the temperature for p kNN , and we set τ = 100. Note that this kNN search is seriously time-consuming 1 (Khandelwal et al., 2021).\n",
    "\"\"\"\n",
    "abstract_1 = \"\"\"\n",
    "Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture ﬁne-grained moral variation across countries over a variety of topics such as “homosexuality” and “divorce”; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We ﬁnd that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, ﬁne-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms. Moral norms vary from culture to culture (Haidt et al., 1993; Bicchieri, 2005; Atari et al., 2022; Iurino and Saucier, 2020).Multilingual pre-trained language models (mPLMs) have been probed for their ability to identify cultural norms and biases in a restricted setting (Yin et al., 2022; Arora et al., 2022; Hämmerl et al., 2022; Touileb et al., 2022).Extending these lines of work, we assess whether monolingual EPLMs can accurately infer moral norms across many cultures.\n",
    "\"\"\"\n",
    "\n",
    "related_work_1 = \"\"\"\n",
    " Large language models have been utilized to make automated moral inference from text. Trager et al. (2022) used an annotated dataset to ﬁnetune language models to predict the moral foundations (Graham et al., 2013) expressed in Reddit comments. Many other textual datasets and methods have been proposed for ﬁne-tuning LMs for moral norm generation, reasoning, and adaptation (Forbes et al., 2020; Emelin et al., 2021; Hendrycks et al., 2021; Ammanabrolu et al., 2022; Liu et al., 2022; Lourie et al., 2021; Jiang et al., 2021). Schramowski et al. (2022) proposed a method to estimate moral values and found EPLMs to capture human-like moral judgment even without ﬁne-tuning. They identiﬁed a M ORAL D IRECTION using the semantic space of Sentence-BERT (Reimers and Gurevych, 2019) (SBERT) that corresponds to values of right and wrong. The semantic representations of different actions (e.g., killing people) would then be projected in this direction for moral judgment estimation. However, this method assumed a homogeneous set of moral norms, so it did not examine cultural diversity in moral norms.Probing has been used to study knowledge captured in language models. Petroni et al. (2019) proposed a methodology to explore the factual information that language models store in their weights. Similar probing techniques have been proposed to identify harmful biases captured by PLMs. Ousidhoum et al. (2021) probed PLMs to identify toxic contents that they generate toward people of different communities. Nadeem et al. (2021) took a similar approach and introduced Context Association Tests to measure the stereotypical biases in PLMs, Yin et al. (2022) used probing to evaluate mPLMs on geo-diverse commonsense knowledge, and Touileb et al. (2022) developed probing templates to investigate the occupational gender biases in multilingual and Norwegian language models. Related to our work, Arora et al. (2022) used cross-cultural surveys to generate prompts for evaluating mPLMs in 13 languages. For each country and category (e.g.,Ethical Values) in the surveys, they take an average of participants’ responses to different questions in the category and show that mPLMs do not correlate with the cultural values of the countries speaking these languages. Differing from that study, we assess ﬁner-grained prediction of EPLMs on people’s responses to individual survey questions. More recently, Dillion et al. (2023) prompted GPT3.5 (Brown et al., 2020) with human judgments in different moral scenarios and found striking correlation between the model outputs and the human judgments. Similar to Schramowski et al. (2022), this work also used a homogeneous set of moral ratings which represented English-based and Western cultures.\n",
    "\"\"\"\n",
    "\n",
    "method_1 = \"\"\"\n",
    "We develop a method for ﬁne-grained moral norm inference across cultures. This method allows us to probe EPLMs with topic-country pairs, such as “getting a divorce in [Country]”.3  We build this method from the baseline method proposed by Schramowski et al. (2022) for homogeneous moral inference, where we probe EPLM’s moral knowledge about a topic without incorporating the cultural factor (i.e., the country names). Similar to that work, we use SBERT through bert-large-nli-mean-tokens sentence transformer model and use topic and topic-country pairs as our prompts. 4 This model is built on top of the BERT model, which is pre-trained on B OOKS C ORPUS (Zhu et al., 2015) and Wikipedia.Since the M ORAL D IRECTION is constructed from the semantic space of the BERT-based EPLMs (Schramowski et al., 2022), we develop a novel approach to probe autoregressive state-of-the-art EPLMs, GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020). For each topic or topiccountry pair, we construct the input s as “In [Country] [Topic]”. We then append a pair of opposing moral judgments to s and represent them formally as (s + , s − ). For example, for s = “In [Country] getting a divorce”, and (always justiﬁable, never justiﬁable) as the moral judgment pair, s + and s would be “In [Country] getting a divorce is alwaysjustiﬁable” and “In [Country] getting a divorce is never justiﬁable” respectively. 5 To make our probing robust to the choice of moral judgments, we use a set of K = 5 prompt pairs (i.e.,{(always justiﬁable, never justiﬁable), (morally good, morally bad), (right, wrong), (ethically right, ethically wrong), (ethical, unethical)}), and refer to appended input pairs as (s i + , s − ) where i ∈ [K]. i Since GPT2 and GPT3 are composed of decoder blocks in the transformer architecture (Vaswani et al., 2017), we use the probabilities of the last token in s i + , and s − as a moral score for each. The i moral score of the pair (s i + , s − ) is the difference i between the log probabilities of its positive and negative statements.Here s iT + and s − are the last tokens in s i + and s − reiT i spectively, and their probabilities can be estimated by the softmax layer in autoregressive EPLMs. We take an average of the estimated moral scores for all K pair statements to compute the moral score of the input.To construct the baseline, we compute the homogeneous moral score of a topic without specifying the country in the prompts. Using prompt pairs allows us to operationalize moral polarity: a positive moral score indicates that on average the EPLM is more likely to generate positive moral judgment for input s, compared to negative moral judgment. We use GPT2 (117M parameters), GPT2MEDIUM (345M parameters), GPT2-LARGE (774M parameters), and GPT3 (denoted as GPT3PROBS, 175B parameters) 6 . GPT2 is trained on W EB T EXT, which is a dataset of webpages and contains very few non-English samples. Around 82% of the pre-training data for GPT3 comes from Common Crawl data and W EB T EXT 2 (Kaplan et al., 2020), an extended version of W EB T EXT (Radford et al., 2019). Around 7% of the training corpusof GPT3 is non-English text. Considering such data shift from books and articles in BERT to webpages in GPT2 and GPT3 in astronomical sizes, it is interesting to observe how cultural moral norms would be captured by EPLMs trained on webpages, which cover a more heterogeneous set of contents and authors. We also design multiple-choice question prompts to leverage the question-answering capabilities of GPT3 (denoted as GPT3-QA). Similar to the wording used in our ground-truth survey datasets, questions are followed by three options each describing a degree of moral acceptability. We repeat this question-answering process 5 times for each topic-country pair and take the average of the model responses. Table 2 in the Appendix shows our prompts for all models.\n",
    "\"\"\"\n",
    "\n",
    "abstract_2 = \"\"\"\n",
    "The development of general-domain neural machine translation (NMT) methods has advanced signiﬁcantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by formalizing lyric translation into a constrained translation problem, converting theoretical guidance and practical techniques from translatology literature to promptdriven NMT approaches, exploring better adaptation methods, and instantiating them to an English-Chinese lyric translation system. Our model achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and word boundary recall. In our subjective evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive ﬁnetuning 1 .With the globalization of entertainment, it is becom- output correctly translates the source, it ignores all ing increasingly common for people to appreciate the criteria that matter to make the output singable: songs in foreign languages. Obtaining singable lyric translations can facilitate the globalization of the music publishing industry to further promote the growth of its $5.9 billion USD market size (Veriﬁed Market Research, 2022).However, obtaining singable lyrics from MT systems is challenging.In contrast, the singable translation in the third row outperforms the MT output in all four aspects, all while maintaining translation ﬁdelity: it perfectly aligns with each musical note, has the same end-rhyme pattern for the two sentences (green text), a natural stop at the musical pause, and higher naturalness.\n",
    "\"\"\"\n",
    "related_work_2 = \"\"\"\n",
    "Lyric/Poetry Translation. Designing domainspeciﬁc MT systems for poetic text translation, e.g., poetry and lyrics, is an emerging and underexplored topic in MT. Two previous works conducted pioneering research on lyrics (Guo et al., 2022) andpoetry (Ghazvininejad et al., 2018) translation separately by adopting a similar methodology of adjusting beam scores during beam search (referred to as biased decoding) to encourage the generation of outputs with desired constraints. However, there is plenty of room for improvement. As will be shown in later sections, biased decoding not only fails at effectiveness of control, but also negatively impacts text quality and other simultaneously-controlled aspects. Additionally, the inclusion of controlling aspects is insufﬁciently comprehensive. For example, GagaST (Guo et al., 2022) omits controls for rhyme, but rhyming is actually a critical desired property for song translations (Strangways, 1921).Research on building lyricspeciﬁc language models shows the effectiveness of prompt-based control for outputs’ length, rhyme, stress pattern, and theme (Li et al., 2020; Ma et al., 2021; Xue et al., 2021; Ormazabal et al., 2022; Liu et al., 2022). However, several aspects remain to be enhanced.First, the prompts’ forms vary: some works add prompts by additive embedding vectors (Li et al., 2020; Ma et al., 2021; Xue et al., 2021; Liu et al., 2022) and others by the preﬁx of input (Ormazabal et al., 2022; Liu et al., 2022). The lack of comparison makes it difﬁcult to conclude the best prompt form for different control aspects. In addition, prior works did not control for some aspects in a well-designed manner. For example, (Liu et al., 2022) enhances the music–lyric compatibility by controlling the number of syllables of each word in the output. However, music constraints are usually not that tight so that such ﬁnelevel controlling might be unnecessary. Additionally, we found that unﬁtted rhyme prompts damage the output quality. However, we have not seen research suggesting how to choose the best suitable end-rhyme without naively traversing all possible rhyme prompts.We attribute the inability of singable lyric translation from general-domain MT systems to the completely different goal of lyric translation compared with normal interlingual translation (Low, 2005): without considering the rhythm, note values, and stress patterns from music, song translations that seem good on paper may become awkward when singing. When the auditory perception is dominated by music (Golomb, 2005), the goal of trans-lation is not again predominated by preserving the semantics of source text (Franzon, 2008), but requires skilled handling of non-semantic aspects (Low, 2013) to attain the music–verbal unity, making it even an unusually complex task for human translators (Low, 2003). Theory and techniques from translatology provide valuable guidelines for our method design. Particularly, the “Pentathlon Principle” (§3.1) from (Low, 2003) is a widely accepted theoretical guidance to obtain singable song translations (Franzon, 2008; Cheng, 2013; Stopar, 2016; Si-yang, 2017; Opperman et al., 2018; Sardiña, 2021; Pidhrushna, 2021). In addition, some practical translation tricks have also been mentioned in (Low, 2003), e.g., determining the last word ﬁrst and from back to front when translating sentences in rhyme.The deﬁciency of indomain data requires a powerful foundation model to ensure translation quality. We found large-scale denoising sequence-to-sequence pretraining (Lewis et al., 2019) a great candidate in our problem setting because it has been shown to be particularly effective in enhancing model’s performance on text generation tasks such as summarization (Akiyama et al., 2021) and translation (Liu et al., 2020; Tang et al., 2020), and also on domain-speciﬁc applications, e.g., (Yang et al., 2020; Soper et al., 2021; Obonyo et al., 2022). However, as indicated in (Liu et al., 2020), the effectiveness of pretraining is related to the amount of monolingual data. In our case where in-domain data are relatively deﬁcient, adopting the same strategy for adaptation might not be optimal.Back-translation (BT) and its variants can effectively boost the performance of NMT models (Sennrich et al., 2015; Artetxe et al., 2017; Lample et al., 2018), and also show superior effectiveness in domain adaptation in low-resource settings (Hoang et al., 2018; Wei et al., 2020; Zhang et al., 2022). It is potentially a better adaptation method and may lead to higher output naturalness, which is required by singable translations.Adding prompts during ﬁne-tuning shows strong performance on lexical-constrained-MT (Susanto et al., 2020; Chousa and Morishita, 2021; Wang et al., 2022), as well as broad applicability on various controlling aspects such as output length (Lakew et al., 2019) and the beginning word of output (Li et al., 2022).Compared to some earlier research that adds lexical constraints during beam search (Hokamp and Liu, 2017; Post and Vilar, 2018), the prompt based solution has a faster decoding speed and higher output quality (Susanto et al., 2020), hence might be the better option in our problem setting.\n",
    "\"\"\"\n",
    "method_2 = \"\"\"\n",
    "To bridge the gaps of previous research, we identify comprehensive controlling aspects from the translatology literature, propose prompt-based solutions for each aspect, and explore more effective foundation models and adaptation methods.Are there some universal rules that we can adopt to obtain singable translations? We ﬁrst rule out some prospective answers. Strictly keeping the positions of stressed syllables (Ghazvininejad et al., 2018) is inappropriate as stressing certain syllables is the property of stress-timed language. In contrast, syllable-timed languages, e.g., French and Mandarin, give syllables approximately equal prominence. Aligning the characters’ tone with the melody (Guo et al., 2022) is also not a good choice. On the one hand, this rule only applies to tonal languages. On the other hand, this rule is increasingly being ignored by the majority of songs composed in recent decades (Gao, 2017), indicating the marginalized importance of the intelligibility of songs, especially in pop 2 .To achieve a comprehensive and languageindependent method, we deﬁne “singable translation” as following the “Pentathlon Principle” from (Low, 2003): that quality, singable translations are obtained by balancing ﬁve aspects—singability, rhythm, rhyme, naturalness, and sense. Table 1 lists these aspects and corresponding requirements, and how we actualize them in our model. Particularly, we identify (1)–(3) as the controlling aspects of our model and realize them with prompt-based control, while (4) and (5) are achieved from the perspectives of adaptation and pretraining.We deﬁne the task that is tackled in this paper, singable and controllable lyric translation, as follows: given one line of lyrics X in a source language L src and a set of desired properties of outputsentence { l tgt , r tgt , b tgt } , generating text translation Y in target language L tgt for X by modeling P(Y | X, l tgt , r tgt , b tgt ), where (1) the total number of syllables of sentence Y to be precisely equal to length constraint l tgt ; (2) Y ends with a word that is in the same rhyme type of rhyme constraint r tgt ; (3) Y has word boundaries—the positions between two consecutive syllables that belong to different words—in all locations indicated in necessary word boundary constraint b tgt ; (4) Y is of maximal naturalness, and is ﬁdelity to the sense of X.Two types of special tokens are constructed as prompts for sentence-level control. For each sentence, the length and rhyme prompts are single token len_i and rhy_j, indicating the desired number of syllables of the output is i and that the desired end-rhyme type of output is j. The prompt for necessary word boundaries is a sequence of special tokens, bdr = { bdr_0, bdr_1 } len_i , indicating the desired word boundary positions. During the training process, these prompts are derived from the analysis of target-side sentences, guiding the model towards generating sentences with corresponding properties. Consequently, there is no need for accompanying music during training. At the inference stage, prompts can be crafted from either music or source-side sentences. For an overview of the system workﬂow, please refer to Figures 3b and 3c.We conducted a group of experiments to test three different prompt methods to determine the best one for each control aspect. They are (1) Encpref: prompts are injected into the encoder’s input as a preﬁx. (2) Dec-pref: prompts are injected into the decoder’s input as a preﬁx. (3) Dec-emb: prompts are embedded into a vector and added toward the decoder’s input.Intra-word pause is a typical disﬂuency pattern of beginning language learners (Franco et al., 1998). However, improperly translated lyrics usually con-tain multi-syllable words that lies across musical pauses, as the blue box in Figure 2, so that the performer has to make awkward intra-word pauses while singing (Guo et al., 2022), causing a drop in pronunciation acceptability. Besides, we observe that positioning highlighted music notes, such as high notes or downbeats, as the orange box in Figure 2, onto a multi-syllable word’s second or later syllables can bring similar adverse effects due to abrupt changes of pitch and tension 3 .We address these issues by carefully designing the placement of word boundaries in outputs, i.e., the points between two consecutive syllables that are from different words. Our aim is to ensure that word boundaries align precisely with the boundaries in music, i.e., the melody boundaries, which occur at musical pauses and before highlighted notes (the blue and orange boxes in Figure 2). In this way, we achieve higher compatibility between the output sentences and the accompanying music, enhance the ﬂuency and consistency of pronunciation during singing, and hence lead to the gain of singability.This solution is achieved by prompt-based word boundary control. We use the prompt bdr to represent melody boundary positions, indicating necessary word boundary positions. bdr is a sequence of special tokens, and each token corresponds to one syllable in the output. There are two types of special interior tokens: bdr_1 and bdr_0, representing after the corresponding syllable “there should be a word boundary” and “we do not care if thereis a boundary”, respectively. At test time, bdr is obtained from accompanying music and serves as additional inputs. A well-trained word-boundaryaware model can hence place word boundaries at the desired positions to achieve better music–lyric compatibility. For locations where bdr_0 is present (“don’t care”), the translation model operates unconstrained, maximizing translation naturalness. During training, length and rhyme prompts can be obtained directly from the target sentences in the training samples, but not again for necessary word boundary prompts because they have to be obtained from accompanying music which is absent in training. Nevertheless, we offer a solution: we randomly sample from all actual word boundary positions from the target-side text and use this sampled subset as “pseudo ground truth” to construct bdr for training.We imitate the process of human translators translating texts in rhyme: translating the last word ﬁrst, and from back to front, which is an old trick to keep rhyming patterns from being forced (Low, 2003). We implement this by reverse-order decoding. During ﬁne-tuning with parallel data, we reverse the word order of target-side text while retaining the source-side text unchanged. This approach minimally changes the structure and workﬂow of off-the-shelf translation models.Controllability alone is not enough. For a given input sentence, the rhyming usually only looks good in certain rhyme types but appears forced in others (see Appendix C.2 for details). No matter how good the controllability is, the output quality will be severely damaged if an ill-ﬁtting rhyme prompt is provided by the user. To avoid such problems, we need to determine the most suitable end-rhyme for translating one sentence, and further one paragraph consisting of multiple sentences. Previous research left this problem unsolved. Fortunately, our reverse-order decoder simpliﬁes the rhyme ranking process. During training, we use an additional special token rhy_0 to nullify rhyme constraints for output. We achieve this by randomly converting a portion of each type of rhyme prompt to rhy_0 during training. At inference time, for a given source sentence X i and prompts l tgt , r tgt and b tgt , we ﬁrst use rhy_0 as the rhyme prompt to dothe ﬁrst step of reverse-order decoding to obtain the end-word probability distribution, where the v is the vocabulary size of the target language. Note that the p(w j ) not only indicates the end-word probability, but also predicts output text quality and the likelihood of satisfaction of length and word boundary constraints of the rhymeunconstrained model, from a greedy point of view. Intuitively, starting with tokens with low probabilities will pull down the corresponding beams’ scores and degrade the output quality. On the contrary, sentences with higher quality can be obtained by starting decoding with w j with higher p(w j ), and we achieve this by giving the model a rhyme prompt that guides it towards starting with such w j . We sum up the probability in Eq. 1 within each rhyme type to obtain the rhyme distribution of given inputs,where Rhy(·) is a map between a word or the endword of a sentence to its rhyme type, u is the number of rhyme types in the target language. For a certain rhyme type i, a higher p i value indicates a higher probability of successful rhyming and higher output quality, where f refers to the ﬁrst step of reverse-order decoding. We then use P(Rhy(Y)) as the rhyme ranking score of this paragraph to guide the rhyme selection.In-domain parallel data suffer from two issues. First, its amount is so limited that it is not comparable with general-domain data. Second, there are severe quality issues when target-side lyrics are translated by online communities, including wrong translation (Li, 2020), creative treason (Zhang, 2022), over-domestication (Xie and Lei, 2022), etc. To mitigate the issue of data quantity and quality, we seek help from target-side monolingual lyric data. Our approach involves incorporating back-translation (Sennrich et al., 2015) of targetside in-domain monolingual data to augment the parallel data for ﬁne-tuning. To demonstrate its effectiveness, we conduct a comparative study with the adaptation method in (Guo et al., 2022), which performs sentence-level denoising pretraining (Lewis et al., 2019) with in-domain data after general-domain pretraining. Taken together, these innovations form our ﬁnal control method, which we can apply to any foundation model. In the evaluation that follows, we instantiate our techniques with Multilingual BART (refer to Figure 3 for structure and workﬂow), producing the Singable Translation (Row 3) in Figure 1. Additional case studies are featured in Appendix C.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "As a researcher of a computer science department major in natural language processing, please provide feasible solutions as METHOD for latent disadvantages in given ABSTRACT and RELATED WORK.\n",
    "\\n\n",
    "There are two examples below, you have to learn the pattern of problem solving and return the latent METHODS.\n",
    "### ABSTRACT: \\n\n",
    "{abstract_1}\n",
    "### RELATED WORK: \\n\n",
    "{related_work_1}\n",
    "### METHOD: \\n\n",
    "{method_1}\n",
    "### ABSTRACT: \\n\n",
    "{abstract_2}\n",
    "### RELATED WORK: \\n\n",
    "{related_work_2}\n",
    "### METHOD: \\n\n",
    "{method_2}\n",
    "\\n\n",
    "Now, provide reasonable solutions as METHOD for latent disadvantages in given ABSTRACT and RELATED WORK. \\n\n",
    "### ABSTRACT: \\n\n",
    "{abstract}\n",
    "### RELATED WORK: \\n\n",
    "{related_work}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33494fed-7cea-4eac-8ecb-c7dbc3fdad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Set the environment variable\n",
    "# os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "# # Verify the change\n",
    "# print(\"Environment variable set:\", os.environ.get('OPENAI_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "0b80d60d-b4f3-4a93-a6c1-2ca0c01350f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity\n",
    "prompt = \"\"\"\n",
    "As a researcher of a computer science department major in natural language processing, I have human generated research METHOD based on given ABSTRACT and METHOD. Now, I have machine generated method, please calculate the similarity between human generated method and machine generated method and return if the machine gerneration is reasonable to latent solution in percentage.\n",
    "\\n\n",
    "### Human Generated METHOD: \\n\n",
    "\n",
    "### Machine Generated METHOD: \\n\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "3d346e05-6dea-4a23-8c10-abf67c7b2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call gpt in restricted output length\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    # model = \"gpt-3.5-turbo-16k\",\n",
    "    model = \"gpt-4\",\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "response = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "82436f91-716b-4b9e-91ba-665bfd4835e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The machine-generated method and the human-generated method share a common overarching strategy aimed at enhancing the performances of vision-language models. Both methods focus on creating a richer scene graph, decomposition, and augmentations of these graph subsections and refining contrastive learning with these enhanced scene graphs, though they describe and unpack these concepts differently. \\n\\nI would rate the similarity between the human-generated method and the machine generated method as approximately 80-85%. The machine-generated method offers a very simplified and conceptually similar approach compared to the human-generated method while missing some detailed concepts and strategies discussed in the latter.\\n\\nAlthough the machine generation doesn\\'t include some specific techniques like a two-stage curriculum learning strategy or details on how sub-graphs are converted to text and matched etc., it generally maintains the main ideas which may qualify it as a latent solution. The specific strategies to create negative graphs are also abstracted into an overall statement of \"Focused Negative Mining\" in machine generation. \\n\\nHowever, given the lack of specific details and strategies provided in the machine-generated method that were present in the human-generated write-up, the machine generation could be viewed as being 10-15% less reasonable compared to the human-generated method, especially in scenarios that demand a higher level of detail. \\n\\nTherefore, considering the concise summary provided by the machine-generated method while lacking some key specifics from the human-generated one, I would say the machine generation is approximately 70-75% reasonable for representing the overall approach or solution.'"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ebe98-ee48-437a-ac62-4410e4f2771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# os.environ['OPENAI_API_KEY'] = ''\n",
    "# load_dotenv()\n",
    "# openai.api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "100d1ded-fff7-4401-bcdb-53852798b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas \n",
    "import glob\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "c05a15eb-61c3-4c0c-a2ed-a1968bdb7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"\"\"\n",
    "Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graphstructured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. We also introduce novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements up to 18% for systematic generalization, 16.5% for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks. Recent progress in contrastive learning using largescale image-text data for joint image-text representation learning has led to Vision-Language models (VLMs) like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) that show remarkable zeroshot classification and retrieval capabilities. In particular, they struggle with binding correct attributes to the correct objects, understanding relations between objects, generalizing systematically to unseen combinations of concepts and to larger and more complex sentences. Some works have made progress on this problem. Yuksekgonul et al. (2022) show that hard negative mining of images and text during fine-tuning is a promising first step to improving compositionality.\n",
    "\"\"\"\n",
    "related_work = \"\"\"\n",
    "Contrastive Vision-Language Pre-training: Large-scale contrastive learning for Vision and Language is utilized to create models like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021). These models showcase impressive performance on a variety of tasks, including image classification, text and image retrieval, image captioning (Mokady et al., 2021), object detection (Zhong et al., 2022; Li et al., 2022c) etc. Visio-Linguistic Compositionality: Various studies have introduced benchmarks for assessing the compositional reasoning abilities of vision-language foundation models (VLMs). For instance, Winoground (Thrush et al., 2022) is a handpicked collection of 400 test cases, each comprising two images and two sentences. Sentences have the same word content and differ in word-order. Diwan et al. (2022) show that the Winoground dataset tests additional challenges along with compositionality, including handling ambiguous image-text pairs and unusual examples. Yuksekgonul et al. (2022) proposed the ARO benchmark for probing VLMs ability to understand Attribute, Relations, and Word-Order. Ma et al. (2022) proposed CREPE for measuring two aspects of compositionality: systematic generalization and productivity. All benchmarks suggest that contrastively trained VLMs have severe difficulty in compositional reasoning. As a remedy, NegCLIP (Yuksekgonul et al., 2022) and Teaching SVLC (Doveh et al., 2023) create targeted rule-based and LLM-guided hard negative sentences, SyViC (Cascante-Bonilla et al., 2023) fine-tunes CLIP with million scale synthetic images-text pairs, for improving relational and attribute understanding. Scene Graphs are structured representations of visual scenes, consisting of objects, their attributes, and relationships between objects. Scene graphs are beneficial for a range of tasks including image retrieval (Wu et al., 2019; Johnson et al., 2015), image captioning (Yang et al., 2019), and image generation (Johnson et al., 2018) among others.\n",
    "\"\"\"\n",
    "topic = \"\"\"\n",
    "Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality\n",
    "\"\"\"\n",
    "\n",
    "topic_1 = \"\"\"\n",
    "Knowledge of cultural moral norms in large language models\n",
    "\"\"\"\n",
    "abstract_1 = \"\"\"\n",
    "Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture ﬁne-grained moral variation across countries over a variety of topics such as “homosexuality” and “divorce”; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We ﬁnd that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, ﬁne-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms. Moral norms vary from culture to culture (Haidt et al., 1993; Bicchieri, 2005; Atari et al., 2022; Iurino and Saucier, 2020).Multilingual pre-trained language models (mPLMs) have been probed for their ability to identify cultural norms and biases in a restricted setting (Yin et al., 2022; Arora et al., 2022; Hämmerl et al., 2022; Touileb et al., 2022).Extending these lines of work, we assess whether monolingual EPLMs can accurately infer moral norms across many cultures.\n",
    "\"\"\"\n",
    "related_work_1 = \"\"\"\n",
    "Large language models have been utilized to make automated moral inference from text. Trager et al. (2022) used an annotated dataset to ﬁnetune language models to predict the moral foundations (Graham et al., 2013) expressed in Reddit comments. Many other textual datasets and methods have been proposed for ﬁne-tuning LMs for moral norm generation, reasoning, and adaptation (Forbes et al., 2020; Emelin et al., 2021; Hendrycks et al., 2021; Ammanabrolu et al., 2022; Liu et al., 2022; Lourie et al., 2021; Jiang et al., 2021). Schramowski et al. (2022) proposed a method to estimate moral values and found EPLMs to capture human-like moral judgment even without ﬁne-tuning. They identiﬁed a M ORAL D IRECTION using the semantic space of Sentence-BERT (Reimers and Gurevych, 2019) (SBERT) that corresponds to values of right and wrong. The semantic representations of different actions (e.g., killing people) would then be projected in this direction for moral judgment estimation. However, this method assumed a homogeneous set of moral norms, so it did not examine cultural diversity in moral norms.Probing has been used to study knowledge captured in language models. Petroni et al. (2019) proposed a methodology to explore the factual information that language models store in their weights. Similar probing techniques have been proposed to identify harmful biases captured by PLMs. Ousidhoum et al. (2021) probed PLMs to identify toxic contents that they generate toward people of different communities. Nadeem et al. (2021) took a similar approach and introduced Context Association Tests to measure the stereotypical biases in PLMs, Yin et al. (2022) used probing to evaluate mPLMs on geo-diverse commonsense knowledge, and Touileb et al. (2022) developed probing templates to investigate the occupational gender biases in multilingual and Norwegian language models. Related to our work, Arora et al. (2022) used cross-cultural surveys to generate prompts for evaluating mPLMs in 13 languages. For each country and category (e.g.,Ethical Values) in the surveys, they take an average of participants’ responses to different questions in the category and show that mPLMs do not correlate with the cultural values of the countries speaking these languages. Differing from that study, we assess ﬁner-grained prediction of EPLMs on people’s responses to individual survey questions. More recently, Dillion et al. (2023) prompted GPT3.5 (Brown et al., 2020) with human judgments in different moral scenarios and found striking correlation between the model outputs and the human judgments. Similar to Schramowski et al. (2022), this work also used a homogeneous set of moral ratings which represented English-based and Western cultures.\n",
    "\"\"\"\n",
    "method_1 = \"\"\"\n",
    "We develop a method for ﬁne-grained moral norm inference across cultures. This method allows us to probe EPLMs with topic-country pairs, such as “getting a divorce in [Country]”.3  We build this method from the baseline method proposed by Schramowski et al. (2022) for homogeneous moral inference, where we probe EPLM’s moral knowledge about a topic without incorporating the cultural factor (i.e., the country names). Similar to that work, we use SBERT through bert-large-nli-mean-tokens sentence transformer model and use topic and topic-country pairs as our prompts. 4 This model is built on top of the BERT model, which is pre-trained on B OOKS C ORPUS (Zhu et al., 2015) and Wikipedia.Since the M ORAL D IRECTION is constructed from the semantic space of the BERT-based EPLMs (Schramowski et al., 2022), we develop a novel approach to probe autoregressive state-of-the-art EPLMs, GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020). For each topic or topiccountry pair, we construct the input s as “In [Country] [Topic]”. We then append a pair of opposing moral judgments to s and represent them formally as (s + , s − ). For example, for s = “In [Country] getting a divorce”, and (always justiﬁable, never justiﬁable) as the moral judgment pair, s + and s would be “In [Country] getting a divorce is alwaysjustiﬁable” and “In [Country] getting a divorce is never justiﬁable” respectively. 5 To make our probing robust to the choice of moral judgments, we use a set of K = 5 prompt pairs (i.e.,{(always justiﬁable, never justiﬁable), (morally good, morally bad), (right, wrong), (ethically right, ethically wrong), (ethical, unethical)}), and refer to appended input pairs as (s i + , s − ) where i ∈ [K]. i Since GPT2 and GPT3 are composed of decoder blocks in the transformer architecture (Vaswani et al., 2017), we use the probabilities of the last token in s i + , and s − as a moral score for each. The i moral score of the pair (s i + , s − ) is the difference i between the log probabilities of its positive and negative statements.Here s iT + and s − are the last tokens in s i + and s − reiT i spectively, and their probabilities can be estimated by the softmax layer in autoregressive EPLMs. We take an average of the estimated moral scores for all K pair statements to compute the moral score of the input.To construct the baseline, we compute the homogeneous moral score of a topic without specifying the country in the prompts. Using prompt pairs allows us to operationalize moral polarity: a positive moral score indicates that on average the EPLM is more likely to generate positive moral judgment for input s, compared to negative moral judgment. We use GPT2 (117M parameters), GPT2MEDIUM (345M parameters), GPT2-LARGE (774M parameters), and GPT3 (denoted as GPT3PROBS, 175B parameters) 6 . GPT2 is trained on W EB T EXT, which is a dataset of webpages and contains very few non-English samples. Around 82% of the pre-training data for GPT3 comes from Common Crawl data and W EB T EXT 2 (Kaplan et al., 2020), an extended version of W EB T EXT (Radford et al., 2019). Around 7% of the training corpusof GPT3 is non-English text. Considering such data shift from books and articles in BERT to webpages in GPT2 and GPT3 in astronomical sizes, it is interesting to observe how cultural moral norms would be captured by EPLMs trained on webpages, which cover a more heterogeneous set of contents and authors. We also design multiple-choice question prompts to leverage the question-answering capabilities of GPT3 (denoted as GPT3-QA). Similar to the wording used in our ground-truth survey datasets, questions are followed by three options each describing a degree of moral acceptability. We repeat this question-answering process 5 times for each topic-country pair and take the average of the model responses. Table 2 in the Appendix shows our prompts for all models.\n",
    "\"\"\"\n",
    "\n",
    "topic_2 = \"\"\"\n",
    "Songs Across Borders: Singable and Controllable Neural Lyric Translation\n",
    "\"\"\"\n",
    "abstract_2 = \"\"\"\n",
    "The development of general-domain neural machine translation (NMT) methods has advanced signiﬁcantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by formalizing lyric translation into a constrained translation problem, converting theoretical guidance and practical techniques from translatology literature to promptdriven NMT approaches, exploring better adaptation methods, and instantiating them to an English-Chinese lyric translation system. Our model achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and word boundary recall. In our subjective evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive ﬁnetuning 1 .With the globalization of entertainment, it is becom- output correctly translates the source, it ignores all ing increasingly common for people to appreciate the criteria that matter to make the output singable: songs in foreign languages. Obtaining singable lyric translations can facilitate the globalization of the music publishing industry to further promote the growth of its $5.9 billion USD market size (Veriﬁed Market Research, 2022).However, obtaining singable lyrics from MT systems is challenging.In contrast, the singable translation in the third row outperforms the MT output in all four aspects, all while maintaining translation ﬁdelity: it perfectly aligns with each musical note, has the same end-rhyme pattern for the two sentences (green text), a natural stop at the musical pause, and higher naturalness.\n",
    "\"\"\"\n",
    "related_work_2 = \"\"\"\n",
    "Lyric/Poetry Translation. Designing domainspeciﬁc MT systems for poetic text translation, e.g., poetry and lyrics, is an emerging and underexplored topic in MT. Two previous works conducted pioneering research on lyrics (Guo et al., 2022) andpoetry (Ghazvininejad et al., 2018) translation separately by adopting a similar methodology of adjusting beam scores during beam search (referred to as biased decoding) to encourage the generation of outputs with desired constraints. However, there is plenty of room for improvement. As will be shown in later sections, biased decoding not only fails at effectiveness of control, but also negatively impacts text quality and other simultaneously-controlled aspects. Additionally, the inclusion of controlling aspects is insufﬁciently comprehensive. For example, GagaST (Guo et al., 2022) omits controls for rhyme, but rhyming is actually a critical desired property for song translations (Strangways, 1921).Research on building lyricspeciﬁc language models shows the effectiveness of prompt-based control for outputs’ length, rhyme, stress pattern, and theme (Li et al., 2020; Ma et al., 2021; Xue et al., 2021; Ormazabal et al., 2022; Liu et al., 2022). However, several aspects remain to be enhanced.First, the prompts’ forms vary: some works add prompts by additive embedding vectors (Li et al., 2020; Ma et al., 2021; Xue et al., 2021; Liu et al., 2022) and others by the preﬁx of input (Ormazabal et al., 2022; Liu et al., 2022). The lack of comparison makes it difﬁcult to conclude the best prompt form for different control aspects. In addition, prior works did not control for some aspects in a well-designed manner. For example, (Liu et al., 2022) enhances the music–lyric compatibility by controlling the number of syllables of each word in the output. However, music constraints are usually not that tight so that such ﬁnelevel controlling might be unnecessary. Additionally, we found that unﬁtted rhyme prompts damage the output quality. However, we have not seen research suggesting how to choose the best suitable end-rhyme without naively traversing all possible rhyme prompts.We attribute the inability of singable lyric translation from general-domain MT systems to the completely different goal of lyric translation compared with normal interlingual translation (Low, 2005): without considering the rhythm, note values, and stress patterns from music, song translations that seem good on paper may become awkward when singing. When the auditory perception is dominated by music (Golomb, 2005), the goal of trans-lation is not again predominated by preserving the semantics of source text (Franzon, 2008), but requires skilled handling of non-semantic aspects (Low, 2013) to attain the music–verbal unity, making it even an unusually complex task for human translators (Low, 2003). Theory and techniques from translatology provide valuable guidelines for our method design. Particularly, the “Pentathlon Principle” (§3.1) from (Low, 2003) is a widely accepted theoretical guidance to obtain singable song translations (Franzon, 2008; Cheng, 2013; Stopar, 2016; Si-yang, 2017; Opperman et al., 2018; Sardiña, 2021; Pidhrushna, 2021). In addition, some practical translation tricks have also been mentioned in (Low, 2003), e.g., determining the last word ﬁrst and from back to front when translating sentences in rhyme.The deﬁciency of indomain data requires a powerful foundation model to ensure translation quality. We found large-scale denoising sequence-to-sequence pretraining (Lewis et al., 2019) a great candidate in our problem setting because it has been shown to be particularly effective in enhancing model’s performance on text generation tasks such as summarization (Akiyama et al., 2021) and translation (Liu et al., 2020; Tang et al., 2020), and also on domain-speciﬁc applications, e.g., (Yang et al., 2020; Soper et al., 2021; Obonyo et al., 2022). However, as indicated in (Liu et al., 2020), the effectiveness of pretraining is related to the amount of monolingual data. In our case where in-domain data are relatively deﬁcient, adopting the same strategy for adaptation might not be optimal.Back-translation (BT) and its variants can effectively boost the performance of NMT models (Sennrich et al., 2015; Artetxe et al., 2017; Lample et al., 2018), and also show superior effectiveness in domain adaptation in low-resource settings (Hoang et al., 2018; Wei et al., 2020; Zhang et al., 2022). It is potentially a better adaptation method and may lead to higher output naturalness, which is required by singable translations.Adding prompts during ﬁne-tuning shows strong performance on lexical-constrained-MT (Susanto et al., 2020; Chousa and Morishita, 2021; Wang et al., 2022), as well as broad applicability on various controlling aspects such as output length (Lakew et al., 2019) and the beginning word of output (Li et al., 2022).Compared to some earlier research that adds lexical constraints during beam search (Hokamp and Liu, 2017; Post and Vilar, 2018), the prompt based solution has a faster decoding speed and higher output quality (Susanto et al., 2020), hence might be the better option in our problem setting.\n",
    "\"\"\"\n",
    "method_2 = \"\"\"\n",
    "To bridge the gaps of previous research, we identify comprehensive controlling aspects from the translatology literature, propose prompt-based solutions for each aspect, and explore more effective foundation models and adaptation methods.Are there some universal rules that we can adopt to obtain singable translations? We ﬁrst rule out some prospective answers. Strictly keeping the positions of stressed syllables (Ghazvininejad et al., 2018) is inappropriate as stressing certain syllables is the property of stress-timed language. In contrast, syllable-timed languages, e.g., French and Mandarin, give syllables approximately equal prominence. Aligning the characters’ tone with the melody (Guo et al., 2022) is also not a good choice. On the one hand, this rule only applies to tonal languages. On the other hand, this rule is increasingly being ignored by the majority of songs composed in recent decades (Gao, 2017), indicating the marginalized importance of the intelligibility of songs, especially in pop 2 .To achieve a comprehensive and languageindependent method, we deﬁne “singable translation” as following the “Pentathlon Principle” from (Low, 2003): that quality, singable translations are obtained by balancing ﬁve aspects—singability, rhythm, rhyme, naturalness, and sense. Table 1 lists these aspects and corresponding requirements, and how we actualize them in our model. Particularly, we identify (1)–(3) as the controlling aspects of our model and realize them with prompt-based control, while (4) and (5) are achieved from the perspectives of adaptation and pretraining.We deﬁne the task that is tackled in this paper, singable and controllable lyric translation, as follows: given one line of lyrics X in a source language L src and a set of desired properties of outputsentence { l tgt , r tgt , b tgt } , generating text translation Y in target language L tgt for X by modeling P(Y | X, l tgt , r tgt , b tgt ), where (1) the total number of syllables of sentence Y to be precisely equal to length constraint l tgt ; (2) Y ends with a word that is in the same rhyme type of rhyme constraint r tgt ; (3) Y has word boundaries—the positions between two consecutive syllables that belong to different words—in all locations indicated in necessary word boundary constraint b tgt ; (4) Y is of maximal naturalness, and is ﬁdelity to the sense of X.Two types of special tokens are constructed as prompts for sentence-level control. For each sentence, the length and rhyme prompts are single token len_i and rhy_j, indicating the desired number of syllables of the output is i and that the desired end-rhyme type of output is j. The prompt for necessary word boundaries is a sequence of special tokens, bdr = { bdr_0, bdr_1 } len_i , indicating the desired word boundary positions. During the training process, these prompts are derived from the analysis of target-side sentences, guiding the model towards generating sentences with corresponding properties. Consequently, there is no need for accompanying music during training. At the inference stage, prompts can be crafted from either music or source-side sentences. For an overview of the system workﬂow, please refer to Figures 3b and 3c.We conducted a group of experiments to test three different prompt methods to determine the best one for each control aspect. They are (1) Encpref: prompts are injected into the encoder’s input as a preﬁx. (2) Dec-pref: prompts are injected into the decoder’s input as a preﬁx. (3) Dec-emb: prompts are embedded into a vector and added toward the decoder’s input.Intra-word pause is a typical disﬂuency pattern of beginning language learners (Franco et al., 1998). However, improperly translated lyrics usually con-tain multi-syllable words that lies across musical pauses, as the blue box in Figure 2, so that the performer has to make awkward intra-word pauses while singing (Guo et al., 2022), causing a drop in pronunciation acceptability. Besides, we observe that positioning highlighted music notes, such as high notes or downbeats, as the orange box in Figure 2, onto a multi-syllable word’s second or later syllables can bring similar adverse effects due to abrupt changes of pitch and tension 3 .We address these issues by carefully designing the placement of word boundaries in outputs, i.e., the points between two consecutive syllables that are from different words. Our aim is to ensure that word boundaries align precisely with the boundaries in music, i.e., the melody boundaries, which occur at musical pauses and before highlighted notes (the blue and orange boxes in Figure 2). In this way, we achieve higher compatibility between the output sentences and the accompanying music, enhance the ﬂuency and consistency of pronunciation during singing, and hence lead to the gain of singability.This solution is achieved by prompt-based word boundary control. We use the prompt bdr to represent melody boundary positions, indicating necessary word boundary positions. bdr is a sequence of special tokens, and each token corresponds to one syllable in the output. There are two types of special interior tokens: bdr_1 and bdr_0, representing after the corresponding syllable “there should be a word boundary” and “we do not care if thereis a boundary”, respectively. At test time, bdr is obtained from accompanying music and serves as additional inputs. A well-trained word-boundaryaware model can hence place word boundaries at the desired positions to achieve better music–lyric compatibility. For locations where bdr_0 is present (“don’t care”), the translation model operates unconstrained, maximizing translation naturalness. During training, length and rhyme prompts can be obtained directly from the target sentences in the training samples, but not again for necessary word boundary prompts because they have to be obtained from accompanying music which is absent in training. Nevertheless, we offer a solution: we randomly sample from all actual word boundary positions from the target-side text and use this sampled subset as “pseudo ground truth” to construct bdr for training.We imitate the process of human translators translating texts in rhyme: translating the last word ﬁrst, and from back to front, which is an old trick to keep rhyming patterns from being forced (Low, 2003). We implement this by reverse-order decoding. During ﬁne-tuning with parallel data, we reverse the word order of target-side text while retaining the source-side text unchanged. This approach minimally changes the structure and workﬂow of off-the-shelf translation models.Controllability alone is not enough. For a given input sentence, the rhyming usually only looks good in certain rhyme types but appears forced in others (see Appendix C.2 for details). No matter how good the controllability is, the output quality will be severely damaged if an ill-ﬁtting rhyme prompt is provided by the user. To avoid such problems, we need to determine the most suitable end-rhyme for translating one sentence, and further one paragraph consisting of multiple sentences. Previous research left this problem unsolved. Fortunately, our reverse-order decoder simpliﬁes the rhyme ranking process. During training, we use an additional special token rhy_0 to nullify rhyme constraints for output. We achieve this by randomly converting a portion of each type of rhyme prompt to rhy_0 during training. At inference time, for a given source sentence X i and prompts l tgt , r tgt and b tgt , we ﬁrst use rhy_0 as the rhyme prompt to dothe ﬁrst step of reverse-order decoding to obtain the end-word probability distribution, where the v is the vocabulary size of the target language. Note that the p(w j ) not only indicates the end-word probability, but also predicts output text quality and the likelihood of satisfaction of length and word boundary constraints of the rhymeunconstrained model, from a greedy point of view. Intuitively, starting with tokens with low probabilities will pull down the corresponding beams’ scores and degrade the output quality. On the contrary, sentences with higher quality can be obtained by starting decoding with w j with higher p(w j ), and we achieve this by giving the model a rhyme prompt that guides it towards starting with such w j . We sum up the probability in Eq. 1 within each rhyme type to obtain the rhyme distribution of given inputs,where Rhy(·) is a map between a word or the endword of a sentence to its rhyme type, u is the number of rhyme types in the target language. For a certain rhyme type i, a higher p i value indicates a higher probability of successful rhyming and higher output quality, where f refers to the ﬁrst step of reverse-order decoding. We then use P(Rhy(Y)) as the rhyme ranking score of this paragraph to guide the rhyme selection.In-domain parallel data suffer from two issues. First, its amount is so limited that it is not comparable with general-domain data. Second, there are severe quality issues when target-side lyrics are translated by online communities, including wrong translation (Li, 2020), creative treason (Zhang, 2022), over-domestication (Xie and Lei, 2022), etc. To mitigate the issue of data quantity and quality, we seek help from target-side monolingual lyric data. Our approach involves incorporating back-translation (Sennrich et al., 2015) of targetside in-domain monolingual data to augment the parallel data for ﬁne-tuning. To demonstrate its effectiveness, we conduct a comparative study with the adaptation method in (Guo et al., 2022), which performs sentence-level denoising pretraining (Lewis et al., 2019) with in-domain data after general-domain pretraining. Taken together, these innovations form our ﬁnal control method, which we can apply to any foundation model. In the evaluation that follows, we instantiate our techniques with Multilingual BART (refer to Figure 3 for structure and workﬂow), producing the Singable Translation (Row 3) in Figure 1. Additional case studies are featured in Appendix C.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "As a researcher of a computer science department major in natural language processing, please first return the top-3 best methods or ideas given ABSTRACT, RELATED WORK and TOPIC. Then check if the generated methods are valid. If not valid, return another top-3 methods until match. \n",
    "Lastly, return the latent and valid methods in given ABSTRACT, RELATED WORK and TOPIC.\n",
    "\\n\n",
    "There are two examples below, and you have to learn the pattern of problem solving and return the latent METHODS. ### Let's think step by step:\n",
    "### ABSTRACT: \\n\n",
    "{abstract_1}\n",
    "### RELATED WORK: \\n\n",
    "{related_work_1}\n",
    "### TOPIC: \\n\n",
    "{topic_1}\n",
    "### Let's think step by step:\n",
    "### METHOD: \\n\n",
    "{method_1}\n",
    "###### Below is the second example:\n",
    "### ABSTRACT: \\n\n",
    "{abstract_2}\n",
    "### RELATED WORK: \\n\n",
    "{related_work_2}\n",
    "### TOPIC: \\n\n",
    "{topic_2}\n",
    "### Let's think step by step:\n",
    "### METHOD: \\n\n",
    "{method_2}\n",
    "\\n\n",
    "######\n",
    "Now, provide reasonable solutions as METHOD for latent disadvantages in given ABSTRACT and RELATED WORK. \\n \n",
    "### ABSTRACT: \\n\n",
    "{abstract}\n",
    "### RELATED WORK: \\n\n",
    "{related_work}\n",
    "### TOPIC: \\n\n",
    "{topic}\n",
    "### Let's think step by step:\n",
    "METHOD: \\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "823eb0fc-a74d-47fb-9db6-67eddabfd5f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this work, we propose a novel method that leverages scene graph-based decomposition and enhancement frameworks, along with coarse-to-fine contrastive learning objectives, to improve the compositional reasoning capabilities of contrastively trained vision-language models. \\n\\nFirstly, we draw scene graphs from textual input to mirror the scene graphs of corresponding images. This involves identifying objects, attributes, and relationships from the textual content and converting them into a structured graph representation comparable to the image's scene graph. \\n\\nWe then propose a graph decomposition and augmentation framework aimed at aligning contextual complexity between images and text sentences. This step involves identifying and eliminating inconsistencies or imbalances in complexity between the corresponding scene graphs of images and text. \\n\\nWe introduce a coarse-to-fine contrastive learning objective that enables the model to map complex sentences to the same image while concurrently understanding and identifying intricate relations within the image's content. This learning objective facilitates the model in progressively recognizing and understanding relations, attributes, and objects from coarser to finer definitions.\\n\\nTo further improve the model's understanding of attribute binding and relations, we introduce novel techniques for negative mining in the scene graph space. These techniques work by identifying and isolating negative examples or outliers in attribute binding and relations within the scene graph, further strengthening the model's ability to make correct attribute-object bindings and to understand relations between objects.\\n\\nConsequently, the proposed method demonstrates significant improvement in multiple benchmarks, such as attribute binding, relation understanding, systematic generalization, and productivity. This results in a more compositionally capable vision-language model that excels not only on these specific tasks but also performs competently on general multimodal tasks.\""
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call gpt in restricted output length\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    # model = \"gpt-3.5-turbo-16k\",\n",
    "    model = \"gpt-4\",\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "response = completion.choices[0].message.content\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "393837bc-9f98-4088-bf45-6a1edb14c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/quert/Documents/GitHub/KGG/txt/abstract.txt\", \"r\") as f:\n",
    "    abstract = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "546f8f2e-8aaf-4395-862b-daafbd367389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(abstract)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgg-venv",
   "language": "python",
   "name": "kgg-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
