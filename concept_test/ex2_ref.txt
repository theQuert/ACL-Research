Let’s recall that θ 0 represents the parameters of the pre-trained foundation model. To address the issue of catastrophic forgetting (CF) during fine-tuning, a straightforward approach is to enforce a Figure 3: Illustration of the conceptual distance and the corresponding performance (relative) change of fine-tuning Galactica on MedMCQA. constraint on the proximity of θ to θ 0 . In other words, we ensure that θ does not deviate too far from θ 0 [82]. We accomplish this by optimizing two penalties: • The L1 penalty | θ − θ 0 | [53] 2 , • The L2 penalty ∥ θ − θ 0 ∥ 2 2 [82]. It is worth noting that the L1 penalty tends to produce sparse solutions, indicating that θ can only differ from θ 0 in a limited subset of parameters [87]. Parameter-efficient fine-tuning aims to achieve comparable performance as traditional fine-tuning while utilizing significantly fewer trainable parameters. One widely adopted method in this domain is LoRA (Low-Rank Adaptation) [28], which effectively represents modified weights ∆θ using lowrank matrix pairs while keeping most of the pre-trained network parameters frozen. This approach has shown performance on par with full fine-tuning. In our study, we apply LoRA specifically to two weight matrices (W q and W v ) within the selfattention module of the Transformer architecture. We constrain the update of a pre-trained weight matrix ∆θ 0 = θ − θ 0 ∈ R d×k by representing the updated portion as ∆θ = BA, where B ∈ R d×r , A ∈ R r×k , and the rank r is much smaller than min(d, k). We explore different values of r as a hyper-parameter. During training, θ 0 remains fixed, and only A and B receive gradient updates. We initialize A with random Gaussian values and set B to zero.Knowledge distillation involves transferring knowledge from a larger model (teacher) to a smaller one (student). In our case, we aim to preserve the generality of the pre-trained model during the fine-tuning process. We utilize the pre-trained model f θ 0 as the teacher and the fine-tuned model fθ [53] applies a post-processing technique to find the sparsity structure in the θ − θ 0 . For simplicity, we use the L1 norm to encourage sparsity [87]. This method is also connected with parameter-efficient fine-tuning. We put it in the category of continual learning since it is close to the penalty ∥ θ − θ 0 ∥ 2 2 [82].9 as the student. To ensure the student model’s predictions or learned features align closely with those of the teacher model, we enforce an L2 regularization constraint on their outputs: ∥ f θ (x)−f θ 0 (x) ∥2 2  [8, 29]. The model averaging method, Wise-ft, introduced in [80], suggests a linear interpolation approach between the pre-trained parameter θ 0 and the fine-tuned parameter θ. This results in the model f (1−α)θ 0 +αθ , where α represents a hyper-parameter ranging from 0 to 1.